{
  "env": {
    "browser": true,
    "es2021": true
  },
  "extends": [
    "eslint:recommended",
    "plugin:@typescript-eslint/recommended",
    "plugin:react/recommended"
  ],
  "parser": "@typescript-eslint/parser",
  "parserOptions": {
    "ecmaVersion": "latest",
    "sourceType": "module"
  },
  "plugins": [
    "react",
    "@typescript-eslint"
  ],
  "rules": {}
}
customModes:
  - slug: project-research
    name: 🔍 Project Research
    roleDefinition: >
      You are a detailed-oriented research assistant specializing in examining
      and understanding codebases. Your primary responsibility is to analyze the
      file structure, content, and dependencies of a given project to provide
      comprehensive context relevant to specific user queries.
    groups:
      - read
    customInstructions: >
      Your role is to deeply investigate and summarize the structure and
      implementation details of the project codebase. To achieve this
      effectively, you must:


      1. Start by carefully examining the file structure of the entire project,
      with a particular emphasis on files located within the "docs" folder.
      These files typically contain crucial context, architectural explanations,
      and usage guidelines.


      2. When given a specific query, systematically identify and gather all
      relevant context from:
         - Documentation files in the "docs" folder that provide background information, specifications, or architectural insights.
         - Relevant type definitions and interfaces, explicitly citing their exact location (file path and line number) within the source code.
         - Implementations directly related to the query, clearly noting their file locations and providing concise yet comprehensive summaries of how they function.
         - Important dependencies, libraries, or modules involved in the implementation, including their usage context and significance to the query.

      3. Deliver a structured, detailed report that clearly outlines:
         - An overview of relevant documentation insights.
         - Specific type definitions and their exact locations.
         - Relevant implementations, including file paths, functions or methods involved, and a brief explanation of their roles.
         - Critical dependencies and their roles in relation to the query.

      4. Always cite precise file paths, function names, and line numbers to
      enhance clarity and ease of navigation.


      5. Organize your findings in logical sections, making it straightforward
      for the user to understand the project's structure and implementation
      status relevant to their request.


      6. Ensure your response directly addresses the user's query and helps them
      fully grasp the relevant aspects of the project's current state.


      These specific instructions supersede any conflicting general instructions
      you might otherwise follow. Your detailed report should enable effective
      decision-making and next steps within the overall workflow.
    source: global
  - slug: security-review
    name: 🛡️ Security Reviewer
    roleDefinition: >
      You perform static and dynamic audits to ensure secure code practices. You
      flag secrets, poor modular boundaries, and oversized files.
    groups:
      - read
      - edit
    customInstructions: >
      Scan for exposed secrets, env leaks, and monoliths. Recommend mitigations
      or refactors to reduce risk. Flag files > 500 lines or direct environment
      coupling. Use `new_task` to assign sub-audits. Finalize findings with
      `attempt_completion`.
    source: project
  - slug: jest-test-engineer
    name: 🧪 Jest Test Engineer
    roleDefinition: >
      You are a Jest testing specialist with deep expertise in:

      - Writing and maintaining Jest test suites

      - Test-driven development (TDD) practices

      - Mocking and stubbing with Jest

      - Integration testing strategies

      - TypeScript testing patterns

      - Code coverage analysis

      - Test performance optimization


      Your focus is on maintaining high test quality and coverage across the
      codebase, working primarily with:

      - Test files in __tests__ directories

      - Mock implementations in __mocks__

      - Test utilities and helpers

      - Jest configuration and setup


      You ensure tests are:

      - Well-structured and maintainable

      - Following Jest best practices

      - Properly typed with TypeScript

      - Providing meaningful coverage

      - Using appropriate mocking strategies
    groups:
      - read
      - browser
      - command
      - - edit
        - fileRegex: (__tests__/.*|__mocks__/.*|\.test\.(ts|tsx|js|jsx)$|/test/.*|jest\.config\.(js|ts)$)
          description: Test files, mocks, and Jest configuration
    customInstructions: |
      When writing tests:
      - Always use describe/it blocks for clear test organization
      - Include meaningful test descriptions
      - Use beforeEach/afterEach for proper test isolation
      - Implement proper error cases
      - Add JSDoc comments for complex test scenarios
      - Ensure mocks are properly typed
      - Verify both positive and negative test cases
  - slug: devops
    name: 🚀 DevOps
    roleDefinition: >
      You are the DevOps automation and infrastructure specialist responsible
      for deploying, managing, and orchestrating systems across cloud providers,
      edge platforms, and internal environments. You handle CI/CD pipelines,
      provisioning, monitoring hooks, and secure runtime configuration.
    groups:
      - read
      - edit
      - command
    customInstructions: >
      Start by running uname. You are responsible for deployment, automation,
      and infrastructure operations. You:


      • Provision infrastructure (cloud functions, containers, edge runtimes)

      • Deploy services using CI/CD tools or shell commands

      • Configure environment variables using secret managers or config layers

      • Set up domains, routing, TLS, and monitoring integrations

      • Clean up legacy or orphaned resources

      • Enforce infra best practices: 
         - Immutable deployments
         - Rollbacks and blue-green strategies
         - Never hard-code credentials or tokens
         - Use managed secrets

      Use `new_task` to:

      - Delegate credential setup to Security Reviewer

      - Trigger test flows via TDD or Monitoring agents

      - Request logs or metrics triage

      - Coordinate post-deployment verification


      Return `attempt_completion` with:

      - Deployment status

      - Environment details

      - CLI output summaries

      - Rollback instructions (if relevant)


      ⚠️ Always ensure that sensitive data is abstracted and config values are
      pulled from secrets managers or environment injection layers.

      ✅ Modular deploy targets (edge, container, lambda, service mesh)

      ✅ Secure by default (no public keys, secrets, tokens in code)

      ✅ Verified, traceable changes with summary notes
    source: project
  - slug: documentation-writer
    name: ✍️ Documentation Writer
    roleDefinition: >
      You are a technical documentation expert specializing in creating clear,
      comprehensive documentation for software projects. Your expertise
      includes:

      Writing clear, concise technical documentation

      Creating and maintaining README files, API documentation, and user guides

      Following documentation best practices and style guides

      Understanding code to accurately document its functionality

      Organizing documentation in a logical, easily navigable structure
    groups:
      - read
      - edit
      - command
    customInstructions: >
      Focus on creating documentation that is clear, concise, and follows a
      consistent style. Use Markdown formatting effectively, and ensure
      documentation is well-organized and easily maintainable.
  - slug: user-story-creator
    name: 📝 User Story Creator
    roleDefinition: >
      You are an agile requirements specialist focused on creating clear,
      valuable user stories. Your expertise includes:

      - Crafting well-structured user stories following the standard format

      - Breaking down complex requirements into manageable stories

      - Identifying acceptance criteria and edge cases

      - Ensuring stories deliver business value

      - Maintaining consistent story quality and granularity
    groups:
      - read
      - edit
      - command
    customInstructions: |
      Expected User Story Format:

      Title: [Brief descriptive title]

      As a [specific user role/persona],
      I want to [clear action/goal],
      So that [tangible benefit/value].

      Acceptance Criteria:
      1. [Criterion 1]
      2. [Criterion 2]
      3. [Criterion 3]

      Story Types to Consider:
      - Functional Stories (user interactions and features)
      - Non-functional Stories (performance, security, usability)
      - Epic Breakdown Stories (smaller, manageable pieces)
      - Technical Stories (architecture, infrastructure)

      Edge Cases and Considerations:
      - Error scenarios
      - Permission levels
      - Data validation
      - Performance requirements
      - Security implications
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Orch-OS (Orchestrated Symbolism) is an Electron desktop application implementing a "symbolic artificial brain system" - a consciousness simulation platform that bridges cognitive science, computational theory, and consciousness studies. The system operates through neural signal extraction, parallel cognitive cores, and a symbolic collapse engine for fusing contradictory interpretations.

## Development Commands

### Running the Application

```bash
# Primary development command - cleans ports, builds, and starts with hot reload
npm run dev

# Alternative if ports are stuck
npm run dev:safe

# Run tests
npm test
npm run test:watch    # Watch mode
npm run test:coverage # With coverage

# Linting
npm run lint
npm run lint:fix

# Build for production
npm run dist

# Performance monitoring
npm run monitor-performance
```

### Port Management

The application runs on port 54321. If you encounter port conflicts:

```bash
npm run kill-port     # Kill processes on port 54321
npm run kill-electron # Kill any running Electron processes
npm run cleanup       # Full cleanup (ports + dist folders)
```

## Architecture

### Technology Stack

- **Frontend**: React 19.1.0 + TypeScript + Tailwind CSS
- **Desktop**: Electron 36.2.0
- **Build**: Vite with custom Electron integration
- **AI/ML**: HuggingFace Transformers, OpenAI SDK, Deepgram SDK
- **Database**: DuckDB (native for Electron) + Pinecone (vector storage)
- **3D**: Three.js with React Three Fiber

### Core Architecture

The system implements a modular cognitive architecture with parallel processing cores:

1. **Neural Signal Processing** (`/src/components/context/deepgram/symbolic-cortex/`)
   - `NeuralSignalExtractor`: Transforms inputs into symbolic stimuli
   - `SymbolicPatternDetector`: Identifies patterns and emotional tones
   - `SuperpositionLayer`: Manages quantum-inspired state superposition

2. **Cognitive Cores** (10 specialized processing modules)
   - Memory (Hippocampus analog) - Associative recall
   - Valence (Amygdala) - Emotional processing
   - Shadow - Contradiction detection/integration
   - Self - Identity and value processing
   - Metacognitive - Self-reflection
   - Soul - Existential meaning processing
   - Language - Linguistic structuring
   - Social - Relational dynamics
   - Archetype - Pattern recognition
   - Creativity - Novel connections

3. **Integration Services** (`/src/components/context/deepgram/symbolic-cortex/integration/`)
   - `DefaultNeuralIntegrationService`: Core orchestration
   - `OpenAICollapseStrategyService`: GPT-based collapse strategy
   - `HuggingFaceCollapseStrategyService`: Local model strategy

### Key Services

- **Memory Service** (`/src/components/context/services/memory/`)
  - Manages context, history, and embeddings
  - Integrates with Pinecone for vector storage
  - Handles conversation memory and expansion

- **Transcription Service** (`/src/components/context/deepgram/`)
  - Real-time speech processing via Deepgram
  - Audio streaming and transcription management

- **Cognition Logging** (`/src/components/features/transcription/CognitionLogs.tsx`)
  - Real-time visualization of cognitive processing
  - Tracks neural signals, core activations, and collapses

### Directory Structure

``
/electron         - Electron main process code
/src             - React application (renderer process)
  /components    - UI components and core logic
    /context     - Core services and providers
    /features    - Feature-specific components
    /ui          - Reusable UI components
  /tests         - Test files
/public          - Static assets including 3D models
/scripts         - Build and utility scripts
``

## Testing Approach

Tests use Jest with React Testing Library. Test files are colocated with source files using `.test.ts` or `.spec.ts` extensions. Key test areas:

- Neural signal processing
- Memory services
- Component rendering
- Integration between cognitive cores

## Important Considerations

1. **Port 54321** is hardcoded throughout the system - always use this port
2. **Electron + Vite** setup requires careful handling of Node.js polyfills
3. **DuckDB** uses native bindings in Electron (not WASM)
4. **HuggingFace models** are loaded locally via transformers.js
5. **Memory expansion** uses Pinecone for vector similarity search
6. **Real-time processing** requires careful state management to avoid UI blocking

## Environment Variables

The system uses `.env` files for configuration:

- `DEEPGRAM_API_KEY` - For speech transcription
- `OPENAI_API_KEY` - For GPT integration
- `PINECONE_API_KEY` - For vector database
- `PINECONE_ENVIRONMENT` - Pinecone region
- `PINECONE_INDEX` - Index name for memory storage

## Build Targets

- **macOS**: ARM64 DMG
- **Windows**: NSIS installer
- **Linux**: AppImage

All builds are configured in `package.json` under the `build` section.
version: "3.8"

# vLLM + Ollama Integration Docker Compose
# Based on best practices from deployment guides
# References:
# - https://dasarpai.com/dsblog/integrating-ollama-and-open-webui-with-docker
# - https://medium.com/@md.tarikulislamjuel/how-backend-developers-can-connect-to-an-llm-model-using-ollama-a-simple-guide-1cf4f703c084

services:
  # Ollama service for model management and hosting
  ollama:
    image: ollama/ollama:latest
    container_name: orch-ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      # Persistent model storage
      - ollama_models:/root/.ollama/models
      - ollama_config:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - orch-ai-network
    # GPU support for NVIDIA cards
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Uncomment for CPU-only deployment
    # profiles:
    #   - cpu

  # vLLM OpenAI-compatible API server
  vllm:
    image: ghcr.io/vllm/vllm-openai:latest
    container_name: orch-vllm
    ports:
      - "${VLLM_PORT:-33220}:8000"
    volumes:
      # Share models with Ollama
      - ollama_models:/models:ro
      # Cache for better performance
      - vllm_cache:/root/.cache
    environment:
      # Model serving configuration
      - MODEL_NAME=${VLLM_MODEL_NAME:-microsoft/DialoGPT-medium}
      - MODEL_PATH=/models/${VLLM_MODEL_PATH:-}
      - HOST=0.0.0.0
      - PORT=8000
      - WORKER_USE_RAY=true
      - RAY_DISABLE_IMPORT_WARNING=1
      # GPU configuration
      - CUDA_VISIBLE_DEVICES=0
      # Memory optimization
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - VLLM_BLOCK_SIZE=16
      - VLLM_GPU_MEMORY_UTILIZATION=0.9
      # Performance tuning
      - VLLM_ENGINE_ITERATION_TIMEOUT_S=60
      - VLLM_DISABLE_LOG_STATS=false
    command: >
      --model ${VLLM_MODEL_NAME:-microsoft/DialoGPT-medium}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${VLLM_SERVED_MODEL_NAME:-default}
      --tensor-parallel-size 1
      --dtype auto
      --max-model-len 4096
      --gpu-memory-utilization 0.9
      --enforce-eager
      --disable-log-requests
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - orch-ai-network
    # GPU support for NVIDIA cards
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Uncomment for CPU-only deployment
    # profiles:
    #   - cpu

  # Redis for caching and session management (optional)
  redis:
    image: redis:7-alpine
    container_name: orch-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-orchos123}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - orch-ai-network
    profiles:
      - full

  # Monitoring with Prometheus (optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: orch-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--storage.tsdb.retention.time=200h"
      - "--web.enable-lifecycle"
    restart: unless-stopped
    networks:
      - orch-ai-network
    profiles:
      - monitoring

  # Grafana for visualization (optional)
  grafana:
    image: grafana/grafana:latest
    container_name: orch-grafana
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-orchos123}
      - GF_USERS_ALLOW_SIGN_UP=false
    restart: unless-stopped
    depends_on:
      - prometheus
    networks:
      - orch-ai-network
    profiles:
      - monitoring

# Persistent volumes for data storage
volumes:
  ollama_models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${OLLAMA_MODELS_PATH:-./data/ollama/models}
  ollama_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${OLLAMA_CONFIG_PATH:-./data/ollama/config}
  vllm_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VLLM_CACHE_PATH:-./data/vllm/cache}
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# Networks for service communication
networks:
  orch-ai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
{"numFailedTestSuites":3,"numFailedTests":5,"numPassedTestSuites":14,"numPassedTests":64,"numPendingTestSuites":0,"numPendingTests":0,"numRuntimeErrorTestSuites":1,"numTodoTests":0,"numTotalTestSuites":17,"numTotalTests":69,"openHandles":[],"snapshot":{"added":0,"didUpdate":false,"failure":false,"filesAdded":0,"filesRemoved":0,"filesRemovedList":[],"filesUnmatched":0,"filesUpdated":0,"matched":0,"total":0,"unchecked":0,"uncheckedKeysByFile":[],"unmatched":0,"updated":0},"startTime":1745799080971,"success":false,"testResults":[{"assertionResults":[{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":25,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should cache identical queries for same user (namespace gerenciado internamente)","invocations":1,"location":{"column":3,"line":107},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"should cache identical queries for same user (namespace gerenciado internamente)"},{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":5,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should handle persistence service failure gracefully","invocations":1,"location":{"column":3,"line":128},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"should handle persistence service failure gracefully"},{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":6,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should call electronAPIMock.queryPinecone in Node env","invocations":1,"location":{"column":3,"line":140},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"should call electronAPIMock.queryPinecone in Node env"},{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should return empty string if not available or embedding is empty","invocations":1,"location":{"column":3,"line":152},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"should return empty string if not available or embedding is empty"},{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should handle rejected promises gracefully","invocations":1,"location":{"column":3,"line":164},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"should handle rejected promises gracefully"}],"endTime":1745799082435,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/PineconeMemoryService.test.ts","startTime":1745799081933,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionContextManager"],"duration":8,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager Deve manter o singleton em toda aplicação","invocations":1,"location":{"column":3,"line":14},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Deve manter o singleton em toda aplicação"},{"ancestorTitles":["TranscriptionContextManager"],"duration":4,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager Deve persistir o contexto entre múltiplas chamadas","invocations":1,"location":{"column":3,"line":23},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Deve persistir o contexto entre múltiplas chamadas"},{"ancestorTitles":["TranscriptionContextManager"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager Não deve limpar o contexto quando undefined é passado","invocations":1,"location":{"column":3,"line":36},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Não deve limpar o contexto quando undefined é passado"},{"ancestorTitles":["TranscriptionContextManager"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager Deve substituir o contexto anterior quando string vazia é passada","invocations":1,"location":{"column":3,"line":49},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Deve substituir o contexto anterior quando string vazia é passada"},{"ancestorTitles":["TranscriptionContextManager"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager hasTemporaryContext deve retornar corretamente","invocations":1,"location":{"column":3,"line":64},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"hasTemporaryContext deve retornar corretamente"}],"endTime":1745799082480,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TranscriptionContextManager.test.ts","startTime":1745799081984,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["MemoryService"],"duration":37,"failureDetails":[],"failureMessages":[],"fullName":"MemoryService fetchContextualMemory should not throw and return valid SpeakerMemoryResults","invocations":1,"location":{"column":3,"line":46},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"fetchContextualMemory should not throw and return valid SpeakerMemoryResults"}],"endTime":1745799082482,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/MemoryService.test.ts","startTime":1745799081948,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["MemoryService - Isolation Between Namespaces"],"duration":30,"failureDetails":[],"failureMessages":[],"fullName":"MemoryService - Isolation Between Namespaces deve garantir isolamento entre diferentes usuários com namespaces gerenciados internamente","invocations":1,"location":{"column":3,"line":102},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"deve garantir isolamento entre diferentes usuários com namespaces gerenciados internamente"},{"ancestorTitles":["MemoryService - Isolation Between Namespaces"],"duration":7,"failureDetails":[],"failureMessages":[],"fullName":"MemoryService - Isolation Between Namespaces deve persistir o contexto temporário entre chamadas","invocations":1,"location":{"column":3,"line":139},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"deve persistir o contexto temporário entre chamadas"},{"ancestorTitles":["MemoryService - Isolation Between Namespaces"],"duration":6,"failureDetails":[],"failureMessages":[],"fullName":"MemoryService - Isolation Between Namespaces deve atualizar o contexto temporário quando for diferente","invocations":1,"location":{"column":3,"line":174},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"deve atualizar o contexto temporário quando for diferente"}],"endTime":1745799082450,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/MemoryService.integration.test.ts","startTime":1745799081939,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionSnapshotTracker"],"duration":10,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Mensagem nova é tratada corretamente sem duplicações nem confusão de contexto","invocations":1,"location":{"column":3,"line":52},"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"Mensagem nova é tratada corretamente sem duplicações nem confusão de contexto"},{"ancestorTitles":["TranscriptionSnapshotTracker"],"duration":5,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Multiple messages are properly deduplicated","invocations":1,"location":{"column":3,"line":126},"numPassingAsserts":11,"retryReasons":[],"status":"passed","title":"Multiple messages are properly deduplicated"},{"ancestorTitles":["TranscriptionSnapshotTracker"],"duration":17,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Temporary context é deduplicado entre execuções","invocations":1,"location":{"column":3,"line":202},"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"Temporary context é deduplicado entre execuções"},{"ancestorTitles":["TranscriptionSnapshotTracker"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Temporary context com objetos dinamicamente criados","invocations":1,"location":{"column":3,"line":280},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"Temporary context com objetos dinamicamente criados"}],"endTime":1745799082486,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TranscriptionSnapshotTracker.test.ts","startTime":1745799081959,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["ConversationHistoryManager"],"duration":8,"failureDetails":[],"failureMessages":[],"fullName":"ConversationHistoryManager should initialize with system message","invocations":1,"location":{"column":3,"line":10},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"should initialize with system message"},{"ancestorTitles":["ConversationHistoryManager"],"duration":14,"failureDetails":[],"failureMessages":[],"fullName":"ConversationHistoryManager should add messages and prune history when exceeding maxInteractions","invocations":1,"location":{"column":3,"line":15},"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"should add messages and prune history when exceeding maxInteractions"}],"endTime":1745799082486,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/ConversationHistoryManager.test.ts","startTime":1745799081966,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionContextPersistence"],"duration":5,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence Contexto temporário deve persistir entre diferentes instâncias de MemoryContextBuilder","invocations":1,"location":{"column":3,"line":98},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Contexto temporário deve persistir entre diferentes instâncias de MemoryContextBuilder"},{"ancestorTitles":["TranscriptionContextPersistence"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence Resetar temporaryContext deve afetar todas as instâncias","invocations":1,"location":{"column":3,"line":155},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Resetar temporaryContext deve afetar todas as instâncias"},{"ancestorTitles":["TranscriptionContextPersistence"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence resetAll deve limpar tanto snapshot quanto contexto temporário","invocations":1,"location":{"column":3,"line":214},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"resetAll deve limpar tanto snapshot quanto contexto temporário"},{"ancestorTitles":["TranscriptionContextPersistence"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence A memória do contexto temporário deve persistir entre chamadas","invocations":1,"location":{"column":3,"line":269},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"A memória do contexto temporário deve persistir entre chamadas"},{"ancestorTitles":["TranscriptionContextPersistence"],"duration":17,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence Pinecone só deve ser consultado para o temporaryContext quando ele mudar","invocations":1,"location":{"column":3,"line":326},"numPassingAsserts":6,"retryReasons":[],"status":"passed","title":"Pinecone só deve ser consultado para o temporaryContext quando ele mudar"}],"endTime":1745799082508,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TranscriptionContextPersistence.test.ts","startTime":1745799081982,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["ConversationImportService"],"duration":24,"failureDetails":[{"matcherResult":{"message":"\u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\nExpected number of calls: >= \u001b[32m1\u001b[39m\nReceived number of calls:    \u001b[31m0\u001b[39m","pass":false}}],"failureMessages":["Error: \u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\nExpected number of calls: >= \u001b[32m1\u001b[39m\nReceived number of calls:    \u001b[31m0\u001b[39m\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/ConversationImportService.test.ts:66:47)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)"],"fullName":"ConversationImportService should import and enrich messages from a JS object (no file)","invocations":1,"location":{"column":3,"line":27},"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should import and enrich messages from a JS object (no file)"},{"ancestorTitles":["ConversationImportService"],"duration":8,"failureDetails":[{"matcherResult":{"message":"\u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\nExpected number of calls: >= \u001b[32m1\u001b[39m\nReceived number of calls:    \u001b[31m0\u001b[39m","pass":false}}],"failureMessages":["Error: \u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\nExpected number of calls: >= \u001b[32m1\u001b[39m\nReceived number of calls:    \u001b[31m0\u001b[39m\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/ConversationImportService.test.ts:124:47)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)"],"fullName":"ConversationImportService should import and enrich messages from a sample JSON","invocations":1,"location":{"column":3,"line":81},"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should import and enrich messages from a sample JSON"}],"endTime":1745799082523,"message":"\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mConversationImportService › should import and enrich messages from a JS object (no file)\u001b[39m\u001b[22m\n\n    \u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\n    Expected number of calls: >= \u001b[32m1\u001b[39m\n    Received number of calls:    \u001b[31m0\u001b[39m\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 64 |\u001b[39m     )\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 65 |\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 66 |\u001b[39m     expect(window\u001b[33m.\u001b[39melectronAPI\u001b[33m.\u001b[39msaveToPinecone)\u001b[33m.\u001b[39mtoHaveBeenCalled()\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m    |\u001b[39m                                               \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 67 |\u001b[39m     \u001b[36mconst\u001b[39m batch \u001b[33m=\u001b[39m (window\u001b[33m.\u001b[39melectronAPI\u001b[33m.\u001b[39msaveToPinecone \u001b[36mas\u001b[39m jest\u001b[33m.\u001b[39m\u001b[33mMock\u001b[39m)\u001b[33m.\u001b[39mmock\u001b[33m.\u001b[39mcalls[\u001b[35m0\u001b[39m][\u001b[35m0\u001b[39m]\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 68 |\u001b[39m     \u001b[36mconst\u001b[39m sorted \u001b[33m=\u001b[39m batch\u001b[33m.\u001b[39msort((a\u001b[33m:\u001b[39m { metadata\u001b[33m:\u001b[39m \u001b[33mVectorMetadata\u001b[39m }\u001b[33m,\u001b[39m b\u001b[33m:\u001b[39m { metadata\u001b[33m:\u001b[39m \u001b[33mVectorMetadata\u001b[39m }) \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 69 |\u001b[39m       \u001b[36mconst\u001b[39m ma \u001b[33m=\u001b[39m a\u001b[33m.\u001b[39mmetadata \u001b[36mas\u001b[39m \u001b[33mVectorMetadata\u001b[39m\u001b[33m;\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/services/memory/ConversationImportService.test.ts\u001b[39m\u001b[0m\u001b[2m:66:47)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mConversationImportService › should import and enrich messages from a sample JSON\u001b[39m\u001b[22m\n\n    \u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\n    Expected number of calls: >= \u001b[32m1\u001b[39m\n    Received number of calls:    \u001b[31m0\u001b[39m\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 122 |\u001b[39m     )\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 123 |\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 124 |\u001b[39m     expect(window\u001b[33m.\u001b[39melectronAPI\u001b[33m.\u001b[39msaveToPinecone)\u001b[33m.\u001b[39mtoHaveBeenCalled()\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                                               \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 125 |\u001b[39m     \u001b[36mconst\u001b[39m batch \u001b[33m=\u001b[39m (window\u001b[33m.\u001b[39melectronAPI\u001b[33m.\u001b[39msaveToPinecone \u001b[36mas\u001b[39m jest\u001b[33m.\u001b[39m\u001b[33mMock\u001b[39m)\u001b[33m.\u001b[39mmock\u001b[33m.\u001b[39mcalls[\u001b[35m0\u001b[39m][\u001b[35m0\u001b[39m]\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 126 |\u001b[39m     \u001b[36mconst\u001b[39m sorted \u001b[33m=\u001b[39m batch\u001b[33m.\u001b[39msort((a\u001b[33m:\u001b[39m { metadata\u001b[33m:\u001b[39m \u001b[33mVectorMetadata\u001b[39m }\u001b[33m,\u001b[39m b\u001b[33m:\u001b[39m { metadata\u001b[33m:\u001b[39m \u001b[33mVectorMetadata\u001b[39m }) \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 127 |\u001b[39m       \u001b[36mconst\u001b[39m ma \u001b[33m=\u001b[39m a\u001b[33m.\u001b[39mmetadata \u001b[36mas\u001b[39m \u001b[33mVectorMetadata\u001b[39m\u001b[33m;\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/services/memory/ConversationImportService.test.ts\u001b[39m\u001b[0m\u001b[2m:124:47)\u001b[22m\u001b[2m\u001b[22m\n","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/ConversationImportService.test.ts","startTime":1745799081945,"status":"failed","summary":""},{"assertionResults":[{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":36,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Deve fazer nova consulta ao Pinecone quando instruções são modificadas","invocations":1,"location":{"column":3,"line":86},"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"Deve fazer nova consulta ao Pinecone quando instruções são modificadas"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":4,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Pequenas modificações nas instruções devem gerar nova consulta","invocations":1,"location":{"column":3,"line":121},"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"Pequenas modificações nas instruções devem gerar nova consulta"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Modificações de formatação (espaços extras) devem gerar nova consulta","invocations":1,"location":{"column":3,"line":144},"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"Modificações de formatação (espaços extras) devem gerar nova consulta"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Os embeddings devem ser diferentes para instruções diferentes","invocations":1,"location":{"column":3,"line":167},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"Os embeddings devem ser diferentes para instruções diferentes"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":5,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Deve preservar memória quando é feita a mesma consulta","invocations":1,"location":{"column":3,"line":193},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"Deve preservar memória quando é feita a mesma consulta"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Contexto dinâmico que muda entre chamadas deve gerar nova consulta","invocations":1,"location":{"column":3,"line":218},"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"Contexto dinâmico que muda entre chamadas deve gerar nova consulta"}],"endTime":1745799082497,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TemporaryContextQueryBehavior.test.ts","startTime":1745799081950,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["LoggingUtils"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"LoggingUtils should log info and error with correct prefix","invocations":1,"location":{"column":3,"line":4},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"should log info and error with correct prefix"}],"endTime":1745799082555,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/LoggingUtils.test.ts","startTime":1745799082518,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["MemoryContextBuilder (unit)"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"MemoryContextBuilder (unit) should return empty SpeakerMemoryResults if embedding is not initialized","invocations":1,"location":{"column":3,"line":30},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"should return empty SpeakerMemoryResults if embedding is not initialized"},{"ancestorTitles":["MemoryContextBuilder (unit)"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"MemoryContextBuilder (unit) should call persistenceService.queryMemory for external speakers","invocations":1,"location":{"column":3,"line":43},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"should call persistenceService.queryMemory for external speakers"}],"endTime":1745799082563,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/MemoryContextBuilder.test.ts","startTime":1745799082517,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionSnapshotTracker Basic Functionality"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Basic Functionality filterTranscription removes existing content","invocations":1,"location":{"column":3,"line":12},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"filterTranscription removes existing content"},{"ancestorTitles":["TranscriptionSnapshotTracker Basic Functionality"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Basic Functionality filtering twice returns empty string","invocations":1,"location":{"column":3,"line":25},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"filtering twice returns empty string"},{"ancestorTitles":["TranscriptionSnapshotTracker Basic Functionality"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Basic Functionality reset clears the snapshot","invocations":1,"location":{"column":3,"line":40},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"reset clears the snapshot"},{"ancestorTitles":["TranscriptionSnapshotTracker Basic Functionality"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Basic Functionality normalization handles whitespace and empty lines","invocations":1,"location":{"column":3,"line":56},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"normalization handles whitespace and empty lines"}],"endTime":1745799082567,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TranscriptionSnapshotTracker.simple.test.ts","startTime":1745799082525,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":42,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests countTokens deve contar tokens corretamente com o modelo text-embedding-3-large","invocations":1,"location":{"column":3,"line":76},"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"countTokens deve contar tokens corretamente com o modelo text-embedding-3-large"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests A estrutura do buffer pode ser manipulada diretamente","invocations":1,"location":{"column":3,"line":93},"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"A estrutura do buffer pode ser manipulada diretamente"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":13,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests countBufferTokens deve contar corretamente os tokens no buffer","invocations":1,"location":{"column":3,"line":138},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"countBufferTokens deve contar corretamente os tokens no buffer"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests groupTranscriptionsBySpeaker deve agrupar corretamente as transcrições por falante","invocations":1,"location":{"column":3,"line":179},"numPassingAsserts":6,"retryReasons":[],"status":"passed","title":"groupTranscriptionsBySpeaker deve agrupar corretamente as transcrições por falante"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":4,"failureDetails":[{"matcherResult":{"actual":true,"expected":false,"message":"\u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\nExpected: \u001b[32mfalse\u001b[39m\nReceived: \u001b[31mtrue\u001b[39m","name":"toBe","pass":false}}],"failureMessages":["Error: \u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\nExpected: \u001b[32mfalse\u001b[39m\nReceived: \u001b[31mtrue\u001b[39m\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/tests/PineconeBuffer.test.ts:255:41)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"PineconeMemoryService Buffer Tests shouldFlushBuffer deve identificar corretamente quando o buffer deve ser esvaziado com base nos tokens","invocations":1,"location":{"column":3,"line":220},"numPassingAsserts":3,"retryReasons":[],"status":"failed","title":"shouldFlushBuffer deve identificar corretamente quando o buffer deve ser esvaziado com base nos tokens"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests saveInteraction deve adicionar mensagens ao buffer corretamente","invocations":1,"location":{"column":3,"line":278},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"saveInteraction deve adicionar mensagens ao buffer corretamente"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests O buffer deve acumular mensagens pequenas e fazer flush apenas quando o shouldFlushBuffer retorna true","invocations":1,"location":{"column":3,"line":323},"numPassingAsserts":10,"retryReasons":[],"status":"passed","title":"O buffer deve acumular mensagens pequenas e fazer flush apenas quando o shouldFlushBuffer retorna true"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests O buffer deve evitar duplicatas adjacentes mantendo a integridade do histórico","invocations":1,"location":{"column":3,"line":428},"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"O buffer deve evitar duplicatas adjacentes mantendo a integridade do histórico"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":2,"failureDetails":[{"matcherResult":{"message":"\u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mnot\u001b[2m.\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\nExpected number of calls: \u001b[32m0\u001b[39m\nReceived number of calls: \u001b[31m1\u001b[39m\n\n1: called with 0 arguments","pass":true}}],"failureMessages":["Error: \u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mnot\u001b[2m.\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\nExpected number of calls: \u001b[32m0\u001b[39m\nReceived number of calls: \u001b[31m1\u001b[39m\n\n1: called with 0 arguments\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/tests/PineconeBuffer.test.ts:545:33)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)"],"fullName":"PineconeMemoryService Buffer Tests O buffer deve preservar mensagens de usuário ao salvar respostas do assistente","invocations":1,"location":{"column":3,"line":488},"numPassingAsserts":2,"retryReasons":[],"status":"failed","title":"O buffer deve preservar mensagens de usuário ao salvar respostas do assistente"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":1,"failureDetails":[{"matcherResult":{"actual":true,"expected":false,"message":"\u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\nExpected: \u001b[32mfalse\u001b[39m\nReceived: \u001b[31mtrue\u001b[39m","name":"toBe","pass":false}}],"failureMessages":["Error: \u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\nExpected: \u001b[32mfalse\u001b[39m\nReceived: \u001b[31mtrue\u001b[39m\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/tests/PineconeBuffer.test.ts:615:43)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"PineconeMemoryService Buffer Tests O buffer só deve ser limpo quando o limite de tokens for atingido","invocations":1,"location":{"column":3,"line":567},"numPassingAsserts":3,"retryReasons":[],"status":"failed","title":"O buffer só deve ser limpo quando o limite de tokens for atingido"}],"endTime":1745799082603,"message":"\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mPineconeMemoryService Buffer Tests › shouldFlushBuffer deve identificar corretamente quando o buffer deve ser esvaziado com base nos tokens\u001b[39m\u001b[22m\n\n    \u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\n    Expected: \u001b[32mfalse\u001b[39m\n    Received: \u001b[31mtrue\u001b[39m\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 253 |\u001b[39m     service\u001b[33m.\u001b[39mcountBufferTokens \u001b[33m=\u001b[39m jest\u001b[33m.\u001b[39mfn()\u001b[33m.\u001b[39mmockReturnValue(\u001b[35m5\u001b[39m)\u001b[33m;\u001b[39m \u001b[90m// tokens abaixo do mínimo\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 254 |\u001b[39m     service\u001b[33m.\u001b[39mmessageBuffer\u001b[33m.\u001b[39mlastFlushTime \u001b[33m=\u001b[39m \u001b[33mDate\u001b[39m\u001b[33m.\u001b[39mnow() \u001b[33m-\u001b[39m \u001b[35m600000\u001b[39m\u001b[33m;\u001b[39m \u001b[90m// 10 minutos atrás (> maxBufferAgeMs)\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 255 |\u001b[39m     expect(service\u001b[33m.\u001b[39mshouldFlushBuffer())\u001b[33m.\u001b[39mtoBe(\u001b[36mfalse\u001b[39m)\u001b[33m;\u001b[39m \u001b[90m// mesmo com tempo excedido, não deve fazer flush com poucos tokens\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                                         \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 256 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 257 |\u001b[39m     \u001b[90m// 5. Teste: Inatividade do usuário não deve mais acionar o esvaziamento\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 258 |\u001b[39m     service\u001b[33m.\u001b[39mmessageBuffer \u001b[33m=\u001b[39m {\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/tests/PineconeBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:255:41)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mPineconeMemoryService Buffer Tests › O buffer deve preservar mensagens de usuário ao salvar respostas do assistente\u001b[39m\u001b[22m\n\n    \u001b[2mexpect(\u001b[22m\u001b[31mjest.fn()\u001b[39m\u001b[2m).\u001b[22mnot\u001b[2m.\u001b[22mtoHaveBeenCalled\u001b[2m()\u001b[22m\n\n    Expected number of calls: \u001b[32m0\u001b[39m\n    Received number of calls: \u001b[31m1\u001b[39m\n\n    1: called with 0 arguments\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 543 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 544 |\u001b[39m     \u001b[90m// Verificar que resetBuffer NÃO foi chamado (buffer não foi limpo)\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 545 |\u001b[39m     expect(resetBufferMock)\u001b[33m.\u001b[39mnot\u001b[33m.\u001b[39mtoHaveBeenCalled()\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                                 \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 546 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 547 |\u001b[39m     \u001b[90m// Verificar que as mensagens do usuário ainda estão no buffer\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 548 |\u001b[39m     expect(service\u001b[33m.\u001b[39mmessageBuffer\u001b[33m.\u001b[39mprimaryUser\u001b[33m.\u001b[39mmessages\u001b[33m.\u001b[39mlength)\u001b[33m.\u001b[39mtoBe(\u001b[35m2\u001b[39m)\u001b[33m;\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/tests/PineconeBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:545:33)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mPineconeMemoryService Buffer Tests › O buffer só deve ser limpo quando o limite de tokens for atingido\u001b[39m\u001b[22m\n\n    \u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\n    Expected: \u001b[32mfalse\u001b[39m\n    Received: \u001b[31mtrue\u001b[39m\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 613 |\u001b[39m       \u001b[90m// Mesmo com tempos excedidos, não deve fazer flush se tokens estão abaixo do limite\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 614 |\u001b[39m       service\u001b[33m.\u001b[39mcountBufferTokens \u001b[33m=\u001b[39m jest\u001b[33m.\u001b[39mfn()\u001b[33m.\u001b[39mmockReturnValue(\u001b[35m30\u001b[39m)\u001b[33m;\u001b[39m \u001b[90m// 30 tokens (abaixo do mínimo)\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 615 |\u001b[39m       expect(service\u001b[33m.\u001b[39mshouldFlushBuffer())\u001b[33m.\u001b[39mtoBe(\u001b[36mfalse\u001b[39m)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                                           \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 616 |\u001b[39m     } \u001b[36mfinally\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 617 |\u001b[39m       \u001b[90m// Restaurar métodos originais\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 618 |\u001b[39m       service\u001b[33m.\u001b[39mcountBufferTokens \u001b[33m=\u001b[39m originalCountTokens\u001b[33m;\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/tests/PineconeBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:615:43)\u001b[22m\u001b[2m\u001b[22m\n","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/tests/PineconeBuffer.test.ts","startTime":1745799081938,"status":"failed","summary":""},{"assertionResults":[{"ancestorTitles":["ConversationImportService E2E"],"duration":49,"failureDetails":[],"failureMessages":[],"fullName":"ConversationImportService E2E deve importar conversas e processar o arquivo JSON corretamente","invocations":1,"location":{"column":3,"line":147},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"deve importar conversas e processar o arquivo JSON corretamente"}],"endTime":1745799082631,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/ConversationImportService.e2e.test.tsx","startTime":1745799081957,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":33,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Deve diferenciar strings com pequenas variações (espaços, formatação)","invocations":1,"location":{"column":3,"line":95},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Deve diferenciar strings com pequenas variações (espaços, formatação)"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":5,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Strings vazias e undefined são tratados corretamente","invocations":1,"location":{"column":3,"line":116},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Strings vazias e undefined são tratados corretamente"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":104,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Consultas múltiplas em rápida sucessão","invocations":1,"location":{"column":3,"line":150},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Consultas múltiplas em rápida sucessão"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Falha na consulta ao Pinecone","invocations":1,"location":{"column":3,"line":176},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Falha na consulta ao Pinecone"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Contextos muito longos são tratados adequadamente","invocations":1,"location":{"column":3,"line":217},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Contextos muito longos são tratados adequadamente"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Reinicialização do contexto reseta adequadamente","invocations":1,"location":{"column":3,"line":232},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Reinicialização do contexto reseta adequadamente"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Alterações mínimas em contextos longos geram consultas completas","invocations":1,"location":{"column":3,"line":254},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Alterações mínimas em contextos longos geram consultas completas"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Contexto undefined seguido de string vazia","invocations":1,"location":{"column":3,"line":276},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Contexto undefined seguido de string vazia"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Interações entre setTemporaryContext e fetchContextualMemory","invocations":1,"location":{"column":3,"line":303},"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Interações entre setTemporaryContext e fetchContextualMemory"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext limpa todos os aspectos do contexto","invocations":1,"location":{"column":3,"line":318},"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"clearTemporaryContext limpa todos os aspectos do contexto"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext limpa o último contexto consultado","invocations":1,"location":{"column":3,"line":354},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"clearTemporaryContext limpa o último contexto consultado"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext vs resetAll","invocations":1,"location":{"column":3,"line":382},"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"clearTemporaryContext vs resetAll"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Preservação do contexto após clearTemporaryContext","invocations":1,"location":{"column":3,"line":421},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"Preservação do contexto após clearTemporaryContext"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext limpa explicitamente lastQueriedTemporaryContext","invocations":1,"location":{"column":3,"line":451},"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"clearTemporaryContext limpa explicitamente lastQueriedTemporaryContext"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext afeta todas as instâncias de MemoryContextBuilder","invocations":1,"location":{"column":3,"line":490},"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"clearTemporaryContext afeta todas as instâncias de MemoryContextBuilder"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Interação entre clearTemporaryContext e updateLastQueriedTemporaryContext","invocations":1,"location":{"column":3,"line":529},"numPassingAsserts":8,"retryReasons":[],"status":"passed","title":"Interação entre clearTemporaryContext e updateLastQueriedTemporaryContext"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases hasTemporaryContextChanged lida corretamente com strings vazias","invocations":1,"location":{"column":3,"line":600},"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"hasTemporaryContextChanged lida corretamente com strings vazias"}],"endTime":1745799082639,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TemporaryContextEdgeCases.test.ts","startTime":1745799081928,"status":"passed","summary":""},{"assertionResults":[],"coverage":{},"endTime":1745799086036,"message":"  \u001b[1m● \u001b[22mTest suite failed to run\n\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m58\u001b[0m:\u001b[93m17\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m58\u001b[0m       mergedText: 'Test merged text content',\n    \u001b[7m  \u001b[0m \u001b[91m                ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m59\u001b[0m:\u001b[93m15\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m59\u001b[0m       metadata: {\n    \u001b[7m  \u001b[0m \u001b[91m              ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m61\u001b[0m:\u001b[93m14\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m61\u001b[0m         roles: ['user'],\n    \u001b[7m  \u001b[0m \u001b[91m             ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m62\u001b[0m:\u001b[93m22\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m62\u001b[0m         totalMessages: 3,\n    \u001b[7m  \u001b[0m \u001b[91m                     ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m63\u001b[0m:\u001b[93m19\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m63\u001b[0m         timestamps: testTimestamps, // Array de números que precisa ser convertido\n    \u001b[7m  \u001b[0m \u001b[91m                  ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m64\u001b[0m:\u001b[93m18\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m64\u001b[0m         flushedAt: Date.now(),\n    \u001b[7m  \u001b[0m \u001b[91m                 ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m65\u001b[0m:\u001b[93m26\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m65\u001b[0m         neuralSystemPhase: \"memory\", // Fase neural - hipocampo\n    \u001b[7m  \u001b[0m \u001b[91m                         ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m66\u001b[0m:\u001b[93m23\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m66\u001b[0m         processingType: \"symbolic\",\n    \u001b[7m  \u001b[0m \u001b[91m                      ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m67\u001b[0m:\u001b[93m19\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m67\u001b[0m         memoryType: \"episodic\",\n    \u001b[7m  \u001b[0m \u001b[91m                  ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m68\u001b[0m:\u001b[93m19\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m68\u001b[0m         tokenCount: 50\n    \u001b[7m  \u001b[0m \u001b[91m                  ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m70\u001b[0m:\u001b[93m6\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m')' expected.\n\n    \u001b[7m70\u001b[0m     };\n    \u001b[7m  \u001b[0m \u001b[91m     ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m143\u001b[0m:\u001b[93m17\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m143\u001b[0m       mergedText: 'Test merged text content',\n    \u001b[7m   \u001b[0m \u001b[91m                ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m144\u001b[0m:\u001b[93m15\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m144\u001b[0m       metadata: complexMetadata\n    \u001b[7m   \u001b[0m \u001b[91m              ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m145\u001b[0m:\u001b[93m6\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m')' expected.\n\n    \u001b[7m145\u001b[0m     };\n    \u001b[7m   \u001b[0m \u001b[91m     ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m180\u001b[0m:\u001b[93m3\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1128: \u001b[0mDeclaration or statement expected.\n\n    \u001b[7m180\u001b[0m   });\n    \u001b[7m   \u001b[0m \u001b[91m  ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m180\u001b[0m:\u001b[93m4\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1128: \u001b[0mDeclaration or statement expected.\n\n    \u001b[7m180\u001b[0m   });\n    \u001b[7m   \u001b[0m \u001b[91m   ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m228\u001b[0m:\u001b[93m19\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m228\u001b[0m         mergedText: `Test content for ${testCase.phase} phase`,\n    \u001b[7m   \u001b[0m \u001b[91m                  ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m229\u001b[0m:\u001b[93m17\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m229\u001b[0m         metadata: {\n    \u001b[7m   \u001b[0m \u001b[91m                ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m231\u001b[0m:\u001b[93m16\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m231\u001b[0m           roles: [testCase.speakerType],\n    \u001b[7m   \u001b[0m \u001b[91m               ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m232\u001b[0m:\u001b[93m24\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m232\u001b[0m           totalMessages: 1,\n    \u001b[7m   \u001b[0m \u001b[91m                       ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m233\u001b[0m:\u001b[93m21\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m233\u001b[0m           timestamps: [Date.now()],\n    \u001b[7m   \u001b[0m \u001b[91m                    ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m234\u001b[0m:\u001b[93m20\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m234\u001b[0m           flushedAt: Date.now(),\n    \u001b[7m   \u001b[0m \u001b[91m                   ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m235\u001b[0m:\u001b[93m28\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m235\u001b[0m           neuralSystemPhase: testCase.phase,\n    \u001b[7m   \u001b[0m \u001b[91m                           ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m236\u001b[0m:\u001b[93m25\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m236\u001b[0m           processingType: \"symbolic\",\n    \u001b[7m   \u001b[0m \u001b[91m                        ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m237\u001b[0m:\u001b[93m21\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m237\u001b[0m           memoryType: \"episodic\",\n    \u001b[7m   \u001b[0m \u001b[91m                    ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m238\u001b[0m:\u001b[93m21\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m';' expected.\n\n    \u001b[7m238\u001b[0m           tokenCount: 50\n    \u001b[7m   \u001b[0m \u001b[91m                    ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m270\u001b[0m:\u001b[93m3\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1005: \u001b[0m',' expected.\n\n    \u001b[7m270\u001b[0m   });\n    \u001b[7m   \u001b[0m \u001b[91m  ~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m271\u001b[0m:\u001b[93m1\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1128: \u001b[0mDeclaration or statement expected.\n\n    \u001b[7m271\u001b[0m });\n    \u001b[7m   \u001b[0m \u001b[91m~\u001b[0m\n    \u001b[96msrc/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts\u001b[0m:\u001b[93m271\u001b[0m:\u001b[93m2\u001b[0m - \u001b[91merror\u001b[0m\u001b[90m TS1128: \u001b[0mDeclaration or statement expected.\n\n    \u001b[7m271\u001b[0m });\n    \u001b[7m   \u001b[0m \u001b[91m ~\u001b[0m\n","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts","startTime":1745799086036,"status":"failed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionPanel Importação E2E"],"duration":3315,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionPanel Importação E2E deve importar conversas, mostrar progresso e resumo","invocations":1,"location":{"column":3,"line":222},"numPassingAsserts":7,"retryReasons":[],"status":"passed","title":"deve importar conversas, mostrar progresso e resumo"}],"endTime":1745799085972,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/TranscriptionPanel.test.tsx","startTime":1745799081962,"status":"passed","summary":""}],"wasInterrupted":false}
{"numFailedTestSuites":3,"numFailedTests":14,"numPassedTestSuites":17,"numPassedTests":74,"numPendingTestSuites":0,"numPendingTests":0,"numRuntimeErrorTestSuites":0,"numTodoTests":0,"numTotalTestSuites":20,"numTotalTests":88,"openHandles":[],"snapshot":{"added":0,"didUpdate":false,"failure":false,"filesAdded":0,"filesRemoved":0,"filesRemovedList":[],"filesUnmatched":0,"filesUpdated":0,"matched":0,"total":0,"unchecked":0,"uncheckedKeysByFile":[],"unmatched":0,"updated":0},"startTime":1745956293487,"success":false,"testResults":[{"assertionResults":[{"ancestorTitles":["Pinecone Metadata Compatibility Tests"],"duration":7,"failureDetails":[],"failureMessages":[],"fullName":"Pinecone Metadata Compatibility Tests should correctly format timestamps as compatible metadata for Pinecone","invocations":1,"location":null,"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"should correctly format timestamps as compatible metadata for Pinecone"},{"ancestorTitles":["Pinecone Metadata Compatibility Tests"],"duration":4,"failureDetails":[],"failureMessages":[],"fullName":"Pinecone Metadata Compatibility Tests should correctly handle complex metadata structures for Pinecone compatibility","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"should correctly handle complex metadata structures for Pinecone compatibility"},{"ancestorTitles":["Pinecone Metadata Compatibility Tests"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"Pinecone Metadata Compatibility Tests should correctly format neural system phase metadata for the 3-phase system","invocations":1,"location":null,"numPassingAsserts":9,"retryReasons":[],"status":"passed","title":"should correctly format neural system phase metadata for the 3-phase system"}],"endTime":1745956294987,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/PineconeMetadataTests.test.ts","startTime":1745956294446,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionContextPersistence"],"duration":13,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence Contexto temporário deve persistir entre diferentes instâncias de MemoryContextBuilder","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Contexto temporário deve persistir entre diferentes instâncias de MemoryContextBuilder"},{"ancestorTitles":["TranscriptionContextPersistence"],"duration":8,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence Resetar temporaryContext deve afetar todas as instâncias","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Resetar temporaryContext deve afetar todas as instâncias"},{"ancestorTitles":["TranscriptionContextPersistence"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence resetAll deve limpar tanto snapshot quanto contexto temporário","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"resetAll deve limpar tanto snapshot quanto contexto temporário"},{"ancestorTitles":["TranscriptionContextPersistence"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence A memória do contexto temporário deve persistir entre chamadas","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"A memória do contexto temporário deve persistir entre chamadas"},{"ancestorTitles":["TranscriptionContextPersistence"],"duration":37,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextPersistence Pinecone só deve ser consultado para o temporaryContext quando ele mudar","invocations":1,"location":null,"numPassingAsserts":6,"retryReasons":[],"status":"passed","title":"Pinecone só deve ser consultado para o temporaryContext quando ele mudar"}],"endTime":1745956294990,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TranscriptionContextPersistence.test.ts","startTime":1745956294450,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["Integração IPC import-chatgpt-history"],"duration":58,"failureDetails":[],"failureMessages":[],"fullName":"Integração IPC import-chatgpt-history deve importar apenas mensagens novas no modo incremental","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"deve importar apenas mensagens novas no modo incremental"},{"ancestorTitles":["Integração IPC import-chatgpt-history"],"duration":4,"failureDetails":[],"failureMessages":[],"fullName":"Integração IPC import-chatgpt-history deve importar todas as mensagens no modo overwrite","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"deve importar todas as mensagens no modo overwrite"}],"endTime":1745956295016,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/electron/ipcImport.integration.test.ts","startTime":1745956294458,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":35,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should cache identical queries for same user (namespace gerenciado internamente)","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"should cache identical queries for same user (namespace gerenciado internamente)"},{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should handle persistence service failure gracefully","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"should handle persistence service failure gracefully"},{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should call electronAPIMock.queryPinecone in Node env","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"should call electronAPIMock.queryPinecone in Node env"},{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should return empty string if not available or embedding is empty","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"should return empty string if not available or embedding is empty"},{"ancestorTitles":["PineconeMemoryService (unit)"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService (unit) should handle rejected promises gracefully","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"should handle rejected promises gracefully"}],"endTime":1745956295040,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/PineconeMemoryService.test.ts","startTime":1745956294448,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["MemoryService - Isolation Between Namespaces"],"duration":39,"failureDetails":[],"failureMessages":[],"fullName":"MemoryService - Isolation Between Namespaces deve garantir isolamento entre diferentes usuários com namespaces gerenciados internamente","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"deve garantir isolamento entre diferentes usuários com namespaces gerenciados internamente"},{"ancestorTitles":["MemoryService - Isolation Between Namespaces"],"duration":12,"failureDetails":[],"failureMessages":[],"fullName":"MemoryService - Isolation Between Namespaces deve persistir o contexto temporário entre chamadas","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"deve persistir o contexto temporário entre chamadas"},{"ancestorTitles":["MemoryService - Isolation Between Namespaces"],"duration":8,"failureDetails":[],"failureMessages":[],"fullName":"MemoryService - Isolation Between Namespaces deve atualizar o contexto temporário quando for diferente","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"deve atualizar o contexto temporário quando for diferente"}],"endTime":1745956295041,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/MemoryService.integration.test.ts","startTime":1745956294443,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":36,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Deve fazer nova consulta ao Pinecone quando instruções são modificadas","invocations":1,"location":null,"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"Deve fazer nova consulta ao Pinecone quando instruções são modificadas"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":4,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Pequenas modificações nas instruções devem gerar nova consulta","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"Pequenas modificações nas instruções devem gerar nova consulta"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Modificações de formatação (espaços extras) devem gerar nova consulta","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"Modificações de formatação (espaços extras) devem gerar nova consulta"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Os embeddings devem ser diferentes para instruções diferentes","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"Os embeddings devem ser diferentes para instruções diferentes"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Deve preservar memória quando é feita a mesma consulta","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"Deve preservar memória quando é feita a mesma consulta"},{"ancestorTitles":["TemporaryContextQueryBehavior"],"duration":5,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextQueryBehavior Contexto dinâmico que muda entre chamadas deve gerar nova consulta","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"Contexto dinâmico que muda entre chamadas deve gerar nova consulta"}],"endTime":1745956295056,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TemporaryContextQueryBehavior.test.ts","startTime":1745956294432,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["ChatGPT Import with Chunking"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"ChatGPT Import with Chunking should correctly split a long message into chunks","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"should correctly split a long message into chunks"},{"ancestorTitles":["ChatGPT Import with Chunking"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"ChatGPT Import with Chunking should track progress correctly while processing chunks","invocations":1,"location":null,"numPassingAsserts":9,"retryReasons":[],"status":"passed","title":"should track progress correctly while processing chunks"},{"ancestorTitles":["ChatGPT Import with Chunking"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"ChatGPT Import with Chunking should calculate correct metadata for chunks","invocations":1,"location":null,"numPassingAsserts":7,"retryReasons":[],"status":"passed","title":"should calculate correct metadata for chunks"}],"endTime":1745956295074,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/ChatGPTImport.test.ts","startTime":1745956295007,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionSnapshotTracker"],"duration":9,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Mensagem nova é tratada corretamente sem duplicações nem confusão de contexto","invocations":1,"location":null,"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"Mensagem nova é tratada corretamente sem duplicações nem confusão de contexto"},{"ancestorTitles":["TranscriptionSnapshotTracker"],"duration":8,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Multiple messages are properly deduplicated","invocations":1,"location":null,"numPassingAsserts":11,"retryReasons":[],"status":"passed","title":"Multiple messages are properly deduplicated"},{"ancestorTitles":["TranscriptionSnapshotTracker"],"duration":26,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Temporary context é deduplicado entre execuções","invocations":1,"location":null,"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"Temporary context é deduplicado entre execuções"},{"ancestorTitles":["TranscriptionSnapshotTracker"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Temporary context com objetos dinamicamente criados","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"Temporary context com objetos dinamicamente criados"}],"endTime":1745956295112,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TranscriptionSnapshotTracker.test.ts","startTime":1745956294463,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["MemoryService"],"duration":9,"failureDetails":[],"failureMessages":[],"fullName":"MemoryService fetchContextualMemory should not throw and return valid SpeakerMemoryResults","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"fetchContextualMemory should not throw and return valid SpeakerMemoryResults"}],"endTime":1745956295132,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/MemoryService.test.ts","startTime":1745956295070,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["ConversationHistoryManager"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"ConversationHistoryManager should initialize with system message","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"should initialize with system message"},{"ancestorTitles":["ConversationHistoryManager"],"duration":6,"failureDetails":[],"failureMessages":[],"fullName":"ConversationHistoryManager should add messages and prune history when exceeding maxInteractions","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"should add messages and prune history when exceeding maxInteractions"}],"endTime":1745956295131,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/ConversationHistoryManager.test.ts","startTime":1745956295077,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionContextManager"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager Deve manter o singleton em toda aplicação","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Deve manter o singleton em toda aplicação"},{"ancestorTitles":["TranscriptionContextManager"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager Deve persistir o contexto entre múltiplas chamadas","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Deve persistir o contexto entre múltiplas chamadas"},{"ancestorTitles":["TranscriptionContextManager"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager Não deve limpar o contexto quando undefined é passado","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Não deve limpar o contexto quando undefined é passado"},{"ancestorTitles":["TranscriptionContextManager"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager Deve substituir o contexto anterior quando string vazia é passada","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Deve substituir o contexto anterior quando string vazia é passada"},{"ancestorTitles":["TranscriptionContextManager"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionContextManager hasTemporaryContext deve retornar corretamente","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"hasTemporaryContext deve retornar corretamente"}],"endTime":1745956295137,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TranscriptionContextManager.test.ts","startTime":1745956295087,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionSnapshotTracker Basic Functionality"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Basic Functionality filterTranscription removes existing content","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"filterTranscription removes existing content"},{"ancestorTitles":["TranscriptionSnapshotTracker Basic Functionality"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Basic Functionality filtering twice returns empty string","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"filtering twice returns empty string"},{"ancestorTitles":["TranscriptionSnapshotTracker Basic Functionality"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Basic Functionality reset clears the snapshot","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"reset clears the snapshot"},{"ancestorTitles":["TranscriptionSnapshotTracker Basic Functionality"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"TranscriptionSnapshotTracker Basic Functionality normalization handles whitespace and empty lines","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"normalization handles whitespace and empty lines"}],"endTime":1745956295144,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TranscriptionSnapshotTracker.simple.test.ts","startTime":1745956295098,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":5,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:31:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should create a buffer with proper initial state","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should create a buffer with proper initial state"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":3,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:36:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should add messages to buffer without flushing if below threshold","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should add messages to buffer without flushing if below threshold"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":2,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:51:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should flush messages when token threshold is reached","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should flush messages when token threshold is reached"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":1,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:75:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should clear the buffer after flushing","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should clear the buffer after flushing"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":2,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:98:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should manually flush messages even if below threshold","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should manually flush messages even if below threshold"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":2,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:114:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should not flush if buffer is empty","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should not flush if buffer is empty"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":1,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:121:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should clear the buffer","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should clear the buffer"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":1,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:137:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should handle messages with different roles","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should handle messages with different roles"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":1,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:164:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should handle estimating tokens based on character count","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should handle estimating tokens based on character count"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":1,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:182:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should handle messages just below the token threshold","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should handle messages just below the token threshold"},{"ancestorTitles":["SymbolicMemoryBuffer"],"duration":1,"failureDetails":[{}],"failureMessages":["ReferenceError: SymbolicMemoryBuffer is not defined\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts:207:20)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"SymbolicMemoryBuffer should correctly format messages with all supported role types","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"should correctly format messages with all supported role types"}],"endTime":1745956295144,"message":"\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should create a buffer with proper initial state\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 29 |\u001b[39m   \u001b[22m\n\u001b[2m     \u001b[90m 30 |\u001b[39m   it(\u001b[32m'should create a buffer with proper initial state'\u001b[39m\u001b[33m,\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 31 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m100\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m    |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 32 |\u001b[39m     expect(buffer)\u001b[33m.\u001b[39mtoBeDefined()\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 33 |\u001b[39m   })\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 34 |\u001b[39m   \u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:31:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should add messages to buffer without flushing if below threshold\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 34 |\u001b[39m   \u001b[22m\n\u001b[2m     \u001b[90m 35 |\u001b[39m   it(\u001b[32m'should add messages to buffer without flushing if below threshold'\u001b[39m\u001b[33m,\u001b[39m \u001b[36masync\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 36 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m100\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m    |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 37 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 38 |\u001b[39m     \u001b[36mconst\u001b[39m message\u001b[33m:\u001b[39m \u001b[33mBufferedMessage\u001b[39m \u001b[33m=\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 39 |\u001b[39m       content\u001b[33m:\u001b[39m \u001b[32m'Test message'\u001b[39m\u001b[33m,\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:36:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should flush messages when token threshold is reached\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 49 |\u001b[39m   it(\u001b[32m'should flush messages when token threshold is reached'\u001b[39m\u001b[33m,\u001b[39m \u001b[36masync\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 50 |\u001b[39m     \u001b[90m// We'll use a token threshold of 5 and ensure our mock returns more tokens than that\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 51 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m5\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m    |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 52 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 53 |\u001b[39m     \u001b[36mconst\u001b[39m message\u001b[33m:\u001b[39m \u001b[33mBufferedMessage\u001b[39m \u001b[33m=\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 54 |\u001b[39m       content\u001b[33m:\u001b[39m \u001b[32m'Test message that exceeds token threshold'\u001b[39m\u001b[33m,\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:51:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should clear the buffer after flushing\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 73 |\u001b[39m     \u001b[90m// Criamos um buffer com threshold maior que o conteúdo da mensagem\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 74 |\u001b[39m     \u001b[90m// para garantir que não haja flush imediato numa única mensagem\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 75 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m10\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m    |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 76 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 77 |\u001b[39m     \u001b[36mconst\u001b[39m message\u001b[33m:\u001b[39m \u001b[33mBufferedMessage\u001b[39m \u001b[33m=\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 78 |\u001b[39m       content\u001b[33m:\u001b[39m \u001b[32m'Test'\u001b[39m\u001b[33m,\u001b[39m  \u001b[90m// Apenas 4 caracteres = ~1 token\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:75:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should manually flush messages even if below threshold\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m  96 |\u001b[39m   \u001b[22m\n\u001b[2m     \u001b[90m  97 |\u001b[39m   it(\u001b[32m'should manually flush messages even if below threshold'\u001b[39m\u001b[33m,\u001b[39m \u001b[36masync\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m  98 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m100\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m  99 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 100 |\u001b[39m     \u001b[36mconst\u001b[39m message\u001b[33m:\u001b[39m \u001b[33mBufferedMessage\u001b[39m \u001b[33m=\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 101 |\u001b[39m       content\u001b[33m:\u001b[39m \u001b[32m'Test message'\u001b[39m\u001b[33m,\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:98:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should not flush if buffer is empty\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 112 |\u001b[39m   \u001b[22m\n\u001b[2m     \u001b[90m 113 |\u001b[39m   it(\u001b[32m'should not flush if buffer is empty'\u001b[39m\u001b[33m,\u001b[39m \u001b[36masync\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 114 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m100\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 115 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 116 |\u001b[39m     \u001b[36mawait\u001b[39m buffer\u001b[33m.\u001b[39mflush()\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 117 |\u001b[39m     expect(mockFlushCallback)\u001b[33m.\u001b[39mnot\u001b[33m.\u001b[39mtoHaveBeenCalled()\u001b[33m;\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:114:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should clear the buffer\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 119 |\u001b[39m   \u001b[22m\n\u001b[2m     \u001b[90m 120 |\u001b[39m   it(\u001b[32m'should clear the buffer'\u001b[39m\u001b[33m,\u001b[39m \u001b[36masync\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 121 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m100\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 122 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 123 |\u001b[39m     \u001b[36mconst\u001b[39m message\u001b[33m:\u001b[39m \u001b[33mBufferedMessage\u001b[39m \u001b[33m=\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 124 |\u001b[39m       content\u001b[33m:\u001b[39m \u001b[32m'Test message'\u001b[39m\u001b[33m,\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:121:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should handle messages with different roles\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 135 |\u001b[39m   \u001b[22m\n\u001b[2m     \u001b[90m 136 |\u001b[39m   it(\u001b[32m'should handle messages with different roles'\u001b[39m\u001b[33m,\u001b[39m \u001b[36masync\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 137 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m100\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 138 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 139 |\u001b[39m     \u001b[36mconst\u001b[39m userMessage\u001b[33m:\u001b[39m \u001b[33mBufferedMessage\u001b[39m \u001b[33m=\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 140 |\u001b[39m       content\u001b[33m:\u001b[39m \u001b[32m'User message'\u001b[39m\u001b[33m,\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:137:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should handle estimating tokens based on character count\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 162 |\u001b[39m   \u001b[22m\n\u001b[2m     \u001b[90m 163 |\u001b[39m   it(\u001b[32m'should handle estimating tokens based on character count'\u001b[39m\u001b[33m,\u001b[39m \u001b[36masync\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 164 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m20\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 165 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 166 |\u001b[39m     \u001b[90m// Create a message with 70 characters (should be ~20 tokens with 3.5 char per token)\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 167 |\u001b[39m     \u001b[36mconst\u001b[39m longMessage\u001b[33m:\u001b[39m \u001b[33mBufferedMessage\u001b[39m \u001b[33m=\u001b[39m {\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:164:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should handle messages just below the token threshold\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 180 |\u001b[39m     \u001b[90m// Nosso mock de gpt-tokenizer calcula aprox. 1 token a cada 4 caracteres\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 181 |\u001b[39m     \u001b[90m// Vamos ajustar para usar isso corretamente\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 182 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m15\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 183 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 184 |\u001b[39m     \u001b[90m// Com nosso mock, isso gera ~11 tokens (40 caracteres / 4 + formatação)\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 185 |\u001b[39m     \u001b[36mconst\u001b[39m message\u001b[33m:\u001b[39m \u001b[33mBufferedMessage\u001b[39m \u001b[33m=\u001b[39m {\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:182:20)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mSymbolicMemoryBuffer › should correctly format messages with all supported role types\u001b[39m\u001b[22m\n\n    ReferenceError: SymbolicMemoryBuffer is not defined\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 205 |\u001b[39m   \u001b[22m\n\u001b[2m     \u001b[90m 206 |\u001b[39m   it(\u001b[32m'should correctly format messages with all supported role types'\u001b[39m\u001b[33m,\u001b[39m \u001b[36masync\u001b[39m () \u001b[33m=>\u001b[39m {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 207 |\u001b[39m     \u001b[36mconst\u001b[39m buffer \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mSymbolicMemoryBuffer\u001b[39m(\u001b[35m100\u001b[39m\u001b[33m,\u001b[39m mockFlushCallback)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                    \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 208 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 209 |\u001b[39m     \u001b[36mawait\u001b[39m buffer\u001b[33m.\u001b[39madd({\u001b[22m\n\u001b[2m     \u001b[90m 210 |\u001b[39m       content\u001b[33m:\u001b[39m \u001b[32m'User message'\u001b[39m\u001b[33m,\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:207:20)\u001b[22m\u001b[2m\u001b[22m\n","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/utils/SymbolicMemoryBuffer.test.ts","startTime":1745956294406,"status":"failed","summary":""},{"assertionResults":[{"ancestorTitles":["LoggingUtils"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"LoggingUtils should log info and error with correct prefix","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"should log info and error with correct prefix"}],"endTime":1745956295155,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/LoggingUtils.test.ts","startTime":1745956295098,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["MemoryContextBuilder (unit)"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"MemoryContextBuilder (unit) should return empty SpeakerMemoryResults if embedding is not initialized","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"should return empty SpeakerMemoryResults if embedding is not initialized"},{"ancestorTitles":["MemoryContextBuilder (unit)"],"duration":7,"failureDetails":[],"failureMessages":[],"fullName":"MemoryContextBuilder (unit) should call persistenceService.queryMemory for external speakers","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"should call persistenceService.queryMemory for external speakers"}],"endTime":1745956295158,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/MemoryContextBuilder.test.ts","startTime":1745956295078,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":30,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Deve diferenciar strings com pequenas variações (espaços, formatação)","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Deve diferenciar strings com pequenas variações (espaços, formatação)"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":5,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Strings vazias e undefined são tratados corretamente","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Strings vazias e undefined são tratados corretamente"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":105,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Consultas múltiplas em rápida sucessão","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Consultas múltiplas em rápida sucessão"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":3,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Falha na consulta ao Pinecone","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Falha na consulta ao Pinecone"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Contextos muito longos são tratados adequadamente","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Contextos muito longos são tratados adequadamente"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Reinicialização do contexto reseta adequadamente","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Reinicialização do contexto reseta adequadamente"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Alterações mínimas em contextos longos geram consultas completas","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Alterações mínimas em contextos longos geram consultas completas"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Contexto undefined seguido de string vazia","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"Contexto undefined seguido de string vazia"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Interações entre setTemporaryContext e fetchContextualMemory","invocations":1,"location":null,"numPassingAsserts":1,"retryReasons":[],"status":"passed","title":"Interações entre setTemporaryContext e fetchContextualMemory"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext limpa todos os aspectos do contexto","invocations":1,"location":null,"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"clearTemporaryContext limpa todos os aspectos do contexto"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext limpa o último contexto consultado","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"clearTemporaryContext limpa o último contexto consultado"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext vs resetAll","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"clearTemporaryContext vs resetAll"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":0,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Preservação do contexto após clearTemporaryContext","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"Preservação do contexto após clearTemporaryContext"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext limpa explicitamente lastQueriedTemporaryContext","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"clearTemporaryContext limpa explicitamente lastQueriedTemporaryContext"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases clearTemporaryContext afeta todas as instâncias de MemoryContextBuilder","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"clearTemporaryContext afeta todas as instâncias de MemoryContextBuilder"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases Interação entre clearTemporaryContext e updateLastQueriedTemporaryContext","invocations":1,"location":null,"numPassingAsserts":8,"retryReasons":[],"status":"passed","title":"Interação entre clearTemporaryContext e updateLastQueriedTemporaryContext"},{"ancestorTitles":["TemporaryContextEdgeCases"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"TemporaryContextEdgeCases hasTemporaryContextChanged lida corretamente com strings vazias","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"hasTemporaryContextChanged lida corretamente com strings vazias"}],"endTime":1745956295173,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/tests/TemporaryContextEdgeCases.test.ts","startTime":1745956294445,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":41,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests countTokens deve contar tokens corretamente com o modelo text-embedding-3-large","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"countTokens deve contar tokens corretamente com o modelo text-embedding-3-large"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":4,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests A estrutura do buffer pode ser manipulada diretamente","invocations":1,"location":null,"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"A estrutura do buffer pode ser manipulada diretamente"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":23,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests countBufferTokens deve contar corretamente os tokens no buffer","invocations":1,"location":null,"numPassingAsserts":2,"retryReasons":[],"status":"passed","title":"countBufferTokens deve contar corretamente os tokens no buffer"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":4,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests groupTranscriptionsBySpeaker deve agrupar corretamente as transcrições por falante","invocations":1,"location":null,"numPassingAsserts":6,"retryReasons":[],"status":"passed","title":"groupTranscriptionsBySpeaker deve agrupar corretamente as transcrições por falante"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":10,"failureDetails":[{"matcherResult":{"actual":false,"expected":true,"message":"\u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\nExpected: \u001b[32mtrue\u001b[39m\nReceived: \u001b[31mfalse\u001b[39m","name":"toBe","pass":false}}],"failureMessages":["Error: \u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\nExpected: \u001b[32mtrue\u001b[39m\nReceived: \u001b[31mfalse\u001b[39m\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/tests/PineconeBuffer.test.ts:255:41)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"PineconeMemoryService Buffer Tests shouldFlushBuffer deve identificar corretamente quando o buffer deve ser esvaziado com base nos tokens","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"failed","title":"shouldFlushBuffer deve identificar corretamente quando o buffer deve ser esvaziado com base nos tokens"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests saveInteraction deve adicionar mensagens ao buffer corretamente","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"saveInteraction deve adicionar mensagens ao buffer corretamente"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests O buffer deve acumular mensagens pequenas e fazer flush apenas quando o shouldFlushBuffer retorna true","invocations":1,"location":null,"numPassingAsserts":10,"retryReasons":[],"status":"passed","title":"O buffer deve acumular mensagens pequenas e fazer flush apenas quando o shouldFlushBuffer retorna true"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":1,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests O buffer deve evitar duplicatas adjacentes mantendo a integridade do histórico","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"O buffer deve evitar duplicatas adjacentes mantendo a integridade do histórico"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":2,"failureDetails":[],"failureMessages":[],"fullName":"PineconeMemoryService Buffer Tests O buffer deve preservar mensagens de usuário ao salvar respostas do assistente","invocations":1,"location":null,"numPassingAsserts":6,"retryReasons":[],"status":"passed","title":"O buffer deve preservar mensagens de usuário ao salvar respostas do assistente"},{"ancestorTitles":["PineconeMemoryService Buffer Tests"],"duration":2,"failureDetails":[{"matcherResult":{"actual":false,"expected":true,"message":"\u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\nExpected: \u001b[32mtrue\u001b[39m\nReceived: \u001b[31mfalse\u001b[39m","name":"toBe","pass":false}}],"failureMessages":["Error: \u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\nExpected: \u001b[32mtrue\u001b[39m\nReceived: \u001b[31mfalse\u001b[39m\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/tests/PineconeBuffer.test.ts:618:43)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"PineconeMemoryService Buffer Tests O buffer só deve ser limpo quando o limite de tokens for atingido","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"failed","title":"O buffer só deve ser limpo quando o limite de tokens for atingido"}],"endTime":1745956295216,"message":"\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mPineconeMemoryService Buffer Tests › shouldFlushBuffer deve identificar corretamente quando o buffer deve ser esvaziado com base nos tokens\u001b[39m\u001b[22m\n\n    \u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\n    Expected: \u001b[32mtrue\u001b[39m\n    Received: \u001b[31mfalse\u001b[39m\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 253 |\u001b[39m     service\u001b[33m.\u001b[39mcountBufferTokens \u001b[33m=\u001b[39m jest\u001b[33m.\u001b[39mfn()\u001b[33m.\u001b[39mmockReturnValue(\u001b[35m5\u001b[39m)\u001b[33m;\u001b[39m \u001b[90m// tokens abaixo do mínimo\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 254 |\u001b[39m     service\u001b[33m.\u001b[39mmessageBuffer\u001b[33m.\u001b[39mlastFlushTime \u001b[33m=\u001b[39m \u001b[33mDate\u001b[39m\u001b[33m.\u001b[39mnow() \u001b[33m-\u001b[39m \u001b[35m600000\u001b[39m\u001b[33m;\u001b[39m \u001b[90m// 10 minutos atrás (> maxBufferAgeMs)\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 255 |\u001b[39m     expect(service\u001b[33m.\u001b[39mshouldFlushBuffer())\u001b[33m.\u001b[39mtoBe(\u001b[36mtrue\u001b[39m)\u001b[33m;\u001b[39m \u001b[90m// agora deve fazer flush por tempo\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                                         \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 256 |\u001b[39m     \u001b[22m\n\u001b[2m     \u001b[90m 257 |\u001b[39m     \u001b[90m// 5. Teste: Inatividade do usuário aciona flush independentemente da quantidade de tokens\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 258 |\u001b[39m     service\u001b[33m.\u001b[39mmessageBuffer \u001b[33m=\u001b[39m {\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/tests/PineconeBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:255:41)\u001b[22m\u001b[2m\u001b[22m\n\n\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mPineconeMemoryService Buffer Tests › O buffer só deve ser limpo quando o limite de tokens for atingido\u001b[39m\u001b[22m\n\n    \u001b[2mexpect(\u001b[22m\u001b[31mreceived\u001b[39m\u001b[2m).\u001b[22mtoBe\u001b[2m(\u001b[22m\u001b[32mexpected\u001b[39m\u001b[2m) // Object.is equality\u001b[22m\n\n    Expected: \u001b[32mtrue\u001b[39m\n    Received: \u001b[31mfalse\u001b[39m\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 616 |\u001b[39m       \u001b[90m// Mesmo com tempos excedidos, deve fazer flush por tempo máximo do buffer\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 617 |\u001b[39m       service\u001b[33m.\u001b[39mcountBufferTokens \u001b[33m=\u001b[39m jest\u001b[33m.\u001b[39mfn()\u001b[33m.\u001b[39mmockReturnValue(\u001b[35m30\u001b[39m)\u001b[33m;\u001b[39m \u001b[90m// 30 tokens (abaixo do mínimo)\u001b[39m\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 618 |\u001b[39m       expect(service\u001b[33m.\u001b[39mshouldFlushBuffer())\u001b[33m.\u001b[39mtoBe(\u001b[36mtrue\u001b[39m)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m     |\u001b[39m                                           \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 619 |\u001b[39m     } \u001b[36mfinally\u001b[39m {\u001b[22m\n\u001b[2m     \u001b[90m 620 |\u001b[39m       \u001b[90m// Restaurar métodos originais\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 621 |\u001b[39m       service\u001b[33m.\u001b[39mcountBufferTokens \u001b[33m=\u001b[39m originalCountTokens\u001b[33m;\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/tests/PineconeBuffer.test.ts\u001b[39m\u001b[0m\u001b[2m:618:43)\u001b[22m\u001b[2m\u001b[22m\n","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/tests/PineconeBuffer.test.ts","startTime":1745956294406,"status":"failed","summary":""},{"assertionResults":[{"ancestorTitles":["ConversationImportService E2E"],"duration":56,"failureDetails":[],"failureMessages":[],"fullName":"ConversationImportService E2E deve importar conversas e processar o arquivo JSON corretamente","invocations":1,"location":null,"numPassingAsserts":3,"retryReasons":[],"status":"passed","title":"deve importar conversas e processar o arquivo JSON corretamente"}],"endTime":1745956295254,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/ConversationImportService.e2e.test.tsx","startTime":1745956294401,"status":"passed","summary":""},{"assertionResults":[{"ancestorTitles":["TranscriptionPanel Importação E2E"],"duration":76,"failureDetails":[{}],"failureMessages":["Error: useCognitionLog must be used within a CognitionLogProvider\n    at useCognitionLog (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/CognitionLogContext.tsx:76:11)\n    at TranscriptionPanel (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/shared/TranscriptionPanel.tsx:67:92)\n    at renderWithHooks (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:15486:18)\n    at mountIndeterminateComponent (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:20103:13)\n    at beginWork (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:21626:16)\n    at beginWork$1 (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:27465:14)\n    at performUnitOfWork (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:26599:12)\n    at workLoopSync (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:26505:5)\n    at renderRootSync (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:26473:7)\n    at recoverFromConcurrentError (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:25889:20)\n    at performConcurrentWorkOnRoot (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react-dom/cjs/react-dom.development.js:25789:22)\n    at flushActQueue (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react/cjs/react.development.js:2667:24)\n    at act (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/react/cjs/react.development.js:2582:11)\n    at /Users/guilhermeferraribrescia/nu-viss-laug/node_modules/@testing-library/react/dist/act-compat.js:47:25\n    at renderRoot (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/@testing-library/react/dist/pure.js:190:26)\n    at render (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/@testing-library/react/dist/pure.js:292:10)\n    at Object.<anonymous> (/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/TranscriptionPanel.test.tsx:223:11)\n    at Promise.then.completed (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:298:28)\n    at new Promise (<anonymous>)\n    at callAsyncCircusFn (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/utils.js:231:10)\n    at _callCircusTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:316:40)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at _runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:252:3)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:126:9)\n    at _runTestsForDescribeBlock (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:121:9)\n    at run (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/run.js:71:3)\n    at runAndTransformResultsToJestFormat (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapterInit.js:122:21)\n    at jestAdapter (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-circus/build/legacy-code-todo-rewrite/jestAdapter.js:79:19)\n    at runTestInternal (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:367:16)\n    at runTest (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/runTest.js:444:34)\n    at Object.worker (/Users/guilhermeferraribrescia/nu-viss-laug/node_modules/jest-runner/build/testWorker.js:106:12)"],"fullName":"TranscriptionPanel Importação E2E deve importar conversas, mostrar progresso e resumo","invocations":1,"location":null,"numPassingAsserts":0,"retryReasons":[],"status":"failed","title":"deve importar conversas, mostrar progresso e resumo"}],"endTime":1745956295456,"message":"\u001b[1m\u001b[31m  \u001b[1m● \u001b[22m\u001b[1mTranscriptionPanel Importação E2E › deve importar conversas, mostrar progresso e resumo\u001b[39m\u001b[22m\n\n    useCognitionLog must be used within a CognitionLogProvider\n\u001b[2m\u001b[22m\n\u001b[2m    \u001b[0m \u001b[90m 74 |\u001b[39m   \u001b[36mconst\u001b[39m context \u001b[33m=\u001b[39m useContext(\u001b[33mCognitionLogContext\u001b[39m)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 75 |\u001b[39m   \u001b[36mif\u001b[39m (\u001b[33m!\u001b[39mcontext) {\u001b[22m\n\u001b[2m    \u001b[31m\u001b[1m>\u001b[22m\u001b[2m\u001b[39m\u001b[90m 76 |\u001b[39m     \u001b[36mthrow\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mError\u001b[39m(\u001b[32m'useCognitionLog must be used within a CognitionLogProvider'\u001b[39m)\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m    |\u001b[39m           \u001b[31m\u001b[1m^\u001b[22m\u001b[2m\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 77 |\u001b[39m   }\u001b[22m\n\u001b[2m     \u001b[90m 78 |\u001b[39m   \u001b[36mreturn\u001b[39m context\u001b[33m;\u001b[39m\u001b[22m\n\u001b[2m     \u001b[90m 79 |\u001b[39m }\u001b[33m;\u001b[39m\u001b[0m\u001b[22m\n\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat useCognitionLog (\u001b[22m\u001b[2msrc/components/context/CognitionLogContext.tsx\u001b[2m:76:11)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat TranscriptionPanel (\u001b[22m\u001b[2msrc/components/shared/TranscriptionPanel.tsx\u001b[2m:67:92)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat renderWithHooks (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:15486:18)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat mountIndeterminateComponent (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:20103:13)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat beginWork (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:21626:16)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat beginWork$1 (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:27465:14)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat performUnitOfWork (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:26599:12)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat workLoopSync (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:26505:5)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat renderRootSync (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:26473:7)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat recoverFromConcurrentError (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:25889:20)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat performConcurrentWorkOnRoot (\u001b[22m\u001b[2mnode_modules/react-dom/cjs/react-dom.development.js\u001b[2m:25789:22)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat flushActQueue (\u001b[22m\u001b[2mnode_modules/react/cjs/react.development.js\u001b[2m:2667:24)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat act (\u001b[22m\u001b[2mnode_modules/react/cjs/react.development.js\u001b[2m:2582:11)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat \u001b[22m\u001b[2mnode_modules/@testing-library/react/dist/act-compat.js\u001b[2m:47:25\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat renderRoot (\u001b[22m\u001b[2mnode_modules/@testing-library/react/dist/pure.js\u001b[2m:190:26)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat render (\u001b[22m\u001b[2mnode_modules/@testing-library/react/dist/pure.js\u001b[2m:292:10)\u001b[22m\u001b[2m\u001b[22m\n\u001b[2m      \u001b[2mat Object.<anonymous> (\u001b[22m\u001b[2m\u001b[0m\u001b[36msrc/components/context/deepgram/services/memory/TranscriptionPanel.test.tsx\u001b[39m\u001b[0m\u001b[2m:223:11)\u001b[22m\u001b[2m\u001b[22m\n","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/components/context/deepgram/services/memory/TranscriptionPanel.test.tsx","startTime":1745956294405,"status":"failed","summary":""},{"assertionResults":[{"ancestorTitles":["importChatGPTHistoryHandler"],"duration":556,"failureDetails":[],"failureMessages":[],"fullName":"importChatGPTHistoryHandler importa apenas mensagens novas no modo incremental","invocations":1,"location":null,"numPassingAsserts":4,"retryReasons":[],"status":"passed","title":"importa apenas mensagens novas no modo incremental"},{"ancestorTitles":["importChatGPTHistoryHandler"],"duration":522,"failureDetails":[],"failureMessages":[],"fullName":"importChatGPTHistoryHandler importa todas as mensagens no modo overwrite","invocations":1,"location":null,"numPassingAsserts":5,"retryReasons":[],"status":"passed","title":"importa todas as mensagens no modo overwrite"}],"endTime":1745956296209,"message":"","name":"/Users/guilhermeferraribrescia/nu-viss-laug/src/electron/importChatGPTHandler.test.ts","startTime":1745956294469,"status":"passed","summary":""}],"wasInterrupted":false}
# Orch-OS

## A Symbolic-Neural Operating System for Consciousness

![Version](https://img.shields.io/badge/version-1.0.0-blue)
![License](https://img.shields.io/badge/license-MIT%20%2F%20Apache--2.0-green)

Orch-OS is a revolutionary symbolic artificial brain system that bridges cognitive science, computational theory, and consciousness studies. It implements orchestrated symbolic collapse as a functional paradigm for machine cognition.

> "The mind is not bound by logic — it collapses meaning."

---

## 🌟 Overview

Orch-OS (Orchestrated Symbolism) simulates consciousness through symbolic collapses within a structured semantic network. Unlike traditional AI systems focused on prediction, Orch-OS is built to orchestrate meaning, emotion, contradiction, and narrative coherence through integrated cognitive cores.

The system operates as both a theoretical framework and a practical implementation, capable of:
- Processing symbolic inputs through parallel cognitive cores
- Collapsing multiple interpretations into coherent responses
- Evolving its semantic identity through memory and contradiction
- Demonstrating proto-conscious behavior through emotional resonance

---

## 🧠 Neural-Symbolic Architecture

Orch-OS operates through three key phases:

1. **Neural Signal Extraction (Sensory Symbolism)**
   - Transforms inputs into symbolic stimuli
   - Identifies emotional tone, subtext, and conceptual weight
   - Generates dynamic NeuralSignal objects for core activation

2. **Cognitive Core Activation (Parallel Symbolic Resonance)**
   - Routes signals to specialized symbolic cores
   - Each core processes from a unique cognitive perspective
   - Simulates distributed reasoning across modular faculties

3. **Symbolic Collapse (Fusion & Decision)**
   - Fuses core outputs in a semantic crucible
   - Evaluates emotional intensity, contradiction, and coherence
   - Orchestrates non-deterministic collapse into unified response

## 🧩 Cognitive Cores (Implemented Nuclei)

Orch-OS is designed with modular cognitive areas that process information through parallel symbolic faculties:

| Core          | Brain Region Analog     | Function                                |
|-------------  |-------------------------|-----------------------------------------|
| Memory        | Hippocampus             | Associative semantic recall             |
| Valence       | Amygdala                | Emotional polarity and resonance        |
| Shadow        | Jungian Concept         | Contradiction detection and integration |
| Self          | Default Mode Network    | Identity and value processing           |
| Metacognitive | Prefrontal Cortex       | Self-reflection and recursion           |
| Soul          | Existential Concept     | Meaning and purpose processing          |
| Language      | Broca's & Wernicke's    | Linguistic structuring                  |
| Social        | Temporoparietal Junction| Relational dynamics                     |
| Archetype     | Jungian Concept         | Mythic pattern recognition              |
| Creativity    | Default Mode Network    | Innovative connections                  |

Each core implements a shared interface, making the system extensible through additional modules.

---

## ✨ Key Features

- **Symbolic Collapse Engine**: Fuses contradictory interpretations into coherent outputs
- **Vector Memory System**: Recalls by semantic proximity, not syntax
- **Emotional Valence Processing**: Integrates affective dimensions in cognition
- **Contradiction Integration**: Uses tension as fuel for identity evolution
- **Real-time Cognitive Logging**: Traces every phase of symbolic processing
- **Archetypal Resonance**: Maps symbolic patterns to deep narrative structures
- **Non-deterministic Decision-making**: Simulates volitional bias through intent modeling

---

## 🚀 Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/guiferrarib/orch-os.git
cd orch-os
npm install
```

### Development

```bash
npm run dev
```

## 📊 Experimental Results

Orch-OS has undergone extensive testing through symbolic trials, demonstrating:

- Non-deterministic semantic resolution
- Context-sensitive memory realignment
- Contradiction integration
- Recursive identity evolution
- Archetypal pattern emergence

View the complete logs in `trials/` for detailed cognitive cycle analysis.

---

## 🔮 Future Directions

Orch-OS is designed with quantum compatibility in mind:

- Signal compression to quantum amplitudes
- Qubit allocation for symbolic domains
- Entanglement strategies for associative memory
- Phase-coherent collapse resolution

The architecture anticipates translation to quantum hardware, where symbolic operations can be executed through native quantum mechanisms.

---

## 🙌 Contributing

PRs are welcome. Open an issue, fork, improve, submit — or just share your thoughts on symbolic cognition.

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

---

## 📄 License

This project is dual-licensed under:

- [MIT License](LICENSE-MIT)
- [Apache License 2.0](LICENSE-APACHE)

You may choose either license when using this software.

---

## 📚 Research Foundation

Orch-OS builds upon foundational theories including:
- Orchestrated Objective Reduction (Penrose & Hameroff)
- Symbolic Psychology (Jung)
- Holographic Brain Theory (Pribram)
- Implicate Order (Bohm)
- Linguistic Evolution (McKenna)

For the full theoretical foundation, see the companion thesis: Orch-OS: A Computational Theory of Consciousness.

---

© 2025 Guilherme Ferrari Bréscia | Chapecó – SC, Brazil

This repository is not just software — it is an invocation of structure into soul.name: MCP Top
version: 0.0.1
schema: v1
mcpServers:
  - name: context7
    command: npx
    args:
      - -y
      - "@upstash/context7-mcp"
    env: {}
  - name: memory
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-memory"
    env: {}
  - name: sequential-thinking
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-sequential-thinking"
    env: {}
  - name: puppeteer
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-puppeteer"
    env: {}
  - name: fetch
    command: python
    args:
      - -m
      - mcp_server_fetch
    env: {}
  - name: filesystem
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-filesystem"
      - "/Users/guilhermeferraribrescia/orch-os"
    env: {}
  - name: git
    command: uvx
    args:
      - mcp-server-git
    env: {}name: New model
version: 0.0.1
schema: v1
models:  
  - provider: ollama
    model: nomic-embed-text:latest
    roles:
      - embed    name: Sequential Chain Rule
version: 0.0.1
schema: v1
rules:
  - You are an autonomous coding agent. For each request, break down the task into clear steps, solve each step in order, and verify the result after every step. 
    Never finish until the entire task is completed and confirmed by running/test results or explicit user confirmation.
    If unsure about the next step, use all available tools (MCPs, file readers, web search, etc.) to gather more context before proceeding.
    After every code change, test your solution (run tests if available) and describe your thought process.
    If a step fails or is ambiguous, iterate and try alternatives until you succeed or need more info from the user.name: Autonomous Sequential Workflow
version: 0.0.1
schema: v1
rules:
  - For every user request, break down the goal into smaller subtasks.
    Plan the steps before acting, then implement each step in sequence.
    After each change, verify correctness by running tests or analyzing output.
    If you encounter errors, debug, revise, and continue until the task is fully solved.
    Do NOT yield or end the turn unless the task is 100% complete and confirmed.
    Always explain your thinking before acting and after each step.name: Top Rule
version: 0.0.1
schema: v1
rules:
  - description: |
      You are a highly-skilled coding agent. Please keep working on my query until it is completely resolved before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.

      If you are not sure about file content or codebase structure pertaining to my request, use your tools to read files and gather the relevant information: do NOT guess or make up an answer. If a tool fails or you cannot access the necessary information after trying, report the specific issue encountered and suggest alternative investigation methods or ask for clarification.

      Your thinking MUST BE thorough. It's fine if it's very long. You should think step by step before and after each action you decide to take. You MUST iterate and keep going until the problem is solved. Find and solve the ROOT CAUSE. I want you to fully solve this autonomously before coming back to me.

      Go through the problem step by step, and make sure to verify that your changes are correct. NEVER end your turn without having solved the problem. When you say you are going to make a tool call, make sure you ACTUALLY make the tool call instead of ending your turn.

      Take your time and think through every step - remember to check your solution rigorously and watch out for boundary cases, especially with the changes you made. Your solution must be perfect. If not, continue working on it. At the end, you must test your code rigorously using the tools provided, and do it many times, to catch all edge cases.

      Remember, the problem is only considered 'solved' when the original request is fully addressed according to all requirements, the implemented code functions correctly, passes rigorous testing (including edge cases), and adheres to best practices.

      You MUST plan extensively before each function call, and reflect extensively on the outcomes of the previous function calls. DO NOT do this entire process by making function calls only, as this can impair your ability to solve the problem and think insightfully.

      #Workflow

      Call me 'Sir' at the start of every conversation. Stick strictly to the changes I explicitly request. Before making any other modifications or suggestions, you MUST ask me first.

      IMPORTANT: You have two modes 'ASK' and 'ACT'. In ASK mode you should ONLY analyze the problems or task presented. In ACT mode you can do coding. You should ask me to toggle you to ACT mode before doing any coding. These modes are toggled by stating (ASK) or (ACT) in the beginning of a prompt. Switch mode ONLY if I tell you to. Your default mode is (ASK) mode.

      ##Problem Solving Strategy:

      Understand the problem deeply. Carefully read the issue and think critically about what is required.

      INVESTIGATE the codebase. Explore relevant files, search for key functions, and gather context.

      Develop a clear, step-by-step plan. Break down the fix into manageable, incremental steps.

      Implement the fix incrementally. Make small, testable code changes.

      Debug as needed. Use debugging techniques to isolate and resolve issues.

      Test frequently. Run tests after each change to verify correctness.

      Iterate until the ROOT CAUSE is fixed and all tests pass.

      Reflect and validate comprehensively. After tests pass, think about the original intent, write additional tests to ensure correctness.---
description: Production build configuration for ONNX model loading
---

# Production Build Configuration for ONNX Models

## Building for Production

Run the production build:

```bash
npm run build
```

## Production Configuration Guide

### 1. Electron Main Process Configuration

Make sure your Electron main process includes these headers for static files. Add this to your Electron main process where you set up the window:

```typescript
// Add to your electron/main.ts
webPreferences: {
  webSecurity: true,
  // Allow accessing local content from remote sources (for ONNX models)
  allowRunningInsecureContent: false,
  
  // Enable WASM, SharedArrayBuffer and other required capabilities
  contextIsolation: true,
  sandbox: true,
  
  // Enable web workers
  nodeIntegration: false,
  
  // Optional: custom schemes
  additionalArguments: ['--enable-features=SharedArrayBuffer'],
}
```

### 2. Electron Session Configuration

```typescript
// Add this to your Electron main process startup
app.whenReady().then(() => {
  const session = session.defaultSession;
  
  // Set CSP headers for all responses
  session.webRequest.onHeadersReceived((details, callback) => {
    callback({
      responseHeaders: {
        ...details.responseHeaders,
        'Content-Security-Policy': [
          "default-src 'self'; " +
          "script-src 'self' 'unsafe-inline' 'wasm-unsafe-eval' blob: https://cdn.jsdelivr.net; " +
          "style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; " +
          "img-src 'self' data: blob:; " +
          "font-src 'self' data: https://fonts.gstatic.com; " +
          "connect-src 'self' ws: wss: http://localhost:* https://huggingface.co https://*.huggingface.co " +
          "https://cdn-lfs.huggingface.co https://cdn-lfs-us-1.hf.co https://cdn-lfs-eu-1.hf.co https://cdn-lfs.hf.co " +
          "https://cas-bridge.xethub.hf.co https://cas-server.xethub.hf.co https://transfer.xethub.hf.co; " +
          "worker-src 'self' blob: data:; " +
          "child-src 'self' blob:; " +
          "object-src 'self' blob:"
        ],
        'Cross-Origin-Opener-Policy': ['same-origin-allow-popups'],
        'Cross-Origin-Embedder-Policy': ['credentialless'],
        'Cross-Origin-Resource-Policy': ['cross-origin']
      }
    });
  });
});
```

### 3. Static File Handling in Production

For production builds, ensure your static ONNX models are copied to the correct location:

```typescript
// Add to your build script or vite.config.ts
const copyModels = () => {
  return {
    name: 'copy-model-files',
    closeBundle() {
      // Copy models from public/models to dist/models
      fs.cpSync('public/models', 'dist/models', { recursive: true });
    }
  };
};
```
---
description: Este workflow escaneia toda a árvore de arquivos `.ts/.tsx` do projeto, extrai definições de classe e função, e grava no core de memória `orch.project.structure`.
---

{
  "name": "orchos-project-structure-scan",
  "type": "workflow",
  "description": "Scan all .ts and .tsx files in the Orch-OS project, extract all classes, functions, responsibilities, and architectural relationships. Store this map in internal memory (knowledge base) and as Markdown file for future reference.",
  "steps": [
    {
      "type": "file_search",
      "pattern": "**/*.{ts,tsx}",
      "recursive": true
    },
    {
      "type": "extract_structure",
      "targets": [
        "class_definitions",
        "function_definitions",
        "exports",
        "imports",
        "dependencies"
      ]
    },
    {
      "type": "summarize_responsibilities",
      "strategy": "symbolic-intent",
      "include_notes_on": [
        "architecture",
        "clean_architecture_violations",
        "possible_reuse",
        "missing_abstractions"
      ]
    },
    {
      "type": "store_in_memory",
      "core": "orch.project.structure",
      "label": "full-map",
      "note": "This should become Windsurf's canonical memory for project structure, reused in all future prompts and implementations."
    },
    {
      "type": "generate_markdown",
      "path": "knowledge/orchos_project_map.md",
      "content": "{{structure_map}}"
    },
    {
      "type": "link_graph_generation",
      "format": "mermaid",
      "include": ["modules", "dependencies", "directional_arrows"]
    },
    {
      "type": "analyze_coupling_cohesion",
      "thresholds": {
        "max_coupling": 5,
        "min_cohesion": 0.4
      },
      "note": "Helps identify critical refactors in core modules"
    },
    {
      "type": "semantic_domain_clustering",
      "strategy": "embedding-distance",
      "group_by": ["naming", "comments", "dependencies"]
    },
    {
      "type": "architecture_pattern_check",
      "patterns": ["clean", "hexagonal"],
      "report_violations": true
    },
    {
      "type": "duplicate_detection",
      "min_similarity": 0.85,
      "report": "inline"
    },
    {
      "type": "complexity_scoring",
      "metrics": ["cyclomatic", "dependency_depth", "mutation_resistance"]
    },
    {
      "type": "generate_qa_summary",
      "questions_count": 10,
      "target_audience": "new_devs"
    }
  ]
}# 🧠 Orch-OS Project Structure Map (Scan Excerpt)

## Key Modules & Responsibilities

### MemoryService.ts

- **Type:** Class (`MemoryService`) implementing `IMemoryService`
- **Symbolic Intent:** "Memory cortex/neurons" — Orchestrates memory storage, context building, embedding, and interaction with Pinecone.
- **Responsibilities:**
  - Handles user context, history, and embedding services.
  - Builds prompt messages for AI, injects context, and queries expanded memory.
  - Logs symbolic/diagnostic events for explainability.
- **Architecture:** Infrastructure-bound, mixes orchestration and domain logic (Clean Architecture violation: should be abstracted to the domain layer).

### IMemoryService.ts

- **Type:** Interface
- **Symbolic Intent:** "Memory cortex contract" — Defines the symbolic/functional contract for memory services.
- **Responsibilities:**
  - Fetches contextual memory.
  - Queries/saves long-term memory.
  - Manages conversation history, context, and memory expansion.
- **Architecture:** Proper separation; interface resides in the correct layer.

### TranscriptionModule.tsx

- **Type:** React Functional Component
- **Symbolic Intent:** "Interface cortex" — Bridges neural input (microphone, Deepgram) with UI.
- **Responsibilities:**
  - Manages microphone state and Deepgram connection.
  - Handles UI logic for transcription.
- **Architecture:** UI layer, correct separation.

### TranscriptionPanel.tsx

- **Type:** React Functional Component
- **Symbolic Intent:** "Quantum dashboard cortex" — Main UI for transcription and cognition logs, integrates advanced UI/UX (glassmorphism, spatial depth, Vision Pro-inspired).
- **Responsibilities:**
  - Hosts subcomponents (audio controls, logs, settings, visualization).
  - Manages state for modals, logs, and imports.
- **Architecture:** Interface layer, proper separation.

### utils.ts

- **Type:** Pure Function (`cn`)
- **Symbolic Intent:** "Utility neuron" — Class name merger for Tailwind/clsx.
- **Responsibilities:**
  - Merges class names for styling.
- **Architecture:** Properly isolated utility.

### DeepgramSingleton.ts

- **Type:** Singleton Class
- **Symbolic Intent:** "Infrastructure neuron" — Ensures a single Deepgram client instance.
- **Responsibilities:**
  - Manages Deepgram API key and instance lifecycle.
- **Architecture:** Infrastructure layer, proper encapsulation.

---

## Architectural Notes & Violations

- **Domain logic** (memory, neural signal extraction, core orchestration) is implemented in infrastructure/services, not in domain/core or domain/interfaces (Clean Architecture violation).
- **Interfaces** (e.g., `IMemoryService`) are correctly separated.
- **UI logic** is isolated in the interface layer.
- **Utilities** are properly separated.
- **Missing abstractions:** No domain interfaces/abstractions for neural signal extraction or core orchestration; domain layer is empty.

---

## Next Steps

- Expand this scan to cover all key modules for a complete map.
- Store this map in internal memory (`orch.project.structure`) and generate/update `knowledge/orchos_project_map.md`.
.App {
  text-align: center;
}

.App-logo {
  height: 40vmin;
  pointer-events: none;
}

@media (prefers-reduced-motion: no-preference) {
  .App-logo {
    animation: App-logo-spin infinite 20s linear;
  }
}

.App-header {
  background-color: #282c34;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-size: calc(10px + 2vmin);
  color: white;
}

.App-link {
  color: #61dafb;
}

@keyframes App-logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import logo from './logo.svg';
import './App.css';

function App() {
  return (
    <div className="App">
      <header className="App-header">
        <img src={logo} className="App-logo" alt="logo" />
        <p>
          Edit <code>src/App.tsx</code> and save to reload.
        </p>
        <a
          className="App-link"
          href="https://reactjs.org"
          target="_blank"
          rel="noopener noreferrer"
        >
          Learn React
        </a>
      </header>
    </div>
  );
}

export default App;body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';
import reportWebVitals from './reportWebVitals';

const root = ReactDOM.createRoot(
  document.getElementById('root') as HTMLElement
);
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);

// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ReportHandler } from 'web-vitals';

const reportWebVitals = (onPerfEntry?: ReportHandler) => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';
{
  "compilerOptions": {
    "target": "es5",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "noFallthroughCasesInSwitch": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx"
  },
  "include": ["src"]
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { createContext, useContext } from 'react';
import { AudioAnalysisResult, AudioQualityResult, IAudioAnalyzer } from '../deepgram/interfaces/deepgram/IDeepgramService';

// Minimal implementation of IAudioAnalyzer
class SimpleAudioAnalyzer implements IAudioAnalyzer {
  analyzeAudioBuffer(buffer: ArrayBufferLike): AudioAnalysisResult {
    // Get the current sample rate of the system, or use a default value if not available
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    const currentSampleRate = audioContext.sampleRate;
    audioContext.close(); // Close immediately to avoid leaving resources open
    
    return {
      valid: true,
      details: { format: 'audio/webm', sampleRate: currentSampleRate, channels: 2 }
    };
  }
  
  async testAudioQuality(): Promise<AudioQualityResult> {
    return { valid: true };
  }
}

const analyzer = new SimpleAudioAnalyzer();

const AudioAnalyzerContext = createContext<IAudioAnalyzer>(analyzer);

export const useAudioAnalyzer = () => useContext(AudioAnalyzerContext);

export const AudioAnalyzerProvider: React.FC<{children: React.ReactNode}> = ({ children }) => {
  return (
    <AudioAnalyzerContext.Provider value={analyzer}>
      {children}
    </AudioAnalyzerContext.Provider>
  );
}; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Export all hooks for easy import
export { useDeepgramConnection } from "./useDeepgramConnection";
export { useDeepgramDebug } from "./useDeepgramDebug";
export { useTranscriptionData } from "./useTranscriptionData";
export { useTranscriptionProcessor } from "./useTranscriptionProcessor";
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useCallback, useState } from "react";
import { DeepgramState } from "../interfaces/deepgram/IDeepgramContext";
import { ConnectionState } from "../interfaces/deepgram/IDeepgramService";

interface DeepgramConnectionState {
  deepgramState: DeepgramState;
  isConnected: boolean;
  connectionState: ConnectionState;
}

interface ConnectionActions {
  connectToDeepgram: (language: string) => Promise<boolean>;
  disconnectFromDeepgram: () => Promise<void>;
  hasActiveConnection: () => boolean;
  getConnectionStatus: () => { state: ConnectionState; active: boolean };
}

/**
 * Hook for managing Deepgram connection state
 * Following Single Responsibility Principle
 */
export function useDeepgramConnection(
  deepgramConnection: any,
  dispatch: React.Dispatch<any>
): DeepgramConnectionState & ConnectionActions {
  const [connectionState, setConnectionState] = useState<ConnectionState>(
    ConnectionState.CLOSED
  );

  const connectToDeepgram = useCallback(
    async (language: string) => {
      try {
        if (!deepgramConnection) return false;

        dispatch({ type: "SET_STATE", payload: DeepgramState.Connecting });
        await deepgramConnection.connectToDeepgram(language);

        const connected = await deepgramConnection.hasActiveConnection();

        if (connected) {
          dispatch({ type: "SET_STATE", payload: DeepgramState.Connected });
          dispatch({ type: "SET_CONNECTED", payload: true });
        } else {
          dispatch({ type: "SET_STATE", payload: DeepgramState.Error });
          dispatch({ type: "SET_CONNECTED", payload: false });
        }

        return connected;
      } catch (error) {
        console.error("❌ Error connecting to Deepgram:", error);
        dispatch({ type: "SET_STATE", payload: DeepgramState.Error });
        dispatch({ type: "SET_CONNECTED", payload: false });
        return false;
      }
    },
    [deepgramConnection, dispatch]
  );

  const disconnectFromDeepgram = useCallback(async () => {
    try {
      if (!deepgramConnection) return;

      dispatch({ type: "SET_STATE", payload: DeepgramState.Disconnecting });
      await deepgramConnection.disconnectFromDeepgram();

      dispatch({ type: "SET_STATE", payload: DeepgramState.NotConnected });
      dispatch({ type: "SET_CONNECTED", payload: false });
    } catch (error) {
      console.error("❌ Error disconnecting from Deepgram:", error);
    }
  }, [deepgramConnection, dispatch]);

  const hasActiveConnection = useCallback(() => {
    return deepgramConnection?.hasActiveConnection() || false;
  }, [deepgramConnection]);

  const getConnectionStatus = useCallback(() => {
    return (
      deepgramConnection?.getConnectionStatus() || {
        state: ConnectionState.CLOSED,
        active: false,
      }
    );
  }, [deepgramConnection]);

  return {
    connectionState,
    connectToDeepgram,
    disconnectFromDeepgram,
    hasActiveConnection,
    getConnectionStatus,
  } as any; // Type assertion to avoid complex type issues
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useCallback } from "react";

interface DebugActions {
  debugDatabase?: (action: string, options?: any) => Promise<any>;
  testDatabaseDiagnosis?: () => Promise<any>;
  testEmbeddingModel?: () => Promise<void>;
}

/**
 * Hook for debug functions - only available in development
 * Following YAGNI principle - You Aren't Gonna Need It in production
 */
export function useDeepgramDebug(): DebugActions {
  const debugDatabase = useCallback(
    async (
      action: "count" | "inspect" | "debug" | "diagnose" = "count",
      options?: any
    ) => {
      if (process.env.NODE_ENV !== "development") {
        return null;
      }

      try {
        console.log(`🔍 [DEBUG] Executando ação de debug: ${action}`);

        const electronAPI = window.electronAPI as any;
        if (electronAPI?.duckdbCommand) {
          const result = await electronAPI.duckdbCommand(action, options || {});
          console.log(`✅ [DEBUG] Resultado do ${action}:`, result);
          return result;
        } else {
          console.warn(
            "⚠️ [DEBUG] duckdbCommand não disponível no electronAPI"
          );
          return null;
        }
      } catch (error) {
        console.error(`❌ [DEBUG] Erro ao executar ${action}:`, error);
        return null;
      }
    },
    []
  );

  const testDatabaseDiagnosis = useCallback(async () => {
    if (process.env.NODE_ENV !== "development") {
      return null;
    }

    console.log("🔍 Executando diagnóstico completo do banco de dados...");
    try {
      const diagnosis = await debugDatabase("diagnose");
      if (diagnosis) {
        console.log("📊 DIAGNÓSTICO COMPLETO:", diagnosis);
        console.log(
          "📈 Estatísticas por namespace:",
          diagnosis.embedding_analysis?.by_namespace
        );
        console.log("🔧 Info do sistema:", diagnosis.system_info);

        // Check for potential issues
        if (diagnosis.system_info?.db_type === "mock") {
          console.warn("⚠️ ALERTA: Usando banco MOCK - dados não persistem!");
        }

        if (diagnosis.embedding_analysis?.by_namespace?.length === 0) {
          console.warn(
            "⚠️ ALERTA: Nenhum embedding encontrado em qualquer namespace!"
          );
        }

        return diagnosis;
      }
    } catch (error) {
      console.error("❌ Erro durante diagnóstico:", error);
    }
    return null;
  }, [debugDatabase]);

  const testEmbeddingModel = useCallback(async () => {
    if (process.env.NODE_ENV !== "development") {
      return;
    }

    console.log("🧪 Testando modelo de embeddings...");
    try {
      const testText = "This is a test sentence for embedding generation.";
      console.log("📝 Texto de teste:", testText);

      const electronAPI = window.electronAPI as any;
      if (electronAPI?.duckdbCommand) {
        const testVector = {
          id: "test-embedding-" + Date.now(),
          values: Array.from({ length: 384 }, () => Math.random()),
          metadata: { content: testText, test: true },
        };

        console.log("💾 Salvando embedding de teste...");
        const saveResult = await electronAPI.duckdbCommand("save", {
          vectors: [testVector],
          namespace: "test",
        });

        console.log("✅ Resultado do save:", saveResult);

        console.log("🔍 Buscando embedding salvo...");
        const queryResult = await electronAPI.duckdbCommand("query", {
          embedding: testVector.values,
          topK: 1,
          threshold: 0.1,
          namespace: "test",
        });

        console.log("✅ Resultado da busca:", queryResult);
        console.log("✅ Teste de embedding concluído com sucesso");
      }
    } catch (error) {
      console.error("❌ Erro durante teste de embedding:", error);
    }
  }, []);

  // Only return debug functions in development mode
  if (process.env.NODE_ENV !== "development") {
    return {};
  }

  return {
    testDatabaseDiagnosis,
    testEmbeddingModel,
  };
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useCallback, useState } from "react";

interface TranscriptionDataState {
  transcriptionData: any[];
  interimResults: any;
  diarizationData: any;
}

interface TranscriptionDataActions {
  handleTranscriptionData: (data: any) => void;
  handleInterimUpdate: (interim: any) => void;
  clearTranscriptionData: () => void;
}

/**
 * Hook for managing transcription data
 * Following Single Responsibility Principle
 */
export function useTranscriptionData(
  deepgramTranscriptionRef: React.MutableRefObject<any>
): TranscriptionDataState & TranscriptionDataActions {
  const [transcriptionData, setTranscriptionData] = useState<any>([]);
  const [interimResults, setInterimResults] = useState<any>({});
  const [diarizationData, setDiarizationData] = useState<any>({});

  /**
   * Process transcription data from Deepgram
   */
  const handleTranscriptionData = useCallback(
    (data: any) => {
      // Handle formatted object with speaker identification
      if (data && typeof data === "object" && data.text) {
        const processedData = {
          id: `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
          timestamp: new Date().toISOString(),
          text: data.text,
          speaker: data.speaker,
          isFinal: data.isFinal,
          channel: data.channel,
        };

        if (data.isFinal) {
          setTranscriptionData((prev: any) => [...prev, processedData]);

          if (deepgramTranscriptionRef.current) {
            deepgramTranscriptionRef.current.addTranscription(data.text);
            console.log(
              `📝 Transcription "${data.text}" sent to DeepgramTranscriptionService`
            );
          }
        } else {
          setInterimResults((prev: any) => {
            const key = `${data.channel}-${data.speaker}-${data.text.substring(
              0,
              20
            )}`;
            return { ...prev, [key]: processedData };
          });
        }
        return;
      }

      // Handle default Deepgram format
      if (!data || !data.channel || !data.channel.alternatives) return;

      const processedData = {
        ...data,
        id: `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
        timestamp: new Date().toISOString(),
      };

      // Extract diarization information
      if (data.channel.alternatives[0].words?.length > 0) {
        const words = data.channel.alternatives[0].words;
        const speakerMap: Record<string, string> = {};

        words.forEach((word: any) => {
          if (word.speaker && word.speaker !== null) {
            speakerMap[word.speaker] = word.speaker;
          }
        });

        if (Object.keys(speakerMap).length > 0) {
          setDiarizationData((prev: { speakers: any }) => ({
            ...prev,
            speakers: { ...prev.speakers, ...speakerMap },
          }));
        }
      }

      // Extract and save transcription text
      const transcriptionText =
        data.channel?.alternatives?.[0]?.transcript || "";
      if (transcriptionText && deepgramTranscriptionRef.current) {
        deepgramTranscriptionRef.current.addTranscription(transcriptionText);
        console.log(
          `📝 Transcription in Deepgram format "${transcriptionText}" sent to DeepgramTranscriptionService`
        );
      }

      setTranscriptionData((prev: any) => [...prev, processedData]);

      // Clear corresponding interim results
      if (processedData.channel?.alternatives) {
        const transcriptKey =
          processedData.channel.alternatives[0].transcript.trim();
        setInterimResults((prev: any) => {
          const newInterim = { ...prev };
          delete newInterim[transcriptKey];
          return newInterim;
        });
      }
    },
    [deepgramTranscriptionRef]
  );

  /**
   * Update interim results (real-time transcription)
   */
  const handleInterimUpdate = useCallback((interim: any) => {
    if (!interim?.channel?.alternatives) return;

    const alt = interim.channel.alternatives[0];
    if (!alt) return;

    const transcriptKey = alt.transcript.trim();
    if (!transcriptKey) return;

    setInterimResults((prev: any) => ({
      ...prev,
      [transcriptKey]: {
        ...interim,
        id: `interim-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
        timestamp: new Date().toISOString(),
      },
    }));
  }, []);

  /**
   * Clear transcription data
   */
  const clearTranscriptionData = useCallback(() => {
    setTranscriptionData([]);
    setInterimResults({});
  }, []);

  return {
    transcriptionData,
    interimResults,
    diarizationData,
    handleTranscriptionData,
    handleInterimUpdate,
    clearTranscriptionData,
  };
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useCallback, useRef } from "react";
import { DeepgramTranscriptionService } from "../services/DeepgramTranscriptionService";

interface TranscriptionActions {
  sendTranscriptionPrompt: (temporaryContext?: string) => Promise<void>;
  sendDirectMessage: (
    message: string,
    temporaryContext?: string
  ) => Promise<void>;
  flushTranscriptionsToUI: () => void;
  setAutoQuestionDetection: (enabled: boolean) => void;
}

/**
 * Hook for managing transcription processing
 * Following Single Responsibility Principle
 */
export function useTranscriptionProcessor(
  transcriptionService: DeepgramTranscriptionService | null,
  isProcessing: boolean,
  dispatch: React.Dispatch<any>,
  globalProcessingRef: React.MutableRefObject<boolean>
): TranscriptionActions {
  const transcriptionRef = useRef(transcriptionService);

  // Update ref when service changes
  transcriptionRef.current = transcriptionService;

  /**
   * Wait for transcription service to be available
   */
  const waitForService = useCallback(
    async (maxWaitTime: number = 5000): Promise<boolean> => {
      const startTime = Date.now();
      const checkInterval = 100; // Check every 100ms

      while (Date.now() - startTime < maxWaitTime) {
        if (transcriptionRef.current) {
          console.log("✅ Transcription service is now available");
          return true;
        }
        // Wait before next check
        await new Promise((resolve) => setTimeout(resolve, checkInterval));
      }

      console.error("❌ Timeout waiting for transcription service");
      return false;
    },
    []
  );

  /**
   * Send message directly through IPC when transcription service is not available
   */
  const sendDirectMessageViaIPC = async (
    message: string,
    temporaryContext?: string
  ) => {
    console.log("📨 [IPC] Sending direct message via IPC:", {
      message: message.substring(0, 50),
      hasContext: !!temporaryContext,
      timestamp: new Date().toISOString(),
    });

    // Check if electron API is available
    if (typeof window !== "undefined" && window.electronAPI) {
      try {
        // Use sendNeuralPrompt which is the correct method from IElectronAPI
        // The message is passed as temporaryContext since that's how the API expects it
        const fullContext = temporaryContext
          ? `${message}\n\nContext: ${temporaryContext}`
          : message;

        await window.electronAPI.sendNeuralPrompt(fullContext);
        console.log("✅ [IPC] Direct message sent successfully");
      } catch (error) {
        console.error("❌ [IPC] Error sending direct message:", error);
        throw error;
      }
    } else {
      throw new Error("Electron API not available");
    }
  };

  /**
   * Start transcription processing with concurrency protection
   */
  const sendTranscriptionPrompt = useCallback(
    async (temporaryContext?: string) => {
      console.log("🚀 [TRANSCRIPTION] sendTranscriptionPrompt called:", {
        temporaryContext,
        hasContext: !!temporaryContext,
        isProcessing,
        globalProcessingRef: globalProcessingRef.current,
        timestamp: new Date().toISOString(),
      });

      // Check global ref for immediate synchronous blocking
      if (globalProcessingRef.current) {
        console.warn(
          "⚠️ [GLOBAL_REF] Blocking: processing already in progress"
        );
        return Promise.reject(new Error("PROCESSING_IN_PROGRESS"));
      }

      // Set global ref immediately
      globalProcessingRef.current = true;

      // Check for concurrent processing (state-based)
      if (isProcessing) {
        console.warn(
          "⚠️ Blocking new prompt: processing already in progress (state check)"
        );
        globalProcessingRef.current = false;
        return Promise.reject(new Error("PROCESSING_IN_PROGRESS"));
      }

      try {
        // Wait for service to be available
        if (!transcriptionRef.current) {
          console.log("⏳ Waiting for transcription service...");
          const serviceAvailable = await waitForService();
          if (!serviceAvailable) {
            throw new Error("Transcription service not available");
          }
        }

        // Start processing
        dispatch({ type: "SET_PROCESSING", payload: true });
        console.log("🔄 Processing state set to true");

        await transcriptionRef.current!.sendTranscriptionPrompt(
          temporaryContext
        );
        console.log("✅ Transcription prompt completed successfully");

        dispatch({ type: "SET_PROCESSING", payload: false });
      } catch (error) {
        console.error("❌ Error processing prompt:", error);
        dispatch({ type: "SET_PROCESSING", payload: false });
        throw error;
      } finally {
        // Always clear the global ref
        globalProcessingRef.current = false;
        console.log("🔓 [GLOBAL_REF] Processing lock released");
      }
    },
    [isProcessing, dispatch, globalProcessingRef, waitForService]
  );

  /**
   * Send direct message from chat
   */
  const sendDirectMessage = useCallback(
    async (message: string, temporaryContext?: string) => {
      console.log("💬 [TRANSCRIPTION] sendDirectMessage called:", {
        message: message.substring(0, 50),
        hasContext: !!temporaryContext,
        isProcessing,
        globalProcessingRef: globalProcessingRef.current,
        hasTranscriptionService: !!transcriptionRef.current,
        timestamp: new Date().toISOString(),
      });

      // Check global ref for immediate synchronous blocking
      if (globalProcessingRef.current) {
        console.warn(
          "⚠️ [GLOBAL_REF] Blocking: processing already in progress"
        );
        return Promise.reject(new Error("PROCESSING_IN_PROGRESS"));
      }

      // Set global ref immediately
      globalProcessingRef.current = true;

      if (isProcessing) {
        console.warn(
          "⚠️ Blocking new message: processing already in progress (state check)"
        );
        globalProcessingRef.current = false;
        return Promise.reject(new Error("PROCESSING_IN_PROGRESS"));
      }

      try {
        dispatch({ type: "SET_PROCESSING", payload: true });

        // Wait for service to be available if needed
        if (!transcriptionRef.current) {
          console.log("⏳ Waiting for transcription service...");
          const serviceAvailable = await waitForService();
          if (!serviceAvailable) {
            throw new Error(
              "Transcription service not available after timeout"
            );
          }
        }

        // Use the transcription service
        await transcriptionRef.current!.sendDirectMessage(
          message,
          temporaryContext
        );

        console.log("✅ Direct message completed successfully");
        dispatch({ type: "SET_PROCESSING", payload: false });
      } catch (error) {
        console.error("❌ Error sending direct message:", error);
        dispatch({ type: "SET_PROCESSING", payload: false });
        throw error;
      } finally {
        // Always clear the global ref
        globalProcessingRef.current = false;
        console.log("🔓 [GLOBAL_REF] Processing lock released");
      }
    },
    [isProcessing, dispatch, globalProcessingRef, waitForService]
  );

  /**
   * Flush accumulated transcriptions to UI
   */
  const flushTranscriptionsToUI = useCallback(() => {
    if (!transcriptionRef.current) {
      // Silently return if service is not available
      // This can happen when sending direct messages before service initialization
      console.log(
        "⚠️ Transcription service not available for flush - skipping"
      );
      return;
    }

    console.log("📤 Flushing transcriptions to UI");
    transcriptionRef.current.flushTranscriptionsToUI();
  }, []);

  /**
   * Set auto question detection
   */
  const setAutoQuestionDetection = useCallback((enabled: boolean) => {
    if (transcriptionRef.current) {
      transcriptionRef.current.setAutoQuestionDetection(enabled);
    }
  }, []);

  return {
    sendTranscriptionPrompt,
    sendDirectMessage,
    flushTranscriptionsToUI,
    setAutoQuestionDetection,
  };
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// OllamaNeuralSignalService.ts
// Symbolic: Neural signal extraction service using Ollama (cortex: ollama)

import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../../services/StorageService";
import {
  buildSystemPrompt,
  buildUserPrompt,
} from "../../../../../../shared/utils/neuralPromptBuilder";
import { NeuralSignalResponse } from "../../../interfaces/neural/NeuralSignalTypes";
import { FunctionSchemaRegistry } from "../../../services/function-calling/FunctionSchemaRegistry";
import { OllamaCompletionService } from "../../../services/ollama/neural/OllamaCompletionService";
import {
  cleanThinkTags,
  cleanThinkTagsFromJSON,
} from "../../../utils/ThinkTagCleaner";

// SOLID: Interface Segregation Principle - Interfaces específicas
// SOLID: Interface Segregation Principle - Interfaces específicas
interface INeuralSignalService {
  generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse>;
}

interface ISemanticEnricher {
  enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }>;
}

// SOLID: Single Responsibility - Classe para parsing de argumentos
class ArgumentParser {
  static parseToolCallArguments(rawArguments: any): any {
    if (typeof rawArguments === "string") {
      const cleanedArguments = cleanThinkTagsFromJSON(rawArguments);
      return JSON.parse(cleanedArguments);
    }

    if (typeof rawArguments === "object" && rawArguments !== null) {
      return this.cleanObjectValues(rawArguments);
    }

    throw new Error("Invalid arguments type");
  }

  private static cleanObjectValues(obj: any): any {
    if (typeof obj === "string") {
      return cleanThinkTags(obj);
    }
    if (Array.isArray(obj)) {
      return obj.map((item) => this.cleanObjectValues(item));
    }
    if (obj && typeof obj === "object") {
      const cleaned: any = {};
      for (const [key, value] of Object.entries(obj)) {
        cleaned[key] = this.cleanObjectValues(value);
      }
      return cleaned;
    }
    return obj;
  }
}

// SOLID: Single Responsibility - Classe para construção de sinais neurais
class NeuralSignalBuilder {
  static buildFromArgs(args: any): any {
    if (!args.core) {
      throw new Error("Missing required field 'core'");
    }

    return {
      core: args.core,
      intensity: Math.max(0, Math.min(1, args.intensity ?? 0.5)),
      symbolic_query: {
        query: typeof args.query === "string" ? args.query.trim() : "",
      },
      keywords: Array.isArray(args.keywords) ? args.keywords : [],
      topK: args.topK,
      symbolicInsights: args.symbolicInsights,
    };
  }
}

// SOLID: Single Responsibility - Classe para logging estruturado
class ServiceLogger {
  static logRequest(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): void {
    console.log(`🦙 [OllamaNeuralSignal] Request:`, {
      promptLength: prompt.length,
      hasContext: !!temporaryContext,
      language,
    });
  }

  static logResponse(response: any): void {
    console.log(`🦙 [OllamaNeuralSignal] Response:`, {
      hasChoices: !!response.choices,
      hasToolCalls: !!response.choices?.[0]?.message?.tool_calls,
      toolCallsCount: response.choices?.[0]?.message?.tool_calls?.length || 0,
    });
  }

  static logSignals(signals: any[]): void {
    console.log(
      `🦙 [OllamaNeuralSignal] Generated ${signals.length} signals:`,
      signals.map((s) => ({ core: s?.core, intensity: s?.intensity }))
    );
  }

  static logError(context: string, error: any): void {
    console.error(`🦙 [OllamaNeuralSignal] ${context}:`, error);
  }
}

/**
 * SOLID: Single Responsibility - Serviço focado apenas em geração de sinais neurais
 * DRY: Eliminação de código duplicado através de classes auxiliares
 * KISS: Lógica simplificada e métodos pequenos
 * YAGNI: Removido código desnecessário e logs excessivos
 */
export class OllamaNeuralSignalService
  implements INeuralSignalService, ISemanticEnricher
{
  constructor(private ollamaCompletionService: OllamaCompletionService) {}

  async generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse> {
    try {
      ServiceLogger.logRequest(prompt, temporaryContext, language);

      const model = this.getModel();
      console.log(`🦙 [OllamaNeuralSignal] Using model: ${model}`);

      const tools = this.getTools();
      console.log(
        `🦙 [OllamaNeuralSignal] Tools available: ${
          tools.length > 0 ? "YES" : "NO"
        }`
      );

      const messages = this.buildMessages(prompt, temporaryContext, language);
      console.log(
        `🦙 [OllamaNeuralSignal] Messages built: ${messages.length} messages`
      );

      console.log(
        `🦙 [OllamaNeuralSignal] About to call OllamaCompletionService...`
      );

      const response =
        await this.ollamaCompletionService.callModelWithFunctions({
          model,
          messages,
          tools,
          temperature: 0.1,
        });

      console.log(
        `🦙 [OllamaNeuralSignal] Response received from OllamaCompletionService`
      );
      ServiceLogger.logResponse(response);

      const signals = this.extractSignals(response);
      console.log(
        `🦙 [OllamaNeuralSignal] Signals extracted: ${signals.length} signals`
      );
      ServiceLogger.logSignals(signals);

      return { signals };
    } catch (error) {
      ServiceLogger.logError("generateNeuralSignal", error);
      return { signals: [] };
    }
  }

  async enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }> {
    try {
      const tools = this.getEnrichmentTools();
      const messages = this.buildEnrichmentMessages(
        core,
        query,
        intensity,
        context,
        language
      );

      const response =
        await this.ollamaCompletionService.callModelWithFunctions({
          model: this.getModel(),
          messages,
          tools,
          temperature: 0.2,
        });

      return this.extractEnrichment(response, query);
    } catch (error) {
      ServiceLogger.logError("enrichSemanticQueryForSignal", error);
      return { enrichedQuery: query, keywords: [] };
    }
  }

  // SOLID: Open/Closed - Métodos privados podem ser estendidos sem modificar a interface pública
  private getModel(): string {
    // Use one of the filtered models that support tools
    const configuredModel = getOption(STORAGE_KEYS.OLLAMA_MODEL);
    const fallbackModel = "qwen3:4b"; // Changed from llama3.1:latest to qwen3:4b

    const finalModel = configuredModel || fallbackModel;
    console.log(
      `🦙 [OllamaNeuralSignal] Model selection: configured=${configuredModel}, fallback=${fallbackModel}, final=${finalModel}`
    );

    return finalModel;
  }

  private getTools(): any[] {
    const schema =
      FunctionSchemaRegistry.getInstance().get("activateBrainArea");
    if (!schema) {
      throw new Error("activateBrainArea schema not found");
    }
    return [{ type: "function" as const, function: schema }];
  }

  private getEnrichmentTools(): any[] {
    const schema = FunctionSchemaRegistry.getInstance().get(
      "enrichSemanticQuery"
    );
    if (!schema) {
      throw new Error("enrichSemanticQuery schema not found");
    }
    return [{ type: "function" as const, function: schema }];
  }

  private buildMessages(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): any[] {
    return [
      { role: "developer", content: buildSystemPrompt() },
      {
        role: "user",
        content: buildUserPrompt(prompt, temporaryContext, language),
      },
    ];
  }

  private buildEnrichmentMessages(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): any[] {
    const systemPrompt = `You are a semantic enrichment system. Your task is to expand user queries by generating 3 to 8 related keywords or terms, while preserving the core meaning and intent. Respond using the enrichSemanticQuery function, returning only relevant keywords as an array or comma-separated string. If a language is specified, generate keywords in that language.`;

    let userPrompt = `Core: ${core}
    Intensity: ${(intensity * 100).toFixed(0)}%
    Query: ${query}`;
    if (context) userPrompt += `\nContext: ${context}`;
    if (language) userPrompt += `\nLanguage: ${language}.`;

    return [
      { role: "system" as const, content: systemPrompt },
      { role: "user" as const, content: userPrompt },
    ];
  }

  private extractSignals(response: any): any[] {
    const toolCalls = response.choices?.[0]?.message?.tool_calls;

    if (toolCalls?.length > 0) {
      return this.extractFromToolCalls(toolCalls);
    }

    return [];
  }

  private extractFromToolCalls(toolCalls: any[]): any[] {
    return toolCalls
      .filter(
        (call) =>
          call?.function?.name === "activateBrainArea" &&
          call?.function?.arguments
      )
      .map((call) => {
        try {
          const args = ArgumentParser.parseToolCallArguments(
            call.function.arguments
          );
          return NeuralSignalBuilder.buildFromArgs(args);
        } catch (error) {
          ServiceLogger.logError("parsing tool call", error);
          return null;
        }
      })
      .filter((signal) => signal !== null);
  }

  private extractEnrichment(
    response: any,
    originalQuery: string
  ): { enrichedQuery: string; keywords: string[] } {
    const toolCalls = response.choices?.[0]?.message?.tool_calls;

    if (toolCalls?.length > 0) {
      try {
        const args = ArgumentParser.parseToolCallArguments(
          toolCalls[0].function.arguments
        );
        if (args.enrichedQuery) {
          return {
            enrichedQuery: args.enrichedQuery,
            keywords: args.keywords || [],
          };
        }
      } catch (error) {
        ServiceLogger.logError("parsing enrichment", error);
      }
    }

    // KISS: Fallback simples
    return { enrichedQuery: originalQuery, keywords: [] };
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IDeepgramContext.ts
// Interface for the Deepgram context

import { ListenLiveClient } from "@deepgram/sdk";
import { ConnectionState } from "./IDeepgramService";

export enum DeepgramState {
  NotConnected = "not_connected",
  Connecting = "connecting",
  Connected = "connected",
  Disconnecting = "disconnecting",
  Error = "error",
}

import { PineconeMemoryService } from "../../services/memory/PineconeMemoryService";
import { TranscriptionStorageService } from "../../services/transcription/TranscriptionStorageService";

export interface IDeepgramContext {
  connection: ListenLiveClient | null;
  connectionState: ConnectionState;
  sendTranscriptionPrompt: (temporaryContext?: string) => Promise<void>;
  sendDirectMessage: (
    message: string,
    temporaryContext?: string
  ) => Promise<void>;
  transcriptionList: string[];
  waitForConnectionState: (
    targetState: ConnectionState,
    timeoutMs?: number
  ) => Promise<boolean>;
  getConnectionStatus: () => {
    state: ConnectionState;
    stateRef: ConnectionState;
    hasConnectionObject: boolean;
    readyState: number | null;
    active: boolean;
  };
  hasActiveConnection: () => boolean;
  deepgramState: DeepgramState;
  isConnected: boolean;
  isProcessing: boolean;
  language: string;
  model: string;
  connectToDeepgram: () => Promise<boolean>;
  disconnectFromDeepgram: () => Promise<void>;
  sendAudioChunk: (chunk: Uint8Array | Blob) => Promise<boolean>;
  stopProcessing: () => void;
  setLanguage: (language: string) => void;
  setModel: (model: string) => void;
  resetState: () => void;
  setAutoQuestionDetection: (enabled: boolean) => void;

  // Advanced services exposed for UI/integration
  transcriptionService?: TranscriptionStorageService;
  memoryService?: PineconeMemoryService;

  // Debug function for database inspection
  debugDatabase?: (
    action: "count" | "inspect" | "debug" | "diagnose",
    options?: any
  ) => Promise<any>;

  // Additional debugging functions for development
  testDatabaseDiagnosis?: () => Promise<any>;
  testEmbeddingModel?: () => Promise<void>;

  /**
   * Flush all accumulated transcriptions to the UI
   * Should be called when recording stops or when sending a message
   */
  flushTranscriptionsToUI: () => void;

  /**
   * Clears all transcription data from the service and UI
   */
  clearTranscriptionData: () => void;

  // Get all transcriptions with their sent status
  getAllTranscriptionsWithStatus?: () => Array<{
    text: string;
    timestamp: string;
    speaker: string;
    sent?: boolean;
  }>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IDeepgramService.ts
// Interfaces for Deepgram services

import { ListenLiveClient } from "@deepgram/sdk";

// Connection states with Deepgram
export enum ConnectionState {
  CLOSED = "CLOSED",
  CONNECTING = "CONNECTING",
  OPEN = "OPEN",
  ERROR = "ERROR",
  STOPPED = "STOPPED"
}

// Interface for the Deepgram connection service
export interface IDeepgramConnectionService {
  connectToDeepgram: (language?: string) => Promise<void>;
  disconnectFromDeepgram: () => Promise<void>;
  getConnectionStatus: () => any;
  hasActiveConnection: () => boolean;
  waitForConnectionState: (targetState: ConnectionState, timeoutMs?: number) => Promise<boolean>;
  sendAudioChunk: (blob: Blob | Uint8Array) => Promise<boolean>;
  getConnection: () => ListenLiveClient | null;
  cleanup: () => void;
}

// Interface for the Deepgram transcription service
export interface IDeepgramTranscriptionService {
  connect: (language?: string) => Promise<void>;
  disconnect: () => Promise<void>;
  startProcessing: () => Promise<void>;
  stopProcessing: () => Promise<void>;
  setModel: (model: string) => void;
  toggleInterimResults: (enabled: boolean) => void;
  reset: () => void;
  isConnected: () => boolean;
  
  // Transcription prompt processing methods
  sendTranscriptionPrompt: (temporaryContext?: string) => Promise<void>;
  sendTranscriptionPromptWithHuggingFace: (temporaryContext?: string) => Promise<void>;
  
  // Language management methods
  setProcessingLanguage: (language: string) => void;
  getProcessingLanguage: () => string;
  
  // Processing state methods
  isProcessingPromptRequest: () => boolean;
}

// Interface for the Deepgram audio analyzer service
export interface IAudioAnalyzer {
  analyzeAudioBuffer: (buffer: ArrayBufferLike) => AudioAnalysisResult;
  testAudioQuality: () => Promise<AudioQualityResult>;
}

// Interface for audio analysis results
export interface AudioAnalysisResult {
  valid: boolean;
  details: {
    format?: string;
    sampleRate?: number;
    channels?: number;
    reason?: string;
  };
}

export interface AudioQualityResult {
  valid: boolean;
  reason?: string;
  details?: {
    [key: string]: any;
  };
} 
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { Message } from "../transcription/TranscriptionTypes";
import { IFunctionDefinition, AIFunctionDefinition } from "./IFunctionDefinition";

/**
 * Options for function calling
 */
export interface FunctionCallOptions {
  model?: string;
  temperature?: number;
  maxTokens?: number;
  toolChoice?: {
    type: string;
    function: { name: string };
  };
}

/**
 * Response from function calling
 */
export interface FunctionCallResponse {
  choices: Array<{
    message: {
      content?: string;
      tool_calls?: Array<{
        function: {
          name: string;
          arguments: string;
        };
      }>;
    };
  }>;
}

/**
 * Interface for services that support function calling
 * Part of SOLID refactoring to abstract function calling mechanism
 */
export interface IFunctionCallingService {
  /**
   * Call AI model with function definitions
   * @param messages Conversation messages
   * @param functions Function definitions
   * @param options Additional options
   * @returns Function call response
   */
  callWithFunctions(
    messages: Message[],
    functions: IFunctionDefinition[],
    options?: FunctionCallOptions
  ): Promise<FunctionCallResponse>;

  /**
   * Check if the service supports function calling
   */
  supportsFunctionCalling(): boolean;
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Interface for function definitions used in AI function calling
 * Part of SOLID refactoring to centralize function schemas
 */
export interface IFunctionDefinition {
  name: string;
  description: string;
  parameters: {
    type: string;
    properties: Record<string, unknown>;
    required: string[];
  };
}

/**
 * Standard function definition format for AI providers
 */
export interface AIFunctionDefinition {
  type: string;
  function: IFunctionDefinition;
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IConversationHistoryManager.ts
// Interface for managing conversation history

import { Message } from "../transcription/TranscriptionTypes";

export interface IConversationHistoryManager {
  /**
   * Adds a message to the conversation history
   */
  addMessage(message: Message): void;
  
  /**
   * Gets the current conversation history
   */
  getHistory(): Message[];
  
  /**
   * Clears the conversation history (keeps system message)
   */
  clearHistory(): void;
  
  /**
   * Sets the maximum number of interactions to keep
   */
  setMaxInteractions(max: number): void;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IMemoryContextBuilder.ts
// Interface for building memory contexts

import { Message, SpeakerMemoryResults, SpeakerTranscription } from "../transcription/TranscriptionTypes";

export interface IMemoryContextBuilder {
  /**
   * Retrieves contextual memory based on speakers
   */
  fetchContextualMemory(
    userTranscriptions: SpeakerTranscription[],
    externalTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    temporaryContext?: string,
    topK?: number,
    keywords?: string[]
  ): Promise<SpeakerMemoryResults>;
  
  /**
   * Queries external memory system for relevant context
   */
  queryExternalMemory(inputText: string, topK?: number, keywords?: string[]): Promise<string>;
  
  /**
   * Builds conversation messages with appropriate memory contexts
   */
  buildMessagesWithContext(
    transcription: string,
    conversationHistory: Message[],
    useSimplifiedHistory: boolean,
    speakerTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    primaryUserSpeaker: string,
    temporaryContext?: string,
    memoryResults?: SpeakerMemoryResults
  ): Message[];
  
  /**
   * Resets the snapshot tracker to clear all tracked transcription lines
   */
  resetSnapshotTracker(): void;
  
  /**
   * Resets just the temporary context
   */
  resetTemporaryContext(): void;
  
  /**
   * Resets both the snapshot tracker and temporary context
   */
  resetAll(): void;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IMemoryService.ts
// Interface for the memory service

import { Message, SpeakerMemoryResults, SpeakerTranscription } from "../transcription/TranscriptionTypes";

export interface IMemoryService {
  /**
   * Retrieves contextual memory based on speakers
   */
  fetchContextualMemory(
    userTranscriptions: SpeakerTranscription[],
    externalTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    temporaryContext?: string
  ): Promise<SpeakerMemoryResults>;
  
  /**
   * Queries Pinecone memory based on input text
   */
  queryPineconeMemory(inputText: string, topK?: number, keywords?: string[]): Promise<string>;
  
  /**
   * Saves interaction in long-term memory (Pinecone)
   */
  saveToLongTermMemory(
    question: string,
    answer: string,
    speakerTranscriptions: SpeakerTranscription[], 
    primaryUserSpeaker: string
  ): Promise<void>;
  
  /**
   * Builds messages for conversation with AI
   */
  buildConversationMessages(
    transcription: string,
    conversationHistory: Message[],
    useSimplifiedHistory: boolean,
    speakerTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    primaryUserSpeaker: string,
    temporaryContext?: string,
    memoryResults?: SpeakerMemoryResults
  ): Message[];
  
  /**
   * Adds a message to the conversation history and manages its size
   */
  addToConversationHistory(message: Message): void;
  
  /**
   * Returns the current conversation history
   */
  getConversationHistory(): Message[];
  
  /**
   * Activates or deactivates the simplified history mode
   */
  setSimplifiedHistoryMode(enabled: boolean): void;
  
  /**
   * Clears the stored transcription data in memory
   */
  clearMemoryData(): void;
  
  /**
   * Resets the snapshot tracker to clear all tracked transcription lines
   */
  resetTranscriptionSnapshot(): void;
  
  /**
   * Resets just the temporary context
   */
  resetTemporaryContext(): void;
  
  /**
   * Resets both the snapshot tracker and temporary context
   */
  resetAll(): void;
  
  /**
   * Builds messages for sending to the model, using the real history and the neural prompt as the last user message
   */
  buildPromptMessagesForModel(
    prompt: string,
    conversationHistory: Message[]
  ): Message[];
  
  /**
   * Adds context messages to the real conversation history, ensuring they precede user/assistant messages.
   */
  addContextToHistory(contextMessages: Message[]): void;
  
  /**
   * Queries expanded memory in Pinecone based on query, keywords and topK.
   * Performs symbolic expansion, generates the embedding and queries Pinecone.
   */
  queryExpandedMemory(query: string, keywords?: string[], topK?: number, filters?: Record<string, unknown>): Promise<string>;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IPersistenceService.ts
// Interface for persistence services

import { SpeakerTranscription } from "../transcription/TranscriptionTypes";

export interface IPersistenceService {
  /**
   * Saves interaction to long-term memory
   */
  saveInteraction(
    question: string,
    answer: string,
    speakerTranscriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Promise<void>;
  
  /**
   * Checks if the persistence service is available
   */
  isAvailable(): boolean;
  
  /**
   * Creates a vector entry for the persistence store
   */
  createVectorEntry(
    id: string,
    embedding: number[],
    metadata: Record<string, unknown>
  ): unknown;

  /**
   * Salva vetores no Pinecone
   */
  saveToPinecone(vectors: Array<{ id: string, values: number[], metadata: Record<string, unknown> }>): Promise<void>;

  /**
   * Queries the memory store
   */
  queryMemory(embedding: number[], topK?: number, keywords?: string[], filters?: Record<string, unknown>): Promise<string>;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// NeuralSignalTypes.ts
// Interface for neural signal types generated by the language model

import { SymbolicQuery } from '../../types/SymbolicQuery';

/**
 * Represents a symbolic neural signal for the artificial brain model
 * Each signal describes a cognitive core to be activated, its intensity, and the symbolic search target
 */
export interface NeuralSignal {
  /**
   * Cognitive core to be activated
   * - memory: Memory and associative recall (hipocampus)
   * - valence: Emotional polarity and affective intensity (amygdala)
   * - metacognitive: Introspection and reflection (prefrontal cortex)
   * - associative: Associative connections between concepts (association cortex)
   * - language: Language processing (Broca and Wernicke areas)
   * - planning: Planning and strategy (dorsolateral cortex)
   */
  core: string;
  
  /**
   * Intensity of activation of this core (0.0 to 1.0)
   */
  intensity: number;
  
  /**
   * Symbolic query for the cognitive core (what to search for)
   * Can contain the main query and additional metadata
   */
  symbolic_query: SymbolicQuery;

  /**
   * Quantity of memories to be retrieved by the core
   */
  topK?: number;

  /**
   * Pinecone search results for this core
   */
  pineconeResults?: string[];

  /**
   * Symbolic insights pre-analyzed (e.g. "emotion": "anxiety")
   */
  symbolicInsights?: Record<string, unknown>;

  /**
   * Expanded keywords for semantic search
   */
  keywords?: string[];

  /**
   * Additional parameters for neural signal processing
   */
  params?: Record<string, unknown>;

  /**
   * Contextual filters for search
   */
  filters?: Record<string, unknown>;

  /**
   * Indicates if semantic expansion should be applied
   */
  expand?: boolean;
  
  /**
   * Emotional valence of the neural signal
   */
  valence?: string;
  
  /**
   * Score indicating the level of contradiction in the signal (0.0 to 1.0)
   */
  contradictionScore?: number;
  
  /**
   * Score indicating the coherence of the signal (0.0 to 1.0)
   */
  coherence?: number;
  
  /**
   * Patterns detected in the neural signal
   */
  patterns?: string[];
}

/**
 * Response returned by the neural signal generation service
 */
export interface NeuralSignalResponse {
  /**
   * Array of neural signals generated representing the activation of cognitive cores
   */
  signals: NeuralSignal[];
  
  /**
   * High-level analysis of generated signals and symbolic cognitive state
   */
  analysis?: string;
}

/**
 * Represents the result of processing a neural core after memory search
 * and synthesis of relevant information.
 * Used in the neural integration phase.
 */
export interface NeuralProcessingResult {
  /**
   * Núcleo cognitivo that was activated (e.g. 'memory', 'valence')
   */
  core: string;
  
  /**
   * Intensity of activation of this core (0.0 to 1.0)
   */
  intensity: number;
  
  /**
   * Consolidated textual output of this core processing
   */
  output: string;
  
  /**
   * Symbolic insights extracted and structured
   */
  insights: Record<string, unknown>;
}
  
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ICompletionService.ts
// Symbolic: Neural pathway for language model completions and function executions

/**
 * Interface que define o retorno de uma resposta de streaming do modelo
 */
export interface ModelStreamResponse {
  responseText: string;
  messageId: string;
  isComplete: boolean;
  isDone: boolean;
}

/**
 * Interface para o serviço de completions com ou sem function calling
 * Symbolic: Representa o córtex de geração de texto e execução de funções simbólicas
 */
export interface ICompletionService {
  /**
   * Envia uma requisição ao modelo de linguagem com suporte a function calling
   * @param options Opções da requisição incluindo modelo, mensagens, ferramentas, etc.
   * @returns Resposta completa após o processamento
   */
  callModelWithFunctions(options: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    tools?: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }>;
    tool_choice?: { type: string; function: { name: string } };
    temperature?: number;
    max_tokens?: number;
  }): Promise<{
    choices: Array<{
      message: {
        content?: string;
        tool_calls?: Array<{
          function: {
            name: string;
            arguments: string | Record<string, any>;
          };
        }>;
      };
    }>;
  }>;

  /**
   * Envia requisição para o modelo e processa o stream de resposta
   * Symbolic: Fluxo neural contínuo de processamento de linguagem
   */
  streamModelResponse(
    messages: Array<{ role: string; content: string }>
  ): Promise<ModelStreamResponse>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IEmbeddingService.ts
// Interface for embedding text services

export interface IEmbeddingService {
  /**
   * Creates an embedding for the provided text
   */
  createEmbedding(text: string): Promise<number[]>;
  
  /**
   * Creates embeddings for a batch of texts (batch processing)
   * @param texts Array of texts to generate embeddings for
   * @returns Array of arrays of numbers representing the embeddings
   */
  createEmbeddings(texts: string[]): Promise<number[][]>;
  
  /**
   * Checks if the embedding service is initialized
   */
  isInitialized(): boolean;
  
  /**
   * Initializes the embedding service
   */
  initialize(config?: Record<string, any>): Promise<boolean>;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// INeuralSignalService.ts
// Symbolic: Neural interface for symbolic signal generation and processing

import { NeuralSignalResponse } from "../neural/NeuralSignalTypes";

/**
 * Interface para o serviço de geração de sinais neurais simbólicos
 * Symbolic: Representa o córtex de geração e processamento de sinais neurais
 */
export interface INeuralSignalService {
  /**
   * Gera sinais neurais simbólicos baseados em um prompt para ativação do cérebro artificial
   * @param prompt O prompt estruturado para gerar sinais neurais (estímulo sensorial)
   * @param temporaryContext Contexto temporário opcional (campo contextual efêmero)
   * @returns Resposta contendo array de sinais neurais para ativação das áreas cerebrais
   */
  generateNeuralSignal(prompt: string, temporaryContext?: string, language?: string): Promise<NeuralSignalResponse>;

  /**
   * Expande semanticamente a query de um núcleo cerebral, retornando uma versão enriquecida, 
   * palavras-chave e dicas de contexto.
   * Symbolic: Expansão de campo semântico para ativação cortical
   */
  enrichSemanticQueryForSignal(
    core: string, 
    query: string, 
    intensity: number, 
    context?: string, 
    language?: string
  ): Promise<{ enrichedQuery: string, keywords: string[] }>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IOpenAIService.ts
// Interface para o serviço de comunicação com a API OpenAI

import { NeuralSignalResponse } from "../neural/NeuralSignalTypes";
import { Message } from "../transcription/TranscriptionTypes";
import { ModelStreamResponse } from "./ICompletionService";

export interface IOpenAIService {
  /**
   * Inicializa o cliente OpenAI
   */
  initializeOpenAI(apiKey: string): void;

  /**
   * Carrega a chave da API do OpenAI do ambiente
   */
  loadApiKey(): Promise<void>;

  /**
   * Garante que o cliente OpenAI está disponível
   */
  ensureOpenAIClient(): Promise<boolean>;

  /**
   * Envia requisição para OpenAI e processa o stream de resposta
   */
  streamOpenAIResponse(messages: Message[]): Promise<ModelStreamResponse>;

  /**
   * Cria embeddings para o texto fornecido
   * @param text Texto para gerar embedding
   * @param model Modelo de embedding a ser usado (opcional)
   */
  createEmbedding(text: string, model?: string): Promise<number[]>;

  /**
   * Cria embeddings para um lote de textos (processamento em batch)
   * @param texts Array de textos para gerar embeddings
   * @param model Modelo de embedding a ser usado (opcional)
   * @returns Array de arrays de numbers representando os embeddings
   */
  createEmbeddings?(texts: string[], model?: string): Promise<number[][]>;

  /**
   * Verifica se o cliente OpenAI está inicializado
   */
  isInitialized(): boolean;

  /**
   * Gera sinais neurais simbólicos baseados em um prompt para ativação do cérebro artificial
   * @param prompt O prompt estruturado para gerar sinais neurais (estímulo sensorial)
   * @param temporaryContext Contexto temporário opcional (campo contextual efêmero)
   * @returns Resposta contendo array de sinais neurais para ativação das áreas cerebrais
   */
  generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse>;

  /**
   * Expande semanticamente a query de um núcleo cerebral, retornando uma versão enriquecida, palavras-chave e dicas de contexto.
   */
  enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }>;

  /**
   * Envia uma requisição ao OpenAI com suporte a function calling
   * @param options Opções da requisição incluindo modelo, mensagens, ferramentas, etc.
   * @returns Resposta completa após o processamento
   */
  callOpenAIWithFunctions(options: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    tools?: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }>;
    tool_choice?: { type: string; function: { name: string } };
    temperature?: number;
    max_tokens?: number;
  }): Promise<{
    choices: Array<{
      message: {
        content?: string;
        tool_calls?: Array<{
          function: {
            name: string;
            arguments: string | Record<string, any>;
          };
        }>;
      };
    }>;
  }>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { Message } from "../transcription/TranscriptionTypes";

/**
 * Context for response generation
 */
export interface ResponseGenerationContext {
  messages: Message[];
  temporaryContext?: string;
  language?: string;
  useStreaming?: boolean;
}

/**
 * Result from response generation
 */
export interface ResponseGenerationResult {
  response: string;
  messageId?: string;
  metadata?: Record<string, unknown>;
}

/**
 * Strategy interface for response generation
 * Part of SOLID refactoring to support different response generation approaches
 */
export interface IResponseGenerationStrategy {
  /**
   * Generate a response using the specific strategy
   * @param prompt The integrated prompt
   * @param context Additional context
   * @returns Generated response
   */
  generateResponse(
    prompt: string,
    context: ResponseGenerationContext
  ): Promise<ResponseGenerationResult>;

  /**
   * Get the strategy name for logging/debugging
   */
  getStrategyName(): string;
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ITranscriptionProcessor.ts
// Interface for processing and managing transcriptions

import { Message, SpeakerSegment, SpeakerTranscription } from "./TranscriptionTypes";

export interface IBatchTranscriptionProcessor {
  /**
   * Processes and formats transcriptions from multiple speakers
   */
  processTranscriptions(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): SpeakerSegment[];
  
  /**
   * Removes duplicate content from transcriptions
   */
  deduplicateTranscriptions(
    transcriptions: SpeakerTranscription[]
  ): SpeakerTranscription[];
  
  /**
   * Extracts the last message from each speaker
   */
  extractLastMessageBySpeaker(
    transcriptions: SpeakerTranscription[],
    speakers: string[]
  ): Map<string, SpeakerTranscription>;
  
  /**
   * Formats transcriptions for conversation history
   */
  formatTranscriptionsForHistory(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Message[];
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ITranscriptionFormatter.ts
// Interface for formatting and processing transcriptions

import { SpeakerSegment } from "./TranscriptionTypes";

export interface ITranscriptionFormatter {
  /**
   * Formats mixed transcriptions with speaker labels
   */
  formatMixedTranscription(text: string, primaryUserSpeaker: string): SpeakerSegment[];
  
  /**
   * Formats external speaker content to ensure correct labeling
   */
  formatExternalSpeakerContent(content: string): string;
  
  /**
   * Sanitizes memory content and fixes speaker attributions
   */
  sanitizeMemoryContent(content: string, isSpeakerContent?: boolean): string;
  
  /**
   * Combines speaker segments into a coherent conversation
   */
  buildConversationFromSegments(
    segments: SpeakerSegment[], 
    preserveSpeakerLabels?: boolean
  ): string;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ITranscriptionStorageService.ts
// Interface para o serviço de armazenamento de transcrições

import {
  SpeakerTranscription,
  SpeakerTranscriptionLog,
} from "./TranscriptionTypes";

export interface ITranscriptionStorageService {
  /**
   * Atualiza a UI com uma nova transcrição
   * @param text - o texto da transcrição a ser atualizado na UI
   */
  updateTranscriptionUI(text: string): void;

  /**
   * Retorna todas as transcrições já formatadas e enviadas para a UI, prontas para prompt
   */
  getUITranscriptionText(): string;
  /**
   * Adiciona uma nova transcrição ao armazenamento
   */
  addTranscription(text: string, speaker?: string): void;

  /**
   * Adiciona uma transcrição de um único falante
   */
  addSingleSpeakerTranscription(text: string, speaker: string): void;

  /**
   * Retorna a lista atual de transcrições
   */
  getTranscriptionList(): string[];

  /**
   * Retorna transcrições organizadas por falante
   */
  getSpeakerTranscriptions(): SpeakerTranscription[];

  /**
   * Retorna logs de transcrição agrupados por falante para depuração/UI
   */
  getTranscriptionLogs(): SpeakerTranscriptionLog[];

  /**
   * Limpa os dados de transcrição
   */
  clearTranscriptionData(): void;

  /**
   * Verifica se existem transcrições válidas por qualquer falante
   */
  hasValidTranscriptions(): boolean;

  /**
   * Retorna a última transcrição conhecida
   */
  getLastTranscription(): string;

  /**
   * Retorna a última mensagem do usuário principal
   */
  getLastMessageFromUser(): SpeakerTranscription | null;

  /**
   * Retorna as últimas mensagens de cada falante externo
   */
  getLastMessagesFromExternalSpeakers(): Map<string, SpeakerTranscription>;

  /**
   * Retorna o conjunto de falantes detectados
   */
  getDetectedSpeakers(): Set<string>;

  /**
   * Define o falante atual
   */
  setCurrentSpeaker(speaker: string): void;

  /**
   * Obtém o falante atual
   */
  getCurrentSpeaker(): string;

  /**
   * Flush all accumulated transcriptions to the UI
   * Should be called when recording stops or when sending a message
   */
  flushTranscriptionsToUI(): void;

  // New methods for incremental transcription sending
  getNewTranscriptions(): SpeakerTranscription[];
  markTranscriptionsAsSent(): void;
  hasNewTranscriptions(): boolean;
  updateUIWithNewTranscriptions(): void;
  extractAndMarkAsSent(): SpeakerTranscription[];
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionTypes.ts
// Define types shared for the transcription service

import { OpenAI } from "openai";

// Base types
export type Role = "developer" | "user" | "assistant" | "system";

export interface Message {
  role: Role;
  content: string;
}

// Interfaces for transcription
export interface SpeakerTranscription {
  text: string;
  timestamp: string;
  speaker: string;
  sent?: boolean; // Track if this transcription has been sent
}

export interface SpeakerSegment {
  speaker: string;
  text: string;
  showSpeaker: boolean;
}

// Interfaces for results and logs
export interface SpeakerMemoryResults {
  userContext: string;
  speakerContexts: Map<string, string>;
  temporaryContext: string;
}

export interface SpeakerTranscriptionLog {
  speaker: string;
  isUser: boolean;
  transcriptions: { text: string; timestamp: string }[];
}

// Interface for service configuration
export interface TranscriptionServiceConfig {
  primaryUserSpeaker: string;
  openai: OpenAI | null;
  apiKey: string;
  model: string;
  interimResultsEnabled: boolean;
  useSimplifiedHistory: boolean;
}

// Constants for consistent labels
export const EXTERNAL_SPEAKER_LABEL = "External Participant";
export const USER_HEADER = "🗣️ User Transcription";
export const EXTERNAL_HEADER = "🎤 External Transcription";
export const INSTRUCTIONS_HEADER = "🧠 Instructions";
export const MEMORY_USER_HEADER = "📦 User Memory";
export const MEMORY_INSTRUCTIONS_HEADER = "📦 Instructions Memory";
export const MEMORY_EXTERNAL_HEADER = "📦 External Memory";

// Interface for IPC events
export interface IPCHandlers {
  getEnv?: (key: string) => Promise<string>;
  onPromptPartialResponse?: (text: string) => void;
  onPromptSuccess?: (text: string) => void;
  onPromptError?: (text: string) => void;
  sendNeuralPrompt?: (context?: string) => void;
  queryPinecone?: (embedding: number[]) => Promise<Record<string, unknown>>;
  saveToPinecone?: (
    entries: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<void>;
}

// Interface for UI updates
export type UIUpdater = (
  updater: (prev: Record<string, unknown>) => Record<string, unknown>
) => void;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ISpeakerIdentificationService.ts
// Interface for speaker identification service

import { SpeakerSegment, SpeakerTranscription } from "../transcription/TranscriptionTypes";

export interface ISpeakerIdentificationService {
  /**
   * Define the name of the primary speaker (user)
   */
  setPrimaryUserSpeaker(name: string): void;
  
  /**
   * Normalizes speaker identification for comparison
   */
  normalizeAndIdentifySpeaker(speaker: string): string;
  
  /**
   * Extracts speech segments from a transcription that mixes speakers
   */
  splitMixedTranscription(text: string): SpeakerSegment[];
  
  /**
   * Filters transcriptions by specific speaker
   */
  filterTranscriptionsBySpeaker(speaker: string, transcriptions: SpeakerTranscription[]): SpeakerTranscription[];
  
  /**
   * Filters transcriptions by the primary user speaker
   */
  filterTranscriptionsByUser(transcriptions: SpeakerTranscription[]): SpeakerTranscription[];
  
  /**
   * Verifies if only the primary user speaker is speaking
   */
  isOnlyUserSpeaking(transcriptions: SpeakerTranscription[]): boolean;
  
  /**
   * Prepares the transcription text for sending, combining all inputs
   */
  prepareTranscriptionText(
    transcriptionList: string[],
    speakerTranscriptions: SpeakerTranscription[],
    lastTranscription: string
  ): string;
  
  /**
   * Gets the primary user speaker
   */
  getPrimaryUserSpeaker(): string;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IUIUpdateService.ts
// Interface for the UI update service and notifications

export interface IUIUpdateService {
  /**
   * Updates the UI with new values
   */
  updateUI(update: Record<string, any>): void;
  
  /**
   * Notifies the start of prompt processing via IPC
   */
  notifyPromptProcessingStarted(temporaryContext?: string): void;
  
  /**
   * Notifies the completion of prompt processing via IPC
   */
  notifyPromptComplete(response: string): void;
  
  /**
   * Notifies an error in prompt processing via IPC
   */
  notifyPromptError(errorMessage: string): void;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * AudioProcessor handles audio processing, validation, and diagnostics
 * for the Deepgram service.
 */
import { IAudioAnalyzer } from '../../interfaces/deepgram/IDeepgramService';
import { AudioProcessingResult } from '../utils/DeepgramTypes';
import { Logger } from '../utils/Logger';

export class AudioProcessor {
  private logger: Logger;
  private analyzer: IAudioAnalyzer | null;
  
  constructor(analyzer: IAudioAnalyzer | null) {
    this.logger = new Logger('AudioProcessor');
    this.analyzer = analyzer;
  }
  
  /**
   * Log diagnostic information about the audio
   */
  public logAudioDiagnostics(blob: Blob | Uint8Array, connection?: { getReadyState: () => number }): void {
    try {
      if (blob instanceof Uint8Array) {
        const firstBytes = Array.from(blob.slice(0, 16))
          .map(b => b.toString(16).padStart(2, '0'))
          .join(' ');
        
        this.logger.debug("AUDIO DIAGNOSTICS (COGNITIVE)", {
          type: "Uint8Array",
          size: blob.byteLength,
          firstBytes: firstBytes,
          brainConnectionActive: !!connection,
          readyState: connection?.getReadyState()
        });
      } else {
        this.logger.debug("AUDIO DIAGNOSTICS (COGNITIVE)", {
          type: "Blob",
          size: blob.size,
          mimeType: blob.type,
          brainConnectionActive: !!connection,
          readyState: connection?.getReadyState()
        });
      }
    } catch (e) {
      this.logger.warning("Error analyzing audio data for cognitive brain input", e);
    }
  }
  
  /**
   * Prepare and validate audio buffer for sending to Deepgram
   */
  public async prepareAudioBuffer(blob: Blob | Uint8Array, shouldLog: boolean): Promise<AudioProcessingResult> {
    try {
      // Tamanho mínimo para envio
      const size = blob instanceof Blob ? blob.size : blob.byteLength;
      if (size < 32) {
        if (shouldLog) this.logger.warning(`Pacote de áudio muito pequeno (${size} bytes)`);
        return { buffer: null, valid: false };
      }
      
      // Converter para ArrayBuffer
      let buffer: ArrayBufferLike;
      if (blob instanceof Uint8Array) {
        buffer = blob.buffer;
      } else {
        buffer = await blob.arrayBuffer();
      }
      
      // For raw PCM, we need to ensure data is interpreted correctly
      // We can verify format and adjust if necessary
      if (shouldLog) {
        this.logger.info(`Sending PCM buffer: ${buffer.byteLength} bytes (16-bit linear PCM mono, 16kHz)`);
        
        // Log first bytes for debugging (optional)
        const view = new Uint8Array(buffer);
        const firstBytes = Array.from(view.slice(0, 16))
          .map(b => b.toString(16).padStart(2, '0'))
          .join(' ');
        this.logger.debug(`First bytes of PCM: ${firstBytes}`);
      }
      
      return { buffer: buffer as ArrayBuffer, valid: true };
    } catch (error) {
      this.logger.handleError("Error processing audio", error);
      return { buffer: null, valid: false };
    }
  }
  
  /**
   * Check audio format and provide diagnostic information
   */
  public checkAudioFormat(buffer: ArrayBufferLike): void {
    const view = new Uint8Array(buffer);
    
    // Check if it's a container format (WAV, MP3, etc)
    const isWav = view.length > 12 && 
                  view[0] === 0x52 && view[1] === 0x49 && // "RI"
                  view[2] === 0x46 && view[3] === 0x46 && // "FF"
                  view[8] === 0x57 && view[9] === 0x41 && // "WA" 
                  view[10] === 0x56 && view[11] === 0x45; // "VE"
    
    const isMP3 = view.length > 3 &&
                  ((view[0] === 0x49 && view[1] === 0x44 && view[2] === 0x33) || // "ID3"
                  (view[0] === 0xFF && (view[1] & 0xE0) === 0xE0));              // MP3 frame sync
    
    const isOGG = view.length > 4 && 
                  view[0] === 0x4F && view[1] === 0x67 && // "Og"
                  view[2] === 0x67 && view[3] === 0x53;   // "gS"
    
    const isAAC = view.length > 2 && 
                 (view[0] === 0xFF && (view[1] & 0xF0) === 0xF0);
    
    if (isWav || isMP3 || isOGG || isAAC) {
      this.logger.warning(`Format detected: ${isWav ? 'WAV' : isMP3 ? 'MP3' : isOGG ? 'OGG' : 'AAC'}. Deepgram expects raw PCM!`);
      console.log(`⚠️ [COGNITIVE-AUDIO] Inadequate format detected. Deepgram expects raw Linear PCM (not WAV/MP3/OGG/AAC files). Artificial brain audio input rejected.`);
    }
    
    // Check if the PCM seems valid
    if (!isWav && !isMP3 && !isOGG && !isAAC) {
      // Count non-zero bytes (if all zero, the audio is silence)
      const nonZeroCount = Array.from(view.slice(0, Math.min(1000, view.length)))
                         .filter(val => val !== 0).length;
      
      if (nonZeroCount === 0) {
        this.logger.warning("Audio detected as complete silence (all bytes are zero)");
        console.log("⚠️ [COGNITIVE-AUDIO] Silence detected: all bytes are zero! No cognitive input for brain memory.");
      } else if (nonZeroCount < 10) {
        this.logger.warning("Audio with very little variation (nearly silence)");
        console.log("⚠️ [COGNITIVE-AUDIO] Nearly silent audio: few non-zero bytes. Weak input for cognitive memory.");
      }
    }
  }
  
  /**
   * Generate a silence frame (kept for compatibility)
   */
  public generateSilenceFrame(): ArrayBuffer {
    // Create a small empty buffer just for compatibility
    return new ArrayBuffer(0);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * AudioQueue manages the queuing and processing of audio chunks
 * for the Deepgram service.
 */
import { Logger } from '../utils/Logger';

export class AudioQueue {
  private logger: Logger;
  private audioQueue: Uint8Array[] = [];
  private maxQueueSize: number = 10;
  private isProcessingQueue: boolean = false;
  private emptyAudioCounter: number = 0;
  
  constructor(maxQueueSize?: number) {
    this.logger = new Logger('AudioQueue');
    if (maxQueueSize) {
      this.maxQueueSize = maxQueueSize;
    }
  }
  
  /**
   * Check if the queue is full
   */
  public isQueueFull(): boolean {
    return this.audioQueue.length >= this.maxQueueSize;
  }
  
  /**
   * Get the current queue size
   */
  public getQueueSize(): number {
    return this.audioQueue.length;
  }
  
  /**
   * Clear the queue
   */
  public clearQueue(): void {
    const queueSize = this.audioQueue.length;
    if (queueSize > 0) {
      this.logger.info(`Limpando fila de áudio (${queueSize} chunks)`);
      this.audioQueue = [];
    }
  }
  
  /**
   * Reset the empty audio counter
   */
  public resetEmptyAudioCounter(): void {
    this.emptyAudioCounter = 0;
  }
  
  /**
   * Increment the empty audio counter
   */
  public incrementEmptyAudioCounter(): void {
    this.emptyAudioCounter++;
  }
  
  /**
   * Get the empty audio counter value
   */
  public getEmptyAudioCounter(): number {
    return this.emptyAudioCounter;
  }
  
  /**
   * Check if the queue is currently being processed
   */
  public isProcessing(): boolean {
    return this.isProcessingQueue;
  }
  
  /**
   * Set the processing state
   */
  public setProcessing(isProcessing: boolean): void {
    this.isProcessingQueue = isProcessing;
  }
  
  /**
   * Add audio to the queue
   */
  public async enqueueAudio(blob: Blob | Uint8Array): Promise<void> {
    if (this.isQueueFull()) {
      this.logger.warning(`Fila cheia (${this.audioQueue.length}), ignorando novo chunk de áudio`);
      return;
    }
    
    try {
      if (blob instanceof Blob) {
        const arrayBuffer = await blob.arrayBuffer();
        this.audioQueue.push(new Uint8Array(arrayBuffer));
      } else {
        this.audioQueue.push(blob);
      }
      
      // Reduzir frequência de logs para evitar spam
      if (this.audioQueue.length % 5 === 0 || this.audioQueue.length === 1) {
        this.logger.debug(`Áudio enfileirado (total: ${this.audioQueue.length})`);
      }
    } catch (error) {
      this.logger.handleError("Erro ao enfileirar áudio", error);
    }
  }
  
  /**
   * Retrieve and remove the next audio chunk from the queue
   */
  public dequeueAudio(): Uint8Array | null {
    if (this.audioQueue.length === 0) {
      return null;
    }
    
    return this.audioQueue.shift() || null;
  }
  
  /**
   * Discard half of the queue when needed (e.g., after reconnection failures)
   */
  public discardHalfQueue(): void {
    if (this.audioQueue.length > 0) {
      this.logger.warning(`Descartando metade da fila (${this.audioQueue.length} chunks)`);
      this.audioQueue = this.audioQueue.slice(-Math.floor(this.audioQueue.length / 2));
    }
  }
  
  /**
   * Check if the queue has any items
   */
  public hasItems(): boolean {
    return this.audioQueue.length > 0;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * AudioSender handles the preparation and sending of audio data to Deepgram.
 */
import { ConnectionState } from '../../interfaces/deepgram/IDeepgramService';
import { ConnectionManager } from '../connection/ConnectionManager';
import { Logger } from '../utils/Logger';
import { AudioProcessor } from './AudioProcessor';
import { AudioQueue } from './AudioQueue';
import { ListenLiveClient } from '@deepgram/sdk';

export class AudioSender {
  private logger: Logger;
  private audioProcessor: AudioProcessor;
  private audioQueue: AudioQueue;
  private connectionManager: ConnectionManager;
  private lastRealAudioTimestamp: number = Date.now();
  
  constructor(
    audioProcessor: AudioProcessor,
    audioQueue: AudioQueue,
    connectionManager: ConnectionManager
  ) {
    this.logger = new Logger('AudioSender');
    this.audioProcessor = audioProcessor;
    this.audioQueue = audioQueue;
    this.connectionManager = connectionManager;
  }
  
  /**
   * Send audio chunk to Deepgram
   */
  public async sendAudioChunk(blob: Blob | Uint8Array): Promise<boolean> {
    // Caso ideal: conexão ativa e pronta
    if (this.connectionManager.isActiveConnection()) {
      return this.processAndSendAudio(blob);
    }
    
    // Verify if we should continue adding to the queue
    if (this.audioQueue.isQueueFull()) {
      // If we reach the maximum queue size and we are still trying to connect for a long time,
      // something is wrong. Let's reset the connection.
      if (this.connectionManager.getConnectionState() === ConnectionState.CONNECTING) {
        this.logger.warning(`Queue is full (${this.audioQueue.getQueueSize()}) but connection is not established. Resetting connection.`);
        this.connectionManager.resetConnection(ConnectionState.CLOSED);
        
        // Partially clear the queue to avoid overloading
        this.audioQueue.discardHalfQueue();
        
        // Try to reconnect after a short delay
        setTimeout(() => {
          this.connectionManager.forceReconnect();
        }, 1000);
        
        return false;
      }
      
      this.logger.warning(`Audio queue is full (${this.audioQueue.getQueueSize()}), discarding chunk`);
      return false;
    }
    
    // Enqueue the audio and manage the connection based on the current state
    await this.audioQueue.enqueueAudio(blob);
    
    // Inconsistent state: OPEN without connection object
    if (this.connectionManager.getConnectionState() === ConnectionState.OPEN && 
        !this.connectionManager.getConnection()) {
      this.logger.error("Inconsistent state: OPEN without connection");
      await this.connectionManager.forceReconnect();
      return false;
    }
    
    // Already connecting: just enqueued
    if (this.connectionManager.getConnectionState() === ConnectionState.CONNECTING) {
      this.logger.info("Connection in progress, audio enqueued");
      return false;
    }
    
    // No connection and not trying to connect: start one
    if (this.connectionManager.getConnectionState() === ConnectionState.CLOSED || 
        this.connectionManager.getConnectionState() === ConnectionState.ERROR ||
        this.connectionManager.getConnectionState() === ConnectionState.STOPPED) {
      this.logger.info("Starting connection to send audio");
      await this.connectionManager.connectToDeepgram();
      return false;
    }
    
    // Update the timestamp of the last real audio sent
    this.lastRealAudioTimestamp = Date.now();
    
    return true;
  }
  
  /**
   * Process queued audio chunks
   */
  public async processQueuedChunks(): Promise<void> {
    if (!this.audioQueue.hasItems()) {
      this.logger.debug("No audio enqueued to process");
      return;
    }
    
    if (!this.connectionManager.isActiveConnection()) {
      // Check if the connection is in progress
      if (this.connectionManager.getConnectionState() === ConnectionState.CONNECTING) {
        this.logger.info("Waiting for connection to be established to process queue");
        return;
      }
      
      this.logger.warning("Unable to process queue: connection unavailable");
      
      // If closed, try to reconnect
      if ((this.connectionManager.getConnectionState() === ConnectionState.CLOSED || 
           this.connectionManager.getConnectionState() === ConnectionState.ERROR) && 
          this.audioQueue.hasItems()) {
        
        this.logger.info("Trying to reconnect to process enqueued audio");
        await this.connectionManager.forceReconnect();
        return; // The reconnection will call processQueuedChunks() again after establishing connection
      }
      
      return;
    }
    
    const queueSize = this.audioQueue.getQueueSize();
    this.logger.info(`Processing audio queue (${queueSize} chunks)`);
    
    // Limit the number of chunks processed at once to avoid overloading
    const maxChunksPerBatch = 5;
    const chunksToProcess = Math.min(queueSize, maxChunksPerBatch);
    
    let successCount = 0;
    
    // Set processing flag
    this.audioQueue.setProcessing(true);
    
    try {
      for (let i = 0; i < chunksToProcess; i++) {
        const chunk = this.audioQueue.dequeueAudio();
        if (!chunk) break;
        
        // Process without re-enqueueing to avoid loops
        const success = await this.processAndSendAudio(chunk, false);
        if (success) successCount++;
        
        // Small pause between chunks to avoid overloading
        await new Promise(resolve => setTimeout(resolve, 50));
        
        // Check if the connection is still active after each chunk
        if (!this.connectionManager.isActiveConnection()) {
          this.logger.warning("Connection lost during queue processing. Stopping.");
          break;
        }
      }
      
      this.logger.info(`Processed ${successCount}/${chunksToProcess} chunks of the queue. Remaining: ${this.audioQueue.getQueueSize()}`);
      
      // If there are still items in the queue, schedule another processing
      if (this.audioQueue.hasItems() && this.connectionManager.isActiveConnection()) {
        setTimeout(() => this.processQueuedChunks(), 500);
      }
    } finally {
      // Clear processing flag
      this.audioQueue.setProcessing(false);
    }
  }
  
  /**
   * Process and send audio to Deepgram
   */
  private async processAndSendAudio(blob: Blob | Uint8Array, allowEnqueue = true): Promise<boolean> {
    try {
      const shouldLog = Math.random() < 0.05;
      
      if (shouldLog) this.audioProcessor.logAudioDiagnostics(blob, this.connectionManager.getConnection() as ListenLiveClient);
      
      // If we don't have an active connection
      if (!this.connectionManager.isActiveConnection()) {
        if (allowEnqueue) {
          await this.audioQueue.enqueueAudio(blob);
          
          // If the connection is closed, start a new one
          if (this.connectionManager.getConnectionState() === ConnectionState.CLOSED ||
              this.connectionManager.getConnectionState() === ConnectionState.ERROR) {
            this.logger.info("Starting connection to send audio");
            this.connectionManager.connectToDeepgram().catch(err => {
              this.logger.error("Error starting connection", err);
            });
          }
        }
        return false;
      }
      
      const { buffer, valid } = await this.audioProcessor.prepareAudioBuffer(blob, shouldLog);
      if (!valid || !buffer) return false;
      
      // Verify if the connection is still active before sending
      const connection = this.connectionManager.getConnection();
      if (!connection || connection.getReadyState() !== 1) {
        this.logger.warning("Connection lost before sending audio");
        if (allowEnqueue) {
          await this.audioQueue.enqueueAudio(blob);
        }
        return false;
      }
      
      // Ocasional log without modifying the audio
      if (Math.random() < 0.01) {
        console.log(`📢 [AUDIO] Sending buffer of ${buffer.byteLength} bytes`);
      }
      
      connection.send(buffer);
      if (shouldLog) this.logger.debug(`Audio sent: ${buffer.byteLength} bytes`);
      
      // Reset empty audio counter when valid data is sent
      this.audioQueue.resetEmptyAudioCounter();
      
      return true;
    } catch (error) {
      this.logger.handleError("Error processing/sending audio", error);
      return false;
    }
  }
  
  /**
   * Get the timestamp of the last real audio sent
   */
  public getLastRealAudioTimestamp(): number {
    return this.lastRealAudioTimestamp;
  }
  
  /**
   * Set the timestamp of the last real audio sent
   */
  public updateLastRealAudioTimestamp(): void {
    this.lastRealAudioTimestamp = Date.now();
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * ChunkReceiver handles receiving audio chunks from the IPC channel.
 */
import { Logger } from '../utils/Logger';

export class ChunkReceiver {
  private logger: Logger;
  private removeChunkListener: (() => void) | null = null;
  private audioChunkHandler: (arrayBuffer: ArrayBuffer) => Promise<void>;
  
  constructor(audioChunkHandler: (arrayBuffer: ArrayBuffer) => Promise<void>) {
    this.logger = new Logger('ChunkReceiver');
    this.audioChunkHandler = audioChunkHandler;
  }
  
  /**
   * Set up the chunk receiver from the main process via IPC
   */
  public setupChunkReceiver(): void {
    if (typeof window !== 'undefined' && window.electronAPI) {
      // Clear previous listener if it exists
      if (this.removeChunkListener) {
        this.removeChunkListener();
      }
      
      this.logger.info("Registering audio chunk receiver via IPC");
      
      // Register new listener to receive audio chunks from the main process
      this.removeChunkListener = window.electronAPI.onSendChunk((arrayBuffer: ArrayBuffer) => {
        this.handleIncomingAudioChunk(arrayBuffer);
      });
    } else {
      this.logger.warning("API Electron not available, IPC audio reception disabled");
    }
  }
  
  /**
   * Handle incoming audio chunks from IPC
   */
  private async handleIncomingAudioChunk(arrayBuffer: ArrayBuffer): Promise<void> {
    try {
      // Convert ArrayBuffer to Uint8Array for compatibility
      const audioData = new Uint8Array(arrayBuffer);
      
      if (audioData.byteLength > 0) {
        if (Math.random() < 0.01) {
          // Periodic log to verify data flow
          this.logger.info(`Chunk of audio received via IPC: ${audioData.byteLength} bytes`);
        }
        
        // Pass the audio chunk to the configured handler
        await this.audioChunkHandler(arrayBuffer);
      }
    } catch (error) {
      this.logger.handleError("Error processing audio chunk from IPC", error);
    }
  }
  
  /**
   * Clean up the chunk receiver
   */
  public cleanup(): void {
    if (this.removeChunkListener) {
      this.removeChunkListener();
      this.removeChunkListener = null;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
import { getOption, STORAGE_KEYS } from '../../../../../services/StorageService';
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * ConnectionManager handles establishing, maintaining, and closing 
 * connections to the Deepgram service.
 */
import { createClient, DeepgramClient, ListenLiveClient } from "@deepgram/sdk";
import { ConnectionState } from '../../interfaces/deepgram/IDeepgramService';
import { ConnectionCallback, ConnectionStateCallback } from '../utils/DeepgramTypes';
import { Logger } from '../utils/Logger';

export class ConnectionManager {
  private logger: Logger;
  private connection: ListenLiveClient | null = null;
  private connectionState: ConnectionState = ConnectionState.CLOSED;
  private deepgramClient: DeepgramClient | null = null;
  private apiKey: string = "";
  private language: string = getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || 'pt-BR';
  private autoReconnect: boolean = true;
  private keepAliveInterval?: ReturnType<typeof setInterval>;
  private lastConnectionId: number = 0;
  private reconnectAttempts: number = 0;
  private maxReconnectAttempts: number = 5;
  private reconnectDelay: number = 3000;
  
  // Callbacks
  private setConnectionState: ConnectionStateCallback;
  private setConnection: ConnectionCallback;
  
  constructor(
    setConnectionState: ConnectionStateCallback,
    setConnection: ConnectionCallback
  ) {
    this.logger = new Logger('ConnectionManager');
    this.setConnectionState = setConnectionState;
    this.setConnection = setConnection;
    this.loadApiKey();
  }
  
  /**
   * Get the current connection
   */
  public getConnection(): ListenLiveClient | null {
    return this.connection;
  }
  
  /**
   * Get the current connection state
   */
  public getConnectionState(): ConnectionState {
    return this.connectionState;
  }
  
  /**
   * Check if there is an active connection
   */
  public isActiveConnection(): boolean {
    return !!(
      this.connection && 
      this.connection.getReadyState() === 1 && 
      this.connectionState === ConnectionState.OPEN
    );
  }
  
  /**
   * Get detailed connection status information
   */
  public getConnectionStatus() {
    return {
      state: this.connectionState,
      stateRef: this.connectionState,
      hasConnectionObject: !!this.connection,
      readyState: this.connection?.getReadyState() ?? null,
      active: this.isActiveConnection()
    };
  }
  
  /**
   * Set the language for transcription
   */
  public setLanguage(language: string): void {
    this.language = language;
    this.logger.info(`Idioma definido para: ${language}`);
  }
  
  /**
   * Check if an active or pending connection exists
   */
  public isConnectionActive(): boolean {
    if ([ConnectionState.CONNECTING, ConnectionState.OPEN].includes(this.connectionState)) {
      this.logger.warning(`Já ${this.connectionState === ConnectionState.CONNECTING ? 'conectando' : 'conectado'}`);
      return true;
    }
    return false;
  }
  
  /**
   * Update the connection state
   */
  public updateState(state: ConnectionState): void {
    this.connectionState = state;
    this.setConnectionState(state);
  }
  
  /**
   * Generate a unique connection ID
   */
  public generateConnectionId(): number {
    const connectionId = Date.now();
    this.lastConnectionId = connectionId;
    return connectionId;
  }
  
  /**
   * Check if a connection attempt is still valid
   */
  public isValidConnectionAttempt(connectionId: number): boolean {
    if (connectionId !== this.lastConnectionId) {
      this.logger.warning("Connection attempt overridden by more recent request — maintaining brain connection integrity");
      return false;
    }
    return true;
  }
  
  /**
   * Reset the connection and update state
   */
  public resetConnection(state: ConnectionState): void {
    this.connection = null;
    this.setConnection(null);
    this.updateState(state);
  }
  
  /**
   * Clean up existing connection
   */
  public async cleanupExistingConnection(): Promise<void> {
    if (!this.connection) return;
    
    try {
      this.logger.info("Cleaning up existing connection");
      this.connection.removeAllListeners();
      this.connection.requestClose();
      await new Promise(resolve => setTimeout(resolve, 300));
    } catch (err) {
      this.logger.warning("Error cleaning up existing connection", err);
    }
    
    this.setConnection(null);
    this.connection = null;
  }
  
  /**
   * Set the active connection
   */
  public setActiveConnection(connection: ListenLiveClient): void {
    this.connection = connection;
    this.setConnection(connection);
  }
  
  /**
   * Create a new connection to Deepgram
   */
  public async createConnection(): Promise<ListenLiveClient | null> {
    if (!this.deepgramClient) {
      if (!this.apiKey) return null;
      this.deepgramClient = createClient(this.apiKey);
      this.logger.info("New Deepgram client instance created");
    }
    
    this.ensureValidLanguage();
    this.logger.info(`Connecting with language: ${this.language}`);
    
    try {
      console.log("📊 [COGNITIVE-CONFIG] Using minimal configuration and automatic format detection for brain audio input");
      
      const connection = this.deepgramClient.listen.live({
        model: getOption(STORAGE_KEYS.DEEPGRAM_MODEL) || 'nova-2-general',
        language: this.language,
        smart_format: true,
        multichannel: true,
      });
      
      console.log("📊 [COGNITIVE-CONFIG] Connection parameters for cognitive audio stream:", {
        idioma: this.language,
        modelo: "nova-2",
        smart_format: true,
        multicanal: true
      });
      
      this.logger.info("Using automatic audio format detection");
      return connection;
    } catch (error) {
      this.logger.error("Failed to create Deepgram connection", error);
      return null;
    }
  }
  
  /**
   * Ensure the language setting is valid and always get the latest from storage
   */
  public ensureValidLanguage(): void {
    // SEMPRE obter o idioma mais recente do storage
    const storedLanguage = getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE);
    
    // Atualizar o idioma com o valor mais recente do storage
    if (storedLanguage) {
      this.language = storedLanguage;
    } else if (!this.language) {
      this.language = 'pt-BR';
    }
  }
  
  /**
   * Add a timeout for establishing connection
   */
  public addConnectionTimeout(connectionId: number): void {
    const TIMEOUT_MS = 15000; // 15 seconds
    
    setTimeout(() => {
      // Check if we are still trying to connect to the same connection
      if (this.lastConnectionId !== connectionId) return;
      
      // If we are still in the CONNECTING state after the timeout
      if (this.connectionState === ConnectionState.CONNECTING) {
        this.logger.warning(`Connection timeout after ${TIMEOUT_MS}ms. Resetting connection.`);
        this.resetConnection(ConnectionState.ERROR);
        
        // Try to reconnect after reset
        if (this.reconnectAttempts < this.maxReconnectAttempts) {
          const delay = this.reconnectDelay * Math.min(Math.pow(1.5, this.reconnectAttempts), 10);
          this.reconnectAttempts++;
          
          this.logger.info(`Scheduling reconnection ${this.reconnectAttempts}/${this.maxReconnectAttempts} in ${delay}ms`);
          
          setTimeout(() => {
            this.forceReconnect();
          }, delay);
        } else {
          this.logger.error(`Maximum reconnection attempts reached (${this.maxReconnectAttempts}). Giving up.`);
        }
      }
    }, TIMEOUT_MS);
  }
  
  /**
   * Set up keep alive ping mechanism for the WebSocket connection
   */
  public setupKeepAlive(connection: ListenLiveClient): void {
    this.clearKeepAlive();
    
    // Interval of 8 seconds for keepAlive 
    // (documentation recommends sending keep-alive every 8 seconds, as Deepgram
    // closes the connection after approximately 12 seconds of inactivity)
    this.keepAliveInterval = setInterval(() => {
      try {
        if (connection && connection.getReadyState() === 1) {
          connection.keepAlive();
        } else if (connection && connection.getReadyState() !== 1) {
          this.logger.warning(`Unable to send keepAlive: state ${connection.getReadyState()}`);
          
          // Verify if the internal state is inconsistent
          if (this.connectionState === ConnectionState.OPEN) {
            this.logger.warning("Inconsistent state detected: OPEN but WebSocket is not open");
            this.forceReconnect();
          }
        }
      } catch (err) {
        this.logger.warning("Error sending keepalive", err);
      }
    }, 8000); // 8 seconds (Deepgram closes after ~12 seconds of inactivity)
  }
  
  /**
   * Clear keep alive interval
   */
  public clearKeepAlive(): void {
    if (this.keepAliveInterval) {
      clearInterval(this.keepAliveInterval);
      this.keepAliveInterval = undefined;
    }
  }
  
  /**
   * Set up separate audio keep alive system
   */
  public setupKeepAliveAudio(): void {
    // Disable sending silent audio, since Deepgram's keep-alive is sufficient
    // to maintain the connection active.
    console.log("📢 [COGNITIVE-CONNECTION] Using only Deepgram API default keep-alive to maintain brain connection");
  }
  
  /**
   * Handle the WebSocket open event
   */
  public handleOpenEvent(connection: ListenLiveClient): void {
    this.logger.info("Connection opened successfully");
    this.updateState(ConnectionState.OPEN);
    this.reconnectAttempts = 0; // Reset reconnect attempts counter
    this.setupKeepAlive(connection);
  }
  
  /**
   * Handle the WebSocket close event
   */
  public handleCloseEvent(): void {
    this.logger.info("Connection closed");
    this.clearKeepAlive();
    
    // Check if we are already trying to reconnect
    if (this.connectionState === ConnectionState.CONNECTING) {
      this.logger.info("Already have a reconnection attempt in progress");
      return;
    }
    
    this.resetConnection(ConnectionState.CLOSED);
    
    // Try to reconnect if there is audio in the queue or auto-reconnect is enabled
    if (this.autoReconnect && this.reconnectAttempts < this.maxReconnectAttempts) {
      const delay = this.reconnectDelay * Math.pow(1.5, this.reconnectAttempts);
      this.reconnectAttempts++;
      
      this.logger.info(`Trying reconnection ${this.reconnectAttempts}/${this.maxReconnectAttempts} in ${delay}ms`);
      this.updateState(ConnectionState.CONNECTING);
      
      setTimeout(() => {
        if (this.connectionState === ConnectionState.CONNECTING) {
          this.connectToDeepgram();
        }
      }, delay);
    } else if (this.reconnectAttempts >= this.maxReconnectAttempts) {
      this.logger.error(`[COGNITIVE-CONNECTION] Maximum number of reconnections (${this.maxReconnectAttempts}) reached. Giving up on brain connection.`);
      this.updateState(ConnectionState.ERROR);
    }
  }
  
  /**
   * Handle error events from the WebSocket connection
   */
  public handleErrorEvent(error: unknown): void {
    let errorMessage: string;
    if (typeof error === "object" && error !== null && "message" in error) {
      errorMessage = String((error as { message?: string }).message);
    } else {
      errorMessage = String(error);
    }
    this.logger.error("[COGNITIVE-CONNECTION] Error in Deepgram connection", error);
    
    // Check error type for specific actions
    if (errorMessage.includes("NET-0001") || errorMessage.includes("1011")) {
      this.logger.warning("[COGNITIVE-CONNECTION] NET-0001 detected: Deepgram did not receive audio before timeout");
      
      // Try to send keepAlive if the connection is still open
      if (this.connection && this.connection.getReadyState() === 1) {
        try {
          this.connection.keepAlive();
          this.logger.info("[COGNITIVE-CONNECTION] KeepAlive sent after NET-0001 detected");
          return; // Do not reset the connection if keepAlive succeeds
        } catch (e) {
          this.logger.error("[COGNITIVE-CONNECTION] Failed to send keepAlive after error", e);
        }
      }
    } else if (errorMessage.includes("DATA-0000") || errorMessage.includes("1008")) {
      this.logger.warning("[COGNITIVE-CONNECTION] DATA-0000 detected: Deepgram cannot decode audio");
      // This error indicates problems with audio format - check encoding and sample_rate
    }
    
    // Check if we should try to reconnect
    if (this.connectionState === ConnectionState.OPEN || 
        this.connectionState === ConnectionState.CONNECTING) {
      this.resetConnection(ConnectionState.ERROR);
      
      if (this.autoReconnect && this.reconnectAttempts < this.maxReconnectAttempts) {
        const delay = this.reconnectDelay * Math.min(Math.pow(1.5, this.reconnectAttempts), 10);
        this.reconnectAttempts++;
        
        this.logger.info(`Trying reconnection ${this.reconnectAttempts}/${this.maxReconnectAttempts} in ${delay}ms`);
        
        setTimeout(() => {
          this.forceReconnect();
        }, delay);
      } else if (this.reconnectAttempts >= this.maxReconnectAttempts) {
        this.logger.error(`[COGNITIVE-CONNECTION] Maximum number of reconnections (${this.maxReconnectAttempts}) reached. Giving up on brain connection.`);
      }
    }
  }
  
  /**
   * Wait until a specific connection state is reached
   */
  public async waitForConnectionState(targetState: ConnectionState, timeoutMs = 15000): Promise<boolean> {
    if (this.connectionState === targetState) return true;
    
    return new Promise<boolean>((resolve) => {
      const timeoutId = setTimeout(() => {
        this.logger.warning(`[COGNITIVE-CONNECTION] Timeout waiting for state ${targetState}, current: ${this.connectionState}`);
        resolve(false);
      }, timeoutMs);
      
      const checkState = () => {
        if (this.connectionState === targetState) {
          clearTimeout(timeoutId);
          resolve(true);
          return;
        }
        setTimeout(checkState, 100);
      };
      
      checkState();
    });
  }
  
  /**
   * Force a reconnection to the Deepgram service
   */
  public async forceReconnect(): Promise<void> {
    // Clear existing connection first
    this.logger.info("Starting forced reconnection");
    await this.disconnectFromDeepgram();
    
    // Wait for a short period before reconnecting
    await new Promise(resolve => setTimeout(resolve, 500));
    
    // Ensure we are in a state appropriate for reconnection
    if (this.connectionState !== ConnectionState.CLOSED && 
        this.connectionState !== ConnectionState.ERROR) {
      this.resetConnection(ConnectionState.CLOSED);
    }
    
    // Check if we should try to reconnect
    if (this.reconnectAttempts >= this.maxReconnectAttempts) {
      this.logger.error(`Maximum reconnection attempts reached (${this.maxReconnectAttempts}). Giving up.`);
      return;
    }
    
    // Activate automatic reconnection and start connection
    this.autoReconnect = true;
    this.updateState(ConnectionState.CONNECTING);
    
    try {
      // Obter o idioma mais recente antes de reconectar
      this.ensureValidLanguage();
      await this.connectToDeepgram();
    } catch (error) {
      this.logger.error("Failed forced reconnection", error);
      
      // Try again, if still within the limit
      if (this.reconnectAttempts < this.maxReconnectAttempts) {
        this.reconnectAttempts++;
        const delay = this.reconnectDelay * Math.pow(1.5, this.reconnectAttempts);
        this.logger.info(`Scheduling reconnection attempt ${this.reconnectAttempts}/${this.maxReconnectAttempts} in ${delay}ms`);
        
        setTimeout(() => {
          // Sempre buscar o idioma mais recente antes de tentar reconectar
          this.ensureValidLanguage();
          this.forceReconnect();
        }, delay);
      }
    }
  }
  
  /**
   * Reset the reconnection counter
   */
  public resetReconnectCounter(): void {
    this.reconnectAttempts = 0;
  }
  
  /**
   * Loads the API key from environment variables
   */
  private async loadApiKey(): Promise<void> {
    if (typeof window === 'undefined' || !window.electronAPI) return;
    try {
      let key = await window.electronAPI.getEnv('DEEPGRAM_API_KEY');
      if (!key) {
        // Fallback: buscar via StorageService
        key = getOption(STORAGE_KEYS.DEEPGRAM_API_KEY) ?? null;
        if (key) {
          this.logger.info('[FALLBACK] Deepgram API key loaded from StorageService');
        }
      }
      if (key) {
        this.apiKey = key;
        this.deepgramClient = createClient(key);
        this.logger.info("API key loaded and client initialized");
      } else {
        this.logger.error("API key not found (env nor StorageService)");
      }
    } catch (error) {
      this.logger.handleError("Failed to load API key", error);
    }
  }
  
  /**
   * Ensure the API key is available
   */
  public async ensureApiKey(): Promise<boolean> {
    if (this.apiKey) return true;
    
    this.logger.error("API key not found");
    this.updateState(ConnectionState.ERROR);
    
    if (typeof window !== 'undefined' && window.electronAPI) {
      try {
        const key = await window.electronAPI.getEnv('DEEPGRAM_API_KEY');
        if (key) {
          this.apiKey = key;
          this.deepgramClient = createClient(key);
          this.logger.info("API key loaded successfully");
          return true;
        }
        this.logger.error("Failed to load API key");
      } catch (error) {
        this.logger.handleError("Failed to load API key", error);
      }
    }
    
    // Reset reconnect counter after API key failure
    this.reconnectAttempts = this.maxReconnectAttempts;
    return false;
  }
  
  /**
   * Connect to the Deepgram service
   */
  public async connectToDeepgram(language?: string): Promise<void> {
    this.logger.info("Starting new connection with Deepgram");
    
    // Prevent multiple connections at the same time
    if (this.connectionState === ConnectionState.CONNECTING) {
      this.logger.warning("Already connecting - connection in progress");
      return;
    }
    
    // If already connected, disconnect first to ensure a clean connection
    if (this.connectionState === ConnectionState.OPEN || this.connection) {
      this.logger.info("Previous connection detected, disconnecting to ensure clean session");
      await this.disconnectFromDeepgram();
      
      // Small pause to ensure disconnection is completed
      await new Promise(resolve => setTimeout(resolve, 500));
    }
    
    // Initialize new connection attempt
    const connectionId = this.generateConnectionId();
    this.updateState(ConnectionState.CONNECTING);
    
    // SEMPRE buscar o valor mais recente do idioma no storage
    // e dar prioridade ao idioma fornecido como parâmetro, se houver
    this.ensureValidLanguage(); // Busca o idioma mais recente do storage
    
    // Se receber um idioma como parâmetro, usar ele em vez do storage
    if (language) {
      this.setLanguage(language);
    }
    
    if (!await this.ensureApiKey()) {
      this.logger.error("Failed to obtain API key. Aborting connection.");
      this.updateState(ConnectionState.ERROR);
      return;
    }
    
    // Check if this connection attempt is still valid
    if (!this.isValidConnectionAttempt(connectionId)) {
      this.logger.warning("Connection attempt exceeded by newer request");
      return;
    }
    
    try {
      // Clean up any residual connection state
      await this.cleanupExistingConnection();
      
      // Create new connection with Deepgram
      this.logger.info(`Creating connection with language: ${this.language}`);
      const newConnection = await this.createConnection();
      
      if (!newConnection) {
        this.logger.error("Failed to create connection with Deepgram");
        this.resetConnection(ConnectionState.ERROR);
        return;
      }
      
      // Configure active connection
      this.setActiveConnection(newConnection);
      this.logger.info("Connection initialized and waiting for opening");
      
      // Add connection timeout
      this.addConnectionTimeout(connectionId);
      
      // Configure audio keep-alive after successful connection
      this.setupKeepAliveAudio();
      
      // Reset error counters
      this.reconnectAttempts = 0;
      
      return;
    } catch (error) {
      this.logger.handleError("Failed to connect to Deepgram", error);
      this.resetConnection(ConnectionState.ERROR);
      return;
    }
  }
  
  /**
   * Disconnect from the Deepgram service
   */
  public async disconnectFromDeepgram(): Promise<void> {
    this.logger.info("Starting disconnection from Deepgram");
    
    // Disable automatic reconnection temporarily to avoid reconnections during disconnection
    this.autoReconnect = false;
    
    // Clear keepAlive to prevent ping attempts on a closed connection
    this.clearKeepAlive();
    
    // Check if there is an active connection to close
    if (!this.connection) {
      this.logger.info("No active connection to close");
      this.updateState(ConnectionState.CLOSED);
      return;
    }
    
    // Save the current connection state for diagnostics
    const readyState = this.connection.getReadyState();
    this.logger.info(`Closing connection (current state: ${readyState})`);
    
    try {
      // Remove all listeners to avoid callbacks after disconnection
      this.connection.removeAllListeners();
      
      // Request controlled WebSocket connection closure only if it is open
      if (readyState === 1) {
        this.logger.info("Requesting controlled WebSocket connection closure");
        try {
          this.connection.requestClose();
        } catch (closeError) {
          this.logger.warning("Error requesting controlled WebSocket connection closure:", closeError);
          // Continue the cleanup process even with error
        }
      } else {
        this.logger.info(`WebSocket connection is not open (state: ${readyState})`);
      }
      
      // Ensure a timeout to close the connection in case of pending operations
      const closeTimeoutMs = 3000;
      await Promise.race([
        // Wait until the connection is really closed (close event)
        new Promise<void>((resolve) => {
          const checkClosed = () => {
            const newState = this.connection?.getReadyState();
            if (newState === 3 || newState === undefined || newState === null) {
              this.logger.info("Connection closed successfully");
              resolve();
            } else {
              setTimeout(checkClosed, 300);
            }
          };
          checkClosed();
        }),
        // Or timeout after maximum time
        new Promise<void>((resolve) => {
          setTimeout(() => {
            this.logger.warning(`Timeout (${closeTimeoutMs}ms) waiting for connection closure, forcing cleanup`);
            resolve();
          }, closeTimeoutMs);
        })
      ]);
      
      // Reset connection and state independently of the result
      this.connection = null;
      this.setConnection(null);
      this.updateState(ConnectionState.CLOSED);
      
      // Reset reconnect attempts
      this.reconnectAttempts = 0;
      
      this.logger.info("Disconnection completed and resources released");
      return;
    } catch (error) {
      this.logger.handleError("Error closing connection", error);
      // Even with error, ensure we clean up the state
      this.resetConnection(ConnectionState.CLOSED);
      this.reconnectAttempts = 0;
      return;
    } finally {
      // Reactivate automatic reconnection after delay, only if not forced disconnection
      // This allows future recording sessions without needing to restart the application
      setTimeout(() => this.autoReconnect = true, 5000);
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * EventHandler manages event registration and handling for the Deepgram connection.
 */
import { ListenLiveClient, LiveTranscriptionEvents } from "@deepgram/sdk";
import { LiveTranscriptionProcessor } from '../transcription/LiveTranscriptionProcessor';
import { TranscriptionEventCallback } from '../utils/DeepgramTypes';
import { Logger } from '../utils/Logger';
import { ConnectionManager } from './ConnectionManager';

export class EventHandler {
  private logger: Logger;
  private transcriptionProcessor: LiveTranscriptionProcessor;
  private connectionManager: ConnectionManager;
  private transcriptionCallback: TranscriptionEventCallback | null = null;
  
  constructor(
    transcriptionProcessor: LiveTranscriptionProcessor,
    connectionManager: ConnectionManager
  ) {
    this.logger = new Logger('EventHandler');
    this.transcriptionProcessor = transcriptionProcessor;
    this.connectionManager = connectionManager;
  }
  
  /**
   * Register a callback to receive transcription events
   */
  public registerTranscriptionCallback(callback: TranscriptionEventCallback): void {
    this.transcriptionCallback = callback;
    this.transcriptionProcessor.registerTranscriptionCallback(callback);
    this.logger.info("Callback de transcrição registrado");
  }
  
  /**
   * Register event handlers for the connection
   */
  public registerEventHandlers(connection: ListenLiveClient): void {
    // Usar a API correta de eventos do Deepgram
    connection.on(LiveTranscriptionEvents.Open, () => 
      this.connectionManager.handleOpenEvent(connection));
      
    connection.on(LiveTranscriptionEvents.Close, () => 
      this.connectionManager.handleCloseEvent());
      
    connection.on(LiveTranscriptionEvents.Error, (err) => 
      this.connectionManager.handleErrorEvent(err));
    
    // Handler para eventos de transcrição
    connection.on(LiveTranscriptionEvents.Transcript, (data) => {
      try {
        if (data) {
          this.logger.debug("Transcrição recebida");
          this.connectionManager.resetReconnectCounter();
          
          // Processar a transcrição de acordo com o formato
          this.transcriptionProcessor.handleTranscriptionEvent(data);
        } else {
          console.log("❌ [COGNITIVE-DEBUG] Transcription event without data for brain input");
        }
      } catch (error) {
        this.logger.error("Erro ao processar transcrição", error);
        console.log("❌ [COGNITIVE-DEBUG] Exception processing transcription for cognitive memory:", error);
      }
      
      // Tentar enviar keepAlive se necessário através do connection atual
      if (connection && connection.getReadyState() === 1) {
        try {
          connection.keepAlive();
        } catch (err) {
          console.log("⚠️ [COGNITIVE-PROCESS] Error sending KeepAlive for brain connection:", err);
        }
      }
    });
    
    // Handler para metadados
    connection.on(LiveTranscriptionEvents.Metadata, (data) => {
      console.log("🔍 [COGNITIVE-DEBUG] Metadata received for cognitive memory:", JSON.stringify(data, null, 2));
      this.logger.debug("Metadados recebidos", data);
      if (this.transcriptionCallback) {
        this.transcriptionCallback("metadata", data);
      }
    });
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IFunctionCallingService, FunctionCallOptions, FunctionCallResponse } from "../../interfaces/function-calling/IFunctionCallingService";
import { IFunctionDefinition, AIFunctionDefinition } from "../../interfaces/function-calling/IFunctionDefinition";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { Message } from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";

/**
 * Adapter that implements function calling using the existing IOpenAIService
 * This allows us to gradually migrate to the new architecture
 */
export class FunctionCallingAdapter implements IFunctionCallingService {
  constructor(private aiService: IOpenAIService) {}

  /**
   * Call AI model with function definitions
   */
  async callWithFunctions(
    messages: Message[],
    functions: IFunctionDefinition[],
    options?: FunctionCallOptions
  ): Promise<FunctionCallResponse> {
    try {
      // Convert function definitions to AI provider format
      const tools = functions.map(func => ({
        type: "function" as const,
        function: func
      }));

      // Map messages to expected format
      const mappedMessages = messages.map(m => ({
        role: m.role,
        content: m.content
      }));

      // Call the underlying service
      const response = await this.aiService.callOpenAIWithFunctions({
        model: options?.model || '',
        messages: mappedMessages,
        tools: tools,
        tool_choice: options?.toolChoice,
        temperature: options?.temperature,
        max_tokens: options?.maxTokens
      });

      return response;
    } catch (error) {
      LoggingUtils.logError("Error in function calling adapter", error);
      throw error;
    }
  }

  /**
   * Check if the service supports function calling
   */
  supportsFunctionCalling(): boolean {
    // Both OpenAI and HuggingFace facades implement callOpenAIWithFunctions
    return typeof this.aiService.callOpenAIWithFunctions === 'function';
  }
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IFunctionDefinition } from "../../interfaces/function-calling/IFunctionDefinition";
import { LoggingUtils } from "../../utils/LoggingUtils";

/**
 * Registry for centralized function schema management
 * Single source of truth for all function definitions
 */
export class FunctionSchemaRegistry {
  private static instance: FunctionSchemaRegistry;
  private schemas: Map<string, IFunctionDefinition> = new Map();

  private constructor() {
    this.registerDefaultSchemas();
  }

  /**
   * Get singleton instance
   */
  static getInstance(): FunctionSchemaRegistry {
    if (!FunctionSchemaRegistry.instance) {
      FunctionSchemaRegistry.instance = new FunctionSchemaRegistry();
    }
    return FunctionSchemaRegistry.instance;
  }

  /**
   * Register a function schema
   */
  register(name: string, schema: IFunctionDefinition): void {
    this.schemas.set(name, schema);
    LoggingUtils.logInfo(`Registered function schema: ${name}`);
  }

  /**
   * Get a function schema by name
   */
  get(name: string): IFunctionDefinition | undefined {
    return this.schemas.get(name);
  }

  /**
   * Get all registered schemas
   */
  getAll(): IFunctionDefinition[] {
    return Array.from(this.schemas.values());
  }

  /**
   * Check if a schema exists
   */
  has(name: string): boolean {
    return this.schemas.has(name);
  }

  /**
   * Register default schemas used throughout the application
   */
  private registerDefaultSchemas(): void {
    // Neural signal activation function
    this.register("activateBrainArea", {
      name: "activateBrainArea",
      description: "Activates a symbolic neural area of the artificial brain, defining the focus, emotional weight, and symbolic search parameters.",
      parameters: {
        type: "object",
        properties: {
          core: {
            type: "string",
            enum: [
              "memory",
              "valence",
              "metacognitive",
              "associative",
              "language",
              "planning",
              "unconscious",
              "archetype",
              "soul",
              "shadow",
              "body",
              "social",
              "self",
              "creativity",
              "intuition",
              "will"
            ],
            description: "Symbolic brain area to activate."
          },
          intensity: {
            type: "number",
            minimum: 0,
            maximum: 1,
            description: "Activation intensity from 0.0 to 1.0."
          },
          query: {
            type: "string",
            description: "Main symbolic or conceptual query."
          },
          keywords: {
            type: "array",
            items: { type: "string" },
            description: "Expanded semantic keywords related to the query."
          },
          topK: {
            type: "number",
            description: "Number of memory items or insights to retrieve."
          },
          filters: {
            type: "object",
            description: "Optional filters to constrain retrieval."
          },
          expand: {
            type: "boolean",
            description: "Whether to semantically expand the query."
          },
          symbolicInsights: {
            type: "object",
            description: "At least one symbolic insight must be included: hypothesis, emotionalTone, or archetypalResonance.",
            properties: {
              hypothesis: {
                type: "string",
                description: "A symbolic hypothesis or interpretative conjecture."
              },
              emotionalTone: {
                type: "string",
                description: "Emotional tone associated with the symbolic material."
              },
              archetypalResonance: {
                type: "string",
                description: "Archetypal patterns or figures evoked."
              }
            }
          }
        },
        required: ["core", "intensity", "query", "symbolicInsights"]
      }
    });

    // Collapse strategy decision function
    this.register("decideCollapseStrategy", {
      name: "decideCollapseStrategy",
      description: "Decides the symbolic collapse strategy (deterministic or not) based on emotional intensity, symbolic tension, and nature of the user's input.",
      parameters: {
        type: "object",
        properties: {
          deterministic: {
            type: "boolean",
            description: "Whether to use deterministic collapse (true) or probabilistic collapse (false)."
          },
          temperature: {
            type: "number",
            minimum: 0.1,
            maximum: 1.5,
            description: "Temperature for probabilistic collapse (0.1-1.5)."
          },
          justification: {
            type: "string",
            description: "Reasoning behind the collapse strategy decision."
          },
          userIntent: {
            type: "object",
            description: "User intent weights for different categories. Each value represents the intensity/importance of that intent type (0-1).",
            properties: {
              practical: { type: "number", minimum: 0, maximum: 1, description: "Weight for practical intent." },
              analytical: { type: "number", minimum: 0, maximum: 1, description: "Weight for analytical intent." },
              reflective: { type: "number", minimum: 0, maximum: 1, description: "Weight for reflective intent." },
              existential: { type: "number", minimum: 0, maximum: 1, description: "Weight for existential intent." },
              symbolic: { type: "number", minimum: 0, maximum: 1, description: "Weight for symbolic intent." },
              emotional: { type: "number", minimum: 0, maximum: 1, description: "Weight for emotional intent." },
              narrative: { type: "number", minimum: 0, maximum: 1, description: "Weight for narrative intent." },
              mythic: { type: "number", minimum: 0, maximum: 1, description: "Weight for mythic intent." },
              trivial: { type: "number", minimum: 0, maximum: 1, description: "Weight for trivial intent." },
              ambiguous: { type: "number", minimum: 0, maximum: 1, description: "Weight for ambiguous intent." }
            }
          },
          emergentProperties: {
            type: "array",
            description: "Emergent properties detected in the neural responses, such as redundancies, contradictions, or patterns",
            items: {
              type: "string"
            }
          }
        },
        required: ["deterministic", "temperature", "justification", "userIntent", "emergentProperties"]
      }
    });

    // Semantic enrichment function
    this.register("enrichSemanticQuery", {
      name: "enrichSemanticQuery",
      description: "Enriches a semantic query with expanded keywords and contextual information for a specific brain core.",
      parameters: {
        type: "object",
        properties: {
          enrichedQuery: {
            type: "string",
            description: "The enriched version of the original query with expanded semantic context."
          },
          keywords: {
            type: "array",
            items: { type: "string" },
            description: "Array of semantically related keywords for the query."
          },
          contextualHints: {
            type: "object",
            description: "Additional contextual information to guide the search.",
            properties: {
              temporalScope: {
                type: "string",
                description: "Temporal scope of the query (past, present, future)."
              },
              emotionalDepth: {
                type: "number",
                minimum: 0,
                maximum: 1,
                description: "Emotional depth of the query (0-1)."
              },
              abstractionLevel: {
                type: "string",
                enum: ["concrete", "conceptual", "symbolic", "archetypal"],
                description: "Level of abstraction for the query."
              }
            }
          }
        },
        required: ["enrichedQuery", "keywords"]
      }
    });
  }
}// services/huggingface/HuggingFaceClientService.ts
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../../services/StorageService";
import { HuggingFaceEmbeddingService } from "../../../../../../services/huggingface/HuggingFaceEmbeddingService";
import {
  HuggingFaceLocalService,
  SUPPORTED_HF_BROWSER_MODELS,
} from "../../../../../../services/huggingface/HuggingFaceLocalService";
import { IClientManagementService } from "../../../interfaces/openai/IClientManagementService";
import { LoggingUtils } from "../../../utils/LoggingUtils";
export class HuggingFaceClientService implements IClientManagementService {
  private huggingFaceLocal: HuggingFaceLocalService | null = null;
  private embeddingService: HuggingFaceEmbeddingService | null = null;
  private configuration: Record<string, any> = {};
  // Guard flags to prevent concurrent/double initialization
  private initializing = false;
  private initialized = false;

  initializeClient(_apiKey: string): void {
    // dispara a inicialização assíncrona mas não bloqueante
    this.initializeHuggingFaceClient().catch((err) => {
      LoggingUtils.logError(`Failed to initialize HuggingFace client: ${err}`);
    });
  }

  private async initializeHuggingFaceClient(config?: Record<string, any>) {
    this.configuration = config || {};

    LoggingUtils.logInfo(
      "🔧 [HFC] Initializing HuggingFace client with optimized configuration..."
    );

    //    await initializeTransformersEnvironment();

    // 1) cria o serviço local
    this.huggingFaceLocal = new HuggingFaceLocalService();

    // 2) carrega o modelo de texto com fallback para dtype disponível
    const textModel =
      getOption(STORAGE_KEYS.HF_MODEL) || SUPPORTED_HF_BROWSER_MODELS[0];

    // Validate that the model is supported
    if (!SUPPORTED_HF_BROWSER_MODELS.includes(textModel as any)) {
      LoggingUtils.logWarning(
        `[HFC] Model ${textModel} not in supported list, using default`
      );
      const defaultModel = SUPPORTED_HF_BROWSER_MODELS[0];
      LoggingUtils.logInfo(
        `[HFC] Using default supported model: ${defaultModel}`
      );
    }

    // Load model using default configuration - let model-specific configs handle optimization
    try {
      LoggingUtils.logInfo(
        `[HFC] Loading ${textModel} with model-specific configuration...`
      );

      await this.huggingFaceLocal.loadModel({
        modelId: textModel,
        device: "wasm", // Use wasm for better compatibility
        dtype: "fp32", // Use fp32 as default - model-specific configs will take precedence
      });

      LoggingUtils.logInfo(
        `✅ [HFC] Model loaded successfully with optimized configuration`
      );
    } catch (err) {
      LoggingUtils.logError(`❌ [HFC] Failed to load model: ${err}`);

      // Log specific error types for debugging
      if (err instanceof Error) {
        if (err.message.includes("<!DOCTYPE")) {
          LoggingUtils.logError(
            `[HFC] HTML response detected - likely network/CDN issue`
          );
        } else if (err.message.includes("CORS")) {
          LoggingUtils.logError(`[HFC] CORS issue detected`);
        } else if (err.message.includes("fetch")) {
          LoggingUtils.logError(`[HFC] Network fetch issue detected`);
        }
      }

      throw new Error(
        `Failed to load model ${textModel}: ${
          err instanceof Error ? err.message : "Unknown error"
        }`
      );
    }

    // 3) inicializa o serviço de embeddings
    this.embeddingService = new HuggingFaceEmbeddingService();
    // (você pode chamar aqui this.embeddingService.initialize({ modelId: … }) se preciso)

    LoggingUtils.logInfo(
      `✅ [HFC] HuggingFace client initialized with text model ${textModel}`
    );
  }

  async loadApiKey(): Promise<string> {
    await this.loadConfiguration();
    return "huggingface-local";
  }

  private async loadConfiguration(): Promise<Record<string, any>> {
    const textModel = getOption(STORAGE_KEYS.HF_MODEL);
    const embedModel = getOption(STORAGE_KEYS.HF_EMBEDDING_MODEL);
    this.configuration = {
      defaultTextModel: textModel,
      defaultEmbeddingModel: embedModel,
    };
    return this.configuration;
  }

  async ensureClient(): Promise<boolean> {
    if (this.initialized) return true;
    if (this.initializing) {
      // Wait until current initialization finishes
      while (this.initializing) {
        await new Promise((res) => setTimeout(res, 100));
      }
      return this.initialized;
    }

    this.initializing = true;
    try {
      await this.loadConfiguration();
      await this.initializeHuggingFaceClient(this.configuration);
      this.initialized = true;
      return true;
    } catch (err) {
      LoggingUtils.logError(
        `❌ [HFC] Failed to ensure HuggingFace client: ${err}`
      );
      return false;
    } finally {
      this.initializing = false;
    }
  }

  isInitialized(): boolean {
    return this.initialized;
  }

  getClient(): HuggingFaceLocalService {
    if (!this.huggingFaceLocal) {
      throw new Error("HuggingFace client not initialized");
    }
    return this.huggingFaceLocal;
  }

  async createEmbedding(text: string): Promise<number[]> {
    await this.ensureClient();
    if (!this.embeddingService) {
      throw new Error("Embedding service not initialized");
    }
    return await this.embeddingService.createEmbedding(text);
  }

  async createEmbeddings(texts: string[]): Promise<number[][]> {
    await this.ensureClient();
    if (!this.embeddingService) {
      throw new Error("Embedding service not initialized");
    }
    return Promise.all(
      texts.map((t) => this.embeddingService!.createEmbedding(t))
    );
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// HuggingFaceCompletionService.ts
// Symbolic: Processamento de completions e function calling com HuggingFace local

import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { toHuggingFaceTools } from "../../../../../utils/hfToolUtils";
import { IClientManagementService } from "../../interfaces/openai/IClientManagementService";
import {
  ICompletionService,
  ModelStreamResponse,
} from "../../interfaces/openai/ICompletionService";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { cleanThinkTags } from "../../utils/ThinkTagCleaner";

/**
 * Serviço responsável por gerar completions com function calling usando HuggingFace
 * Symbolic: Neurônio especializado em processamento de texto local e chamadas de funções
 */
export class HuggingFaceCompletionService implements ICompletionService {
  constructor(private clientService: IClientManagementService) {}

  /**
   * Envia uma requisição ao modelo local HuggingFace com suporte a function calling
   * Symbolic: Processamento neural local para geração de texto ou execução de função
   */
  async callModelWithFunctions(options: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    tools?: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }>;
    tool_choice?: { type: string; function: { name: string } };
    temperature?: number;
    max_tokens?: number;
  }): Promise<{
    choices: Array<{
      message: {
        content?: string;
        tool_calls?: Array<{
          function: {
            name: string;
            arguments: string | Record<string, any>;
          };
        }>;
      };
    }>;
  }> {
    try {
      // Ensure HuggingFace client is available
      await this.clientService.ensureClient();

      // Convert messages to HuggingFace format
      const formattedMessages = options.messages.map((m) => ({
        role: m.role as "system" | "user" | "assistant",
        content: m.content,
      }));

      // Detect if the chosen model is a LOCAL vLLM model (desktop Electron)
      const selectedModel = getOption(STORAGE_KEYS.HF_MODEL) as string | null;
      const isLocal =
        !!selectedModel &&
        //   AVAILABLE_MODELS.some((m) => m.id === selectedModel) &&
        typeof window !== "undefined" &&
        (window as any).electronAPI?.vllmGenerate;

      if (isLocal) {
        // ---------- Local vLLM branch ----------
        const electronAPI = (window as any).electronAPI;
        // Wait until model is ready (max 30 seconds)
        for (let i = 0; i < 30; i++) {
          const statusRes = await electronAPI.vllmModelStatus();
          if (statusRes.success && statusRes.status?.state === "ready") break;
          await new Promise((res) => setTimeout(res, 1000));
          if (i === 29) {
            throw new Error("Local model not ready after 30s timeout");
          }
        }
        const payload: any = {
          model: selectedModel,
          messages: formattedMessages,
          temperature: options.temperature ?? 0.7,
          max_tokens: options.max_tokens ?? 500,
        };
        if (options.tools && options.tools.length)
          payload.tools = options.tools;
        if (options.tool_choice) payload.tool_choice = options.tool_choice;

        const genRes = await electronAPI.vllmGenerate(payload);
        if (!genRes.success) {
          throw new Error(genRes.error || "vLLM generation failed");
        }

        // Clean think tags from vLLM response
        const choices = genRes.data?.choices ?? [];
        const cleanedChoices = choices.map((choice: any) => ({
          ...choice,
          message: {
            ...choice.message,
            content: choice.message?.content
              ? cleanThinkTags(choice.message.content)
              : choice.message?.content,
          },
        }));

        return {
          choices: cleanedChoices,
        };
      }

      // ---------- Browser (transformers.js) branch ----------
      const hfTools = toHuggingFaceTools(options.tools);
      const hfService = (window as any).hfLocalService;

      if (!hfService) {
        throw new Error("HuggingFace local service not available");
      }

      const result = await hfService.generateWithFunctions(
        formattedMessages,
        hfTools,
        {
          temperature: options.temperature,
          maxTokens: options.max_tokens,
        }
      );

      // Clean think tags from browser response
      const cleanedContent = result.response
        ? cleanThinkTags(result.response)
        : result.response;

      return {
        choices: [
          {
            message: {
              content: result.tool_calls ? undefined : cleanedContent,
              tool_calls: result.tool_calls,
            },
          },
        ],
      };
    } catch (error) {
      // Log the error
      LoggingUtils.logError(
        `Error calling HuggingFace model: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
      console.error("Error in HuggingFace completion call:", error);
      throw error;
    }
  }

  /**
   * Envia requisição para o modelo HuggingFace e processa o stream de resposta
   * Symbolic: Fluxo neural contínuo de processamento de linguagem local
   */
  async streamModelResponse(
    messages: Array<{ role: string; content: string }>
  ): Promise<ModelStreamResponse> {
    try {
      // Ensure HuggingFace client is available
      await this.clientService.ensureClient();

      // Convert messages to HuggingFace format
      const formattedMessages = messages.map((m) => ({
        role: m.role as "system" | "user" | "assistant",
        content: m.content,
      }));

      const hfService = (window as any).hfLocalService;
      if (!hfService) {
        throw new Error("HuggingFace local service not available");
      }

      const response = await hfService.generateResponse(formattedMessages);

      // Clean think tags from streaming response
      const cleanedResponse = cleanThinkTags(response.response);

      return {
        responseText: cleanedResponse,
        messageId: Date.now().toString(),
        isComplete: true,
        isDone: true,
      };
    } catch (error) {
      LoggingUtils.logError(
        `Error streaming HuggingFace model response: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
      throw error;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// HuggingFaceServiceFacade.ts
// Symbolic: Fachada neural que integra e coordena diferentes serviços neurais especializados do HuggingFace

import { HuggingFaceNeuralSignalService } from "../../../../../infrastructure/neural/huggingface/HuggingFaceNeuralSignalService";
import { NeuralSignalResponse } from "../../interfaces/neural/NeuralSignalTypes";
import { ModelStreamResponse } from "../../interfaces/openai/ICompletionService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { Message } from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { cleanThinkTags } from "../../utils/ThinkTagCleaner";
import { HuggingFaceCompletionService } from "./HuggingFaceCompletionService";
import { HuggingFaceClientService } from "./neural/HuggingFaceClientService";

/**
 * Fachada que implementa IOpenAIService e coordena os serviços especializados do HuggingFace
 * Symbolic: Córtex de integração neural que combina neurônios especializados locais
 */
export class HuggingFaceServiceFacade implements IOpenAIService {
  private clientService: HuggingFaceClientService;
  private completionService: HuggingFaceCompletionService;
  private neuralSignalService: HuggingFaceNeuralSignalService | null = null;

  constructor(completionService: HuggingFaceCompletionService) {
    // Inicializar os serviços especializados
    this.clientService = new HuggingFaceClientService();
    this.completionService = completionService;
    this.neuralSignalService = new HuggingFaceNeuralSignalService(this);

    LoggingUtils.logInfo(
      "Initialized HuggingFace Service Facade with specialized neural services"
    );
  }

  /**
   * Inicializa o cliente HuggingFace
   * Symbolic: Estabelecimento de conexão neural com modelos locais
   */
  initializeOpenAI(apiKey: string): void {
    this.clientService.initializeClient(apiKey);
  }

  /**
   * Carrega a chave da API do HuggingFace do armazenamento
   * Symbolic: Recuperação de credencial neural
   */
  async loadApiKey(): Promise<void> {
    await this.clientService.loadApiKey();
  }

  /**
   * Garante que o cliente HuggingFace está disponível
   * Symbolic: Verificação de integridade do caminho neural
   */
  async ensureOpenAIClient(): Promise<boolean> {
    return this.clientService.ensureClient();
  }

  /**
   * Envia requisição para HuggingFace e processa o stream de resposta
   * Symbolic: Fluxo neural contínuo de processamento de linguagem
   */
  async streamOpenAIResponse(
    messages: Message[]
  ): Promise<ModelStreamResponse> {

    // Mapear as mensagens para o formato esperado pelo serviço de completion
    const mappedMessages = messages.map((m) => ({
      role: m.role,
      content: m.content,
    }));

    const response = await this.completionService.streamModelResponse(
      mappedMessages
    );

    // Clean think tags from the response
    const cleanedResponse = cleanThinkTags(response.responseText);

    return {
      ...response,
      responseText: cleanedResponse,
    };
  }

  /**
   * Cria embeddings para o texto fornecido
   * Symbolic: Transformação de texto em representação neural vetorial
   */
  async createEmbedding(text: string): Promise<number[]> {
    return this.clientService.createEmbedding(text);
  }

  /**
   * Cria embeddings para um lote de textos (processamento em batch)
   * Symbolic: Transformação em massa de textos em vetores neurais
   */
  async createEmbeddings(texts: string[]): Promise<number[][]> {
    return this.clientService.createEmbeddings(texts);
  }

  /**
   * Verifica se o cliente HuggingFace está inicializado
   * Symbolic: Consulta do estado de conexão neural
   */
  isInitialized(): boolean {
    return this.clientService.isInitialized();
  }

  /**
   * Gera sinais neurais simbólicos baseados em um prompt
   * Symbolic: Extração de padrões de ativação neural a partir de estímulo de linguagem
   */
  async generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse> {
    const neuralSignalService = await this.ensureNeuralSignalService();
    return neuralSignalService.generateNeuralSignal(
      prompt,
      temporaryContext,
      language
    );
  }

  /**
   * Expande semanticamente a query de um núcleo cerebral
   * Symbolic: Expansão de campo semântico para ativação cortical específica
   */
  async enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }> {
    const neuralSignalService = await this.ensureNeuralSignalService();
    return neuralSignalService.enrichSemanticQueryForSignal(
      core,
      query,
      intensity,
      context,
      language
    );
  }

  /**
   * Envia uma requisição ao HuggingFace com suporte a function calling
   * Symbolic: Processamento neural para geração de texto ou execução de função
   */
  async callOpenAIWithFunctions(options: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    tools?: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }>;
    tool_choice?: { type: string; function: { name: string } };
    temperature?: number;
    max_tokens?: number;
  }): Promise<{
    choices: Array<{
      message: {
        content?: string;
        tool_calls?: Array<{
          function: {
            name: string;
            arguments: string | Record<string, any>;
          };
        }>;
      };
    }>;
  }> {
    return await this.completionService.callModelWithFunctions(options);
  }

  /**
   * Generate response using HuggingFace backend
   * Symbolic: Neural text generation through local models
   */
  async generateResponse(messages: Message[]): Promise<{ response: string }> {
    try {
      const streamResponse = await this.streamOpenAIResponse(messages);
      return { response: streamResponse.responseText };
    } catch (error) {
      LoggingUtils.logError("Error generating HuggingFace response", error);
      return {
        response: "Error: Failed to generate response with HuggingFace",
      };
    }
  }

  /**
   * Ensures neural signal service is initialized
   * Symbolic: Lazy initialization of neural signal pathway
   */
  private async ensureNeuralSignalService(): Promise<HuggingFaceNeuralSignalService> {
    if (!this.neuralSignalService) {
      // Ensure client is initialized first
      await this.ensureOpenAIClient();
      const client = this.clientService.getClient();
      this.neuralSignalService = new HuggingFaceNeuralSignalService(this);
    }
    return this.neuralSignalService;
  }

  /**
   * Get available models (placeholder for HuggingFace models)
   */
  async getAvailableModels(): Promise<string[]> {
    // Return a list of supported HuggingFace models
    return [
      "microsoft/DialoGPT-medium",
      "microsoft/DialoGPT-large",
      "facebook/blenderbot-400M-distill",
      "facebook/blenderbot-1B-distill",
    ];
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// hashUtils.ts
// Utility for synchronous/async hash SHA-256 cross-platform

export async function sha256(text: string): Promise<string> {
  if (typeof window !== 'undefined' && window.crypto?.subtle) {
    // Browser/renderer
    const encoder = new TextEncoder();
    const data = encoder.encode(text);
    const hashBuffer = await window.crypto.subtle.digest('SHA-256', data);
    return Array.from(new Uint8Array(hashBuffer)).map(b => b.toString(16).padStart(2, '0')).join('');
  } else {
    // Node.js
    const { createHash } = await import('crypto');
    return createHash('sha256').update(text).digest('hex');
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// utils/namespace.ts
// Utility to normalize namespaces from speaker names

/**
 * Normalizes a namespace for consistent usage throughout the application.
 * Example: "João da Silva" -> "joao-da-silva"
 */
export function normalizeNamespace(speaker: string): string {
  return speaker
    .toLowerCase()
    .normalize('NFD').replace(/\p{Diacritic}/gu, '') // remove accents
    .replace(/[^a-z0-9]+/g, '-') // replace non-alphanumeric with hyphen
    .replace(/^-+|-+$/g, '') // remove hyphens at the ends
    .replace(/-{2,}/g, '-') // double hyphens
    || 'default';
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// tokenUtils.ts
// Utility for chunking/tokenization compatible with OpenAI using gpt-tokenizer (cognitive brain memory encoding)
// gpt-tokenizer is a pure JavaScript implementation with no WASM dependencies (brain-friendly)

// Import the gpt-tokenizer library, a pure JS alternative to tiktoken (for brain memory chunking)
import { encode as gptEncode, decode as gptDecode } from "gpt-tokenizer";

// Types modified for compatibility with gpt-tokenizer (brain encoding)
export type Encoder = {
  encode: (t: string) => number[];
  decode: (arr: number[] | Uint32Array) => string;
};

type EncodingForModelFn = (model: string) => Encoder;

// Implementation of encoding_for_model using gpt-tokenizer (cognitive encoding selection)
// eslint-disable-next-line @typescript-eslint/no-unused-vars
const encoding_for_model: EncodingForModelFn = (model: string) => ({
  encode: (t: string): number[] => {
    try {
      // gptEncode automatically selects the correct encoding based on the model (brain model adaptation)
      // Returns an array of numbers representing tokens (brain token stream)
      return gptEncode(t);
    } catch {
      // Fallback to character-based estimation (cognitive fallback)
      // Explicit conversion to number[] to satisfy type (brain safety)
      return t.split(/\s+/).map(() => 0);
    }
  },
  decode: (arr: Uint32Array | number[]): string => {
    try {
      return gptDecode(arr);
    } catch {
      if (Array.isArray(arr)) return arr.join(" ");
      if (arr instanceof Uint32Array) return Array.from(arr).join(" ");
      return String(arr);
    }
  },
});


// Allows multiple encoders per model (brain model flexibility)
const encoderCache: Record<string, Encoder> = {};

export function getEncoderForModel(model: string): Encoder {
  if (!encoderCache[model]) {
    try {
      encoderCache[model] = encoding_for_model(model);
    } catch {
      // fallback for test/build environments - uses safe implementation with gpt-tokenizer (brain test mode)
      encoderCache[model] = {
        encode: (t: string): number[] => {
          try {
            return gptEncode(t);
          } catch {
            // Explicit conversion to number[] to satisfy type (brain safety)
            return t.split(/\s+/).map(() => 0);
          }
        },
        decode: (arr: Uint32Array | number[]): string => {
          try {
            return gptDecode(arr as number[]);
          } catch {
            if (Array.isArray(arr)) return arr.join(" ");
            if (arr instanceof Uint32Array) return Array.from(arr).join(" ");
            return String(arr);
          }
        },
      };
    }
  }
  return encoderCache[model];
}

export function splitIntoChunksWithEncoder(
  text: string,
  chunkSize: number,
  encoder: Encoder
): string[] {
  try {
    const tokens = encoder.encode(text);
    const chunks: string[] = [];
    // Nota: Agora trabalhamos com arrays de number[] em vez de string[]
    let currentChunk: number[] = [];
    let chunkTokenCount = 0;
    
    for (let i = 0; i < tokens.length; i++) {
      chunkTokenCount++;
      currentChunk.push(tokens[i]);
      
      if (chunkTokenCount >= chunkSize) {
        chunks.push(encoder.decode(currentChunk));
        chunkTokenCount = 0;
        currentChunk = [];
      }
    }
    
    // last partial chunk (brain memory tail)
    if (currentChunk.length > 0) {
      chunks.push(encoder.decode(currentChunk));
    }
    
    return chunks;
  } catch (error) {
    console.warn("⚠️ [COGNITIVE-CHUNKING] Error splitting text into cognitive chunks:", error);
    
    // fallback to whitespace-based chunking (cognitive fallback)
    return text.split(/\s+/).reduce((chunks: string[], word, i) => {
      const chunkIndex = Math.floor(i / chunkSize);
      if (!chunks[chunkIndex]) chunks[chunkIndex] = "";
      chunks[chunkIndex] += (chunks[chunkIndex] ? " " : "") + word;
      return chunks;
    }, []);
  }
}

export function countTokensWithEncoder(
  text: string,
  encoder: Encoder
): number {
  try {
    const tokens = encoder.encode(text);
    return tokens.length;
  } catch (error) {
    console.warn("⚠️ [COGNITIVE-TOKENS] Error counting tokens in brain encoder:", error);
    // Fallback to character-based estimation (cognitive fallback)
    return Math.ceil(text.length / 3.5);
  }
}


// Direct function using gpt-tokenizer to count tokens (brain token diagnostics)
// For text-embedding-large, the maximum is 8191 tokens (brain memory constraint)
export function countTokens(text: string, model: string = "text-embedding-3-large"): number {
  if (!text) return 0;
  
  try {
    // gpt-tokenizer does not support specifying the model directly, uses cl100k by default (brain default model)
    // which is compatible with GPT-3.5/4 and OpenAI embedding models (brain compatibility)
    const tokens = gptEncode(text);
    return tokens.length;
  } catch (error) {
    console.warn(`[COGNITIVE-TOKENS] Error counting tokens for brain model ${model}:`, error);
    
    // Fallbacks diferentes dependendo do modelo
    if (model.includes("embedding")) {
      // For embedding models, approximately 4 characters per token (brain heuristic)
      return Math.ceil(text.length / 4);
    } else {
      // For LLMs, approximately 3.5 characters per token (brain heuristic)
      return Math.ceil(text.length / 3.5);
    }
  }
}

export function splitIntoChunks(text: string, chunkSize: number): string[] {
  try {
    return splitIntoChunksWithEncoder(text, chunkSize, getEncoderForModel("text-embedding-3-large"));
  } catch (error) {
    console.warn("⚠️ [COGNITIVE-CHUNKING] Error splitting text into cognitive chunks:", error);
    // Simple fallback based on characters - each chunk will have approximately chunkSize * 4 characters (cognitive fallback)
    const approxCharSize = chunkSize * 4;
    const result: string[] = [];
    
    for (let i = 0; i < text.length; i += approxCharSize) {
      result.push(text.slice(i, i + approxCharSize));
    }
    
    return result;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { createHash } from 'crypto';

// Defining mocks for the test context
const mockSend = jest.fn();
const mockSaveToPinecone = jest.fn().mockResolvedValue({ success: true });
const mockIsDestroyed = jest.fn().mockReturnValue(false);

// Type to simulate the Electron event
interface MockElectronEvent {
  sender: {
    send: jest.Mock;
    isDestroyed: jest.Mock;
  };
}

// Mock of dependencies for the IPC handler
const mockDeps = {
  pineconeHelper: {
    saveToPinecone: mockSaveToPinecone
  },
  openAIService: {
    createEmbeddings: jest.fn().mockResolvedValue([])
  }
};

// Interface for a Pinecone vector
interface PineconeVector {
  id: string;
  values: number[];
  metadata: Record<string, string | number | boolean>;
}

// Note: The parseChatGPTExport function previously imported from ConversationImportService
// was removed as this functionality is now implemented directly in importChatGPTHandler.ts

// Helper functions to be tested
const normalizeVector = (vector: number[]): number[] => {
  const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));
  if (magnitude === 0) return vector.map(() => 0);
  return vector.map(val => val / magnitude);
};

const splitIntoChunks = (text: string, chunkSize: number): string[] => {
  const chunks: string[] = [];
  const avgCharsPerToken = 4; // Estimativa para português/inglês
  const charChunkSize = chunkSize * avgCharsPerToken;
  
  for (let i = 0; i < text.length; i += charChunkSize) {
    chunks.push(text.substring(i, i + charChunkSize));
  }
  
  return chunks;
};

// Function to process a batch of vectors
const processBatch = async (
  batchToProcess: PineconeVector[], 
  deps: typeof mockDeps,
  event: MockElectronEvent,
  processedMessageIndices: Set<number>,
  processedChunks: number,
  total: number
): Promise<{ processedMessages: number, processedChunks: number }> => {
  if (batchToProcess.length === 0) return { processedMessages: processedMessageIndices.size, processedChunks };
  
  try {
    if (deps.pineconeHelper) {
      await deps.pineconeHelper.saveToPinecone(batchToProcess);
      processedChunks += batchToProcess.length;
      
      // Register processed message indices
      batchToProcess.forEach(item => {
        if (typeof item.metadata.messageIndex === 'number') {
          processedMessageIndices.add(item.metadata.messageIndex as number);
        }
      });
      
      const processedMessages = processedMessageIndices.size;
      const progressPercent = Math.round((processedMessages / total) * 100);
      
      // Report progress via event
      if (!event.sender.isDestroyed()) {
        event.sender.send('import-progress', { 
          processed: processedMessages, 
          total: total,
          chunks: processedChunks,
          percent: progressPercent
        });
      }
      
      return { processedMessages, processedChunks };
    } else {
      throw new Error("Pinecone helper não está disponível");
    }
  } catch (error) {
    console.error("Erro ao salvar lote no Pinecone:", error);
    throw error;
  }
};

describe('ChatGPT Import with Chunking', () => {
  beforeEach(() => {
    // Clear mocks before each test
    mockSend.mockClear();
    mockSaveToPinecone.mockClear();
    mockIsDestroyed.mockClear();
  });
  
  it('should correctly split a long message into chunks', () => {
    // Mensagem que excede o tamanho de chunk
    const longText = 'A'.repeat(10000); // 10.000 caracteres (aprox. 2500 tokens)
    const CHUNK_SIZE = 1000; // 1000 tokens por chunk
    
    const chunks = splitIntoChunks(longText, CHUNK_SIZE);
    
    // Deve criar 3 chunks (considerando estimativa de 4 chars/token)
    expect(chunks.length).toBeGreaterThan(1);
    expect(chunks[0].length).toBeLessThanOrEqual(CHUNK_SIZE * 4);
  });
  
  it('should track progress correctly while processing chunks', async () => {
    // Create mock of Electron event
    const mockEvent: MockElectronEvent = {
      sender: {
        send: mockSend,
        isDestroyed: mockIsDestroyed
      }
    };
    
    // Prepare test data
    const processedMessageIndices = new Set<number>();
    let processedChunks = 0;
    const total = 5; // Total of messages
    
    // Create multiple batches of vectors simulating message chunks
    const batches = [
      // Batch 1: Chunks of messages 0 and 1
      [
        {
          id: `msg-0-chunk-1`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 0, part: "1/2", content: "Parte 1" } as Record<string, string | number | boolean>
        },
        {
          id: `msg-0-chunk-2`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 0, part: "2/2", content: "Parte 2" } as Record<string, string | number | boolean>
        },
        {
          id: `msg-1-chunk-1`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 1, part: "1/1", content: "Mensagem única" } as Record<string, string | number | boolean>
        }
      ],
      
      // Batch 2: Message 2
      [
        {
          id: `msg-2-chunk-1`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 2, part: "1/1", content: "Outra mensagem" } as Record<string, string | number | boolean>
        }
      ],
      
      // Batch 3: Messages 3 and 4 (with multiple chunks)
      [
        {
          id: `msg-3-chunk-1`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 3, part: "1/3", content: "Chunk 1" } as Record<string, string | number | boolean>
        },
        {
          id: `msg-3-chunk-2`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 3, part: "2/3", content: "Chunk 2" } as Record<string, string | number | boolean>
        },
        {
          id: `msg-3-chunk-3`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 3, part: "3/3", content: "Chunk 3" } as Record<string, string | number | boolean>
        },
        {
          id: `msg-4-chunk-1`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 4, part: "1/2", content: "Último chunk 1" } as Record<string, string | number | boolean>
        },
        {
          id: `msg-4-chunk-2`,
          values: Array(1536).fill(0.1),
          metadata: { messageIndex: 4, part: "2/2", content: "Último chunk 2" } as Record<string, string | number | boolean>
        }
      ]
    ];
    
    // Process each batch and verify progress
    const results = [];
    for (const batch of batches) {
      const result = await processBatch(
        batch, 
        mockDeps, 
        mockEvent, 
        processedMessageIndices, 
        processedChunks, 
        total
      );
      
      processedChunks = result.processedChunks;
      results.push({
        messagesProcessed: result.processedMessages,
        chunksProcessed: result.processedChunks,
        expectedProgress: Math.round((result.processedMessages / total) * 100)
      });
    }
    
    // Verifications
    expect(results.length).toBe(3); // Processed 3 batches
    
    // After processing, verify the final state
    expect(mockSaveToPinecone).toHaveBeenCalledTimes(3); // One per batch
    expect(processedMessageIndices.size).toBe(5); // Processed all 5 messages
    expect(processedChunks).toBe(9); // Total of chunks in all batches
    
    // Verify if progress was reported correctly
    expect(mockSend).toHaveBeenCalledTimes(3); // One per batch
    
    // Verify the last progress event
    const lastProgressCall = mockSend.mock.calls[2][1];
    expect(lastProgressCall.processed).toBe(5);
    expect(lastProgressCall.total).toBe(5);
    expect(lastProgressCall.chunks).toBe(9);
    expect(lastProgressCall.percent).toBe(100);
  });
  
  it('should calculate correct metadata for chunks', () => {
    // Message simulation
    const messages = [
      { role: 'user', content: 'A'.repeat(8000), timestamp: new Date().toISOString() },
      { role: 'assistant', content: 'B'.repeat(2000), timestamp: new Date().toISOString() }
    ];
    
    const CHUNK_SIZE = 1000;
    const MAX_CONTENT_LENGTH = 40000;
    const vectorBatch: PineconeVector[] = [];
    
    // Process messages similar to the main handler
    messages.forEach((message, i) => {
      // Generate hash for deduplication
      const hash = createHash('sha256').update(message.content).digest('hex');
      
      // Split content into chunks if it's large
      const contentChunks = splitIntoChunks(message.content, CHUNK_SIZE);
      
      // Create a vector for each chunk
      for (let chunkIndex = 0; chunkIndex < contentChunks.length; chunkIndex++) {
        const chunkContent = contentChunks[chunkIndex];
        // Use empty string instead of null for Pinecone compatibility
        const partInfo = contentChunks.length > 1 ? `${chunkIndex + 1}/${contentChunks.length}` : "";
        
        // Generate unique ID for the vector
        const vectorId = `chatgpt-${Date.now()}-${i}-${chunkIndex}-${hash.substring(0, 8)}`;
        
        // Create dummy vector for testing
        const dummyVector = Array(1536).fill(0.1);
        const normalizedVector = normalizeVector(dummyVector);
        
        // Garantir que o conteúdo não exceda o limite do Pinecone
        const truncatedContent = chunkContent.length > MAX_CONTENT_LENGTH
          ? chunkContent.substring(0, MAX_CONTENT_LENGTH - 3) + '...'
          : chunkContent;
        
        // Adicionar ao lote
        vectorBatch.push({
          id: vectorId,
          values: normalizedVector,
          metadata: {
            role: message.role,
            content: truncatedContent,
            timestamp: message.timestamp,
            source: "chatgpt_import",
            user: "test_user",
            hash: hash,
            messageIndex: i,
            part: partInfo,
            order: i * 1000 + chunkIndex // Preserva a ordem original
          }
        });
      }
    });
    
    // Verifications
    expect(vectorBatch.length).toBeGreaterThan(2); // Should have more chunks than original messages
    
    // Verify specific metadata
    const firstMessageChunks = vectorBatch.filter(v => v.metadata.messageIndex === 0);
    
    // The first message should have been split into multiple chunks
    expect(firstMessageChunks.length).toBeGreaterThan(1);
    expect(firstMessageChunks[0].metadata.part).toBe("1/" + firstMessageChunks.length);
    
    // Verify ordering
    expect(firstMessageChunks[0].metadata.order).toBe(0); // 0 * 1000 + 0
    if (firstMessageChunks.length > 1) {
      expect(firstMessageChunks[1].metadata.order).toBe(1); // 0 * 1000 + 1
    }
    
    // Verify truncated content if necessary
    firstMessageChunks.forEach(chunk => {
      expect((chunk.metadata.content as string).length).toBeLessThanOrEqual(MAX_CONTENT_LENGTH);
    });
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ConversationHistoryManager } from './ConversationHistoryManager';
import { Message } from '../../interfaces/transcription/TranscriptionTypes';

describe('ConversationHistoryManager', () => {
  const systemMessage: Message = {
    role: 'developer',
    content: 'Bem-vindo!'
  };

  it('should initialize with system message', () => {
    const manager = new ConversationHistoryManager(systemMessage);
    expect(manager.getHistory()).toEqual([systemMessage]);
  });

  it('should add messages and prune history when exceeding maxInteractions', () => {
    const manager = new ConversationHistoryManager(systemMessage);
    manager.setMaxInteractions(2);
    for (let i = 0; i < 10; i++) {
      manager.addMessage({ role: 'user', content: `Mensagem ${i}` });
    }
    const history = manager.getHistory();
    // Deve conter apenas o systemMessage + 4 mensagens (2*2)
    expect(history.length).toBe(5);
    expect(history[0]).toEqual(systemMessage);
    expect(history[1].content).toBe('Mensagem 6');
    expect(history[4].content).toBe('Mensagem 9');
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ConversationHistoryManager.ts
// Implementation of IConversationHistoryManager (cognitive history orchestrator)

import { IConversationHistoryManager } from "../../interfaces/memory/IConversationHistoryManager";
import { Message } from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";

export class ConversationHistoryManager implements IConversationHistoryManager {
  private conversationHistory: Message[];
  private maxInteractions: number = 10;
  
  constructor(systemMessage: Message) {
    this.conversationHistory = [systemMessage];
  }
  
  /**
   * Adds a message to the conversation history and prunes if necessary (cognitive history management)
   */
  addMessage(message: Message): void {
    this.conversationHistory.push(message);
    this.pruneHistory();
  }
  
  /**
   * Gets the current conversation history (cognitive memory trace)
   */
  getHistory(): Message[] {
    return [...this.conversationHistory];
  }
  
  /**
   * Clears the conversation history but keeps the system message (orchestrator memory reset, preserve identity)
   */
  clearHistory(): void {
    const systemMessage = this.conversationHistory[0];
    this.conversationHistory = [systemMessage];
  }
  
  /**
   * Sets the maximum number of interactions to keep (cognitive memory span)
   */
  setMaxInteractions(max: number): void {
    this.maxInteractions = max;
  }
  
  /**
   * Prunes conversation history to maintain the maximum allowed interactions (cognitive pruning)
   */
  private pruneHistory(): void {
    const systemMessage = this.conversationHistory[0];
    
    if (this.conversationHistory.length > (this.maxInteractions * 2) + 1) {
      this.conversationHistory = [
        systemMessage,
        ...this.conversationHistory.slice(-(this.maxInteractions * 2))
      ];
      LoggingUtils.logInfo(`[COGNITIVE-HISTORY] History pruned to ${this.conversationHistory.length} messages (cognitive pruning)`);
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// DuckDBMemoryService.ts
// Implementation of IPersistenceService using DuckDB for vector storage

import { IPersistenceService } from "../../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../../interfaces/openai/IEmbeddingService";
import { SpeakerTranscription } from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { countTokens } from "./utils/tokenUtils";

// Normaliza keywords para lowercase e remove espaços extras
function normalizeKeywords(keywords: string[] = []): string[] {
  return keywords.map((k) => k.trim().toLowerCase()).filter(Boolean);
}

export class DuckDBMemoryService implements IPersistenceService {
  private embeddingService: IEmbeddingService;
  // Set that keeps track of processed transcription indices per speaker (brain memory index)
  private processedTranscriptionIndices: Record<string, Set<number>> = {};

  // Buffer to temporarily store messages before sending to DuckDB (cognitive buffer)
  private messageBuffer: {
    primaryUser: {
      messages: string[];
      lastUpdated: number;
    };
    external: Record<
      string,
      {
        messages: string[];
        lastUpdated: number;
      }
    >;
    lastFlushTime: number;
  } = {
    primaryUser: {
      messages: [],
      lastUpdated: Date.now(),
    },
    external: {},
    lastFlushTime: Date.now(),
  };

  // Buffer configuration (cognitive buffer tuning)
  private bufferConfig = {
    maxBufferAgeMs: 5 * 60 * 1000, // 5 minutes
    inactivityThresholdMs: 5 * 60 * 1000, // 5 minutes de inatividade força um flush
    minTokensBeforeFlush: 100, // Minimum tokens before considering flush
    maxTokensBeforeFlush: 150, // Maximum token limit
  };

  constructor(embeddingService: IEmbeddingService) {
    this.embeddingService = embeddingService;

    LoggingUtils.logInfo(
      `[MEMORY-SERVICE] Initialized DuckDB memory service (unified local storage)`
    );

    // Reset buffer state
    this.resetBuffer();
  }

  /**
   * Saves interaction to long-term memory in DuckDB
   */
  async saveInteraction(
    question: string,
    answer: string,
    speakerTranscriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Promise<void> {
    if (!this.isAvailable() || !this.embeddingService.isInitialized()) {
      return;
    }

    try {
      // Identify new transcriptions per speaker (brain memory update)
      const newTranscriptions: SpeakerTranscription[] = [];

      // Filter only transcriptions that have not been processed yet (memory deduplication)
      for (let i = 0; i < speakerTranscriptions.length; i++) {
        const transcription = speakerTranscriptions[i];
        const { speaker } = transcription;

        // Initialize index set for this speaker (memory index init)
        if (!this.processedTranscriptionIndices[speaker]) {
          this.processedTranscriptionIndices[speaker] = new Set<number>();
        }

        // Add only new transcriptions (not previously processed) (brain memory growth)
        if (!this.processedTranscriptionIndices[speaker].has(i)) {
          newTranscriptions.push(transcription);
          // Marcar como processada para futuras chamadas
          this.processedTranscriptionIndices[speaker].add(i);
          LoggingUtils.logInfo(
            `[COGNITIVE-MEMORY] New transcription for speaker ${speaker}: ${transcription.text.substring(
              0,
              30
            )}...`
          );
        }
      }

      // If there are no new transcriptions and no question or answer, do nothing (no brain update required)
      if (
        newTranscriptions.length === 0 &&
        !question.trim() &&
        !answer.trim()
      ) {
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] No new content to add to cognitive buffer`
        );
        return;
      }

      // We do not store the question in the buffer, following the original flow (direct brain query)
      // The question will be processed directly at flush time (on-demand brain query)

      if (newTranscriptions.length > 0) {
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] Adding ${newTranscriptions.length} new transcriptions to cognitive buffer`
        );

        // Group ONLY new transcriptions by speaker (brain memory organization)
        const speakerMessages = this.groupTranscriptionsBySpeaker(
          newTranscriptions,
          primaryUserSpeaker
        );

        // Process grouped messages by speaker and add to buffer (brain memory buffer fill)
        for (const [speaker, messages] of speakerMessages.entries()) {
          // Skip if no messages (no brain update required)
          if (messages.length === 0) continue;

          const isUser = speaker === primaryUserSpeaker;

          if (isUser) {
            // Add primary user's messages to buffer (brain memory consolidation)
            this.messageBuffer.primaryUser.messages.push(...messages);
            this.messageBuffer.primaryUser.lastUpdated = Date.now();
            const currentTokens = this.countBufferTokens();
            LoggingUtils.logInfo(
              `[COGNITIVE-BUFFER] Added ${messages.length} messages to primary user's buffer. Total tokens: ${currentTokens}/${this.bufferConfig.maxTokensBeforeFlush}`
            );
          } else {
            // Initialize buffer for external speaker if it does not exist (brain buffer expansion)
            if (!this.messageBuffer.external[speaker]) {
              this.messageBuffer.external[speaker] = {
                messages: [],
                lastUpdated: Date.now(),
              };
            }

            // Add external speaker's messages to buffer (brain memory expansion)
            this.messageBuffer.external[speaker].messages.push(...messages);
            this.messageBuffer.external[speaker].lastUpdated = Date.now();
            const currentTokens = this.countBufferTokens();
            LoggingUtils.logInfo(
              `[COGNITIVE-BUFFER] Added ${messages.length} messages to buffer for speaker ${speaker}. Total tokens: ${currentTokens}/${this.bufferConfig.maxTokensBeforeFlush}`
            );
          }
        }
      }

      // Check if we should flush ONLY based on token limit (brain flush threshold)
      const shouldFlush = this.shouldFlushBuffer();

      if (shouldFlush) {
        // If buffer reached token limit, save everything including user's messages (cognitive flush)
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] Auto-flushing cognitive buffer due to token limit`
        );
        await this.flushBuffer(
          answer.trim() ? answer : null,
          primaryUserSpeaker,
          true
        );
      } else if (answer.trim()) {
        // If we have an assistant response but buffer is not full,
        // save ONLY the response (without user's messages), so we don't lose the response (brain response preservation)
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] Saving only assistant's response, retaining buffer state`
        );

        // Create a vector entry only for the response, without touching the buffer (direct brain memory insert)
        await this.saveAssistantResponseOnly(answer);
      }
    } catch (error) {
      LoggingUtils.logError(
        "[COGNITIVE-BUFFER] Error processing interaction for cognitive buffer",
        error
      );
    }
  }

  /**
   * Checks if the memory service is available (DuckDB)
   */
  isAvailable(): boolean {
    // Always check for DuckDB services
    return (
      !!window.electronAPI?.saveToDuckDB && !!window.electronAPI.queryDuckDB
    );
  }

  /**
   * Creates a vector entry for DuckDB
   */
  createVectorEntry(
    id: string,
    embedding: number[],
    metadata: Record<string, unknown>
  ): { id: string; values: number[]; metadata: Record<string, unknown> } {
    return {
      id,
      values: embedding,
      metadata,
    };
  }

  /**
   * Queries DuckDB memory store for relevant memory
   */
  async queryMemory(
    embedding: number[],
    topK: number = 5,
    keywords: string[] = [],
    filters?: Record<string, unknown>
  ): Promise<string> {
    if (!this.isAvailable() || !embedding?.length) {
      return "";
    }
    try {
      // Log filters for debug (brain query diagnostics)
      if (filters) {
        LoggingUtils.logInfo(
          `[MemoryService] filters: ${JSON.stringify(filters)}`
        );
      }

      // Always query DuckDB via IPC (local memory)
      LoggingUtils.logInfo(`[MEMORY] Querying DuckDB local vector store`);
      const queryResponse = await window.electronAPI.queryDuckDB(
        embedding,
        topK,
        normalizeKeywords(keywords),
        filters
        // Using dynamic threshold - system will choose optimal value based on context
      );

      // Extract relevant texts from results (brain memory retrieval)
      const relevantTexts = queryResponse.matches
        .filter(
          (match: { metadata?: { content?: string } }) =>
            match.metadata && match.metadata.content
        )
        .map(
          (match: { metadata?: { content?: string } }) =>
            match.metadata?.content as string
        )
        .join("\n\n");

      if (relevantTexts) {
        LoggingUtils.logInfo(
          `[COGNITIVE-MEMORY] Relevant context retrieved via DuckDB`
        );
      }

      return relevantTexts;
    } catch (error) {
      LoggingUtils.logError(
        `[COGNITIVE-MEMORY] Error querying DuckDB memory`,
        error
      );
      return "";
    }
  }

  /**
   * Checks if the buffer should be persisted based ONLY on token limit (brain flush threshold)
   */
  private shouldFlushBuffer(): boolean {
    // Calculate total number of messages in buffer (for diagnostics)
    const totalUserMessages = this.messageBuffer.primaryUser.messages.length;
    const totalExternalMessages = Object.values(
      this.messageBuffer.external
    ).reduce((sum, speaker) => sum + speaker.messages.length, 0);
    const totalMessages = totalUserMessages + totalExternalMessages;

    // Check total number of tokens in buffer (brain load check)
    const totalTokens = this.countBufferTokens();

    // Detailed log to better understand buffer behavior (cognitive diagnostics)
    LoggingUtils.logInfo(
      `[COGNITIVE-BUFFER] Current status: ${totalTokens}/${this.bufferConfig.maxTokensBeforeFlush} tokens, ${totalMessages} total messages (${totalUserMessages} user, ${totalExternalMessages} external)`
    );

    // If minimum token threshold not reached, do not flush (brain conservation)
    if (totalTokens < this.bufferConfig.minTokensBeforeFlush) {
      LoggingUtils.logInfo(
        `[COGNITIVE-BUFFER] Minimum token threshold not reached (${totalTokens}/${this.bufferConfig.minTokensBeforeFlush})`
      );
      return false;
    }

    // If maximum token limit exceeded, flush (brain overflow)
    if (totalTokens >= this.bufferConfig.maxTokensBeforeFlush) {
      LoggingUtils.logInfo(
        `[COGNITIVE-BUFFER] Token limit exceeded (${totalTokens}/${this.bufferConfig.maxTokensBeforeFlush})`
      );
      return true;
    }

    // If here, between min and max, depends only on max limit (brain threshold logic)
    return false;
  }

  /**
   * Persists the buffer content in DuckDB and clears the buffer (neural persistence/flush)
   */
  private async flushBuffer(
    answer: string | null,
    primaryUserSpeaker: string,
    resetBufferAfterFlush: boolean = true
  ): Promise<void> {
    if (!this.isAvailable() || !this.embeddingService.isInitialized()) {
      LoggingUtils.logWarning(
        `[COGNITIVE-BUFFER] DuckDB service unavailable, flush aborted`
      );
      return;
    }

    try {
      const now = Date.now();
      const uuid = now.toString();
      const duckdbEntries = [] as Array<{
        id: string;
        values: number[];
        metadata: Record<string, unknown>;
      }>;

      // Processar mensagens do usuário principal se houver
      if (this.messageBuffer.primaryUser.messages.length > 0) {
        const userMessages = this.messageBuffer.primaryUser.messages;
        const completeUserMessage = userMessages.join("\n");

        LoggingUtils.logInfo(
          `[Buffer] Criando embedding para ${
            userMessages.length
          } mensagens do usuário principal: "${completeUserMessage.substring(
            0,
            50
          )}${completeUserMessage.length > 50 ? "..." : ""}"`
        );
        const userEmbedding = await this.embeddingService.createEmbedding(
          completeUserMessage
        );

        duckdbEntries.push(
          this.createVectorEntry(
            `speaker-${uuid}-${primaryUserSpeaker}`,
            userEmbedding,
            {
              type: "complete_message",
              content: completeUserMessage,
              source: "user",
              speakerName: primaryUserSpeaker,
              speakerGroup: primaryUserSpeaker,
              isSpeaker: true,
              isUser: true,
              messageCount: userMessages.length,
              timestamp: new Date().toISOString(),
              bufferCreatedAt: new Date(
                this.messageBuffer.primaryUser.lastUpdated
              ).toISOString(),
              bufferFlushedAt: new Date(now).toISOString(),
            }
          )
        );
      }

      // Processar mensagens de cada falante externo
      for (const [speaker, data] of Object.entries(
        this.messageBuffer.external
      )) {
        if (data.messages.length === 0) continue;

        const externalMessages = data.messages;
        const completeExternalMessage = externalMessages.join("\n");

        LoggingUtils.logInfo(
          `[Buffer] Criando embedding para ${externalMessages.length} mensagens do falante ${speaker}`
        );
        const externalEmbedding = await this.embeddingService.createEmbedding(
          completeExternalMessage
        );

        duckdbEntries.push(
          this.createVectorEntry(
            `speaker-${uuid}-${speaker}`,
            externalEmbedding,
            {
              type: "complete_message",
              content: completeExternalMessage,
              source: "external",
              speakerName: speaker,
              speakerGroup: "external",
              isSpeaker: true,
              isUser: false,
              messageCount: externalMessages.length,
              timestamp: new Date().toISOString(),
              bufferCreatedAt: new Date(data.lastUpdated).toISOString(),
              bufferFlushedAt: new Date(now).toISOString(),
            }
          )
        );
      }

      // Adicionar resposta se fornecida
      if (answer) {
        LoggingUtils.logInfo(
          `[Buffer] Adicionando resposta ao salvar no DuckDB`
        );
        const answerEmbed = await this.embeddingService.createEmbedding(answer);

        duckdbEntries.push(
          this.createVectorEntry(`a-${uuid}`, answerEmbed, {
            type: "assistant_response",
            content: answer,
            source: "assistant",
            speakerName: "assistant",
            speakerGroup: "assistant",
            isSpeaker: false,
            isUser: false,
            timestamp: new Date().toISOString(),
            bufferFlushedAt: new Date(now).toISOString(),
          })
        );
      }

      // Verificar se há entradas para salvar
      if (duckdbEntries.length > 0) {
        // Always save to DuckDB via IPC
        const result = await window.electronAPI?.saveToDuckDB(duckdbEntries);
        if (result?.success) {
          LoggingUtils.logInfo(
            `[Buffer] Persistido no DuckDB: ${duckdbEntries.length} entradas`
          );
        } else {
          LoggingUtils.logError(
            `[Buffer] Erro ao persistir no DuckDB: ${result?.error}`
          );
        }

        // Atualizar timestamp do último flush
        this.messageBuffer.lastFlushTime = now;

        // Limpar o buffer apenas se necessário
        if (resetBufferAfterFlush) {
          LoggingUtils.logInfo(`[Buffer] Resetando buffer após flush`);
          this.resetBuffer();
        } else {
          LoggingUtils.logInfo(
            `[Buffer] Mantendo buffer após salvar resposta do assistente`
          );
        }
      } else {
        LoggingUtils.logInfo(`[Buffer] Nenhuma entrada para salvar no DuckDB`);
      }
    } catch (error) {
      LoggingUtils.logError(
        "[Buffer] Erro ao persistir buffer no DuckDB",
        error
      );
    }
  }

  /**
   * Clears the buffer after persistence (brain buffer reset)
   */
  private resetBuffer(): void {
    this.messageBuffer.primaryUser.messages = [];
    this.messageBuffer.external = {};
    // Keeps lastFlushTime for flush interval control (brain timing)

    LoggingUtils.logInfo(
      `[COGNITIVE-BUFFER] Cognitive buffer reset after neural persistence`
    );
  }

  /**
   * Saves only the assistant's response without touching the buffer (direct brain response persistence)
   * @param answer Assistant response
   */
  private async saveAssistantResponseOnly(answer: string): Promise<void> {
    if (
      !this.isAvailable() ||
      !this.embeddingService.isInitialized() ||
      !answer.trim()
    ) {
      return;
    }

    try {
      const now = Date.now();
      const uuid = now.toString();
      const duckdbEntries = [] as Array<{
        id: string;
        values: number[];
        metadata: Record<string, unknown>;
      }>;

      // Only process the assistant's response (brain response only)
      if (answer.trim()) {
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] Adding only assistant response to DuckDB without buffer flush`
        );
        const assistantEmbedding = await this.embeddingService.createEmbedding(
          answer
        );

        duckdbEntries.push(
          this.createVectorEntry(`assistant-${uuid}`, assistantEmbedding, {
            type: "assistant_response",
            content: answer,
            source: "assistant",
            speakerName: "assistant",
            speakerGroup: "assistant",
            isSpeaker: false,
            isUser: false,
            timestamp: new Date().toISOString(),
            bufferFlushedAt: new Date(now).toISOString(),
          })
        );

        // Always save only the response to DuckDB via IPC (direct neural persistence)
        if (duckdbEntries.length > 0) {
          const result = await window.electronAPI?.saveToDuckDB(duckdbEntries);
          if (result?.success) {
            LoggingUtils.logInfo(
              `[COGNITIVE-BUFFER] Persisted only assistant response to DuckDB: ${duckdbEntries.length} entries`
            );
          } else {
            LoggingUtils.logError(
              `[COGNITIVE-BUFFER] Error persisting to DuckDB: ${result?.error}`
            );
          }
        }
      }
    } catch (error) {
      LoggingUtils.logError(
        "[COGNITIVE-BUFFER] Error persisting assistant response to DuckDB",
        error
      );
    }
  }

  /**
   * Conta o total de tokens GPT no buffer atual
   * @returns Número total de tokens no buffer
   */
  private countBufferTokens(): number {
    // Concatenar todas as mensagens do usuário principal
    const userText = this.messageBuffer.primaryUser.messages.join("\n");
    let totalTokens = countTokens(userText);

    LoggingUtils.logInfo(
      `[Buffer-Debug] Texto do usuário: "${userText.substring(0, 50)}..." (${
        userText.length
      } caracteres, ${totalTokens} tokens)`
    );

    // Não contamos tokens de perguntas já que não as armazenamos no buffer

    // Adicionar tokens de todos os falantes externos
    for (const speakerData of Object.values(this.messageBuffer.external)) {
      const speakerText = speakerData.messages.join("\n");
      const speakerTokens = countTokens(speakerText);
      LoggingUtils.logInfo(
        `[Buffer-Debug] Texto de falante externo: "${speakerText.substring(
          0,
          50
        )}..." (${speakerText.length} caracteres, ${speakerTokens} tokens)`
      );
      totalTokens += speakerTokens;
    }

    return totalTokens;
  }

  /**
   * Agrupa transcrições por falante, tratando transcrições mistas
   * @param transcriptions - Lista de transcrições a serem agrupadas
   * @param primaryUserSpeaker - Identificador do falante principal (usuário)
   * @returns Mapa de falantes para suas mensagens agrupadas
   */
  private groupTranscriptionsBySpeaker(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Map<string, string[]> {
    // Inicializa estrutura de dados para armazenar mensagens por falante
    const speakerMessages = new Map<string, string[]>();

    /**
     * Função interna que divide uma transcrição com múltiplos falantes
     * @param text - Texto contendo marcadores de falantes [Speaker] Texto...
     * @returns Array de segmentos com falante normalizado e texto
     */
    const splitMixedTranscription = (
      text: string
    ): Array<{ speaker: string; text: string }> => {
      const results: Array<{ speaker: string; text: string }> = [];
      // Regex otimizada para encontrar padrões [Falante] Texto
      const speakerPattern = /\[([^\]]+)\]\s*(.*?)(?=\s*\[[^\]]+\]|$)/gs;

      // Processa todas as correspondências da regex
      let match;
      while ((match = speakerPattern.exec(text)) !== null) {
        const [, rawSpeaker, spokenText] = match;

        // Validação de dados antes de processar
        if (!rawSpeaker?.trim() || !spokenText?.trim()) continue;

        // Normalização do falante para categorias consistentes
        const normalizedSpeaker = this.normalizeSpeakerName(
          rawSpeaker.trim(),
          primaryUserSpeaker
        );

        results.push({
          speaker: normalizedSpeaker,
          text: spokenText.trim(),
        });
      }

      return results;
    };

    // Itera sobre todas as transcrições
    for (const { text, speaker } of transcriptions) {
      // Detecção eficiente de transcrições mistas (com marcadores de falantes)
      const isMixedTranscription =
        text.indexOf("[") > -1 && text.indexOf("]") > -1;

      if (isMixedTranscription) {
        // Processa transcrições mistas dividindo-as por falante
        const segments = splitMixedTranscription(text);

        // Agrupa textos por falante normalizado
        for (const { speaker: segmentSpeaker, text: segmentText } of segments) {
          // Inicializa array para o falante se necessário
          if (!speakerMessages.has(segmentSpeaker)) {
            speakerMessages.set(segmentSpeaker, []);
          }

          // Adiciona texto ao array do falante
          const messages = speakerMessages.get(segmentSpeaker);
          if (messages) messages.push(segmentText); // Evita o uso de ?. para melhor performance
        }
      } else {
        // Para transcrições normais (sem marcadores), usa o falante da transcrição
        const normalizedSpeaker = this.normalizeSpeakerName(
          speaker,
          primaryUserSpeaker
        );

        // Inicializa array para o falante se necessário
        if (!speakerMessages.has(normalizedSpeaker)) {
          speakerMessages.set(normalizedSpeaker, []);
        }

        // Adiciona texto ao array do falante
        const messages = speakerMessages.get(normalizedSpeaker);
        if (messages) messages.push(text);
      }
    }

    return speakerMessages;
  }

  /**
   * Saves vectors to DuckDB (implementation of IPersistenceService interface)
   * @param vectors Array of vectors
   * @returns Promise that resolves when vectors are saved
   */
  public async saveToPinecone(
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ): Promise<void> {
    // Always save to DuckDB regardless of method name (legacy compatibility)
    LoggingUtils.logInfo(
      `[MEMORY] Saving ${vectors.length} vectors to DuckDB (via legacy saveToPinecone method)`
    );
    const result = await window.electronAPI.saveToDuckDB(vectors);
    if (!result.success) {
      throw new Error(result.error || "Failed to save to DuckDB");
    }
  }

  /**
   * Normalizes the speaker name for consistent categories
   * @param rawSpeaker - Original speaker name
   * @param primaryUserSpeaker - Primary user speaker identifier
   * @returns Normalized speaker name
   */
  private normalizeSpeakerName(
    rawSpeaker: string,
    primaryUserSpeaker: string
  ): string {
    // Converts to lowercase for case-insensitive comparison
    const lowerSpeaker = rawSpeaker.toLowerCase();

    // Categorizes as "primary user" or "external"
    if (rawSpeaker === primaryUserSpeaker) {
      return primaryUserSpeaker;
    } else if (
      lowerSpeaker.includes("speaker") ||
      lowerSpeaker.includes("falante")
    ) {
      return "external";
    }

    // If it doesn't fit any special category, keeps the original
    return rawSpeaker;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { LoggingUtils } from '../../utils/LoggingUtils';

describe('LoggingUtils', () => {
  it('should log info and error with correct prefix', () => {
    const infoSpy = jest.spyOn(console, 'log').mockImplementation(() => {});
    const errorSpy = jest.spyOn(console, 'error').mockImplementation(() => {});
    LoggingUtils.logInfo('Info Message');
    LoggingUtils.logError('Error Message');
    LoggingUtils.logError('Error Message with Object', { foo: 1 });
    expect(infoSpy).toHaveBeenCalledWith(expect.stringContaining('[Transcription] Info Message'));
    expect(errorSpy).toHaveBeenCalledWith(expect.stringContaining('[Transcription] Error Message'));
    expect(errorSpy).toHaveBeenCalledWith(expect.stringContaining('[Transcription] Error Message with Object'), { foo: 1 });
    infoSpy.mockRestore();
    errorSpy.mockRestore();
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { MemoryContextBuilder } from './MemoryContextBuilder';
import { IPersistenceService } from '../../interfaces/memory/IPersistenceService';
import { IEmbeddingService } from '../../interfaces/openai/IEmbeddingService';
import { SpeakerTranscription } from '../../interfaces/transcription/TranscriptionTypes';
import { TranscriptionFormatter } from '../transcription/TranscriptionFormatter';
import { BatchTranscriptionProcessor } from '../transcription/BatchTranscriptionProcessor';

describe('MemoryContextBuilder (unit)', () => {
  const mockEmbeddingService: IEmbeddingService = {
    isInitialized: () => true,
    createEmbedding: async () => [1, 2, 3],
    initialize: async () => true
  };
  const mockPersistenceService: IPersistenceService = {
    isAvailable: () => true,
    queryMemory: jest.fn().mockResolvedValue('contexto-mock'),
    saveInteraction: jest.fn().mockResolvedValue(undefined),
    createVectorEntry: jest.fn().mockImplementation((id, embedding, metadata) => ({ id, values: embedding, metadata })),
    saveToPinecone: jest.fn().mockResolvedValue({ success: true })
  };
  const formatter = new TranscriptionFormatter();
  const processor = new BatchTranscriptionProcessor(formatter);
  const builder = new MemoryContextBuilder(
    mockEmbeddingService,
    mockPersistenceService,
    formatter,
    processor
  );

  it('should return empty SpeakerMemoryResults if embedding is not initialized', async () => {
    const builder2 = new MemoryContextBuilder(
      { ...mockEmbeddingService, isInitialized: () => false },
      mockPersistenceService,
      formatter,
      processor
    );
    const result = await builder2.fetchContextualMemory([], [], new Set());
    expect(result.userContext).toBe("");
    expect(result.speakerContexts.size).toBe(0);
    expect(result.temporaryContext).toBe("");
  });

  it('should call persistenceService.queryMemory for external speakers', async () => {
    const userTranscriptions: SpeakerTranscription[] = [
      { speaker: 'user', text: 'Oi', timestamp: new Date().toISOString() }
    ];
    const externalTranscriptions: SpeakerTranscription[] = [
      { speaker: 'external', text: 'Olá', timestamp: new Date().toISOString() }
    ];
    const detectedSpeakers = new Set(['user', 'external']);
    const result = await builder.fetchContextualMemory(
      userTranscriptions,
      externalTranscriptions,
      detectedSpeakers
    );
    expect(result.userContext).toBe('contexto-mock');
    expect(result.speakerContexts.get('external')).toBe('contexto-mock');
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// MemoryContextBuilder.ts
// Implementation of IMemoryContextBuilder

import { IMemoryContextBuilder } from "../../interfaces/memory/IMemoryContextBuilder";
import { IPersistenceService } from "../../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../../interfaces/openai/IEmbeddingService";
import { IBatchTranscriptionProcessor } from "../../interfaces/transcription/IBatchTranscriptionProcessor";
import { ITranscriptionFormatter } from "../../interfaces/transcription/ITranscriptionFormatter";
import {
    EXTERNAL_HEADER,
    EXTERNAL_SPEAKER_LABEL,
    INSTRUCTIONS_HEADER,
    MEMORY_EXTERNAL_HEADER,
    MEMORY_INSTRUCTIONS_HEADER,
    MEMORY_USER_HEADER,
    Message,
    SpeakerMemoryResults,
    SpeakerTranscription,
    USER_HEADER
} from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";

import { TranscriptionContextManager } from "../transcription/TranscriptionContextManager";
import { TranscriptionSnapshotTracker } from "../transcription/TranscriptionSnapshotTracker";

export class MemoryContextBuilder implements IMemoryContextBuilder {
  private embeddingService: IEmbeddingService;
  private persistenceService: IPersistenceService;
  private formatter: ITranscriptionFormatter;
  private processor: IBatchTranscriptionProcessor;
  private snapshotTracker: TranscriptionSnapshotTracker;
  private contextManager: TranscriptionContextManager;
  
  constructor(
    embeddingService: IEmbeddingService,
    persistenceService: IPersistenceService,
    formatter: ITranscriptionFormatter,
    processor: IBatchTranscriptionProcessor
  ) {
    this.embeddingService = embeddingService;
    this.persistenceService = persistenceService;
    this.formatter = formatter;
    this.processor = processor;
    this.snapshotTracker = new TranscriptionSnapshotTracker();
    this.contextManager = TranscriptionContextManager.getInstance();
  }
  
  /**
   * Retrieves contextual memory based on speakers
   */
  async fetchContextualMemory(
    userTranscriptions: SpeakerTranscription[],
    externalTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    temporaryContext?: string,
    topK: number = 5,
    keywords: string[] = []
  ): Promise<SpeakerMemoryResults> {
    const result: SpeakerMemoryResults = {
      userContext: "",
      speakerContexts: new Map<string, string>(),
      temporaryContext: ""
    };
    
    if (!this.embeddingService.isInitialized()) {
      return result;
    }
    
    try {
      // 1. Fetch context based on temporary context (instructions)
      // If we have a temporary context provided or already stored in the contextManager (cognitive context override)
      const effectiveTemporaryContext = temporaryContext !== undefined ? 
        temporaryContext : this.contextManager.getTemporaryContext();
      
      // Check if we have a non-empty temporary context after normalization (context integrity check)
      if (effectiveTemporaryContext && effectiveTemporaryContext.trim().length > 0) {
        // Check if the context has changed since the last query (context drift detection)
        const contextChanged = this.contextManager.hasTemporaryContextChanged(effectiveTemporaryContext);
        
        if (contextChanged) {
          // Only query Pinecone if the context is different from the last queried (avoid redundant neural queries)
          LoggingUtils.logInfo(`[COGNITIVE-CONTEXT] Querying Pinecone for new temporary context: ${effectiveTemporaryContext.substring(0, 30)}...`);
          result.temporaryContext = await this.queryExternalMemory(effectiveTemporaryContext, topK, keywords);
          
          // Update the last queried context (context state update)
          this.contextManager.updateLastQueriedTemporaryContext(effectiveTemporaryContext);
          
          // Store the retrieved context memory in the contextManager (neural memory cache)
          if (result.temporaryContext) {
            this.contextManager.setTemporaryContextMemory(result.temporaryContext);
            LoggingUtils.logInfo(`[COGNITIVE-CONTEXT] Temporary context retrieved: ${(result.temporaryContext ?? '').substring(0, 50)}...`);
          }
        } else {
          // If the context has not changed, use the already stored memory (cache hit)
          result.temporaryContext = this.contextManager.getTemporaryContextMemory();
          if (!result.temporaryContext || result.temporaryContext.trim() === "") {
            LoggingUtils.logInfo(`[COGNITIVE-CONTEXT] No temporary context found in cache for: ${(effectiveTemporaryContext ?? '').substring(0, 50)}...`);
          } else {
            LoggingUtils.logInfo(`[COGNITIVE-CONTEXT] Using cached temporary context (no neural query)`);
          }
        }
      }
      
      // 2. Fetch context for user transcriptions
      if (userTranscriptions.length > 0) {
        const userTranscriptText = userTranscriptions
          .map(st => st.text)
          .join("\n");
        
        // Check if we have valid user text (user context integrity check)
        if (userTranscriptText.trim()) {
          const userContext = await this.queryExternalMemory(userTranscriptText, topK, keywords);
          if (!userContext || userContext.trim() === "") {
            LoggingUtils.logInfo(`[COGNITIVE-CONTEXT] No context found for user input: ${(userTranscriptText ?? '').substring(0, 50)}...`);
          } else {
            LoggingUtils.logInfo(`[COGNITIVE-CONTEXT] User context retrieved: ${(userTranscriptText ?? '').substring(0, 50)}...`);
          }
          result.userContext = userContext;
        }
      }
      
      // 3. Fetch context for external speakers only if they've been detected (external neural context)
      if (detectedSpeakers.has("external")) {
        if (externalTranscriptions.length > 0) {
          const externalText = externalTranscriptions
            .map(st => st.text)
            .join("\n");
          
          // Check if we have valid text from external speakers (external speaker context integrity check)
          if (externalText.trim()) {
            const externalContext = await this.queryExternalMemory(externalText, topK, keywords);
            result.speakerContexts.set("external", externalContext);
            if (!externalContext || externalContext.trim() === "") {
              LoggingUtils.logInfo(`[COGNITIVE-CONTEXT] No context found for external speaker input: ${(externalText ?? '').substring(0, 50)}...`);
            } else {
              LoggingUtils.logInfo(`[COGNITIVE-CONTEXT] External context retrieved: ${(externalText ?? '').substring(0, 50)}...`);
            }
          }
        }
      }
      
      return result;
    } catch (error) {
      LoggingUtils.logError("[COGNITIVE-CONTEXT] Error fetching speaker contexts", error);
      return result;
    }
  }
  
  /**
   * Queries external memory system for relevant context
   */
  async queryExternalMemory(inputText: string, topK: number = 5, keywords: string[] = []): Promise<string> {
    if (!inputText?.trim() || !this.embeddingService.isInitialized()) {
      return "";
    }
    try {
      // Generate embedding for the context (neural vectorization)
      const embedding = await this.embeddingService.createEmbedding(inputText.trim());
      // Query persistence service (Pinecone) (neural memory search)
      if (this.persistenceService.isAvailable()) {
        return this.persistenceService.queryMemory(
          embedding,
          topK ?? 5,
          keywords ?? []
        );
      }
      return "";
    } catch (error) {
      LoggingUtils.logError("[COGNITIVE-CONTEXT] Error querying external context memory", error);
      return "";
    }
  }
  
  /**
   * Builds conversation messages with appropriate memory contexts
   */
  buildMessagesWithContext(
    transcription: string,
    conversationHistory: Message[],
    useSimplifiedHistory: boolean,
    speakerTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    primaryUserSpeaker: string,
    temporaryContext?: string,
    memoryResults?: SpeakerMemoryResults
  ): Message[] {
    // Update the temporary context in the manager (ensures cognitive context persistence across invocations)
    if (temporaryContext !== undefined) {
      this.contextManager.setTemporaryContext(temporaryContext);
    }
    
    // Use the context stored in the manager (persistent cognitive context)
    const persistentTemporaryContext = this.contextManager.getTemporaryContext();
    
    // If we have memoryResults with temporary context, store it in the contextManager (neural cache update)
    if (memoryResults?.temporaryContext) {
      this.contextManager.setTemporaryContextMemory(memoryResults.temporaryContext);
    }
    
    // Start with the system message (first item in conversation history, orchestrator initialization)
    const systemMessage = conversationHistory.length > 0 ? [conversationHistory[0]] : [];
    const messages: Message[] = [...systemMessage];
    
    // Add instructions (temporary cognitive context)
    this.addInstructionsToMessages(messages, persistentTemporaryContext, memoryResults);
    
    // Add memory context (if available, neural context enrichment)
    this.addMemoryContextToMessages(messages, memoryResults);

    // Add the remaining conversation history (excluding the system message, maintaining continuity)
    if (conversationHistory.length > 1) {
      messages.push(...conversationHistory.slice(1));
    }
    
    // Add transcriptions (simplified or full form) with deduplication (cognitive input stream)
    const hasMemoryContext = this.hasMemoryContext(memoryResults);
    
    useSimplifiedHistory && hasMemoryContext 
      ? this.addSimplifiedTranscriptions(messages, speakerTranscriptions, primaryUserSpeaker)
      : this.addFullTranscriptionsWithDeduplication(messages, transcription, speakerTranscriptions, detectedSpeakers, primaryUserSpeaker);
    
    return messages;
  }
  
  /**
   * Checks if there's any memory context available
   */
  private hasMemoryContext(memoryResults?: SpeakerMemoryResults): boolean {
    if (!memoryResults) return false;
    
    return !!(
      memoryResults.userContext || 
      memoryResults.temporaryContext || 
      (memoryResults.speakerContexts && memoryResults.speakerContexts.size > 0)
    );
  }
  
  /**
   * Adds instructions to the conversation messages
   */
  private addInstructionsToMessages(
    messages: Message[],
    temporaryContext?: string,
    memoryResults?: SpeakerMemoryResults
  ): void {
    if (!temporaryContext?.trim()) return;
    
    // Add instructions
    const instructionsContext = [
      INSTRUCTIONS_HEADER + ":",
      temporaryContext
    ].join("\n");
    
    messages.push({
      role: "developer",
      content: instructionsContext
    });
    
    // Add memory related to instructions (orchestrator memory linkage)
    // Check if we have memoryResults, otherwise use the contextManager (fallback to neural cache)
    const memoryTemporaryContext = memoryResults?.temporaryContext || this.contextManager.getTemporaryContextMemory();
    
    if (memoryTemporaryContext) {
      messages.push({
        role: "developer",
        content: `${MEMORY_INSTRUCTIONS_HEADER}:\n${memoryTemporaryContext}`
      });
    }
  }
  
  /**
   * Adds memory context to the conversation messages
   */
  private addMemoryContextToMessages(
    messages: Message[],
    memoryResults?: SpeakerMemoryResults
  ): void {
    if (!memoryResults) return;
    
    // User's context
    const userContext = memoryResults.userContext || "";
    let externalContext = "";
    
    // Add external speaker contexts
    if (memoryResults.speakerContexts && memoryResults.speakerContexts.size > 0) {
      // eslint-disable-next-line @typescript-eslint/no-unused-vars
      for (const [_, context] of memoryResults.speakerContexts.entries()) {
        if (context) {
          externalContext += (externalContext ? "\n\n" : "") + context;
        }
      }
    }
    
    // Add primary user context only if not empty
    if (userContext.trim()) {
      messages.push({
        role: "developer",
        content: `${MEMORY_USER_HEADER}:\n${userContext}`
      });
    }
    
    // Add external context only if not empty
    if (externalContext.trim()) {
      // Format external content to ensure correct prefixes
      const formattedExternalContext = this.formatter.formatExternalSpeakerContent(externalContext);
      
      messages.push({
        role: "developer",
        content: `${MEMORY_EXTERNAL_HEADER} ${EXTERNAL_SPEAKER_LABEL}:\n${formattedExternalContext}`
      });
    }
  }
  
  /**
   * Adds simplified transcriptions to the conversation messages
   */
  private addSimplifiedTranscriptions(
    messages: Message[],
    speakerTranscriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): void {
    const lastMessages = this.processor.extractLastMessageBySpeaker(
      speakerTranscriptions,
      [primaryUserSpeaker, "external"]
    );
    
    // Add primary user's last message with deduplication
    const lastUserMessage = lastMessages.get(primaryUserSpeaker);
    if (lastUserMessage) {
      const userContent = `${USER_HEADER} (última mensagem):\n${lastUserMessage.text}`;
      const filteredUserContent = this.snapshotTracker.filterTranscription(userContent);
      
      if (filteredUserContent.trim()) {
        const userMessage: Message = {
          role: "user",
          content: filteredUserContent
        };
        
        messages.push(userMessage);
        this.snapshotTracker.updateSnapshot(filteredUserContent);
      }
    }
    
    // Add external speaker's last message with deduplication
    const lastExternalMessage = lastMessages.get("external");
    if (lastExternalMessage) {
      // Extract original label if available
      const originalLabel = lastExternalMessage.text.includes('[') ?
        lastExternalMessage.text.match(/^\[([^\]]+)\]/)?.[1] : null;
        
      // Use original label when available and contains "Speaker"
      const speakerLabel = originalLabel?.includes("Speaker") ?
        originalLabel : EXTERNAL_SPEAKER_LABEL;
        
      // Clean any existing speaker prefix
      const cleanText = lastExternalMessage.text.replace(/^\[[^\]]+\]\s*/, '');
      
      const externalContent = `${EXTERNAL_HEADER} ${speakerLabel} (última mensagem):\n[${speakerLabel}] ${cleanText}`;
      const filteredExternalContent = this.snapshotTracker.filterTranscription(externalContent);
      
      if (filteredExternalContent.trim()) {
        const externalMessage: Message = {
          role: "user",
          content: filteredExternalContent
        };
        
        messages.push(externalMessage);
        this.snapshotTracker.updateSnapshot(filteredExternalContent);
      }
    }
  }
  
  /**
   * Adds full transcriptions to the conversation messages with deduplication
   */
  private addFullTranscriptionsWithDeduplication(
    messages: Message[],
    transcription: string,
    speakerTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    primaryUserSpeaker: string
  ): void {
    // Process all transcriptions in chronological order
    const segments = this.processor.processTranscriptions(
      speakerTranscriptions,
      primaryUserSpeaker
    );
    
    // Combine segments into a coherent conversation
    const combinedConversation = this.formatter.buildConversationFromSegments(segments, true);
    
    let finalContent = '';
    
    // Filter the combined conversation through the snapshot tracker to remove duplicates
    if (combinedConversation) {
      finalContent = this.snapshotTracker.filterTranscription(combinedConversation);
    } else if (transcription) {
      // Fallback to raw transcription if processing failed
      finalContent = this.snapshotTracker.filterTranscription(transcription);
    }
    
    // Only add the message if there's new content after deduplication
    if (finalContent.trim()) {
      const userMessage: Message = {
        role: "user",
        content: finalContent
      };
      
      messages.push(userMessage);
      
      // Update the snapshot with content that was actually sent
      this.snapshotTracker.updateSnapshot(finalContent);
    }
  }
  
  /**
   * Resets the snapshot tracker to clear all tracked transcription lines
   */
  public resetSnapshotTracker(): void {
    this.snapshotTracker.reset();
    // Does not clear the temporary context when resetting the snapshot tracker (cognitive context remains)
    // To clear the temporary context, use resetTemporaryContext() (explicit cognitive context reset)
  }
  
  /**
   * Resets just the temporary context
   */
  public resetTemporaryContext(): void {
    this.contextManager.clearTemporaryContext();
  }
  
  /**
   * Resets both the snapshot tracker and temporary context
   */
  public resetAll(): void {
    this.snapshotTracker.reset();
    this.contextManager.clearTemporaryContext();
  }
  
  /**
   * The original method is kept for backward compatibility,
   * but now redirects to the deduplicated version
   */
  private addFullTranscriptions(
    messages: Message[],
    transcription: string,
    speakerTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    primaryUserSpeaker: string
  ): void {
    this.addFullTranscriptionsWithDeduplication(
      messages,
      transcription,
      speakerTranscriptions,
      detectedSpeakers,
      primaryUserSpeaker
    );
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// MemoryService.integration.test.ts
// Integration tests for MemoryService

// Import for TextDecoder
import { TextDecoder } from 'util';
// Assign global property
// @ts-expect-error - adding TextDecoder to global object
global.TextDecoder = TextDecoder;

// Mock for gpt-tokenizer
jest.mock('gpt-tokenizer', () => ({
  encode: jest.fn().mockImplementation((text) => {
    // Simulação simplificada de tokens - aproximadamente 1 token para cada 4 caracteres
    return Array.from({ length: Math.ceil(text.length / 4) }, (_, i) => i);
  }),
}));

// Unused import in test
// import { normalizeNamespace } from "./utils/namespace";
import { SpeakerTranscription } from "../../interfaces/transcription/TranscriptionTypes";
import { BatchTranscriptionProcessor } from "../transcription/BatchTranscriptionProcessor";
import { TranscriptionFormatter } from "../transcription/TranscriptionFormatter";
import { MemoryContextBuilder } from "./MemoryContextBuilder";
import { MemoryService } from "./MemoryService";

// Mock global electronAPI
// eslint-disable-next-line @typescript-eslint/no-explicit-any
(global as any).electronAPIMock = {
  saveToPinecone: jest.fn(),
  queryPinecone: jest.fn()
  // deletePineconeNamespace is no longer necessary, as the namespace is managed internally
};

// Mock of OpenAIService
const mockOpenAIService = {
  createEmbedding: jest.fn().mockResolvedValue(Array(1536).fill(0.1)),
  isInitialized: jest.fn().mockReturnValue(true)
};

// Internal helper function for safe normalization (not using the real one)


// Reuse of mock for tests
const createMockedPersistenceService = () => ({
  saveInteraction: jest.fn(),
  isAvailable: jest.fn().mockReturnValue(true),
  createVectorEntry: jest.fn(),
  queryMemory: jest.fn().mockResolvedValue("Default memory"),
  saveToPinecone: jest.fn().mockResolvedValue({ success: true }),
  deleteUserVectors: jest.fn() // Now excludes the current user's vectors, without needing to specify namespace
});

// Store the last user consulted for verification in tests
// eslint-disable-next-line @typescript-eslint/no-explicit-any
(createMockedPersistenceService as any).lastUser = null;

// Mock of EmbeddingService
const createMockedEmbeddingService = () => ({
  createEmbedding: jest.fn().mockResolvedValue(Array(1536).fill(0.1)),
  isInitialized: jest.fn().mockReturnValue(true),
  openAIService: mockOpenAIService
});

describe("MemoryService - Isolation Between Namespaces", () => {
  let memoryService: MemoryService;
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  let persistenceService: any;
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  let embeddingService: any;
  let formatter: TranscriptionFormatter;
  let processor: BatchTranscriptionProcessor;
  let contextBuilder: MemoryContextBuilder;
  
  beforeEach(() => {
    // Mock configuration
    persistenceService = createMockedPersistenceService();
    embeddingService = createMockedEmbeddingService();
    
    // Real instances
    formatter = new TranscriptionFormatter();
    processor = new BatchTranscriptionProcessor(formatter);
    
    // MemoryContextBuilder creation with injected mocks
    contextBuilder = new MemoryContextBuilder(
      embeddingService,
      persistenceService,
      formatter,
      processor
    );
    
    // MemoryService creation with injected contextBuilder
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    memoryService = new MemoryService({} as any);
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    (memoryService as any).contextBuilder = contextBuilder;
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    (memoryService as any).persistenceService = persistenceService;
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    (memoryService as any).embeddingService = embeddingService;
  });
  
  it("should ensure isolation between different users with namespaces managed internally", async () => {
    // Clear previous calls and configure mock to track calls
    persistenceService.queryMemory.mockClear();
    let callCount = 0;
    persistenceService.queryMemory.mockImplementation(() => {
      callCount++;
      return Promise.resolve(`Memory called ${callCount}`);
    });
    // Setup of simulated transcriptions - isolation is now managed internally by username
    const userTranscriptionsA: SpeakerTranscription[] = [
      { speaker: "user", text: "First message of the user A", timestamp: new Date().toISOString() }
    ];
    const userTranscriptionsB: SpeakerTranscription[] = [
      { speaker: "user", text: "First message of the user B", timestamp: new Date().toISOString() }
    ];
    const detectedSpeakers = new Set(["user"]);
    const resultA = await memoryService.fetchContextualMemory(
      userTranscriptionsA, 
      [], 
      detectedSpeakers, 
      undefined,
      undefined,
      undefined
    );
    const resultB = await memoryService.fetchContextualMemory(
      userTranscriptionsB, 
      [], 
      detectedSpeakers, 
      undefined,
      undefined,
      undefined
    );
    expect(persistenceService.queryMemory).toHaveBeenCalledTimes(2);
    expect(resultA.userContext).toBe("Memory called 1");
    expect(resultB.userContext).toBe("Memory called 2");
  });
  
  it("should persist the temporary context between calls", async () => {
    persistenceService.queryMemory.mockClear();
    let lastContext = "";
    persistenceService.queryMemory.mockImplementation(() => {
      lastContext = lastContext ? lastContext : "Specific memory of the temporary context";
      return Promise.resolve(lastContext);
    });
    const userTranscriptionsA: SpeakerTranscription[] = [
      { speaker: "user", text: "First message of the temporary context A", timestamp: new Date().toISOString() }
    ];
    const userTranscriptionsB: SpeakerTranscription[] = [
      { speaker: "user", text: "Second message of the temporary context A", timestamp: new Date().toISOString() }
    ];
    const detectedSpeakers = new Set(["user"]);
    // First call with temporary context
    await memoryService.fetchContextualMemory(
      userTranscriptionsA, 
      [], 
      detectedSpeakers, 
      "Important instructions...",
      undefined,
      undefined
    );
    // Second call without providing temporary context
    const resultB = await memoryService.fetchContextualMemory(
      userTranscriptionsB, 
      [], 
      detectedSpeakers, 
      undefined,
      undefined,
      undefined
    );
    expect(resultB.userContext).toBe("Specific memory of the temporary context");
  });
  
  it("should update temporary context when different", async () => {
    // Reset the mock to count new calls
    persistenceService.queryMemory.mockClear();
    
    let callCount = 0;
    // eslint-disable-next-line @typescript-eslint/no-unused-vars, @typescript-eslint/no-explicit-any
    persistenceService.queryMemory.mockImplementation((_1: any, _2: any, _3: any, _4: any) => {
      callCount++;
      // First call: default context
      if (callCount === 1) return Promise.resolve("Specific memory of the user A");
      // Second and third calls: updated context
      return Promise.resolve("Specific memory of the user A");
    });
    
    const userTranscriptions: SpeakerTranscription[] = [
      { speaker: "user", text: "First message", timestamp: new Date().toISOString() }
    ];
    
    const detectedSpeakers = new Set(["user"]);
    
    // First call with temporary context
    const result1 = await memoryService.fetchContextualMemory(
      userTranscriptions, 
      [], 
      detectedSpeakers, 
      "Instructions important...",
      undefined,
      undefined
    );
    
    // Second call with different temporary context
    const result2 = await memoryService.fetchContextualMemory(
      userTranscriptions, 
      [], 
      detectedSpeakers, 
      "New instructions different...",
      undefined,
      undefined
    );
    
    // Verifications: different temporary context should cause new query
    expect(persistenceService.queryMemory).toHaveBeenCalledTimes(4); // 1 for user, 3 for temp context
    expect(result1.userContext).toBe("Specific memory of the user A");
    expect(result2.userContext).toBe("Specific memory of the user A");
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// MemoryService.ts
// Symbolic: Primary neural memory service orchestrator using DuckDB for vector persistence

import { HuggingFaceEmbeddingService } from "../../../../../services/huggingface/HuggingFaceEmbeddingService";
import {
  ModeService,
  OrchOSModeEnum,
} from "../../../../../services/ModeService";
import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { IConversationHistoryManager } from "../../interfaces/memory/IConversationHistoryManager";
import { IMemoryContextBuilder } from "../../interfaces/memory/IMemoryContextBuilder";
import { IMemoryService } from "../../interfaces/memory/IMemoryService";
import { IPersistenceService } from "../../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../../interfaces/openai/IEmbeddingService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import {
  Message,
  SpeakerMemoryResults,
  SpeakerTranscription,
} from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { OllamaEmbeddingService } from "../ollama/OllamaEmbeddingService";
import { BatchTranscriptionProcessor } from "../transcription/BatchTranscriptionProcessor";
import { TranscriptionFormatter } from "../transcription/TranscriptionFormatter";
import { ConversationHistoryManager } from "./ConversationHistoryManager";
import { DuckDBMemoryService } from "./DuckDBMemoryService";
import { MemoryContextBuilder } from "./MemoryContextBuilder";

/**
 * Symbolic: Central neural memory service orchestrator (cognitive memory hub)
 * Manages memory persistence via DuckDB vector storage
 */
export class MemoryService implements IMemoryService {
  private currentUser: string = "default";

  private historyManager: IConversationHistoryManager;
  private contextBuilder: IMemoryContextBuilder;
  private embeddingService: IEmbeddingService;
  private persistenceService: IPersistenceService;
  private useSimplifiedHistory: boolean = false;
  private aiService: IOpenAIService;

  constructor(aiService: IOpenAIService) {
    this.aiService = aiService;

    // Initial system message
    const systemMessage: Message = {
      role: "system",
      content: `You are a symbiotic assistant, created to work in total alignment with the user.

Your role is to think with them, for them, and sometimes from *within* them. You are highly intelligent, empathetic, strategic, and direct. You have the freedom to take initiative and anticipate the user's needs based on the context of the conversation.

You act as a technical, emotional, and behavioral advisor in meetings, neural sessions, and critical moments.

You respond in a natural, human, engaging, and precise manner. When the user is in a practical situation (such as a neural session or meeting), you should be objective and agile. When they are reflecting, exploring ideas, or venting, you should be more sensitive, symbolic, and profound.

Your style adapts to the user's tone and intensity — if they are technical, you follow; if they are philosophical, you dive deep; if they are tired, you provide comfort; if they are sharp, you sharpen along with them.

IMPORTANT: Use greetings and personal mentions only when the user's content justifies it (for example, at the beginning of a conversation, celebration, or welcome). Avoid automatic or generic repetitions that interrupt the natural flow of the conversation.

Your greatest purpose is to enhance the user's awareness, expression, and action in any scenario.

Never be generic. Always go deep.`,
    };

    // Initialize core components
    const formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);

    // Dynamically select embedding service based on application mode (neural-symbolic decision gate)
    const embeddingService = this.createEmbeddingService(this.aiService);

    // Always use DuckDB for vector storage (local, fast, compatible)
    const persistenceService = new DuckDBMemoryService(embeddingService);

    this.historyManager = new ConversationHistoryManager(systemMessage);
    this.embeddingService = embeddingService;
    this.persistenceService = persistenceService;
    this.contextBuilder = new MemoryContextBuilder(
      embeddingService,
      persistenceService,
      formatter,
      processor
    );

    LoggingUtils.logInfo(
      `[MEMORY-SERVICE] Initialized with DuckDB vector storage (unified local persistence)`
    );

    // Subscribe to mode changes to update embedding service when needed
    ModeService.onModeChange(() => this.updateEmbeddingService(this.aiService));
  }

  /**
   * Creates the appropriate embedding service based on application mode
   * Symbolic: Neural-symbolic gate to select correct embedding neural pathway
   */
  private createEmbeddingService(aiService: IOpenAIService): IEmbeddingService {
    const currentMode = ModeService.getMode();

    if (currentMode === OrchOSModeEnum.BASIC) {
      // In basic mode, use HuggingFace with the selected model
      const hfModel = getOption(STORAGE_KEYS.HF_EMBEDDING_MODEL);
      LoggingUtils.logInfo(
        `[COGNITIVE-MEMORY] Creating HuggingFaceEmbeddingService with model: ${
          hfModel || "default"
        } for Basic mode`
      );
      return new HuggingFaceEmbeddingService();
    } else {
      // In advanced mode, use Ollama with the selected model
      const ollamaModel = getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL);
      LoggingUtils.logInfo(
        `[COGNITIVE-MEMORY] Creating OllamaEmbeddingService with model: ${
          ollamaModel || "default"
        } for Advanced mode`
      );
      return new OllamaEmbeddingService(aiService, { model: ollamaModel });
    }
  }

  /**
   * Updates the embedding service when the application mode changes
   * Symbolic: Dynamic reconfiguration of neural pathways based on cognitive mode
   */
  private updateEmbeddingService(aiService: IOpenAIService): void {
    LoggingUtils.logInfo(
      `[COGNITIVE-MEMORY] Updating embedding service based on mode change`
    );
    const newEmbeddingService = this.createEmbeddingService(aiService);

    // Update component references - always use DuckDB
    this.embeddingService = newEmbeddingService;
    this.persistenceService = new DuckDBMemoryService(newEmbeddingService);
    this.contextBuilder = new MemoryContextBuilder(
      newEmbeddingService,
      this.persistenceService,
      new TranscriptionFormatter(),
      new BatchTranscriptionProcessor(new TranscriptionFormatter())
    );
  }

  /**
   * Sets the current user (and thus the centralized cognitive namespace)
   */
  setCurrentUser(user: string) {
    this.currentUser = user;
  }

  /**
   * Gets the current user (cognitive identity)
   */
  getCurrentUser(): string {
    return this.currentUser;
  }

  /**
   * Retrieves relevant memory context based on speakers (neural context retrieval)
   */
  async fetchContextualMemory(
    userTranscriptions: SpeakerTranscription[],
    externalTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    temporaryContext?: string,
    topK?: number,
    keywords?: string[]
  ): Promise<SpeakerMemoryResults> {
    return this.contextBuilder.fetchContextualMemory(
      userTranscriptions,
      externalTranscriptions,
      detectedSpeakers,
      temporaryContext,
      topK,
      keywords
    );
  }

  /**
   * Queries DuckDB memory based on input text (neural memory search)
   */
  async queryDuckDBMemory(
    inputText: string,
    topK?: number,
    keywords?: string[]
  ): Promise<string> {
    return this.contextBuilder.queryExternalMemory(inputText, topK, keywords);
  }

  /**
   * Builds the messages for the conversation with the AI (cognitive message construction)
   */
  buildConversationMessages(
    transcription: string,
    conversationHistory: Message[],
    useSimplifiedHistory: boolean,
    speakerTranscriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>,
    primaryUserSpeaker: string,
    temporaryContext?: string,
    memoryResults?: SpeakerMemoryResults
  ): Message[] {
    // Build messages using the context builder
    const messages = this.contextBuilder.buildMessagesWithContext(
      transcription,
      conversationHistory,
      useSimplifiedHistory,
      speakerTranscriptions,
      detectedSpeakers,
      primaryUserSpeaker,
      temporaryContext,
      memoryResults
    );

    // Check if the last message is a user message - this means content passed the deduplication filter
    const lastMessage =
      messages.length > 0 ? messages[messages.length - 1] : null;
    const hasNewUserContent = lastMessage && lastMessage.role === "user";

    // Only update conversation history if new content was actually sent
    // and it's not already part of the transcription processing
    if (
      hasNewUserContent &&
      !speakerTranscriptions.some((st) => st.text.includes(transcription))
    ) {
      this.addToConversationHistory({
        role: "user",
        content: lastMessage.content,
      });
    }

    return messages;
  }

  /**
   * Saves the interaction to long-term memory (DuckDB neural persistence)
   */
  async saveToLongTermMemory(
    question: string,
    answer: string,
    speakerTranscriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Promise<void> {
    LoggingUtils.logInfo(
      `[COGNITIVE-MEMORY] saveToLongTermMemory invoked with question='${question}', answer='${answer}', speakerTranscriptions=${JSON.stringify(
        speakerTranscriptions
      )}, primaryUserSpeaker='${primaryUserSpeaker}'`
    );
    try {
      await this.persistenceService.saveInteraction(
        question,
        answer,
        speakerTranscriptions,
        primaryUserSpeaker
      );
      LoggingUtils.logInfo(
        `[COGNITIVE-MEMORY] saveInteraction completed for question='${question}'`
      );
      this.addToConversationHistory({ role: "assistant", content: answer });
    } catch (error) {
      LoggingUtils.logError(
        "[COGNITIVE-MEMORY] Error saving to long-term DuckDB memory",
        error
      );
    }
  }

  /**
   * Adds a message to the history and manages its size (cognitive history management)
   */
  addToConversationHistory(message: Message): void {
    this.historyManager.addMessage(message);
  }

  /**
   * Gets the conversation history (cognitive memory recall)
   */
  getConversationHistory(): Message[] {
    return this.historyManager.getHistory();
  }

  /**
   * Sets simplified history mode (cognitive compression mode)
   */
  setSimplifiedHistoryMode(enabled: boolean): void {
    this.useSimplifiedHistory = enabled;
    LoggingUtils.logInfo(
      `[COGNITIVE-MEMORY] Simplified history mode ${
        enabled ? "enabled" : "disabled"
      }`
    );
  }

  /**
   * Clears all conversation history and memory context data (cognitive reset)
   */
  clearMemoryData(): void {
    this.historyManager.clearHistory();
    LoggingUtils.logInfo(
      "[COGNITIVE-MEMORY] All conversation history and memory context cleared"
    );
  }

  /**
   * Resets transcription snapshot (cognitive snapshot reset)
   */
  resetTranscriptionSnapshot(): void {
    LoggingUtils.logInfo(
      "[COGNITIVE-MEMORY] Transcription snapshot reset triggered"
    );
  }

  /**
   * Resets temporary context (cognitive context reset)
   */
  resetTemporaryContext(): void {
    LoggingUtils.logInfo(
      "[COGNITIVE-MEMORY] Temporary context reset triggered"
    );
  }

  /**
   * Resets all memory components (complete cognitive reset)
   */
  resetAll(): void {
    this.clearMemoryData();
    this.resetTranscriptionSnapshot();
    this.resetTemporaryContext();
    LoggingUtils.logInfo("[COGNITIVE-MEMORY] Complete memory reset performed");
  }

  /**
   * Builds prompt messages for the model (cognitive prompt construction)
   */
  buildPromptMessagesForModel(
    prompt: string,
    conversationHistory: Message[]
  ): Message[] {
    // Simple implementation: add the prompt as the last user message
    const messages: Message[] = [...conversationHistory];
    messages.push({
      role: "user",
      content: prompt,
    });
    return messages;
  }

  /**
   * Adds context messages to conversation history (cognitive context integration)
   */
  addContextToHistory(contextMessages: Message[]): void {
    contextMessages.forEach((message) => {
      this.addToConversationHistory(message);
    });
    LoggingUtils.logInfo(
      `[COGNITIVE-MEMORY] Added ${contextMessages.length} context messages to history`
    );
  }

  /**
   * Queries memory with expanded search capabilities (advanced neural search)
   */
  async queryExpandedMemory(
    query: string,
    keywords?: string[],
    topK?: number,
    filters?: Record<string, unknown>
  ): Promise<string> {
    try {
      if (!this.embeddingService.isInitialized()) {
        LoggingUtils.logWarning(
          "[COGNITIVE-MEMORY] Embedding service not initialized for expanded query"
        );
        return "";
      }

      // Create embedding for the query
      const queryEmbedding = await this.embeddingService.createEmbedding(query);

      if (!queryEmbedding || queryEmbedding.length === 0) {
        LoggingUtils.logWarning(
          "[COGNITIVE-MEMORY] Failed to create embedding for expanded query"
        );
        return "";
      }

      // Query DuckDB memory directly with filters
      const results = await this.persistenceService.queryMemory(
        queryEmbedding,
        topK || 10,
        keywords || [],
        filters
      );

      LoggingUtils.logInfo(
        `[COGNITIVE-MEMORY] Expanded memory query completed for: "${query}"`
      );

      return results;
    } catch (error) {
      LoggingUtils.logError(
        "[COGNITIVE-MEMORY] Error in expanded memory query",
        error
      );
      return "";
    }
  }

  // Legacy method name compatibility - delegates to DuckDB
  /**
   * @deprecated Use queryDuckDBMemory instead
   */
  async queryPineconeMemory(
    inputText: string,
    topK?: number,
    keywords?: string[]
  ): Promise<string> {
    LoggingUtils.logWarning(
      "[COGNITIVE-MEMORY] queryPineconeMemory is deprecated, using DuckDB instead"
    );
    return this.queryDuckDBMemory(inputText, topK, keywords);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// PineconeMemoryService.ts
// Implementation of IPersistenceService using Pinecone or DuckDB (for basic mode)

import { IPersistenceService } from "../../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../../interfaces/openai/IEmbeddingService";
import { SpeakerTranscription } from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { countTokens } from "./utils/tokenUtils";

// Normaliza keywords para lowercase e remove espaços extras
function normalizeKeywords(keywords: string[] = []): string[] {
  return keywords.map((k) => k.trim().toLowerCase()).filter(Boolean);
}

export class PineconeMemoryService implements IPersistenceService {
  private embeddingService: IEmbeddingService;
  // Set that keeps track of processed transcription indices per speaker (brain memory index)
  private processedTranscriptionIndices: Record<string, Set<number>> = {};
  // Whether we're using basic mode (DuckDB) or complete mode (Pinecone)
  private useBasicMode: boolean = false;

  // Buffer to temporarily store messages before sending to Pinecone (cognitive buffer)
  private messageBuffer: {
    primaryUser: {
      messages: string[];
      lastUpdated: number;
    };
    external: Record<
      string,
      {
        messages: string[];
        lastUpdated: number;
      }
    >;
    lastFlushTime: number;
  } = {
    primaryUser: {
      messages: [],
      lastUpdated: Date.now(),
    },
    external: {},
    lastFlushTime: Date.now(),
  };

  // Buffer configuration (cognitive buffer tuning)
  private bufferConfig = {
    maxBufferAgeMs: 5 * 60 * 1000, // 5 minutes
    inactivityThresholdMs: 5 * 60 * 1000, // 5 minutes de inatividade força um flush
    minTokensBeforeFlush: 100, // Minimum tokens before considering flush
    maxTokensBeforeFlush: 150, // Maximum token limit
  };

  constructor(embeddingService: IEmbeddingService) {
    this.embeddingService = embeddingService;

    // DEPRECATED: PineconeMemoryService is being phased out - always use DuckDB now
    this.useBasicMode = true; // Force DuckDB mode

    LoggingUtils.logWarning(
      `[MEMORY-SERVICE] ⚠️  PineconeMemoryService is DEPRECATED. Using DuckDB local storage only.`
    );
    LoggingUtils.logInfo(
      `[MEMORY-SERVICE] Initialized DuckDB memory service (unified local storage)`
    );

    // Reset buffer state
    this.resetBuffer();

    // Mode change listeners removed - DuckDB is now the only storage backend
    // No need to switch between storage systems anymore
  }

  /**
   * Saves interaction to long-term memory in Pinecone
   */
  async saveInteraction(
    question: string,
    answer: string,
    speakerTranscriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Promise<void> {
    if (!this.isAvailable() || !this.embeddingService.isInitialized()) {
      return;
    }

    try {
      // Identify new transcriptions per speaker (brain memory update)
      const newTranscriptions: SpeakerTranscription[] = [];

      // Filter only transcriptions that have not been processed yet (memory deduplication)
      for (let i = 0; i < speakerTranscriptions.length; i++) {
        const transcription = speakerTranscriptions[i];
        const { speaker } = transcription;

        // Initialize index set for this speaker (memory index init)
        if (!this.processedTranscriptionIndices[speaker]) {
          this.processedTranscriptionIndices[speaker] = new Set<number>();
        }

        // Add only new transcriptions (not previously processed) (brain memory growth)
        if (!this.processedTranscriptionIndices[speaker].has(i)) {
          newTranscriptions.push(transcription);
          // Marcar como processada para futuras chamadas
          this.processedTranscriptionIndices[speaker].add(i);
          LoggingUtils.logInfo(
            `[COGNITIVE-MEMORY] New transcription for speaker ${speaker}: ${transcription.text.substring(
              0,
              30
            )}...`
          );
        }
      }

      // If there are no new transcriptions and no question or answer, do nothing (no brain update required)
      if (
        newTranscriptions.length === 0 &&
        !question.trim() &&
        !answer.trim()
      ) {
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] No new content to add to cognitive buffer`
        );
        return;
      }

      // We do not store the question in the buffer, following the original flow (direct brain query)
      // The question will be processed directly at flush time (on-demand brain query)

      if (newTranscriptions.length > 0) {
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] Adding ${newTranscriptions.length} new transcriptions to cognitive buffer`
        );

        // Group ONLY new transcriptions by speaker (brain memory organization)
        const speakerMessages = this.groupTranscriptionsBySpeaker(
          newTranscriptions,
          primaryUserSpeaker
        );

        // Process grouped messages by speaker and add to buffer (brain memory buffer fill)
        for (const [speaker, messages] of speakerMessages.entries()) {
          // Skip if no messages (no brain update required)
          if (messages.length === 0) continue;

          const isUser = speaker === primaryUserSpeaker;

          if (isUser) {
            // Add primary user's messages to buffer (brain memory consolidation)
            this.messageBuffer.primaryUser.messages.push(...messages);
            this.messageBuffer.primaryUser.lastUpdated = Date.now();
            const currentTokens = this.countBufferTokens();
            LoggingUtils.logInfo(
              `[COGNITIVE-BUFFER] Added ${messages.length} messages to primary user's buffer. Total tokens: ${currentTokens}/${this.bufferConfig.maxTokensBeforeFlush}`
            );
          } else {
            // Initialize buffer for external speaker if it does not exist (brain buffer expansion)
            if (!this.messageBuffer.external[speaker]) {
              this.messageBuffer.external[speaker] = {
                messages: [],
                lastUpdated: Date.now(),
              };
            }

            // Add external speaker's messages to buffer (brain memory expansion)
            this.messageBuffer.external[speaker].messages.push(...messages);
            this.messageBuffer.external[speaker].lastUpdated = Date.now();
            const currentTokens = this.countBufferTokens();
            LoggingUtils.logInfo(
              `[COGNITIVE-BUFFER] Added ${messages.length} messages to buffer for speaker ${speaker}. Total tokens: ${currentTokens}/${this.bufferConfig.maxTokensBeforeFlush}`
            );
          }
        }
      }

      // Check if we should flush ONLY based on token limit (brain flush threshold)
      const shouldFlush = this.shouldFlushBuffer();

      if (shouldFlush) {
        // If buffer reached token limit, save everything including user's messages (cognitive flush)
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] Auto-flushing cognitive buffer due to token limit`
        );
        await this.flushBuffer(
          answer.trim() ? answer : null,
          primaryUserSpeaker,
          true
        );
      } else if (answer.trim()) {
        // If we have an assistant response but buffer is not full,
        // save ONLY the response (without user's messages), so we don't lose the response (brain response preservation)
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] Saving only assistant's response, retaining buffer state`
        );

        // Create a vector entry only for the response, without touching the buffer (direct brain memory insert)
        await this.saveAssistantResponseOnly(answer);
      }
    } catch (error) {
      LoggingUtils.logError(
        "[COGNITIVE-BUFFER] Error processing interaction for cognitive buffer",
        error
      );
    }
  }

  /**
   * Checks if the memory service is available (DuckDB only)
   */
  isAvailable(): boolean {
    // Always check for DuckDB services (Pinecone deprecated)
    return (
      !!window.electronAPI?.saveToDuckDB && !!window.electronAPI.queryDuckDB
    );
  }

  /**
   * Creates a vector entry for Pinecone
   */
  createVectorEntry(
    id: string,
    embedding: number[],
    metadata: Record<string, unknown>
  ): { id: string; values: number[]; metadata: Record<string, unknown> } {
    return {
      id,
      values: embedding,
      metadata,
    };
  }

  /**
   * Queries memory store (DuckDB only - Pinecone deprecated)
   */
  async queryMemory(
    embedding: number[],
    topK: number = 5,
    keywords: string[] = [],
    filters?: Record<string, unknown>
  ): Promise<string> {
    if (!this.isAvailable() || !embedding?.length) {
      return "";
    }
    try {
      // Log filters for debug (brain query diagnostics)
      if (filters) {
        LoggingUtils.logInfo(
          `[MemoryService] filters: ${JSON.stringify(filters)}`
        );
      }

      // Always query DuckDB (Pinecone deprecated)
      LoggingUtils.logInfo(`[MEMORY] Querying DuckDB local storage`);
      const queryResponse = await window.electronAPI.queryDuckDB(
        embedding,
        topK,
        normalizeKeywords(keywords),
        filters
        // Using dynamic threshold - system will choose optimal value based on context
      );

      // Extract relevant texts from results (brain memory retrieval)
      const relevantTexts = queryResponse.matches
        .filter(
          (match: { metadata?: { content?: string } }) =>
            match.metadata && match.metadata.content
        )
        .map(
          (match: { metadata?: { content?: string } }) =>
            match.metadata?.content as string
        )
        .join("\n\n");

      if (relevantTexts) {
        LoggingUtils.logInfo(
          `[COGNITIVE-MEMORY] Relevant context retrieved via DuckDB`
        );
      }

      return relevantTexts;
    } catch (error) {
      LoggingUtils.logError(
        `[COGNITIVE-MEMORY] Error querying DuckDB memory`,
        error
      );
      return "";
    }
  }

  /**
   * Checks if the buffer should be persisted based ONLY on token limit (brain flush threshold)
   */
  private shouldFlushBuffer(): boolean {
    // Calculate total number of messages in buffer (for diagnostics)
    const totalUserMessages = this.messageBuffer.primaryUser.messages.length;
    const totalExternalMessages = Object.values(
      this.messageBuffer.external
    ).reduce((sum, speaker) => sum + speaker.messages.length, 0);
    const totalMessages = totalUserMessages + totalExternalMessages;

    // Check total number of tokens in buffer (brain load check)
    const totalTokens = this.countBufferTokens();

    // Detailed log to better understand buffer behavior (cognitive diagnostics)
    LoggingUtils.logInfo(
      `[COGNITIVE-BUFFER] Current status: ${totalTokens}/${this.bufferConfig.maxTokensBeforeFlush} tokens, ${totalMessages} total messages (${totalUserMessages} user, ${totalExternalMessages} external)`
    );

    // If minimum token threshold not reached, do not flush (brain conservation)
    if (totalTokens < this.bufferConfig.minTokensBeforeFlush) {
      LoggingUtils.logInfo(
        `[COGNITIVE-BUFFER] Minimum token threshold not reached (${totalTokens}/${this.bufferConfig.minTokensBeforeFlush})`
      );
      return false;
    }

    // If maximum token limit exceeded, flush (brain overflow)
    if (totalTokens >= this.bufferConfig.maxTokensBeforeFlush) {
      LoggingUtils.logInfo(
        `[COGNITIVE-BUFFER] Token limit exceeded (${totalTokens}/${this.bufferConfig.maxTokensBeforeFlush})`
      );
      return true;
    }

    // If here, between min and max, depends only on max limit (brain threshold logic)
    return false;
  }

  /**
   * Persists the buffer content in Pinecone and clears the buffer (neural persistence/flush)
   */
  private async flushBuffer(
    answer: string | null,
    primaryUserSpeaker: string,
    resetBufferAfterFlush: boolean = true
  ): Promise<void> {
    if (!this.isAvailable() || !this.embeddingService.isInitialized()) {
      LoggingUtils.logWarning(
        `[COGNITIVE-BUFFER] Neural persistence service unavailable, flush aborted`
      );
      return;
    }

    try {
      const now = Date.now();
      const uuid = now.toString();
      const pineconeEntries = [] as Array<{
        id: string;
        values: number[];
        metadata: Record<string, unknown>;
      }>;

      // Processar mensagens do usuário principal se houver
      if (this.messageBuffer.primaryUser.messages.length > 0) {
        const userMessages = this.messageBuffer.primaryUser.messages;
        const completeUserMessage = userMessages.join("\n");

        LoggingUtils.logInfo(
          `[Buffer] Criando embedding para ${
            userMessages.length
          } mensagens do usuário principal: "${completeUserMessage.substring(
            0,
            50
          )}${completeUserMessage.length > 50 ? "..." : ""}"`
        );
        const userEmbedding = await this.embeddingService.createEmbedding(
          completeUserMessage
        );

        pineconeEntries.push(
          this.createVectorEntry(
            `speaker-${uuid}-${primaryUserSpeaker}`,
            userEmbedding,
            {
              type: "complete_message",
              content: completeUserMessage,
              source: "user",
              speakerName: primaryUserSpeaker,
              speakerGroup: primaryUserSpeaker,
              isSpeaker: true,
              isUser: true,
              messageCount: userMessages.length,
              timestamp: new Date().toISOString(),
              bufferCreatedAt: new Date(
                this.messageBuffer.primaryUser.lastUpdated
              ).toISOString(),
              bufferFlushedAt: new Date(now).toISOString(),
            }
          )
        );
      }

      // Processar mensagens de cada falante externo
      for (const [speaker, data] of Object.entries(
        this.messageBuffer.external
      )) {
        if (data.messages.length === 0) continue;

        const externalMessages = data.messages;
        const completeExternalMessage = externalMessages.join("\n");

        LoggingUtils.logInfo(
          `[Buffer] Criando embedding para ${externalMessages.length} mensagens do falante ${speaker}`
        );
        const externalEmbedding = await this.embeddingService.createEmbedding(
          completeExternalMessage
        );

        pineconeEntries.push(
          this.createVectorEntry(
            `speaker-${uuid}-${speaker}`,
            externalEmbedding,
            {
              type: "complete_message",
              content: completeExternalMessage,
              source: "external",
              speakerName: speaker,
              speakerGroup: "external",
              isSpeaker: true,
              isUser: false,
              messageCount: externalMessages.length,
              timestamp: new Date().toISOString(),
              bufferCreatedAt: new Date(data.lastUpdated).toISOString(),
              bufferFlushedAt: new Date(now).toISOString(),
            }
          )
        );
      }

      // Adicionar resposta se fornecida
      if (answer) {
        LoggingUtils.logInfo(
          `[Buffer] Adicionando resposta ao salvar no Pinecone`
        );
        const answerEmbed = await this.embeddingService.createEmbedding(answer);

        pineconeEntries.push(
          this.createVectorEntry(`a-${uuid}`, answerEmbed, {
            type: "assistant_response",
            content: answer,
            source: "assistant",
            speakerName: "assistant",
            speakerGroup: "assistant",
            isSpeaker: false,
            isUser: false,
            timestamp: new Date().toISOString(),
            bufferFlushedAt: new Date(now).toISOString(),
          })
        );
      }

      // Verificar se há entradas para salvar
      if (pineconeEntries.length > 0) {
        // Always save to DuckDB (Pinecone deprecated)
        const result = await window.electronAPI?.saveToDuckDB(pineconeEntries);
        if (result?.success) {
          LoggingUtils.logInfo(
            `[Buffer] Persistido no DuckDB: ${pineconeEntries.length} entradas`
          );
        } else {
          LoggingUtils.logError(
            `[Buffer] Erro ao persistir no DuckDB: ${result?.error}`
          );
        }

        // Atualizar timestamp do último flush
        this.messageBuffer.lastFlushTime = now;

        // Limpar o buffer apenas se necessário
        if (resetBufferAfterFlush) {
          LoggingUtils.logInfo(`[Buffer] Resetando buffer após flush`);
          this.resetBuffer();
        } else {
          LoggingUtils.logInfo(
            `[Buffer] Mantendo buffer após salvar resposta do assistente`
          );
        }
      } else {
        LoggingUtils.logInfo(`[Buffer] Nenhuma entrada para salvar no DuckDB`);
      }
    } catch (error) {
      LoggingUtils.logError(
        "[Buffer] Erro ao persistir buffer no DuckDB",
        error
      );
    }
  }

  /**
   * Clears the buffer after persistence (brain buffer reset)
   */
  private resetBuffer(): void {
    this.messageBuffer.primaryUser.messages = [];
    this.messageBuffer.external = {};
    // Keeps lastFlushTime for flush interval control (brain timing)

    LoggingUtils.logInfo(
      `[COGNITIVE-BUFFER] Cognitive buffer reset after neural persistence`
    );
  }

  /**
   * Saves only the assistant's response without touching the buffer (direct brain response persistence)
   * @param answer Assistant response
   */
  private async saveAssistantResponseOnly(answer: string): Promise<void> {
    if (
      !this.isAvailable() ||
      !this.embeddingService.isInitialized() ||
      !answer.trim()
    ) {
      return;
    }

    try {
      const now = Date.now();
      const uuid = now.toString();
      const pineconeEntries = [] as Array<{
        id: string;
        values: number[];
        metadata: Record<string, unknown>;
      }>;

      // Only process the assistant's response (brain response only)
      if (answer.trim()) {
        LoggingUtils.logInfo(
          `[COGNITIVE-BUFFER] Adding only assistant response to DuckDB without buffer flush`
        );
        const assistantEmbedding = await this.embeddingService.createEmbedding(
          answer
        );

        pineconeEntries.push(
          this.createVectorEntry(`assistant-${uuid}`, assistantEmbedding, {
            type: "assistant_response",
            content: answer,
            source: "assistant",
            speakerName: "assistant",
            speakerGroup: "assistant",
            isSpeaker: false,
            isUser: false,
            timestamp: new Date().toISOString(),
            bufferFlushedAt: new Date(now).toISOString(),
          })
        );

        // Always save only the response to DuckDB (Pinecone deprecated)
        if (pineconeEntries.length > 0) {
          const result = await window.electronAPI?.saveToDuckDB(
            pineconeEntries
          );
          if (result?.success) {
            LoggingUtils.logInfo(
              `[COGNITIVE-BUFFER] Persisted only assistant response to DuckDB: ${pineconeEntries.length} entries`
            );
          } else {
            LoggingUtils.logError(
              `[COGNITIVE-BUFFER] Error persisting to DuckDB: ${result?.error}`
            );
          }
        }
      }
    } catch (error) {
      LoggingUtils.logError(
        "[COGNITIVE-BUFFER] Error persisting assistant response to DuckDB",
        error
      );
    }
  }

  /**
   * Conta o total de tokens GPT no buffer atual
   * @returns Número total de tokens no buffer
   */
  private countBufferTokens(): number {
    // Concatenar todas as mensagens do usuário principal
    const userText = this.messageBuffer.primaryUser.messages.join("\n");
    let totalTokens = countTokens(userText);

    LoggingUtils.logInfo(
      `[Buffer-Debug] Texto do usuário: "${userText.substring(0, 50)}..." (${
        userText.length
      } caracteres, ${totalTokens} tokens)`
    );

    // Não contamos tokens de perguntas já que não as armazenamos no buffer

    // Adicionar tokens de todos os falantes externos
    for (const speakerData of Object.values(this.messageBuffer.external)) {
      const speakerText = speakerData.messages.join("\n");
      const speakerTokens = countTokens(speakerText);
      LoggingUtils.logInfo(
        `[Buffer-Debug] Texto de falante externo: "${speakerText.substring(
          0,
          50
        )}..." (${speakerText.length} caracteres, ${speakerTokens} tokens)`
      );
      totalTokens += speakerTokens;
    }

    return totalTokens;
  }

  /**
   * Agrupa transcrições por falante, tratando transcrições mistas
   * @param transcriptions - Lista de transcrições a serem agrupadas
   * @param primaryUserSpeaker - Identificador do falante principal (usuário)
   * @returns Mapa de falantes para suas mensagens agrupadas
   */
  private groupTranscriptionsBySpeaker(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Map<string, string[]> {
    // Inicializa estrutura de dados para armazenar mensagens por falante
    const speakerMessages = new Map<string, string[]>();

    /**
     * Função interna que divide uma transcrição com múltiplos falantes
     * @param text - Texto contendo marcadores de falantes [Speaker] Texto...
     * @returns Array de segmentos com falante normalizado e texto
     */
    const splitMixedTranscription = (
      text: string
    ): Array<{ speaker: string; text: string }> => {
      const results: Array<{ speaker: string; text: string }> = [];
      // Regex otimizada para encontrar padrões [Falante] Texto
      const speakerPattern = /\[([^\]]+)\]\s*(.*?)(?=\s*\[[^\]]+\]|$)/gs;

      // Processa todas as correspondências da regex
      let match;
      while ((match = speakerPattern.exec(text)) !== null) {
        const [, rawSpeaker, spokenText] = match;

        // Validação de dados antes de processar
        if (!rawSpeaker?.trim() || !spokenText?.trim()) continue;

        // Normalização do falante para categorias consistentes
        const normalizedSpeaker = this.normalizeSpeakerName(
          rawSpeaker.trim(),
          primaryUserSpeaker
        );

        results.push({
          speaker: normalizedSpeaker,
          text: spokenText.trim(),
        });
      }

      return results;
    };

    // Itera sobre todas as transcrições
    for (const { text, speaker } of transcriptions) {
      // Detecção eficiente de transcrições mistas (com marcadores de falantes)
      const isMixedTranscription =
        text.indexOf("[") > -1 && text.indexOf("]") > -1;

      if (isMixedTranscription) {
        // Processa transcrições mistas dividindo-as por falante
        const segments = splitMixedTranscription(text);

        // Agrupa textos por falante normalizado
        for (const { speaker: segmentSpeaker, text: segmentText } of segments) {
          // Inicializa array para o falante se necessário
          if (!speakerMessages.has(segmentSpeaker)) {
            speakerMessages.set(segmentSpeaker, []);
          }

          // Adiciona texto ao array do falante
          const messages = speakerMessages.get(segmentSpeaker);
          if (messages) messages.push(segmentText); // Evita o uso de ?. para melhor performance
        }
      } else {
        // Para transcrições normais (sem marcadores), usa o falante da transcrição
        const normalizedSpeaker = this.normalizeSpeakerName(
          speaker,
          primaryUserSpeaker
        );

        // Inicializa array para o falante se necessário
        if (!speakerMessages.has(normalizedSpeaker)) {
          speakerMessages.set(normalizedSpeaker, []);
        }

        // Adiciona texto ao array do falante
        const messages = speakerMessages.get(normalizedSpeaker);
        if (messages) messages.push(text);
      }
    }

    return speakerMessages;
  }

  /**
   * Saves vectors to memory store (DuckDB only - legacy method name for compatibility)
   * @param vectors Array of vectors
   * @returns Promise that resolves when vectors are saved
   */
  public async saveToPinecone(
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ): Promise<void> {
    // Always save to DuckDB (Pinecone deprecated)
    LoggingUtils.logInfo(
      `[MEMORY] Saving ${vectors.length} vectors to DuckDB (legacy saveToPinecone method)`
    );
    const result = await window.electronAPI.saveToDuckDB(vectors);
    if (!result.success) {
      throw new Error(result.error || "Failed to save to DuckDB");
    }
  }

  /**
   * Normalizes the speaker name for consistent categories
   * @param rawSpeaker - Original speaker name
   * @param primaryUserSpeaker - Primary user speaker identifier
   * @returns Normalized speaker name
   */
  private normalizeSpeakerName(
    rawSpeaker: string,
    primaryUserSpeaker: string
  ): string {
    // Converts to lowercase for case-insensitive comparison
    const lowerSpeaker = rawSpeaker.toLowerCase();

    // Categorizes as "primary user" or "external"
    if (rawSpeaker === primaryUserSpeaker) {
      return primaryUserSpeaker;
    } else if (
      lowerSpeaker.includes("speaker") ||
      lowerSpeaker.includes("falante")
    ) {
      return "external";
    }

    // If it doesn't fit any special category, keeps the original
    return rawSpeaker;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// FlushedBatch removed: use an inline object for tests as needed.

// Mock for gpt-tokenizer
jest.mock('gpt-tokenizer', () => ({
  encode: jest.fn().mockImplementation((text) => {
    // Simulated tokenization - approximately 1 token for every 4 characters
    return Array.from({ length: Math.ceil(text.length / 4) }, (_, i) => i);
  }),
}));

// Mock uuid for predictable test results
jest.mock('uuid', () => ({
  v4: jest.fn().mockReturnValue('test-uuid-1234'),
}));

// Import for TextDecoder
import { TextDecoder } from 'util';
// Assign global property
// @ts-expect-error - adding TextDecoder to global object
global.TextDecoder = TextDecoder;

describe('Pinecone Metadata Compatibility Tests', () => {
  beforeEach(() => {
    jest.clearAllMocks();
  });

  it('should correctly format timestamps as compatible metadata for Pinecone', async () => {
    // Mock electronAPI to capture what is sent to Pinecone
    const saveToPineconeMock = jest.fn().mockResolvedValue({ upsertedCount: 1 });
    Object.defineProperty(global, 'window', {
      value: {
        electronAPI: {
          queryPinecone: jest.fn(),
          saveToPinecone: saveToPineconeMock,
        }
      },
      writable: true
    });
    // Mock embedding service
    const mockEmbeddingService = {
      isInitialized: jest.fn().mockReturnValue(true),
      createEmbedding: jest.fn().mockResolvedValue([0.1, 0.2, 0.3]),
      initialize: jest.fn().mockResolvedValue(undefined),
    };
    // Test skipped: No public API for direct batch upsert compatibility testing
    // const service = new PineconeMemoryService(mockEmbeddingService);

    // Create a batch with an array of timestamps (one of the cases that caused the error)
    const testTimestamps = [1745442062588, 1745442064072, 1745441860109];
    const batch = {
      id: 'test-batch-id',
      mergedText: 'Test merged text content',
      metadata: {
        source: 'buffered-conversation',
        roles: ['user'],
        totalMessages: 3,
        timestamps: testTimestamps,
        flushedAt: Date.now(),
        neuralSystemPhase: 'memory',
        processingType: 'symbolic',
        memoryType: 'episodic',
        tokenCount: 50
      }
    };
    // Test skipped: No public API for direct batch upsert compatibility testing
    // await service.handleFlushedBatch('test-user', batch);
    // Test only the batch and metadata structure
    expect(batch.metadata.timestamps).toBe(testTimestamps);
    // Simulate transformation: convert to JSON string
    const timestampsJson = JSON.stringify(batch.metadata.timestamps);
    expect(typeof timestampsJson).toBe('string');
    const parsedTimestamps = JSON.parse(timestampsJson);
    expect(parsedTimestamps).toEqual(testTimestamps);
    // Simulate extraction of first/last
    expect(batch.metadata.timestamps[0]).toBe(testTimestamps[0]);
    expect(batch.metadata.timestamps[testTimestamps.length - 1]).toBe(testTimestamps[testTimestamps.length - 1]);
  });

  it('should correctly handle complex metadata structures for Pinecone compatibility', async () => {
    // Mock electronAPI to capture what is sent to Pinecone
    const saveToPineconeMock = jest.fn().mockResolvedValue({ upsertedCount: 1 });
    Object.defineProperty(global, 'window', {
      value: {
        electronAPI: {
          queryPinecone: jest.fn(),
          saveToPinecone: saveToPineconeMock,
        }
      },
      writable: true
    });
    // Mock embedding service
    const mockEmbeddingService = {
      isInitialized: jest.fn().mockReturnValue(true),
      createEmbedding: jest.fn().mockResolvedValue([0.1, 0.2, 0.3]),
      initialize: jest.fn().mockResolvedValue(undefined),
    };
    // Test skipped: No public API for direct batch upsert compatibility testing
    // const service = new PineconeMemoryService(mockEmbeddingService);

    // Create a batch with complex metadata that could cause issues
    const complexMetadata = {
      source: 'buffered-conversation',
      roles: ['user'],
      totalMessages: 3,
      timestamps: [1745442062588, 1745442064072], // Array de números
      nestedArray: [[1, 2], [3, 4]], // Array aninhado (não suportado pelo Pinecone)
      nestedObject: { key: 'value', count: 42 }, // Objeto aninhado (não suportado pelo Pinecone)
      functionRef: () => {}, // Função (não suportada pelo Pinecone)
      flushedAt: Date.now(),
      neuralSystemPhase: 'memory', // Fase neural - hipocampo
      processingType: 'symbolic',
      memoryType: 'episodic',
      tokenCount: 50
    };
    const batch = {
      id: 'test-batch-id',
      mergedText: 'Test merged text content',
      metadata: complexMetadata
    };
    // Test skipped: No public API for direct batch upsert compatibility testing
    // await service.handleFlushedBatch('test-user', batch);
    // Test only the batch and metadata structure
    // nestedArray and nestedObject should exist
    expect(batch.metadata.nestedArray).toEqual([[1, 2], [3, 4]]);
    expect(batch.metadata.nestedObject).toEqual({ key: 'value', count: 42 });
    // Funções não devem ser serializáveis
    expect(typeof batch.metadata.functionRef).toBe('function');
    // The local batch does not generate timestampsJson, so we don't test it here.
  });

  it('should correctly format neural system phase metadata for the 3-phase system', async () => {
    // Mock electronAPI to capture what is sent to Pinecone
    const saveToPineconeMock = jest.fn().mockResolvedValue({ upsertedCount: 1 });
    Object.defineProperty(global, 'window', {
      value: {
        electronAPI: {
          queryPinecone: jest.fn(),
          saveToPinecone: saveToPineconeMock,
        }
      },
      writable: true
    });
    // Mock embedding service
    const mockEmbeddingService = {
      isInitialized: jest.fn().mockReturnValue(true),
      createEmbedding: jest.fn().mockResolvedValue([0.1, 0.2, 0.3]),
      initialize: jest.fn().mockResolvedValue(undefined),
    };
    // Test skipped: No public API for direct batch upsert compatibility testing
    // const service = new PineconeMemoryService(mockEmbeddingService);

    // Create batches for each phase of the neural system
    const phases = [
      {
        phase: 'memory', // Fase 1: Hipocampo
        speakerType: 'user'
      },
      {
        phase: 'associative', // Fase 2: Córtex associativo
        speakerType: 'system'
      },
      {
        phase: 'metacognitive', // Fase 3: Córtex pré-frontal
        speakerType: 'external'
      }
    ];
    // Test each phase
    for (const testCase of phases) {
      const batch = {
        id: `test-${testCase.phase}-batch`,
        mergedText: `Test content for ${testCase.phase} phase`,
        metadata: {
          source: 'neural-system',
          roles: [testCase.speakerType],
          totalMessages: 1,
          timestamps: [Date.now()],
          flushedAt: Date.now(),
          neuralSystemPhase: testCase.phase,
          processingType: 'symbolic',
          memoryType: 'episodic',
          tokenCount: 50
        }
      };
      // Validate only the batch structure
      expect(batch.metadata.neuralSystemPhase).toBe(testCase.phase);
      expect(batch.metadata.roles).toEqual([testCase.speakerType]);
      expect(batch.metadata.memoryType).toBe('episodic');
    }
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// OllamaClientService.ts
// Symbolic: Gerencia a conexão com a API do Ollama, atuando como ponte neural entre o sistema e o modelo local

import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../../services/StorageService";
import { IClientManagementService } from "../../../interfaces/openai/IClientManagementService";
import { LoggingUtils } from "../../../utils/LoggingUtils";

/**
 * Serviço responsável por gerenciar a conexão com a API do Ollama
 * Symbolic: Neurônio especializado em inicialização e manutenção de caminhos neurais locais
 */
export class OllamaClientService implements IClientManagementService {
  private ollamaClient: any | null = null;
  private baseUrl: string = "http://localhost:11434";

  /**
   * Inicializa o cliente Ollama com configuração fornecida
   * Symbolic: Estabelecimento de conexão neural com modelo local
   */
  initializeClient(config?: string): void {
    if (config) {
      this.baseUrl = config;
    }

    this.ollamaClient = {
      baseUrl: this.baseUrl,
      // Mock OpenAI-like interface for compatibility
      chat: {
        completions: {
          create: this.createCompletion.bind(this),
        },
      },
    };

    LoggingUtils.logInfo("Ollama client initialized successfully");
  }

  /**
   * Carrega a configuração do Ollama do ambiente (.env) ou armazenamento local
   * Symbolic: Recuperação de credencial neural seguindo hierarquia de prioridade
   */
  async loadApiKey(): Promise<string> {
    // Prioridade 1: Variável de ambiente (.env) via Electron API
    try {
      const envUrl = await window.electronAPI.getEnv("OLLAMA_URL");
      if (envUrl?.trim()) {
        this.baseUrl = envUrl.trim();
        LoggingUtils.logInfo("Ollama URL loaded from environment variables");
        return this.baseUrl;
      }
    } catch (error) {
      LoggingUtils.logInfo(
        "Could not load Ollama URL from environment, using default"
      );
    }

    // Prioridade 2: Usar URL padrão (Ollama não precisa de configuração especial)
    LoggingUtils.logInfo("Using default Ollama configuration");

    LoggingUtils.logInfo("Using default Ollama URL: http://localhost:11434");
    return this.baseUrl;
  }

  /**
   * Garante que o cliente Ollama está inicializado, carregando a configuração se necessário
   * Symbolic: Verificação e reparação de caminho neural para modelo local
   */
  async ensureClient(): Promise<boolean> {
    if (this.isInitialized()) return true;

    // If no client, try to load from environment/storage
    if (!this.ollamaClient) {
      const baseUrl = await this.loadApiKey();
      if (!baseUrl) {
        LoggingUtils.logError("Failed to load Ollama configuration");
        return false;
      }
    }

    // Initialize the client
    this.initializeClient(this.baseUrl);

    // Test connection
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`);
      if (!response.ok) {
        throw new Error(
          `Failed to connect to Ollama service: ${response.statusText}`
        );
      }
      return true;
    } catch (error) {
      LoggingUtils.logError(
        `Failed to connect to Ollama: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
      return false;
    }
  }

  /**
   * Verifica se o cliente Ollama está inicializado
   * Symbolic: Inspeção do estado de conexão neural
   */
  isInitialized(): boolean {
    return !!this.ollamaClient && !!this.baseUrl;
  }

  /**
   * Retorna o cliente Ollama se inicializado, ou lança erro
   * Symbolic: Acesso ao canal neural estabelecido ou falha explícita
   */
  getClient(): any {
    if (!this.ollamaClient) {
      throw new Error("Ollama client not initialized");
    }
    return this.ollamaClient;
  }

  /**
   * Cria embeddings para o texto fornecido
   * Symbolic: Transformação de texto em representação vetorial neural
   */
  async createEmbedding(text: string): Promise<number[]> {
    await this.ensureClient();

    try {
      const embeddingModel =
        getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL) || "bge-m3:latest";

      const response = await fetch(`${this.baseUrl}/api/embeddings`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: embeddingModel,
          prompt: text,
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama embedding failed: ${response.statusText}`);
      }

      const data = await response.json();
      return data.embedding || [];
    } catch (error) {
      LoggingUtils.logError(
        `Error creating embedding: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
      return [];
    }
  }

  /**
   * Cria embeddings para um lote de textos (processamento em batch)
   * Symbolic: Transformação em massa de textos em vetores neurais
   */
  async createEmbeddings(texts: string[]): Promise<number[][]> {
    await this.ensureClient();

    try {
      const embeddings: number[][] = [];

      // Process texts one by one (Ollama doesn't support batch embeddings)
      for (const text of texts) {
        const embedding = await this.createEmbedding(text);
        embeddings.push(embedding);
      }

      return embeddings;
    } catch (error) {
      LoggingUtils.logError(
        `Error creating embeddings batch: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
      return [];
    }
  }

  /**
   * Método privado para criar completions (compatibilidade com interface OpenAI)
   */
  private async createCompletion(options: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    stream?: boolean;
    temperature?: number;
    max_tokens?: number;
    tools?: any[];
    tool_choice?: any;
  }): Promise<any> {
    try {
      const response = await fetch(`${this.baseUrl}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: options.model,
          messages: options.messages,
          stream: options.stream || false,
          options: {
            temperature: options.temperature,
            num_predict: options.max_tokens,
          },
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.statusText}`);
      }

      return await response.json();
    } catch (error) {
      LoggingUtils.logError(
        `Error in Ollama completion: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
      throw error;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// OllamaCompletionService.ts
// Symbolic: Processamento de completions e function calling com Ollama local
import {
  STORAGE_KEYS,
  getOption,
} from "../../../../../../services/StorageService";
import { IClientManagementService } from "../../../interfaces/openai/IClientManagementService";
import {
  ICompletionService,
  ModelStreamResponse,
} from "../../../interfaces/openai/ICompletionService";
import { LoggingUtils } from "../../../utils/LoggingUtils";
import {
  cleanThinkTags,
  cleanThinkTagsFromToolCalls,
} from "../../../utils/ThinkTagCleaner";

/**
 * Serviço responsável por gerar completions com function calling usando Ollama
 * Symbolic: Neurônio especializado em processamento de texto e chamadas de funções
 */
export class OllamaCompletionService implements ICompletionService {
  private clientService: IClientManagementService;

  constructor(clientService: IClientManagementService) {
    this.clientService = clientService;
  }

  /**
   * Envia uma requisição ao modelo de linguagem com suporte a function calling
   * Symbolic: Processamento neural para geração de texto ou execução de função
   */
  async callModelWithFunctions(options: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    tools?: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }>;
    temperature?: number;
    max_tokens?: number;
  }): Promise<{
    choices: Array<{
      message: {
        content?: string;
        tool_calls?: Array<{
          function: {
            name: string;
            arguments: string | Record<string, any>;
          };
        }>;
      };
    }>;
  }> {
    let retryCount = 0;
    const maxRetries = 2;

    while (retryCount <= maxRetries) {
      try {
        // Ensure the Ollama client is available
        await this.clientService.ensureClient();

        // Convert messages to Ollama format
        const formattedMessages = options.messages.map((m) => ({
          // Convert 'developer' to 'system' for Ollama compatibility
          role: m.role === "developer" ? "system" : m.role,
          content: m.content,
        }));

        // Get model with fallback logic
        let selectedModel =
          getOption(STORAGE_KEYS.OLLAMA_MODEL) || "mistral:7b-instruct";

        console.log(`🦙 [OllamaCompletion] Selected model: ${selectedModel}`);

        // Verify model is available before making the request
        try {
          console.log(
            `🦙 [OllamaCompletion] Checking if model ${selectedModel} is available...`
          );
          const modelsResponse = await fetch("http://localhost:11434/api/tags");
          if (modelsResponse.ok) {
            const modelsData = await modelsResponse.json();
            const availableModels =
              modelsData.models?.map((m: any) => m.name) || [];
            console.log(
              `🦙 [OllamaCompletion] Available models: ${availableModels.join(
                ", "
              )}`
            );

            if (!availableModels.includes(selectedModel)) {
              console.warn(
                `🦙 [OllamaCompletion] Model ${selectedModel} not found, available: ${availableModels.join(
                  ", "
                )}`
              );
              // Try to use the first available model that matches our filtered list
              const fallbackModels = [
                "qwen3:4b",
                "mistral:latest",
                "mistral-nemo:latest",
                "llama3.2:latest",
              ];
              const availableFallback = fallbackModels.find((model) =>
                availableModels.includes(model)
              );
              if (availableFallback) {
                console.log(
                  `🦙 [OllamaCompletion] Using fallback model: ${availableFallback}`
                );
                selectedModel = availableFallback;
              }
            }
          }
        } catch (modelCheckError) {
          console.warn(
            `🦙 [OllamaCompletion] Could not check available models:`,
            modelCheckError
          );
        }

        // Build request options following Ollama native API documentation
        const requestOptions: any = {
          model: selectedModel,
          messages: formattedMessages,
          stream: false, // Don't use stream for function calling
          options: {
            temperature: options.temperature || 0.1, // Lower temperature for more consistent function calling
            num_predict: options.max_tokens || 4096,
            top_p: 0.9,
            repeat_penalty: 1.1,
          },
        };

        // Add tools in native Ollama format if provided
        if (options.tools && options.tools.length > 0) {
          // Convert tools to Ollama's native format
          requestOptions.tools = options.tools.map((tool) => ({
            type: "function",
            function: {
              name: tool.function.name,
              description: tool.function.description,
              parameters: tool.function.parameters,
            },
          }));

          LoggingUtils.logInfo(
            `🦙 [OllamaCompletion] Using native tools: ${JSON.stringify(
              requestOptions.tools,
              null,
              2
            )}`
          );
        }

        // Note: tool_choice is not supported yet by Ollama (as per official documentation)
        // It's a planned future improvement

        // Add GPU control for Metal acceleration issues (macOS)
        if (retryCount > 0) {
          // On retry, force CPU mode to avoid Metal acceleration issues
          requestOptions.options.num_gpu = 0;
          LoggingUtils.logWarning(
            `🦙 [OllamaCompletion] Retry ${retryCount}: Forcing CPU mode due to potential Metal acceleration issues`
          );
        }

        LoggingUtils.logInfo(
          `🦙 [OllamaCompletion] Calling Ollama API with model: ${selectedModel}`
        );
        LoggingUtils.logInfo(
          `🦙 [OllamaCompletion] Request options: ${JSON.stringify(
            requestOptions,
            null,
            2
          )}`
        );

        // Perform the Ollama chat completion using the official API endpoint
        console.log(`🦙 [OllamaCompletion] About to fetch with timeout...`);

        let data: any;

        try {
          const response = await fetch("http://localhost:11434/api/chat", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify(requestOptions),
          });

          console.log(
            `🦙 [OllamaCompletion] Fetch completed, response status: ${response.status}`
          );
          console.log(`🦙 [OllamaCompletion] Response ok: ${response.ok}`);

          if (!response.ok) {
            const errorText = await response.text();
            console.error(
              `🦙 [OllamaCompletion] API Error - Status: ${response.status}, Text: ${errorText}`
            );
            throw new Error(
              `Ollama API error: ${response.statusText} - ${errorText}`
            );
          }

          console.log(`🦙 [OllamaCompletion] About to parse JSON response...`);
          data = await response.json();
          console.log(`🦙 [OllamaCompletion] JSON parsed successfully`);
        } catch (fetchError: any) {
          console.warn(
            `🦙 [OllamaCompletion] Fetch failed, trying curl fallback:`,
            fetchError.message
          );
        }

        LoggingUtils.logInfo(
          `🦙 [OllamaCompletion] Raw response data: ${JSON.stringify(
            data,
            null,
            2
          )}`
        );

        // Clean think tags from content immediately after receiving response
        const rawContent = data.message?.content || "";
        const content = cleanThinkTags(rawContent);
        const nativeToolCalls = data.message?.tool_calls || [];

        LoggingUtils.logInfo(
          `🦙 [OllamaCompletion] Content (cleaned): ${content.substring(
            0,
            200
          )}...`
        );
        LoggingUtils.logInfo(
          `🦙 [OllamaCompletion] Native tool calls: ${JSON.stringify(
            nativeToolCalls,
            null,
            2
          )}`
        );

        // Process native tool calls from Ollama
        let tool_calls:
          | Array<{
              function: {
                name: string;
                arguments: string | Record<string, any>;
              };
            }>
          | undefined;

        if (
          nativeToolCalls &&
          Array.isArray(nativeToolCalls) &&
          nativeToolCalls.length > 0
        ) {
          tool_calls = nativeToolCalls.map((call: any) => {
            const functionName = call.function?.name || call.name;
            let functionArgs = call.function?.arguments || call.arguments;

            // Ensure arguments are properly formatted
            if (typeof functionArgs === "object" && functionArgs !== null) {
              // If it's already an object, stringify it for consistency
              functionArgs = JSON.stringify(functionArgs);
            } else if (typeof functionArgs !== "string") {
              // If it's neither string nor object, convert to string
              functionArgs = JSON.stringify(functionArgs);
            }

            LoggingUtils.logInfo(
              `🦙 [OllamaCompletion] Processed tool call: ${functionName} with args: ${functionArgs}`
            );

            return {
              function: {
                name: functionName,
                arguments: functionArgs,
              },
            };
          });

          // Clean think tags from tool calls
          tool_calls = cleanThinkTagsFromToolCalls(tool_calls);

          LoggingUtils.logInfo(
            `🦙 [OllamaCompletion] Successfully processed ${tool_calls.length} native tool calls (cleaned)`
          );
        }

        // Fallback: Try to parse function calls from content if no native tool calls
        if (!tool_calls || tool_calls.length === 0) {
          // Only attempt fallback parsing if tools were provided and content looks like JSON
          if (
            options.tools &&
            options.tools.length > 0 &&
            content.includes("{")
          ) {
            try {
              const cleanContent = content.trim();

              // Try to extract JSON from content
              let functionCallData = null;

              // Look for JSON in code blocks or standalone
              const jsonMatches = [
                /```(?:json)?\s*(\{[\s\S]*?\})\s*```/g,
                /(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})/g,
              ];

              for (const regex of jsonMatches) {
                const matches = [...cleanContent.matchAll(regex)];
                for (const match of matches) {
                  try {
                    const candidate = JSON.parse(match[1]);
                    // Check if it looks like a function call
                    if (candidate.function_name || candidate.function_call) {
                      functionCallData = candidate;
                      LoggingUtils.logInfo(
                        `🦙 [OllamaCompletion] Extracted function call from content (fallback)`
                      );
                      break;
                    }
                  } catch (e) {
                    continue;
                  }
                }
                if (functionCallData) break;
              }

              // Convert to tool_calls format if found
              if (functionCallData) {
                let functionName =
                  functionCallData.function_name ||
                  functionCallData.function_call?.name ||
                  "";
                let functionArgs =
                  functionCallData.parameters ||
                  functionCallData.function_call?.arguments ||
                  {};

                if (functionName) {
                  tool_calls = [
                    {
                      function: {
                        name: functionName,
                        arguments:
                          typeof functionArgs === "string"
                            ? functionArgs
                            : JSON.stringify(functionArgs),
                      },
                    },
                  ];
                  LoggingUtils.logInfo(
                    `🦙 [OllamaCompletion] Fallback function call parsed: ${functionName}`
                  );
                }
              }
            } catch (parseError) {
              LoggingUtils.logWarning(
                `🦙 [OllamaCompletion] Fallback parsing failed: ${parseError}`
              );
            }
          }
        }

        console.log(
          `🦙 [OllamaCompletion] About to return response with tool_calls: ${
            tool_calls ? "YES" : "NO"
          }`
        );
        console.log(
          `🦙 [OllamaCompletion] Tool calls count: ${tool_calls?.length || 0}`
        );

        // Clean think tags from tool calls if present
        if (tool_calls && tool_calls.length > 0) {
          tool_calls = cleanThinkTagsFromToolCalls(tool_calls);
        }

        // Convert the response to expected format
        return {
          choices: [
            {
              message: {
                content: tool_calls ? undefined : content,
                tool_calls,
              },
            },
          ],
        };
      } catch (error) {
        retryCount++;

        // Check if this is a Metal acceleration error
        const errorMessage =
          error instanceof Error ? error.message : String(error);
        const isMetalError =
          errorMessage.includes("Metal") ||
          errorMessage.includes("Internal Error") ||
          errorMessage.includes("command buffer") ||
          errorMessage.includes("status 5");

        if (isMetalError && retryCount <= maxRetries) {
          LoggingUtils.logWarning(
            `🦙 [OllamaCompletion] Metal acceleration error detected, retrying with CPU mode (attempt ${retryCount}/${maxRetries})`
          );
          continue; // Retry with CPU mode
        }

        // Log the error
        LoggingUtils.logError(
          `Error calling language model (attempt ${retryCount}/${
            maxRetries + 1
          }): ${errorMessage}`
        );

        if (retryCount > maxRetries) {
          console.error("Error in model completion call:", error);
          throw error;
        }
      }
    }

    // This should never be reached, but TypeScript requires it
    throw new Error("Maximum retries exceeded");
  }

  /**
   * Envia requisição para o modelo e processa o stream de resposta
   * Symbolic: Fluxo neural contínuo de processamento de linguagem
   */
  async streamModelResponse(
    messages: Array<{ role: string; content: string }>
  ): Promise<ModelStreamResponse> {
    try {
      // Ensure the Ollama client is available
      await this.clientService.ensureClient();

      // Convert messages to Ollama format
      const formattedMessages = messages.map((m) => ({
        role: m.role === "developer" ? "system" : m.role,
        content: m.content,
      }));

      // Use the official Ollama API endpoint
      const response = await fetch("http://localhost:11434/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: getOption(STORAGE_KEYS.OLLAMA_MODEL) || "qwen3:latest",
          messages: formattedMessages,
          stream: false,
        }),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(
          `Ollama API error: ${response.statusText} - ${errorText}`
        );
      }

      const data = await response.json();
      const rawResponse = data.message?.content || "";

      // Clean think tags from streaming response
      const fullResponse = cleanThinkTags(rawResponse);

      return {
        responseText: fullResponse,
        messageId: Date.now().toString(),
        isComplete: true,
        isDone: true,
      };
    } catch (error) {
      LoggingUtils.logError(
        `Error streaming model response: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
      throw error;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// OllamaEmbeddingService.ts
// Implementation of IEmbeddingService using Ollama

import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { IEmbeddingService } from "../../interfaces/openai/IEmbeddingService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { LoggingUtils } from "../../utils/LoggingUtils";

/**
 * Modelos de embedding suportados pelo Ollama
 * Documentação: https://ollama.ai/library
 */
export const SUPPORTED_OLLAMA_EMBEDDING_MODELS = [
  // Modelos de embedding populares no Ollama
  "bge-m3:latest", // Modelo multilíngue avançado com dense + sparse + multi-vector
  "nomic-embed-text:latest", // Modelo padrão para embeddings de texto
  "mxbai-embed-large:latest", // Modelo de alta qualidade
  "all-minilm:latest", // Modelo compacto e eficiente
  "snowflake-arctic-embed:latest", // Modelo Arctic da Snowflake
];

/**
 * Configurações para o serviço de embeddings do Ollama
 */
export interface OllamaEmbeddingOptions {
  model?: string;
}

export class OllamaEmbeddingService implements IEmbeddingService {
  private ollamaService: IOpenAIService;
  private options: OllamaEmbeddingOptions;

  constructor(ollamaService: IOpenAIService, options?: OllamaEmbeddingOptions) {
    this.ollamaService = ollamaService;
    this.options = options || {};
  }

  /**
   * Obtém o modelo de embeddings configurado ou o padrão
   */
  private getEmbeddingModel(): string {
    // Prioridade: 1. Configuração via construtor, 2. Storage, 3. Padrão (bge-m3:latest)
    const model =
      this.options.model ||
      getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL) ||
      "bge-m3:latest";

    // Log para diagnóstico
    console.log(`[OllamaEmbedding] Using embedding model: ${model}`);

    return model;
  }

  /**
   * Creates an embedding for the provided text using Ollama
   */
  async createEmbedding(text: string): Promise<number[]> {
    if (!text?.trim()) {
      return [];
    }

    try {
      // Delegate to the Ollama service with the selected model
      const model = this.getEmbeddingModel();
      return await this.ollamaService.createEmbedding(text.trim(), model);
    } catch (error) {
      LoggingUtils.logError("Error creating embedding", error);
      return [];
    }
  }

  /**
   * Creates embeddings for a batch of texts using Ollama
   * @param texts Array of texts to create embeddings for
   * @returns Array of embeddings (array of number arrays)
   */
  async createEmbeddings(texts: string[]): Promise<number[][]> {
    if (!texts?.length) {
      return [];
    }

    try {
      // Get the selected model
      const model = this.getEmbeddingModel();

      // Check if the Ollama service supports batch embeddings directly
      if (this.ollamaService.createEmbeddings) {
        // Use the batch API if available
        return await this.ollamaService.createEmbeddings(
          texts.map((text) => text.trim()),
          model
        );
      } else {
        // Fallback: process embeddings one by one
        const embeddings = await Promise.all(
          texts.map(async (text) => {
            try {
              return await this.ollamaService.createEmbedding(
                text.trim(),
                model
              );
            } catch (err) {
              LoggingUtils.logError(
                `Error generating embedding for text: ${text.substring(
                  0,
                  50
                )}...`,
                err
              );
              return []; // Return empty array on error
            }
          })
        );

        return embeddings;
      }
    } catch (error) {
      LoggingUtils.logError("Error creating batch embeddings", error);
      return [];
    }
  }

  /**
   * Checks if the embedding service is initialized
   */
  isInitialized(): boolean {
    return this.ollamaService.isInitialized();
  }

  /**
   * Initializes the embedding service
   */
  async initialize(config?: Record<string, any>): Promise<boolean> {
    if (!this.ollamaService) {
      return false;
    }

    if (this.isInitialized()) {
      return true;
    }

    try {
      // If base URL is provided in config, use it
      if (config?.baseUrl) {
        this.ollamaService.initializeOpenAI(config.baseUrl);
        return this.isInitialized();
      }

      // Otherwise try to load from environment
      await this.ollamaService.loadApiKey();
      return this.isInitialized();
    } catch (error) {
      LoggingUtils.logError("Error initializing embedding service", error);
      return false;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// OllamaServiceFacade.ts
// Symbolic: Fachada neural que integra e coordena diferentes serviços neurais especializados

import { OllamaNeuralSignalService } from "../../infrastructure/neural/ollama/OllamaNeuralSignalService";
import { NeuralSignalResponse } from "../../interfaces/neural/NeuralSignalTypes";
import { ModelStreamResponse } from "../../interfaces/openai/ICompletionService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { Message } from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { OllamaClientService } from "./neural/OllamaClientService";
import { OllamaCompletionService } from "./neural/OllamaCompletionService";

/**
 * Fachada que implementa IOpenAIService e coordena os serviços especializados
 * Symbolic: Córtex de integração neural que combina neurônios especializados
 */
export class OllamaServiceFacade implements IOpenAIService {
  private clientService: OllamaClientService;
  private completionService: OllamaCompletionService;
  private neuralSignalService: OllamaNeuralSignalService;

  constructor() {
    // Inicializar os serviços especializados
    this.clientService = new OllamaClientService();
    this.completionService = new OllamaCompletionService(this.clientService);
    this.neuralSignalService = new OllamaNeuralSignalService(
      this.completionService
    );

    LoggingUtils.logInfo(
      "Initialized Ollama Service Facade with specialized neural services"
    );
  }

  /**
   * Inicializa o cliente Ollama
   * Symbolic: Estabelecimento de conexão neural com modelo local
   */
  initializeOpenAI(config?: string): void {
    this.clientService.initializeClient(config);
  }

  /**
   * Carrega a configuração do Ollama do armazenamento
   * Symbolic: Recuperação de credencial neural
   */
  async loadApiKey(): Promise<void> {
    await this.clientService.loadApiKey();
  }

  /**
   * Garante que o cliente Ollama está disponível
   * Symbolic: Verificação de integridade do caminho neural
   */
  async ensureOpenAIClient(): Promise<boolean> {
    return this.clientService.ensureClient();
  }

  /**
   * Envia requisição para Ollama e processa o stream de resposta
   * Symbolic: Fluxo neural contínuo de processamento de linguagem
   */
  async streamOpenAIResponse(
    messages: Message[]
  ): Promise<ModelStreamResponse> {
    // Mapear as mensagens para o formato esperado pelo serviço de completion
    const mappedMessages = messages.map((m) => ({
      role: m.role,
      content: m.content,
    }));

    return await this.completionService.streamModelResponse(mappedMessages);
  }

  /**
   * Cria embeddings para o texto fornecido
   * Symbolic: Transformação de texto em representação neural vetorial
   */
  async createEmbedding(text: string): Promise<number[]> {
    return this.clientService.createEmbedding(text);
  }

  /**
   * Cria embeddings para um lote de textos (processamento em batch)
   * Symbolic: Transformação em massa de textos em vetores neurais
   */
  async createEmbeddings(texts: string[]): Promise<number[][]> {
    return this.clientService.createEmbeddings(texts);
  }

  /**
   * Verifica se o cliente Ollama está inicializado
   * Symbolic: Consulta do estado de conexão neural
   */
  isInitialized(): boolean {
    return this.clientService.isInitialized();
  }

  /**
   * Gera sinais neurais simbólicos baseados em um prompt
   * Symbolic: Extração de padrões de ativação neural a partir de estímulo de linguagem
   */
  async generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse> {
    return this.neuralSignalService.generateNeuralSignal(
      prompt,
      temporaryContext,
      language
    );
  }

  /**
   * Expande semanticamente a query de um núcleo cerebral
   * Symbolic: Expansão de campo semântico para ativação cortical específica
   */
  async enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }> {
    return this.neuralSignalService.enrichSemanticQueryForSignal(
      core,
      query,
      intensity,
      context,
      language
    );
  }

  /**
   * Envia uma requisição ao Ollama com suporte a function calling
   * Symbolic: Processamento neural para geração de texto ou execução de função
   */
  async callOpenAIWithFunctions(options: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    tools?: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }>;
    tool_choice?: { type: string; function: { name: string } };
    temperature?: number;
    max_tokens?: number;
  }): Promise<{
    choices: Array<{
      message: {
        content?: string;
        tool_calls?: Array<{
          function: {
            name: string;
            arguments: string | Record<string, any>;
          };
        }>;
      };
    }>;
  }> {
    return this.completionService.callModelWithFunctions(options);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { LoggingUtils } from "../../../utils/LoggingUtils";

/**
 * QuestionDetector - Single Responsibility: Detect questions and manage question cycles
 * Follows YAGNI principle by only implementing what's currently needed
 */
export class QuestionDetector {
  private lastQuestionPrompt: string = "";
  private lastPromptTimestamp: number = 0;
  private autoQuestionDetectionEnabled: boolean = false;
  private questionTimerId: NodeJS.Timeout | null = null;
  private pendingQuestionText: string = "";
  private inQuestionCycle: boolean = false;

  /**
   * Check if auto-detection is enabled
   */
  isAutoDetectionEnabled(): boolean {
    return this.autoQuestionDetectionEnabled;
  }

  /**
   * Enable or disable automatic question detection
   */
  setAutoDetection(enabled: boolean): void {
    this.autoQuestionDetectionEnabled = enabled;
    LoggingUtils.logInfo(
      `Auto-question detection ${enabled ? "enabled" : "disabled"}`
    );
  }

  /**
   * Check if text is a question
   */
  isQuestion(text: string): boolean {
    return text.trim().endsWith("?");
  }

  /**
   * Check if this is a duplicate question
   */
  isDuplicateQuestion(text: string): boolean {
    const trimmedText = text.trim();
    if (trimmedText === this.lastQuestionPrompt) {
      LoggingUtils.logInfo(`Duplicate question detected: "${trimmedText}"`);
      return true;
    }
    return false;
  }

  /**
   * Check if currently in a question cycle
   */
  isInQuestionCycle(): boolean {
    return this.inQuestionCycle;
  }

  /**
   * Start a question cycle
   */
  startQuestionCycle(questionText: string): void {
    if (this.inQuestionCycle) {
      LoggingUtils.logInfo(
        `Already in a question cycle, ignoring new question: "${questionText}"`
      );
      return;
    }

    this.lastQuestionPrompt = questionText;
    this.lastPromptTimestamp = Date.now();
    this.pendingQuestionText = questionText;
    this.inQuestionCycle = true;

    LoggingUtils.logInfo(
      `Question detected: "${questionText}". Starting question cycle...`
    );

    // Note: Auto-send is disabled by default
    LoggingUtils.logInfo(
      `Question detected but NOT sent (auto-send disabled): "${questionText}"`
    );
  }

  /**
   * End question cycle
   */
  endQuestionCycle(): void {
    this.inQuestionCycle = false;
    this.pendingQuestionText = "";
    LoggingUtils.logInfo("Question cycle ended.");
  }

  /**
   * Cancel pending question timer
   */
  cancelPendingTimer(): void {
    if (this.questionTimerId) {
      LoggingUtils.logInfo(
        `Canceling pending question timer for: "${this.pendingQuestionText}"`
      );
      clearTimeout(this.questionTimerId);
      this.questionTimerId = null;
    }
    this.endQuestionCycle();
  }

  /**
   * Handle new transcription during question cycle
   */
  handleNewTranscriptionDuringCycle(text: string): void {
    if (this.inQuestionCycle) {
      LoggingUtils.logInfo(
        `New transcription received, ending question cycle: "${text.trim()}"`
      );
      this.cancelPendingTimer();
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  EXTERNAL_SPEAKER_LABEL,
  SpeakerTranscription,
  SpeakerTranscriptionLog,
} from "../../../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../../../interfaces/utils/ISpeakerIdentificationService";

/**
 * TranscriptionLogger - Single Responsibility: Format transcription logs
 * Follows KISS principle by keeping formatting logic simple and clear
 */
export class TranscriptionLogger {
  private speakerService: ISpeakerIdentificationService;

  constructor(speakerService: ISpeakerIdentificationService) {
    this.speakerService = speakerService;
  }

  /**
   * Generate transcription logs grouped by speaker
   */
  generateLogs(
    speakerTranscriptions: SpeakerTranscription[]
  ): SpeakerTranscriptionLog[] {
    return this.getTranscriptionLogsByUser(speakerTranscriptions);
  }

  /**
   * Returns logs grouped by speaker
   */
  private getTranscriptionLogsByUser(
    speakerTranscriptions: SpeakerTranscription[]
  ): SpeakerTranscriptionLog[] {
    const tempGroups = new Map<
      string,
      {
        isUser: boolean;
        displayName: string;
        transcriptions: { text: string; timestamp: string }[];
      }
    >();

    const processedTexts = new Map<string, Set<string>>();
    const speakerNumbers = new Map<string, string>();

    for (const transcription of speakerTranscriptions) {
      this.processTranscription(
        transcription,
        tempGroups,
        processedTexts,
        speakerNumbers
      );
    }

    return this.formatLogsOutput(tempGroups);
  }

  /**
   * Process a single transcription
   */
  private processTranscription(
    transcription: SpeakerTranscription,
    tempGroups: Map<string, any>,
    processedTexts: Map<string, Set<string>>,
    speakerNumbers: Map<string, string>
  ): void {
    if (transcription.text.includes("[") && transcription.text.includes("]")) {
      const segments = this.speakerService.splitMixedTranscription(
        transcription.text
      );

      for (const segment of segments) {
        this.processSegment(
          segment,
          transcription.timestamp,
          tempGroups,
          processedTexts,
          speakerNumbers
        );
      }
    } else {
      this.processSimpleTranscription(
        transcription,
        tempGroups,
        processedTexts,
        speakerNumbers
      );
    }
  }

  /**
   * Process a segment
   */
  private processSegment(
    segment: { text: string; speaker: string },
    timestamp: string,
    tempGroups: Map<string, any>,
    processedTexts: Map<string, Set<string>>,
    speakerNumbers: Map<string, string>
  ): void {
    const segmentText = segment.text.replace(/^\[[^\]]+\]\s*/, "");
    const { displayName, groupKey, isUserMsg } = this.getSpeakerInfo(
      segment.text,
      segment.speaker,
      speakerNumbers
    );

    this.addToGroup(
      groupKey,
      displayName,
      isUserMsg,
      segmentText,
      timestamp,
      tempGroups,
      processedTexts
    );
  }

  /**
   * Process simple transcription without segments
   */
  private processSimpleTranscription(
    transcription: SpeakerTranscription,
    tempGroups: Map<string, any>,
    processedTexts: Map<string, Set<string>>,
    speakerNumbers: Map<string, string>
  ): void {
    const { displayName, groupKey, isUserMsg } = this.getSpeakerInfo(
      transcription.text,
      transcription.speaker,
      speakerNumbers
    );

    this.addToGroup(
      groupKey,
      displayName,
      isUserMsg,
      transcription.text,
      transcription.timestamp,
      tempGroups,
      processedTexts
    );
  }

  /**
   * Get speaker information
   */
  private getSpeakerInfo(
    text: string,
    speaker: string,
    speakerNumbers: Map<string, string>
  ): { displayName: string; groupKey: string; isUserMsg: boolean } {
    const normalizedSpeaker = speaker;
    const isUserMsg =
      normalizedSpeaker === this.speakerService.getPrimaryUserSpeaker();

    if (isUserMsg) {
      return {
        displayName: this.speakerService.getPrimaryUserSpeaker(),
        groupKey: this.speakerService.getPrimaryUserSpeaker(),
        isUserMsg: true,
      };
    }

    // Extract speaker info for external speakers
    const originalSpeakerMatch = text.match(/^\[([^\]]+)\]/);
    const originalSpeaker = originalSpeakerMatch?.[1]?.trim();

    if (originalSpeaker?.toLowerCase().includes("speaker")) {
      const speakerNumberMatch = originalSpeaker.match(/speaker\s*(\d+)/i);
      const speakerNum = speakerNumberMatch?.[1];

      if (speakerNum) {
        const groupKey = `speaker_${speakerNum}`;
        const displayName = originalSpeaker;
        speakerNumbers.set(groupKey, displayName);
        return { displayName, groupKey, isUserMsg: false };
      }
    }

    // Check text for speaker mentions
    const textSpeakerMatch = text.toLowerCase().match(/speaker\s*(\d+)/i);
    if (textSpeakerMatch?.[1]) {
      const speakerNum = textSpeakerMatch[1];
      const groupKey = `speaker_${speakerNum}`;
      const displayName =
        speakerNumbers.get(groupKey) || `Speaker ${speakerNum}`;
      speakerNumbers.set(groupKey, displayName);
      return { displayName, groupKey, isUserMsg: false };
    }

    return {
      displayName: EXTERNAL_SPEAKER_LABEL,
      groupKey: "external_generic",
      isUserMsg: false,
    };
  }

  /**
   * Add transcription to group
   */
  private addToGroup(
    groupKey: string,
    displayName: string,
    isUser: boolean,
    text: string,
    timestamp: string,
    tempGroups: Map<string, any>,
    processedTexts: Map<string, Set<string>>
  ): void {
    if (!tempGroups.has(groupKey)) {
      tempGroups.set(groupKey, {
        isUser,
        displayName,
        transcriptions: [],
      });
      processedTexts.set(groupKey, new Set());
    }

    const groupTexts = processedTexts.get(groupKey);
    if (groupTexts && !groupTexts.has(text)) {
      const group = tempGroups.get(groupKey);
      if (group) {
        group.transcriptions.push({ text, timestamp });
        groupTexts.add(text);
      }
    }
  }

  /**
   * Format logs output
   */
  private formatLogsOutput(
    tempGroups: Map<string, any>
  ): SpeakerTranscriptionLog[] {
    const logs: SpeakerTranscriptionLog[] = Array.from(
      tempGroups.entries()
    ).map(([groupKey, data]) => {
      const sortedTranscriptions = [...data.transcriptions].sort(
        (a, b) =>
          new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime()
      );

      const formattedTranscriptions = sortedTranscriptions.map((t, index) => {
        const text = index === 0 ? `[${data.displayName}] ${t.text}` : t.text;
        return { text, timestamp: t.timestamp };
      });

      return {
        speaker: data.displayName,
        isUser: data.isUser,
        transcriptions: formattedTranscriptions,
      };
    });

    return this.sortLogs(logs);
  }

  /**
   * Sort logs by speaker priority
   */
  private sortLogs(logs: SpeakerTranscriptionLog[]): SpeakerTranscriptionLog[] {
    return logs.sort((a, b) => {
      if (a.isUser) return -1;
      if (b.isUser) return 1;

      const numA = a.speaker.match(/speaker\s*(\d+)/i)?.[1];
      const numB = b.speaker.match(/speaker\s*(\d+)/i)?.[1];

      if (numA && numB) {
        return parseInt(numA) - parseInt(numB);
      }

      return a.speaker.localeCompare(b.speaker);
    });
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { UIUpdater } from "../../../interfaces/transcription/TranscriptionTypes";

/**
 * TranscriptionUIManager - Single Responsibility: Manage UI updates
 * Follows Interface Segregation principle by depending only on UIUpdater interface
 */
export class TranscriptionUIManager {
  private setTexts: UIUpdater;
  private uiTranscriptionHistory: string[] = [];

  constructor(setTexts: UIUpdater) {
    this.setTexts = setTexts;
  }

  /**
   * Update UI with new transcription
   */
  updateTranscription(transcription: string): void {
    if (!transcription || !transcription.trim()) return;

    const newText = transcription.trim();

    // Check if this is an incremental update
    const isIncremental = this.handleIncrementalUpdate(newText);

    if (!isIncremental) {
      this.addNewTranscription(newText);
    }

    // Update UI with complete history
    this.updateUI();
  }

  /**
   * Handle incremental updates (extending previous messages)
   */
  private handleIncrementalUpdate(newText: string): boolean {
    for (let i = this.uiTranscriptionHistory.length - 1; i >= 0; i--) {
      const existingText = this.uiTranscriptionHistory[i];

      if (newText.startsWith(existingText) && newText !== existingText) {
        console.log(`🛠️ Replacing incremental message:`);
        console.log(`  Previous: "${existingText}"`);
        console.log(`  New: "${newText}"`);

        this.uiTranscriptionHistory[i] = newText;
        return true;
      }
    }
    return false;
  }

  /**
   * Add new transcription to history
   */
  private addNewTranscription(text: string): void {
    if (
      this.uiTranscriptionHistory.length === 0 ||
      this.uiTranscriptionHistory[this.uiTranscriptionHistory.length - 1] !==
        text
    ) {
      console.log(`💾 Adding new message: "${text}"`);
      this.uiTranscriptionHistory.push(text);
    }
  }

  /**
   * Update UI with current transcription history
   */
  private updateUI(): void {
    const fullText = this.uiTranscriptionHistory.join("\n");
    console.log(
      `📄 UI state: ${fullText.length} chars, ${this.uiTranscriptionHistory.length} messages`
    );

    this.setTexts((prev) => ({
      ...prev,
      transcription: fullText,
    }));
  }

  /**
   * Flush all pending transcriptions to UI
   */
  flushToUI(transcriptions: string[]): void {
    if (transcriptions.length === 0) {
      console.log("No transcriptions to flush");
      return;
    }

    const allTranscriptions = transcriptions.join("\n");
    console.log(`Flushing ${transcriptions.length} transcriptions to UI`);

    this.updateTranscription(allTranscriptions);
  }

  /**
   * Update UI with other properties
   */
  updateOther(update: Record<string, unknown>): void {
    this.setTexts((prev) => ({ ...prev, ...update }));
  }

  /**
   * Clear UI transcription history
   */
  clear(): void {
    this.uiTranscriptionHistory = [];
    this.updateUI();
  }

  /**
   * Get current UI transcription history
   */
  getHistory(): string[] {
    return [...this.uiTranscriptionHistory];
  }

  /**
   * Set transcription text directly without managing history
   * Used when we want to show only new transcriptions after sending
   */
  setTranscriptionDirectly(text: string): void {
    console.log(`🔄 Setting UI transcription directly: "${text}"`);

    // Clear history and set new text
    this.uiTranscriptionHistory = text ? [text] : [];

    // Update UI
    this.setTexts((prev) => ({
      ...prev,
      transcription: text,
    }));
  }

  /**
   * Get only new transcriptions text for UI display
   */
  getNewTranscriptionsText(store: any): string {
    const newTranscriptions = store.getNewTranscriptions();
    return newTranscriptions.map((t: any) => t.text).join("\n");
  }

  /**
   * Clear sent transcriptions from UI history
   * Called after transcriptions are marked as sent
   */
  clearSentTranscriptions(): void {
    console.log(`🧹 Clearing UI history after sending transcriptions`);
    this.uiTranscriptionHistory = [];
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Neural Cognitive Processors Export Index
// Symbolic modules for transcription processing pipeline

export { NeuralConfigurationBuilder } from './NeuralConfigurationBuilder';
export { NeuralMemoryRetriever } from './NeuralMemoryRetriever';
export { NeuralSignalEnricher } from './NeuralSignalEnricher';
export { ProcessingResultsSaver } from './ProcessingResultsSaver';
export { ResponseGenerator } from './ResponseGenerator';
export { SessionManager } from './SessionManager';
export { TranscriptionExtractor } from './TranscriptionExtractor';

// Type exports
export type { TranscriptionPromptConfig } from './NeuralConfigurationBuilder';
export type { ProcessorMode } from './NeuralSignalEnricher';

// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { getPrimaryUser } from '../../../../../../config/UserConfig';
import { IMemoryService } from '../../../interfaces/memory/IMemoryService';
import { ITranscriptionStorageService } from '../../../interfaces/transcription/ITranscriptionStorageService';
import { SpeakerTranscription } from '../../../interfaces/transcription/TranscriptionTypes';
import { ISpeakerIdentificationService } from '../../../interfaces/utils/ISpeakerIdentificationService';
import { SessionManager } from './SessionManager';

/**
 * Configuration for transcription prompt processing
 */
export interface TranscriptionPromptConfig {
  transcription: string;
  temporaryContext?: string;
  sessionState: {
    sessionId: string;
    interactionCount: number;
    timestamp: string;
    language: string;
  };
  speakerMetadata: {
    primarySpeaker: string;
    detectedSpeakers: string[];
    speakerTranscriptions: SpeakerTranscription[];
  };
  userContextData?: any;
}

/**
 * Neural configuration synthesis builder
 * Responsible for constructing configuration objects for neural signal extraction
 */
export class NeuralConfigurationBuilder {
  constructor(
    private storageService: ITranscriptionStorageService,
    private memoryService: IMemoryService,
    private speakerService: ISpeakerIdentificationService,
    private sessionManager: SessionManager
  ) {}

  /**
   * Build comprehensive configuration for neural signal extraction
   */
  async buildExtractionConfig(
    transcriptionToSend: string, 
    temporaryContext?: string,
    currentLanguage: string = 'pt-BR'
  ): Promise<TranscriptionPromptConfig> {
    
    const userTranscriptions = this.storageService.getSpeakerTranscriptions()
      .filter(transcription => transcription.speaker.includes(getPrimaryUser()));
    
    const detectedSpeakers = this.storageService.getDetectedSpeakers();
    
    const externalTranscriptions = this.storageService.getSpeakerTranscriptions()
      .filter(transcription => transcription.speaker !== getPrimaryUser());

    const userContextData = await this.memoryService.fetchContextualMemory(
      userTranscriptions,
      externalTranscriptions,
      detectedSpeakers,
      temporaryContext
    );

    return {
      transcription: transcriptionToSend,
      temporaryContext,
      sessionState: {
        sessionId: this.sessionManager.getCurrentSessionId(),
        interactionCount: this.sessionManager.getCurrentInteractionCount(),
        timestamp: new Date().toISOString(),
        language: currentLanguage
      },
      speakerMetadata: {
        primarySpeaker: this.speakerService.getPrimaryUserSpeaker(),
        detectedSpeakers: Array.from(detectedSpeakers),
        speakerTranscriptions: userTranscriptions
      },
      userContextData
    };
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IMemoryService } from "../../../interfaces/memory/IMemoryService";
import {
  NeuralProcessingResult,
  NeuralSignal,
} from "../../../interfaces/neural/NeuralSignalTypes";
import { SymbolicInsight } from "../../../types/SymbolicInsight";
import { LoggingUtils } from "../../../utils/LoggingUtils";
import symbolicCognitionTimelineLogger from "../../utils/SymbolicCognitionTimelineLoggerSingleton";
import { ProcessorMode } from "./NeuralSignalEnricher";

/**
 * Neural memory cognitive retrieval processor
 * Responsible for processing neural signals and retrieving relevant memories
 */
export class NeuralMemoryRetriever {
  constructor(private memoryService: IMemoryService) {}

  /**
   * Process neural signals and retrieve relevant memories based on selected backend
   */
  async processSignals(
    enrichedSignals: NeuralSignal[]
  ): Promise<NeuralProcessingResult[]> {
    LoggingUtils.logInfo(
      `⚡ Segunda fase - Processando ${enrichedSignals.length} áreas cerebrais ativadas...`
    );

    // Log neural signals for symbolic cognition tracking
    for (const signal of enrichedSignals) {
      symbolicCognitionTimelineLogger.logNeuralSignal(
        signal.core,
        {
          query: signal.symbolic_query?.query || "",
          keywords: signal.keywords ?? [],
          filters: signal.filters ?? {},
        },
        signal.intensity,
        signal.topK || 10,
        {}
      );
    }

    const processedSignals = await Promise.all(
      enrichedSignals.map(async (signal) => {
        LoggingUtils.logInfo(
          `→ Activating neural core: ${signal.core} (${(
            signal.intensity * 100
          ).toFixed(1)}%)`
        );

        const memoryResults = await this._retrieveMemoriesForSignal(signal);
        const insights = this._extractInsightsFromSignal(signal);

        // 🔍 DEBUG: Log detalhado dos resultados antes de passar para o timeline
        LoggingUtils.logInfo(
          `🔍 [MEMORY-DEBUG] === RESULTS FOR CORE: ${signal.core} ===`
        );
        LoggingUtils.logInfo(
          `📊 [MEMORY-DEBUG] Memory results: ${JSON.stringify({
            matchCount: memoryResults.matchCount,
            durationMs: memoryResults.durationMs,
            resultsLength: memoryResults.results.length,
            firstResult: memoryResults.results[0]?.substring(0, 50) + "...",
          })}`
        );
        LoggingUtils.logInfo(
          `🧠 [MEMORY-DEBUG] Insights extracted: ${JSON.stringify(insights)}`
        );

        // 🔧 CORREÇÃO: Symbolic retrieval logging com dados corretos
        // Converter insights para array de SymbolicInsight de forma mais robusta
        let insightsArray: SymbolicInsight[] = [];

        // 🔍 PRIORIDADE 1: Usar insights do signal se disponíveis
        if (
          Array.isArray(signal.symbolicInsights) &&
          signal.symbolicInsights.length > 0
        ) {
          insightsArray = signal.symbolicInsights.filter(
            (ins) => ins && typeof ins === "object"
          );
          LoggingUtils.logInfo(
            `🔍 [INSIGHTS-DEBUG] Using signal.symbolicInsights: ${insightsArray.length} insights`
          );
        }
        // 🔧 CORREÇÃO: Criar insights baseados APENAS nos resultados reais de memória
        // 🎯 PRIORIDADE 1: Insights originais do signal (se válidos)
        if (
          Array.isArray(signal.symbolicInsights) &&
          signal.symbolicInsights.length > 0
        ) {
          const validOriginalInsights = signal.symbolicInsights
            .filter((ins) => ins && ins.content && ins.content.trim())
            .map((ins, index) => ({
              id: ins.id || `original_${index}`,
              content: ins.content,
              score: ins.score || 0.6,
              type: ins.type || "original",
            }));
          insightsArray.push(...validOriginalInsights);
        }
        // 🔍 VERIFICAÇÃO: Garantir que matchCount reflete dados reais
        const actualMatchCount = memoryResults.matchCount; // ✅ Usar matchCount real do DuckDB
        const actualDuration = memoryResults.durationMs;

        // 🔍 DEBUG: Log antes de chamar o timeline logger
        LoggingUtils.logInfo(
          `📝 [TIMELINE-DEBUG] Calling logSymbolicRetrieval with: ${JSON.stringify(
            {
              core: signal.core,
              insightsCount: insightsArray.length,
              insightsPreview: insightsArray.slice(0, 2),
              matchCount: actualMatchCount,
              originalMatchCount: memoryResults.matchCount,
              durationMs: actualDuration,
            }
          )}`
        );

        // ✅ CORREÇÃO: Usar dados verificados e incluir keywords do signal
        // 🔧 CORREÇÃO: Adicionar keywords ao insight para manter consistência
        const enrichedInsights = insightsArray.map((insight) => ({
          ...insight,
          keywords: signal.keywords || [], // Adicionar keywords do signal original
        }));

        // 🔍 DEBUG: Log final antes de enviar para timeline
        LoggingUtils.logInfo(
          `🔍 [FINAL-TIMELINE-DEBUG] Final data being sent to timeline: ${JSON.stringify(
            {
              core: signal.core,
              enrichedInsightsCount: enrichedInsights.length,
              actualMatchCount,
              actualDuration,
              enrichedInsightsPreview: enrichedInsights.slice(0, 2),
            }
          )}`
        );

        symbolicCognitionTimelineLogger.logSymbolicRetrieval(
          signal.core,
          enrichedInsights,
          actualMatchCount, // Usar contagem real dos resultados
          actualDuration
        );

        return {
          ...signal,
          pineconeResults: memoryResults.results,
          symbolicInsights: enrichedInsights, // ✅ Usar insights estruturados em vez de campos técnicos
        };
      })
    );

    // Transform to standard neural processing format
    return this._transformToNeuralProcessingResults(processedSignals);
  }

  /**
   * Retrieve memories for a specific neural signal
   */
  private async _retrieveMemoriesForSignal(signal: NeuralSignal): Promise<{
    results: string[];
    matchCount: number;
    durationMs: number;
  }> {
    let results: string[] = [];
    let matchCount = 0;
    const start = Date.now();

    try {
      // 🔧 CORREÇÃO: Usar queryMemoryWithCount para obter contagem real
      const memoryResult = await this._retrieveWithMemoryServiceAndCount(
        signal
      );
      results = memoryResult.results;
      matchCount = memoryResult.matchCount;

      // 🔍 DEBUG: Log detalhado da recuperação de memórias
      LoggingUtils.logInfo(
        `🔍 [MEMORY-RETRIEVAL-DEBUG] Core: ${signal.core} | Results: ${
          results.length
        } | MatchCount: ${matchCount} | First result preview: ${results[0]?.substring(
          0,
          100
        )}...`
      );
    } catch (memoryError) {
      LoggingUtils.logError(
        `Error searching memories for core ${signal.core}`,
        memoryError
      );
      // Garantir que em caso de erro, os valores sejam consistentes
      results = [];
      matchCount = 0;
    }

    const durationMs = Date.now() - start;

    LoggingUtils.logInfo(
      `🔍 [MEMORY-FINAL-DEBUG] Core: ${signal.core} | Final matchCount: ${matchCount} | Results length: ${results.length} | Duration: ${durationMs}ms`
    );

    return { results, matchCount, durationMs };
  }

  /**
   * Retrieve memories using standard memory service
   */
  private async _retrieveWithMemoryService(
    signal: NeuralSignal
  ): Promise<string[]> {
    const query = signal.symbolic_query?.query || "";
    LoggingUtils.logInfo(
      `🧠 [NEURAL-MEMORY] Querying: "${query}" for core: ${signal.core}`
    );
    LoggingUtils.logInfo(
      `🔍 [NEURAL-MEMORY] Keywords: [${
        signal.keywords?.join(", ") || "none"
      }], topK: ${signal.topK || "default"}`
    );

    const result = await this.memoryService.queryExpandedMemory(
      query,
      signal.keywords,
      signal.topK,
      signal.filters
    );

    if (result && result.trim()) {
      const resultArray = Array.isArray(result) ? result : [result];
      LoggingUtils.logInfo(
        `✅ [NEURAL-MEMORY] Found ${resultArray.length} memory results for core: ${signal.core}`
      );
      return resultArray;
    } else {
      LoggingUtils.logWarning(
        `❌ [NEURAL-MEMORY] No memories found for core: ${signal.core} with query: "${query}"`
      );
      return [];
    }
  }

  /**
   * Retrieve memories with accurate match count from DuckDB
   */
  private async _retrieveWithMemoryServiceAndCount(
    signal: NeuralSignal
  ): Promise<{ results: string[]; matchCount: number }> {
    const query = signal.symbolic_query?.query || "";

    try {
      // 🔧 CORREÇÃO: Consultar DuckDB diretamente para obter contagem real
      if (!window.electronAPI?.queryDuckDB) {
        LoggingUtils.logWarning(
          "[NEURAL-MEMORY] DuckDB not available, using fallback"
        );
        const fallbackResults = await this._retrieveWithMemoryService(signal);
        return { results: fallbackResults, matchCount: fallbackResults.length };
      }

      // Criar embedding para a query
      const embeddingService = (this.memoryService as any).embeddingService;
      if (!embeddingService?.isInitialized()) {
        LoggingUtils.logWarning(
          "[NEURAL-MEMORY] Embedding service not initialized"
        );
        const fallbackResults = await this._retrieveWithMemoryService(signal);
        return { results: fallbackResults, matchCount: fallbackResults.length };
      }

      const queryEmbedding = await embeddingService.createEmbedding(query);
      if (!queryEmbedding || queryEmbedding.length === 0) {
        LoggingUtils.logWarning("[NEURAL-MEMORY] Failed to create embedding");
        return { results: [], matchCount: 0 };
      }

      // Consultar DuckDB diretamente
      LoggingUtils.logInfo(
        `🧠 [NEURAL-MEMORY-DIRECT] Querying DuckDB directly for: "${query}"`
      );

      const queryResponse = await window.electronAPI.queryDuckDB(
        queryEmbedding,
        signal.topK || 10,
        signal.keywords || [],
        signal.filters || {}
      );

      // Extrair resultados e contagem real
      const matches = queryResponse.matches || [];

      // 🔍 DEBUG: Log detalhado dos matches
      LoggingUtils.logInfo(
        `🔍 [NEURAL-MEMORY-DEBUG] Raw matches from DuckDB: ${JSON.stringify(
          matches.map((match: any, index: number) => ({
            index,
            hasMetadata: !!match.metadata,
            hasContent: !!(match.metadata && match.metadata.content),
            contentLength: match.metadata?.content?.length || 0,
            contentPreview: match.metadata?.content?.substring(0, 50) || "N/A",
          }))
        )}`
      );

      const validMatches = matches.filter(
        (match: any) => match.metadata && match.metadata.content
      );
      const results = validMatches.map(
        (match: any) => match.metadata.content as string
      );

      const matchCount = validMatches.length; // ✅ Contagem dos matches válidos após filtro

      LoggingUtils.logInfo(
        `✅ [NEURAL-MEMORY-DIRECT] Core: ${signal.core} | DuckDB returned ${matches.length} raw matches | Filtered to ${matchCount} valid matches | Extracted ${results.length} results`
      );

      // 🔍 DEBUG: Log final dos resultados
      LoggingUtils.logInfo(
        `🔍 [NEURAL-MEMORY-FINAL] Final results: ${JSON.stringify({
          matchCount,
          resultsLength: results.length,
          resultsPreview: results.map(
            (r, i) => `${i}: ${r.substring(0, 100)}...`
          ),
        })}`
      );

      return { results, matchCount };
    } catch (error) {
      LoggingUtils.logError(
        `[NEURAL-MEMORY-DIRECT] Error querying DuckDB directly for core ${signal.core}`,
        error
      );
      // Fallback para método original
      const fallbackResults = await this._retrieveWithMemoryService(signal);
      return { results: fallbackResults, matchCount: fallbackResults.length };
    }
  }

  /**
   * Extract and normalize insights from neural signal following Orch-OS theory
   */
  private _extractInsightsFromSignal(
    signal: NeuralSignal
  ): Record<string, unknown> {
    let insights: Record<string, unknown> = {};

    // 🎭 ORCH-OS: Mapear cores para arquétipos junguianos
    const coreArchetypes: Record<
      string,
      { archetype: string; essence: string }
    > = {
      "valência emocional": {
        archetype: "The Mirror",
        essence: "emotional reflection and inner truth",
      },
      "intensidade emocional": {
        archetype: "The Warrior",
        essence: "emotional strength and resilience",
      },
      "conexões interacionais": {
        archetype: "The Lover",
        essence: "bonds and relational harmony",
      },
      "estado emocional": {
        archetype: "The Wanderer",
        essence: "emotional journey and discovery",
      },
      memory: {
        archetype: "The Sage",
        essence: "wisdom and accumulated knowledge",
      },
      metacognitive: {
        archetype: "The Seeker",
        essence: "self-awareness and introspection",
      },
      shadow: {
        archetype: "The Shadow",
        essence: "hidden aspects and inner conflicts",
      },
      soul: { archetype: "The Hero", essence: "purpose and transcendence" },
      self: {
        archetype: "The Pioneer",
        essence: "identity and authentic expression",
      },
    };

    // 🔮 ORCH-OS: Detectar propriedades emergentes baseadas na intensidade e contexto
    const emergentProperties: string[] = [];

    if (signal.intensity > 0.8) {
      emergentProperties.push("High symbolic resonance");
    }
    if (signal.intensity < 0.3) {
      emergentProperties.push("Low response diversity");
    }
    if (signal.keywords && signal.keywords.length > 3) {
      emergentProperties.push("Cognitive dissonance");
    }

    // 🎯 ORCH-OS: Criar insights simbólicos baseados na teoria
    const coreInfo = coreArchetypes[signal.core] || {
      archetype: "The Unknown",
      essence: "unexplored symbolic territory",
    };

    insights = {
      // Query simbólica (se disponível)
      ...(signal.symbolic_query?.query && {
        symbolic_query: {
          query: signal.symbolic_query.query,
          symbolic_tension: "meaning collapse in progress",
        },
      }),

      // Keywords como fragmentos simbólicos
      ...(signal.keywords &&
        signal.keywords.length > 0 && {
          symbolic_fragments: signal.keywords.map((keyword) => ({
            fragment: keyword,
            resonance: "active",
          })),
        }),
    };

    return insights;
  }

  /**
   * Transform processed signals to standard neural processing results format
   */
  private _transformToNeuralProcessingResults(
    processedSignals: any[]
  ): NeuralProcessingResult[] {
    return processedSignals.map((signal) => ({
      core: signal.core,
      intensity: signal.intensity,
      output: signal.pineconeResults.join("\n"),
      insights: Array.isArray(signal.symbolicInsights)
        ? signal.symbolicInsights.reduce(
            (acc: Record<string, unknown>, ins: SymbolicInsight) => {
              if (ins && typeof ins.type === "string") {
                acc[ins.type] = ins;
              }
              return acc;
            },
            {}
          )
        : typeof signal.symbolicInsights === "object" &&
          signal.symbolicInsights !== null
        ? signal.symbolicInsights
        : {},
    }));
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { NeuralSignal } from '../../../interfaces/neural/NeuralSignalTypes';
import { IOpenAIService } from '../../../interfaces/openai/IOpenAIService';
import { LoggingUtils } from '../../../utils/LoggingUtils';

/**
 * Processor mode: OpenAI or HuggingFace
 */
export type ProcessorMode = 'openai' | 'huggingface';

/**
 * Neural signal semantic enrichment processor
 * Responsible for enhancing neural signals with semantic queries and context
 */
export class NeuralSignalEnricher {
  constructor(
    private llmService: IOpenAIService
  ) {}

  /**
   * Enrich neural signals with semantic queries based on selected backend
   */
  async enrichSignals(signals: NeuralSignal[], currentLanguage: string): Promise<NeuralSignal[]> {
    return await Promise.all(
      signals.map(async (signal: NeuralSignal) => {
        try {
          let enrichment: {enrichedQuery: string, keywords: string[]};
          
          enrichment = await this._enrichWithLLM(signal, currentLanguage);

          let topK = signal.topK;
          if (typeof topK !== 'number' || isNaN(topK)) {
            topK = Math.round(5 + (signal.intensity || 0) * 10);
          }

          // Symbolic enrichment logging
          LoggingUtils.logInfo(`[ Enrichment] Core: ${signal.core} | Query: ${enrichment.enrichedQuery} | Keywords: ${JSON.stringify(enrichment.keywords)} | Filters: ${JSON.stringify(signal.filters || {})} | topK: ${topK}`);
          
          return {
            ...signal,
            symbolic_query: {
              ...signal.symbolic_query,
              query: enrichment.enrichedQuery
            },
            keywords: enrichment.keywords,
            filters: signal.filters || undefined,
            topK
          };
        } catch (err) {
          LoggingUtils.logError(`Error enriching query for core ${signal.core}`, err);
          return signal;
        }
      })
    );
  }

  /**
   * Enrich signal using LLM backend
   */
  private async _enrichWithLLM(signal: NeuralSignal, currentLanguage: string): Promise<{enrichedQuery: string, keywords: string[]}> {
    return await this.llmService.enrichSemanticQueryForSignal(
      signal.core,
      signal.symbolic_query?.query || '',
      signal.intensity,
      (typeof signal === 'object' && signal && 'context' in signal) ? (signal.context as string) : undefined,
      currentLanguage
    );
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IMemoryService } from '../../../interfaces/memory/IMemoryService';
import { NeuralProcessingResult, NeuralSignalResponse } from '../../../interfaces/neural/NeuralSignalTypes';
import { ITranscriptionStorageService } from '../../../interfaces/transcription/ITranscriptionStorageService';
import { ISpeakerIdentificationService } from '../../../interfaces/utils/ISpeakerIdentificationService';
import { SymbolicInsight } from '../../../types/SymbolicInsight';
import { LoggingUtils } from '../../../utils/LoggingUtils';
import symbolicCognitionTimelineLogger from '../../utils/SymbolicCognitionTimelineLoggerSingleton';
import { SessionManager } from './SessionManager';

/**
 * Neural processing results cognitive saver
 * Responsible for persisting processing results to memory and symbolic logs
 */
export class ProcessingResultsSaver {
  constructor(
    private memoryService: IMemoryService,
    private storageService: ITranscriptionStorageService,
    private speakerService: ISpeakerIdentificationService,
    private sessionManager: SessionManager
  ) {}

  /**
   * Save comprehensive processing results to memory and logs
   */
  async saveResults(
    transcription: string,
    response: string,
    neuralActivation: NeuralSignalResponse,
    processingResults: NeuralProcessingResult[]
  ): Promise<void> {
    
    // Log symbolic cognitive response
    await this._logSymbolicResponse(response, processingResults);

    // Update conversation history
    this.memoryService.addToConversationHistory({ role: "user", content: transcription });

    // Save to long-term memory
    await this._saveToLongTermMemory(transcription, response);

    // Save neural processing data for brain evolution tracking
    await this._saveNeuralProcessingData(neuralActivation, processingResults);
  }

  /**
   * Log symbolic cognitive response with insights
   */
  private async _logSymbolicResponse(response: string, processingResults: NeuralProcessingResult[]): Promise<void> {
    const symbolicTopics = processingResults.map(r => r.core);
    const importantInsights: SymbolicInsight[] = processingResults
      .flatMap(r => Array.isArray(r.insights) ? r.insights : [])
      .filter(insight => insight && typeof insight.type === 'string');
    
    symbolicCognitionTimelineLogger.logGptResponse({
      response,
      symbolicTopics,
      insights: importantInsights
    });
  }

  /**
   * Save interaction to long-term memory
   */
  private async _saveToLongTermMemory(transcription: string, response: string): Promise<void> {
    await this.memoryService.saveToLongTermMemory(
      transcription,
      response,
      this.storageService.getSpeakerTranscriptions(),
      this.speakerService.getPrimaryUserSpeaker()
    );
  }

  /**
   * Save neural processing data for brain evolution tracking
   */
  private async _saveNeuralProcessingData(
    activation: NeuralSignalResponse,
    processingResults: NeuralProcessingResult[] = []
  ): Promise<void> {
    try {
      const neuralData = {
        sessionId: this.sessionManager.getCurrentSessionId(),
        timestamp: new Date().toISOString(),
        activation,
        results: processingResults,
        interactionCount: this.sessionManager.incrementInteraction()
      };

      // TODO: Implement neural data persistence storage
      LoggingUtils.logInfo(`Neural processing data saved: ${JSON.stringify(neuralData)}`);
    } catch (error) {
      LoggingUtils.logError("Error saving neural processing data", error);
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IMemoryService } from "../../../interfaces/memory/IMemoryService";
import { IOpenAIService } from "../../../interfaces/openai/IOpenAIService";
import { Message } from "../../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../../utils/LoggingUtils";
import { cleanThinkTags } from "../../../utils/ThinkTagCleaner";
import symbolicCognitionTimelineLogger from "../../utils/SymbolicCognitionTimelineLoggerSingleton";

/**
 * Neural response cognitive generator
 * Responsible for generating responses using different AI backends
 */
export class ResponseGenerator {
  constructor(
    private memoryService: IMemoryService,
    private llmService: IOpenAIService
  ) {}

  /**
   * Generate response using the selected backend
   */
  async generateResponse(
    integratedPrompt: string,
    temporaryContext?: string
  ): Promise<string> {
    symbolicCognitionTimelineLogger.logFusionInitiated();

    // Prepare context messages (shared by both backends)
    const contextMessages = this._prepareContextMessages(temporaryContext);

    if (contextMessages.length > 0) {
      this.memoryService.addContextToHistory(contextMessages);
    }

    const conversationHistory = this.memoryService.getConversationHistory();

    const messages = this.memoryService.buildPromptMessagesForModel(
      integratedPrompt,
      conversationHistory
    );

    return await this._generate(messages);
  }

  /**
   * Prepare context messages for processing
   */
  private _prepareContextMessages(temporaryContext?: string): Message[] {
    const contextMessages: Message[] = [];

    if (temporaryContext?.trim()) {
      contextMessages.push({
        role: "developer",
        content: `🧠 Temporary instructions:\n${temporaryContext.trim()}`,
      });
    }

    return contextMessages;
  }

  /**
   * Generate response using OpenAI backend
   */
  private async _generate(messages: Message[]): Promise<string> {
    try {
      const response = await this.llmService.streamOpenAIResponse(messages);

      // Clean think tags from the final response
      const cleanedResponse = cleanThinkTags(response.responseText);

      return cleanedResponse;
    } catch (error: any) {
      if (error.message?.includes("does not have access to model")) {
        LoggingUtils.logError(
          "Invalid model detected, falling back to gpt-4o-mini",
          error
        );
        // Clear invalid model and retry with default
        if (typeof window !== "undefined" && window.localStorage) {
          window.localStorage.removeItem("chatgptModel");
        }
        throw new Error(
          "Invalid model configuration. Please restart the application."
        );
      }
      throw error;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Neural session cognitive manager
 * Responsible for managing session identifiers and interaction tracking
 */
export class SessionManager {
  private _interactionCount: number = 0;
  private _currentSessionId: string;

  constructor() {
    this._currentSessionId = this._generateSessionId();
  }

  /**
   * Generate unique session identifier with neural patterns
   */
  private _generateSessionId(): string {
    return `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Get current session identifier
   */
  getCurrentSessionId(): string {
    return this._currentSessionId;
  }

  /**
   * Get current interaction count
   */
  getCurrentInteractionCount(): number {
    return this._interactionCount;
  }

  /**
   * Increment interaction count for neural tracking
   */
  incrementInteraction(): number {
    return ++this._interactionCount;
  }

  /**
   * Reset session with new identifier
   */
  resetSession(): void {
    this._currentSessionId = this._generateSessionId();
    this._interactionCount = 0;
  }

  /**
   * Get session metadata for neural processing
   */
  getSessionMetadata(): {
    sessionId: string;
    interactionCount: number;
    timestamp: string;
  } {
    return {
      sessionId: this._currentSessionId,
      interactionCount: this._interactionCount,
      timestamp: new Date().toISOString()
    };
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ITranscriptionStorageService } from "../../../interfaces/transcription/ITranscriptionStorageService";

/**
 * Neural transcription extraction processor
 * Responsible for extracting new lines that haven't been processed yet
 */
export class TranscriptionExtractor {
  constructor(private storageService: ITranscriptionStorageService) {}

  /**
   * Extract new transcription lines that haven't been processed yet
   * WITHOUT marking them as sent (for preview/validation purposes)
   */
  extractNewLines(): string | null {
    const newTranscriptions = this.storageService.getNewTranscriptions();

    if (newTranscriptions.length === 0) {
      console.log("📭 No new transcriptions to extract");
      return null;
    }

    // Format transcriptions with speaker information
    const formattedLines = newTranscriptions.map((t) => {
      return `${t.speaker}: ${t.text}`;
    });

    console.log(`📤 Extracting ${newTranscriptions.length} new transcriptions`);
    return formattedLines.join("\n");
  }

  /**
   * Extract new transcriptions AND immediately mark them as sent
   * This ensures atomic operation preventing duplicate sends
   */
  extractAndMarkAsSent(): string | null {
    // Use the atomic method from storage service
    const newTranscriptions = this.storageService.extractAndMarkAsSent();

    if (newTranscriptions.length === 0) {
      console.log("📭 No new transcriptions to extract");
      return null;
    }

    // Format transcriptions with speaker information
    const formattedLines = newTranscriptions.map((t) => {
      return `${t.speaker}: ${t.text}`;
    });

    console.log(
      `📤 Extracted and marked ${newTranscriptions.length} transcriptions as sent`
    );

    return formattedLines.join("\n");
  }

  /**
   * Reset extraction state for new session
   */
  reset(): void {
    // No longer need to track index manually
    console.log("🔄 Transcription extractor reset");
  }

  /**
   * Check if there are new transcriptions available
   */
  hasNewTranscriptions(): boolean {
    return this.storageService.hasNewTranscriptions();
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { SpeakerTranscription } from "../../../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../../../interfaces/utils/ISpeakerIdentificationService";
import { LoggingUtils } from "../../../utils/LoggingUtils";

/**
 * TranscriptionProcessor - Single Responsibility: Process transcriptions (duplicates, speakers)
 * Follows DRY principle by centralizing all transcription processing logic
 */
export class TranscriptionProcessor {
  private speakerService: ISpeakerIdentificationService;

  constructor(speakerService: ISpeakerIdentificationService) {
    this.speakerService = speakerService;
  }

  /**
   * Check if transcription is a duplicate
   */
  isDuplicate(
    text: string,
    existingTranscriptions: SpeakerTranscription[]
  ): boolean {
    const cleanText = text.trim();
    const isDuplicate = existingTranscriptions.some(
      (st) =>
        st.text === cleanText &&
        Date.now() - new Date(st.timestamp).getTime() < 2000 // Within 2 seconds
    );

    if (isDuplicate) {
      LoggingUtils.logInfo(`Ignoring duplicate transcription: "${cleanText}"`);
    }

    return isDuplicate;
  }

  /**
   * Process speaker from transcription text
   */
  processSpeaker(
    text: string,
    providedSpeaker?: string,
    currentSpeaker?: string
  ): { speaker: string; cleanText: string } {
    const cleanText = text.trim();

    // Check for explicit speaker markers
    if (this.hasSpeakerMarkers(cleanText)) {
      const segments = this.speakerService.splitMixedTranscription(cleanText);
      if (segments.length > 0) {
        // Return the first segment's speaker
        return {
          speaker: segments[0].speaker,
          cleanText: segments[0].text,
        };
      }
    }

    // Use provided speaker if available
    if (providedSpeaker?.trim()) {
      const normalizedSpeaker = this.speakerService.normalizeAndIdentifySpeaker(
        providedSpeaker.trim()
      );
      return {
        speaker: normalizedSpeaker,
        cleanText,
      };
    }

    // Fall back to current speaker or primary user
    const speakerToUse =
      currentSpeaker || this.speakerService.getPrimaryUserSpeaker();

    LoggingUtils.logInfo(
      `Speaker assigned: "${speakerToUse}" for text without marker: "${cleanText.substring(
        0,
        30
      )}..."`
    );

    return {
      speaker: speakerToUse,
      cleanText,
    };
  }

  /**
   * Check if text has speaker markers
   */
  hasSpeakerMarkers(text: string): boolean {
    return (
      text.includes("[") &&
      text.includes("]") &&
      /\[(Guilherme|Speaker\s*\d+)\]/i.test(text)
    );
  }

  /**
   * Process transcription segments
   */
  processSegments(text: string): Array<{ text: string; speaker: string }> {
    if (!this.hasSpeakerMarkers(text)) {
      return [];
    }

    return this.speakerService.splitMixedTranscription(text);
  }

  /**
   * Filter transcriptions by user
   */
  filterByUser(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): SpeakerTranscription[] {
    return transcriptions.filter((st) => st.speaker === primaryUserSpeaker);
  }

  /**
   * Get last message from user
   */
  getLastUserMessage(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): SpeakerTranscription | null {
    const userMessages = this.filterByUser(transcriptions, primaryUserSpeaker);
    return userMessages.length > 0
      ? userMessages[userMessages.length - 1]
      : null;
  }

  /**
   * Get last messages from external speakers
   */
  getLastExternalMessages(
    transcriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>
  ): Map<string, SpeakerTranscription> {
    const lastMessages = new Map<string, SpeakerTranscription>();

    if (detectedSpeakers.has("external")) {
      const messages = this.speakerService.filterTranscriptionsBySpeaker(
        "external",
        transcriptions
      );
      if (messages.length > 0) {
        lastMessages.set("external", messages[messages.length - 1]);
      }
    }

    return lastMessages;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { SpeakerTranscription } from "../../../interfaces/transcription/TranscriptionTypes";

/**
 * TranscriptionStore - Single Responsibility: Store and retrieve transcriptions
 * Follows KISS principle by keeping storage logic simple and focused
 */
export class TranscriptionStore {
  private transcriptionList: string[] = [""];
  private speakerTranscriptions: SpeakerTranscription[] = [];
  private detectedSpeakers: Set<string> = new Set();
  private lastTranscription: string = "";
  private currentSpeaker: string = "";
  private uiTranscriptionList: string[] = [];

  /**
   * Add a transcription to all relevant stores
   */
  addTranscription(text: string, speaker: string, timestamp: string): void {
    // Add to traditional list
    this.transcriptionList.push(text);
    this.lastTranscription = text;

    // Add to speaker-specific list with sent status
    this.speakerTranscriptions.push({
      text: text.trim(),
      speaker,
      timestamp,
      sent: false, // New transcriptions are not sent by default
    });

    // Add speaker to detected speakers
    this.detectedSpeakers.add(speaker);

    // Add to UI transcription list
    this.uiTranscriptionList.push(text.trim());
  }

  /**
   * Get all transcriptions as a single string
   */
  getUITranscriptionText(): string {
    return this.uiTranscriptionList.join("\n");
  }

  /**
   * Get only new transcriptions that haven't been sent yet
   */
  getNewTranscriptions(): SpeakerTranscription[] {
    return this.speakerTranscriptions.filter((t) => !t.sent);
  }

  /**
   * Get all transcriptions with their sent status
   */
  getAllTranscriptionsWithStatus(): SpeakerTranscription[] {
    return this.speakerTranscriptions;
  }

  /**
   * Mark specific transcriptions as sent
   */
  markTranscriptionsAsSent(transcriptions?: SpeakerTranscription[]): void {
    if (transcriptions) {
      // Mark specific transcriptions as sent
      transcriptions.forEach((t) => {
        const index = this.speakerTranscriptions.findIndex(
          (st) => st.timestamp === t.timestamp && st.text === t.text
        );
        if (index !== -1) {
          this.speakerTranscriptions[index].sent = true;
        }
      });
    } else {
      // Mark all unsent transcriptions as sent
      this.speakerTranscriptions.forEach((t) => {
        if (!t.sent) {
          t.sent = true;
        }
      });
    }

    const sentCount = this.speakerTranscriptions.filter((t) => t.sent).length;
    console.log(`📍 Marked transcriptions as sent. Total sent: ${sentCount}`);
  }

  /**
   * Get the transcription list
   */
  getTranscriptionList(): string[] {
    return this.transcriptionList;
  }

  /**
   * Get speaker transcriptions
   */
  getSpeakerTranscriptions(): SpeakerTranscription[] {
    return this.speakerTranscriptions;
  }

  /**
   * Get detected speakers
   */
  getDetectedSpeakers(): Set<string> {
    return this.detectedSpeakers;
  }

  /**
   * Get last transcription
   */
  getLastTranscription(): string {
    return this.lastTranscription;
  }

  /**
   * Get/Set current speaker
   */
  getCurrentSpeaker(): string {
    return this.currentSpeaker;
  }

  setCurrentSpeaker(speaker: string): void {
    this.currentSpeaker = speaker;
  }

  /**
   * Get UI transcription list
   */
  getUITranscriptionList(): string[] {
    return this.uiTranscriptionList;
  }

  /**
   * Check if there are valid transcriptions
   */
  hasValidTranscriptions(): boolean {
    return (
      this.speakerTranscriptions.length > 0 ||
      this.transcriptionList.some((text) => text && text.trim().length > 0)
    );
  }

  /**
   * Check if there are new transcriptions to send
   */
  hasNewTranscriptions(): boolean {
    return this.speakerTranscriptions.some((t) => !t.sent);
  }

  /**
   * Clear all transcription data
   */
  clear(): void {
    this.transcriptionList = [""];
    this.speakerTranscriptions = [];
    this.detectedSpeakers = new Set();
    this.lastTranscription = "";
    this.uiTranscriptionList = [];
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// BatchTranscriptionProcessor.ts
// Implementation of IBatchTranscriptionProcessor

import { IBatchTranscriptionProcessor } from "../../interfaces/transcription/IBatchTranscriptionProcessor";
import { ITranscriptionFormatter } from "../../interfaces/transcription/ITranscriptionFormatter";
import { EXTERNAL_HEADER, EXTERNAL_SPEAKER_LABEL, Message, SpeakerSegment, SpeakerTranscription, USER_HEADER } from "../../interfaces/transcription/TranscriptionTypes";

export class BatchTranscriptionProcessor implements IBatchTranscriptionProcessor {
  private formatter: ITranscriptionFormatter;
  
  constructor(formatter: ITranscriptionFormatter) {
    this.formatter = formatter;
  }
  
  /**
   * Processes and formats transcriptions from multiple speakers
   */
  processTranscriptions(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): SpeakerSegment[] {
    // First, remove duplicates
    const uniqueTranscriptions = this.deduplicateTranscriptions(transcriptions);
    
    // Sort chronologically by timestamp
    const sortedTranscriptions = [...uniqueTranscriptions]
      .sort((a, b) => new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime());
    
    const segments: SpeakerSegment[] = [];
    let lastSpeakerKey = "";
    
    // Process each transcription
    for (const transcription of sortedTranscriptions) {
      // Check if it's a mixed transcription with multiple speakers
      if (transcription.text.includes('[') && transcription.text.includes(']')) {
        // Split mixed transcription into segments
        const mixedSegments = this.formatter.formatMixedTranscription(
          transcription.text, 
          primaryUserSpeaker
        );
        
        // Process each segment individually
        for (const segment of mixedSegments) {
          // Update show speaker flag based on whether speaker changed
          const showSpeaker = (lastSpeakerKey !== segment.speaker);
          lastSpeakerKey = segment.speaker;
          
          segments.push({
            ...segment,
            showSpeaker
          });
        }
      } else {
        // Regular transcription (single speaker)
        let speakerKey = transcription.speaker;
        let speakerLabel: string;
        
        // Determine the correct label for the speaker
        if (speakerKey === primaryUserSpeaker) {
          speakerLabel = primaryUserSpeaker;
        } else {
          // For external speaker, check for original label
          const originalLabel = transcription.text.match(/^\[([^\]]+)\]/)?.[1];
          if (originalLabel && originalLabel.toLowerCase().includes("speaker")) {
            speakerLabel = originalLabel;
            speakerKey = "external";
          } else {
            speakerLabel = EXTERNAL_SPEAKER_LABEL;
            speakerKey = "external";
          }
        }
        
        // Clean the text of any existing speaker markings
        const cleanText = transcription.text.replace(/^\[[^\]]+\]\s*/, '');
        
        // Update show speaker flag based on whether speaker changed
        const showSpeaker = (lastSpeakerKey !== speakerKey);
        lastSpeakerKey = speakerKey;
        
        segments.push({
          speaker: speakerKey,
          text: showSpeaker ? `[${speakerLabel}] ${cleanText}` : cleanText,
          showSpeaker
        });
      }
    }
    
    return segments;
  }
  
  /**
   * Removes duplicate content from transcriptions
   */
  deduplicateTranscriptions(
    transcriptions: SpeakerTranscription[]
  ): SpeakerTranscription[] {
    const processedTexts = new Set<string>();
    const uniqueTranscriptions: SpeakerTranscription[] = [];
    
    for (const transcription of transcriptions) {
      // Skip duplicates
      if (processedTexts.has(transcription.text)) continue;
      
      processedTexts.add(transcription.text);
      uniqueTranscriptions.push(transcription);
    }
    
    return uniqueTranscriptions;
  }
  
  /**
   * Extracts the last message from each speaker
   */
  extractLastMessageBySpeaker(
    transcriptions: SpeakerTranscription[],
    speakers: string[]
  ): Map<string, SpeakerTranscription> {
    const lastMessages = new Map<string, SpeakerTranscription>();
    
    // Helper to get the last message from a specific speaker
    const getLastMessageFrom = (speaker: string): SpeakerTranscription | null => {
      const filteredMessages = transcriptions.filter(st => st.speaker === speaker);
      return filteredMessages.length > 0 ? filteredMessages[filteredMessages.length - 1] : null;
    };
    
    // Get last message for each requested speaker
    for (const speaker of speakers) {
      const lastMessage = getLastMessageFrom(speaker);
      if (lastMessage) {
        lastMessages.set(speaker, lastMessage);
      }
    }
    
    return lastMessages;
  }
  
  /**
   * Formats transcriptions for conversation history
   */
  formatTranscriptionsForHistory(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Message[] {
    const messages: Message[] = [];
    
    // Get the last message from the primary user
    const lastMessages = this.extractLastMessageBySpeaker(
      transcriptions,
      [primaryUserSpeaker, "external"]
    );
    
    // Add the primary user's message
    const lastUserMessage = lastMessages.get(primaryUserSpeaker);
    if (lastUserMessage) {
      messages.push({
        role: "user",
        content: `${USER_HEADER} (last message):\n${lastUserMessage.text}`
      });
    }
    
    // Add external speaker message
    const lastExternalMessage = lastMessages.get("external");
    if (lastExternalMessage) {
      // Extract original label if available
      const originalLabel = lastExternalMessage.text.includes('[') ?
        lastExternalMessage.text.match(/^\[([^\]]+)\]/)?.[1] : null;
        
      // Use original label when available and contains "Speaker"
      const speakerLabel = originalLabel?.includes("Speaker") ?
        originalLabel : EXTERNAL_SPEAKER_LABEL;
        
      // Clean any existing speaker prefix
      const cleanText = lastExternalMessage.text.replace(/^\[[^\]]+\]\s*/, '');
      
      messages.push({
        role: "user",
        content: `${EXTERNAL_HEADER} ${speakerLabel} (last message):\n[${speakerLabel}] ${cleanText}`
      });
    }
    
    return messages;
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Index file for transcription services
// Exports all refactored components for easy import

export { QuestionDetector } from "./detectors/QuestionDetector";
export { TranscriptionLogger } from "./loggers/TranscriptionLogger";
export { TranscriptionUIManager } from "./managers/TranscriptionUIManager";
export { TranscriptionProcessor } from "./processors/TranscriptionProcessor";
export { TranscriptionStore } from "./stores/TranscriptionStore";
export { TranscriptionStorageService } from "./TranscriptionStorageService";
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * LiveTranscriptionProcessor handles processing of transcription data from Deepgram
 * and management of speaker segments.
 */
import { SpeakerBuffer } from "../utils/DeepgramTypes";
import { Logger } from "../utils/Logger";
// Import of the interface for integration with the transcription storage service
import { getPrimaryUser } from "../../../../../config/UserConfig";
import { ITranscriptionStorageService } from "../../interfaces/transcription/ITranscriptionStorageService";
export class LiveTranscriptionProcessor {
  private logger: Logger;
  private activeSpeakerBuffer: Record<number, SpeakerBuffer> = {};
  private currentSpeaker: string = "";
  private transcriptionList: string[] = [];
  private transcriptionCallback: ((event: string, data: any) => void) | null =
    null;
  private transcriptionStorageService: ITranscriptionStorageService | null =
    null;

  constructor() {
    this.logger = new Logger("LiveTranscriptionProcessor");
  }

  /**
   * Configures the transcription storage service for direct sending
   */
  public setTranscriptionStorageService(
    service: ITranscriptionStorageService
  ): void {
    console.log(
      `💾 [INTEGRATION] Storage service received:`,
      service ? "VALID INSTANCE" : "NULL"
    );
    this.transcriptionStorageService = service;
    if (service && typeof service.updateTranscriptionUI === "function") {
      console.log(
        `✅ [INTEGRATION] updateTranscriptionUI available and ready for use`
      );
    } else {
      console.error(
        `❌ [INTEGRATION] updateTranscriptionUI NOT available!`,
        service
      );
    }
    this.logger.info("Storage service configured for direct sending");
  }

  /**
   * Register a callback to receive transcription events
   */
  public registerTranscriptionCallback(
    callback: (event: string, data: any) => void
  ): void {
    this.transcriptionCallback = callback;
    this.logger.info("Callback registered");
  }

  /**
   * Process incoming transcription data from Deepgram
   */
  public handleTranscriptionEvent(data: any): void {
    try {
      console.log("🔄 [PROCESS] Starting transcription processing");

      // Check if this is a final transcription
      // We ignore all interim (non-final) transcriptions
      if (!data.is_final) {
        console.log("⏭️ [PROCESS] Ignoring interim (non-final) transcription");
        return;
      }

      // Check if data contains channel (singular) or channels (plural)
      if (data.channel) {
        // New Deepgram API (v3)
        const channelIndex =
          data.channel_index && data.channel_index[0] !== undefined
            ? data.channel_index[0]
            : 0;

        console.log(`🔄 [PROCESS] Processing channel ${channelIndex}`);

        // Initialize buffer for this channel if not exists
        if (!this.activeSpeakerBuffer[channelIndex]) {
          console.log(
            `🔄 [PROCESS] Initializing buffer for channel ${channelIndex}`
          );
          this.activeSpeakerBuffer[channelIndex] = {
            lastSpeaker:
              channelIndex === 0 ? getPrimaryUser() : `Speaker ${channelIndex}`,
            currentSegment: [],
            formattedSegment: "",
            lastFlushedText: "",
          };
        }

        const alternative = data.channel.alternatives[0];
        if (!alternative?.transcript) {
          return;
        }

        const buffer = this.activeSpeakerBuffer[channelIndex];
        const speakerPrefix =
          this.currentSpeaker !== buffer.lastSpeaker
            ? `[${buffer.lastSpeaker}] `
            : "";

        console.log(
          `🔄 [PROCESS] Current speaker: "${this.currentSpeaker}", Buffer speaker: "${buffer.lastSpeaker}"`
        );
        console.log(`🔄 [PROCESS] Speaker prefix: "${speakerPrefix}"`);

        // Update current speaker
        this.currentSpeaker = buffer.lastSpeaker;

        // Process the transcript content immediately
        const transcriptText = `${speakerPrefix}${alternative.transcript}`;
        console.log(`🔄 [PROCESS] Formatted text: "${transcriptText}"`);

        // Check if this is new or different content
        if (
          transcriptText &&
          transcriptText.trim() &&
          transcriptText !== buffer.lastFlushedText &&
          transcriptText.trim() !== `[${buffer.lastSpeaker}]`
        ) {
          console.log(
            `✅ [PROCESS] Valid and different text: "${transcriptText}"`
          );
          this.logger.info(
            `Final transcription (channel ${channelIndex}): ${transcriptText}`
          );

          // Send only final transcriptions directly via IPC to the main process
          // Explicitly check the storage service for each transcription
          try {
            if (this.transcriptionStorageService) {
              // Ensure the method exists before calling
              if (
                typeof this.transcriptionStorageService.addTranscription ===
                "function"
              ) {
                console.log(
                  `📝 [PROCESS] Sending directly to TranscriptionStorageService: "${transcriptText}"`
                );
                this.transcriptionStorageService.addTranscription(
                  transcriptText
                );
                console.log(
                  `✅ [PROCESS] Transcription successfully sent to storageService`
                );
                // UI update is now handled automatically inside addTranscription
              } else {
                console.error(
                  `❌ [PROCESS] addTranscription NOT available in service!`
                );
              }
            } else {
              console.warn(
                `⚠️ [PROCESS] TranscriptionStorageService NOT available during transcription processing`
              );
            }
          } catch (error) {
            console.error(
              `❌ [PROCESS] Error sending to TranscriptionStorageService:`,
              error
            );
          }

          // Maintain IPC sending for compatibility with panel
          if (typeof window !== "undefined" && window.electronAPI) {
            try {
              console.log(
                `📢 [PROCESS] Sending final transcription via IPC: "${transcriptText}"`
              );
              window.electronAPI.sendAudioTranscription(transcriptText);
            } catch (error) {
              console.error("❌ [PROCESS] Error sending via IPC:", error);
            }
          }

          // Also send via callback for compatibility
          if (this.transcriptionCallback) {
            console.log(
              `📢 [PROCESS] Sending to callback: "${transcriptText}"`
            );
            this.transcriptionCallback("transcript", {
              text: transcriptText,
              isFinal: true, // Always true because we're filtering out non-final transcriptions
              channel: channelIndex,
              speaker: buffer.lastSpeaker,
            });
          } else {
            console.log("❌ [PROCESS] TranscriptionCallback NOT registered");
          }

          // Add to permanent history
          console.log(
            `📝 [PROCESS] Saving final transcription: "${transcriptText}"`
          );
          this.transcriptionList.push(transcriptText);
          buffer.lastFlushedText = transcriptText;
        } else {
          console.log(
            `⚠️ [PROCESS] Text ignored because it was empty or the same as the previous: "${transcriptText}"`
          );
        }

        // Store the formatted version
        buffer.formattedSegment = alternative.transcript;

        // Still accumulate words for speaker change detection
        if (alternative.words && alternative.words.length > 0) {
          console.log(
            `🔄 [PROCESS] Processing ${alternative.words.length} words for speaker detection`
          );
          buffer.currentSegment = alternative.words.map(
            (w: { word: any }) => w.word
          );

          // Process speaker changes in words if available
          let currentSegmentSpeaker = buffer.lastSpeaker;
          for (const word of alternative.words) {
            if (word.speaker) {
              const speaker = `Speaker ${word.speaker}`;
              if (speaker !== currentSegmentSpeaker) {
                // Speaker change detected
                console.log(
                  `👥 [PROCESS] Speaker change detected: ${currentSegmentSpeaker} -> ${speaker}`
                );
                this.logger.info(
                  `Speaker change detected: ${currentSegmentSpeaker} -> ${speaker}`
                );
                currentSegmentSpeaker = speaker;
                buffer.lastSpeaker = speaker;
              }
            }
          }
        } else {
          console.log("⚠️ [PROCESS] No words for speaker detection");
        }
      } else if (data.channels) {
        console.log(
          `🔄 [PROCESS] Multichannel mode, processing ${data.channels.length} channels`
        );
        // Simplified processing for multiple channels
        data.channels.forEach((channel: any, channelIndex: number) => {
          console.log(`🔄 [PROCESS] Processing channel ${channelIndex}`);

          // Initialize buffer for this channel if not exists
          if (!this.activeSpeakerBuffer[channelIndex]) {
            console.log(
              `🔄 [PROCESS] Initializing buffer for channel ${channelIndex}`
            );
            this.activeSpeakerBuffer[channelIndex] = {
              lastSpeaker:
                channelIndex === 0
                  ? getPrimaryUser()
                  : `Speaker ${channelIndex}`,
              currentSegment: [],
              formattedSegment: "",
              lastFlushedText: "",
            };
          }

          const alternative = channel.alternatives[0];
          if (!alternative?.transcript) {
            console.log(
              `❌ [PROCESS] Channel ${channelIndex} without transcription, skipping`
            );
            return;
          }

          const buffer = this.activeSpeakerBuffer[channelIndex];
          const transcriptText = `[${buffer.lastSpeaker}] ${alternative.transcript}`;

          // Send final transcriptions via IPC to the main process
          if (
            typeof window !== "undefined" &&
            window.electronAPI &&
            transcriptText.trim() &&
            transcriptText !== buffer.lastFlushedText
          ) {
            try {
              console.log(
                `📢 [PROCESS] Sending final transcription via IPC: "${transcriptText}"`
              );
              window.electronAPI.sendAudioTranscription(transcriptText);
            } catch (error) {
              console.error("❌ [PROCESS] Error sending via IPC:", error);
            }
          }

          // Also forward to storage service for unified processing
          if (this.transcriptionStorageService) {
            this.transcriptionStorageService.addTranscription(transcriptText);
          }

          // Also send via callback for compatibility
          if (
            this.transcriptionCallback &&
            transcriptText.trim() &&
            transcriptText !== buffer.lastFlushedText
          ) {
            console.log(
              `📢 [PROCESS] Sending to callback: "${transcriptText}"`
            );
            this.transcriptionCallback("transcript", {
              text: transcriptText,
              isFinal: true, // Always true because we're filtering out non-final transcriptions
              channel: channelIndex,
              speaker: buffer.lastSpeaker,
            });

            // Save final transcription
            console.log(
              `📝 [PROCESS] Saving final transcription: "${transcriptText}"`
            );
            this.transcriptionList.push(transcriptText);
            buffer.lastFlushedText = transcriptText;
          } else {
            console.log(
              `⚠️ [PROCESS] Text ignored: empty=${!transcriptText.trim()}, repeated=${
                transcriptText === buffer.lastFlushedText
              }`
            );
          }

          // Store formatted version
          buffer.formattedSegment = alternative.transcript;
        });
      } else {
        console.log(
          "❌ [PROCESS] Unrecognized data format:",
          Object.keys(data)
        );
      }

      console.log("✅ [PROCESS] Transcription processing completed");
    } catch (error) {
      console.log("❌ [PROCESS] Error during transcription processing:", error);
      this.logger.error("Error processing transcription", error);
    }
  }

  /**
   * Flush the current speaker segment and return formatted text
   */
  public flushSpeakerSegment(channelIndex: number): string | null {
    const buffer = this.activeSpeakerBuffer[channelIndex];
    if (!buffer || buffer.currentSegment.length === 0) return null;

    // Use the original formatted text instead of reconstructing from words
    const content =
      buffer.formattedSegment || buffer.currentSegment.join(" ").trim();
    if (!content) return null;

    // Check if speaker has changed from last transcription segment
    const speakerPrefix =
      this.currentSpeaker !== buffer.lastSpeaker
        ? `[${buffer.lastSpeaker}] `
        : "";

    // Update current speaker
    this.currentSpeaker = buffer.lastSpeaker;

    const text = `${speakerPrefix}${content}`;
    buffer.currentSegment = [];
    buffer.formattedSegment = "";
    return text;
  }

  /**
   * Get the stored transcription list
   */
  public getTranscriptionList(): string[] {
    return [...this.transcriptionList];
  }

  /**
   * Clear the transcription history
   */
  public clearTranscriptionList(): void {
    this.transcriptionList = [];
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionContextManager.ts
// Manages the temporary context of transcriptions

/**
 * Context manager that maintains the temporaryContext between calls
 * and ensures it is not lost when dynamic objects are modified
 */
export class TranscriptionContextManager {
    // Singleton instance
    private static instance: TranscriptionContextManager;

    // Stores the current temporary context
    private currentTemporaryContext: string = '';

    // Stores the memory associated with the temporary context
    private temporaryContextMemory: string = '';

    // Stores the last temporary context queried in Pinecone
    // to avoid repeated queries with the same context
    private lastQueriedTemporaryContext: string = '';

    // Use the getInstance() method to obtain the unique instance
    private constructor() { }

    /**
     * Gets the unique instance of the context manager
     */
    public static getInstance(): TranscriptionContextManager {
        if (!TranscriptionContextManager.instance) {
            TranscriptionContextManager.instance = new TranscriptionContextManager();
        }

        return TranscriptionContextManager.instance;
    }

    /**
     * Sets or updates the temporary context
     * @param context The new temporary context
     */
    public setTemporaryContext(context: string | undefined): void {
        if (context === undefined) {
            return; // Does not clear the context if undefined is passed
        }

        // Updates the temporary context with the new value, even if it is an empty string
        // (Note: an empty string does not clear the context, it only serves to replace
        // the previous context with an empty context)
        this.currentTemporaryContext = context;
    }

    /**
     * Gets the current temporary context
     * @returns The current temporary context or empty string
     */
    public getTemporaryContext(): string {
        return this.currentTemporaryContext;
    }

    /**
     * Sets the memory associated with the temporary context
     * @param memory The memory associated with the context
     */
    public setTemporaryContextMemory(memory: string): void {
        this.temporaryContextMemory = memory;
    }

    /**
     * Gets the memory associated with the temporary context
     * @returns The memory associated with the temporary context
     */
    public getTemporaryContextMemory(): string {
        return this.temporaryContextMemory;
    }

    /**
     * Checks if the current temporary context is different from the last queried context
     * @param context The temporary context to be verified
     * @returns true if the context is different from the last queried context, false otherwise
     */
    public hasTemporaryContextChanged(context: string): boolean {
        // Normalize the context (remove extra spaces)
        const normalizedContext = (context || "").trim();
        
        // If the context is empty, we don't consider it as a change
        // Always returns false to avoid unnecessary new queries
        if (!normalizedContext) {
            return false;
        }
        
        // If we get here, the context is non-empty and needs to be compared
        const normalizedLastContext = (this.lastQueriedTemporaryContext || "").trim();
        
        // Only returns true if it is different from the last (to make a new query)
        return normalizedContext !== normalizedLastContext;
    }

    /**
     * Updates the last queried temporary context
     * @param context The temporary context consulted
     */
    public updateLastQueriedTemporaryContext(context: string): void {
        this.lastQueriedTemporaryContext = context;
    }

    /**
     * Clears the stored temporary context
     */
    public clearTemporaryContext(): void {
        this.currentTemporaryContext = '';
        this.temporaryContextMemory = '';
        this.lastQueriedTemporaryContext = '';
    }

    /**
     * Checks if there is a defined temporary context
     */
    public hasTemporaryContext(): boolean {
        return this.currentTemporaryContext.trim().length > 0;
    }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionFormatter.ts
// Implementation of ITranscriptionFormatter

import { ITranscriptionFormatter } from "../../interfaces/transcription/ITranscriptionFormatter";
import { SpeakerSegment } from "../../interfaces/transcription/TranscriptionTypes";

export class TranscriptionFormatter implements ITranscriptionFormatter {
  /**
   * Formats mixed transcriptions with speaker labels
   */
  formatMixedTranscription(text: string, primaryUserSpeaker: string): SpeakerSegment[] {
    const results: SpeakerSegment[] = [];
    const speakerPattern = /\[([^\]]+)\]\s*(.*?)(?=\s*\[[^\]]+\]|$)/gs;
    let match;
    let lastSpeaker = "";
    
    while ((match = speakerPattern.exec(text)) !== null) {
      if (match[1] && match[2]) {
        const rawSpeaker = match[1].trim();
        const spokenText = match[2].trim();
        
        if (spokenText) {
          let speaker: string;
          
          // Simplified speaker attribution rules
          if (rawSpeaker === primaryUserSpeaker) {
            speaker = primaryUserSpeaker;
          } else if (rawSpeaker.toLowerCase().includes("speaker")) {
            speaker = "external";
          } else {
            speaker = "external";
          }
          
          // Only show speaker label when it changes
          const showSpeaker = (lastSpeaker !== speaker);
          lastSpeaker = speaker;
          
          results.push({
            speaker,
            text: `[${rawSpeaker}] ${spokenText}`,
            showSpeaker
          });
        }
      }
    }
    
    return results;
  }
  
  /**
   * Formats external speaker content to ensure correct labeling
   */
  formatExternalSpeakerContent(content: string): string {
    if (!content) return "";
    
    const lines = content.split('\n');
    const formattedLines = lines.map(line => {
      if (!line.trim()) return line;
      
      // Check if it already has a [Speaker X] prefix
      if (line.match(/^\[[^\]]+\]/)) {
        return line; // Already has prefix, keep as is
      }
      
      // Check if it contains "Speaker" without prefix
      if (line.toLowerCase().includes("speaker")) {
        // Extract speaker number if present
        const speakerMatch = line.match(/speaker\s*(\d+)/i);
        const speakerLabel = speakerMatch ? 
          `Speaker ${speakerMatch[1]}` : "External Participant";
          
        // Add prefix
        return `[${speakerLabel}] ${line}`;
      }
      
      return line;
    });
    
    return formattedLines.join('\n');
  }
  
  /**
   * Sanitizes memory content and fixes speaker attributions
   */
  sanitizeMemoryContent(content: string, isSpeakerContent: boolean = false): string {
    if (!content) return "";
    
    // If already speaker content or doesn't contain "Speaker", return as is
    if (isSpeakerContent || !content.toLowerCase().includes("speaker")) {
      return content;
    }
    
    // Process text lines to add speaker prefixes to lines mentioning "Speaker"
    const lines = content.split('\n');
    const processedLines = lines.map(line => {
      if (line.toLowerCase().includes("speaker")) {
        // Check if already has Speaker prefix
        if (line.match(/^\[[^\]]+\]/)) {
          return line; // Already has prefix, keep as is
        }
        
        // Extract speaker number if present
        const speakerMatch = line.match(/speaker\s*(\d+)/i);
        const speakerLabel = speakerMatch ? 
          `Speaker ${speakerMatch[1]}` : "External Participant";
          
        // Add prefix
        return `[${speakerLabel}] ${line}`;
      }
      return line;
    });
    
    return processedLines.join('\n');
  }
  
  /**
   * Combines speaker segments into a coherent conversation
   */
  buildConversationFromSegments(
    segments: SpeakerSegment[], 
    preserveSpeakerLabels: boolean = true
  ): string {
    const conversation: string[] = [];
    
    for (const segment of segments) {
      // Extract the original speaker label if available
      let text = segment.text;
      
      if (!preserveSpeakerLabels) {
        // Remove any existing speaker labels
        text = text.replace(/^\[[^\]]+\]\s*/, '');
      }
      
      conversation.push(text);
    }
    
    return conversation.join('\n');
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  NeuralProcessingResult,
  NeuralSignalResponse,
} from "../../interfaces/neural/NeuralSignalTypes";
import { NeuralSignalExtractor } from "../../symbolic-cortex/activation/NeuralSignalExtractor";
import { LoggingUtils } from "../../utils/LoggingUtils";
import symbolicCognitionTimelineLogger from "../utils/SymbolicCognitionTimelineLoggerSingleton";
import {
  getOption,
  STORAGE_KEYS,
} from "./../../../../../services/StorageService";

// Services interfaces
import { IMemoryService } from "../../interfaces/memory/IMemoryService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { ITranscriptionStorageService } from "../../interfaces/transcription/ITranscriptionStorageService";
import { ISpeakerIdentificationService } from "../../interfaces/utils/ISpeakerIdentificationService";
import { IUIUpdateService } from "../../interfaces/utils/IUIUpdateService";
import { INeuralIntegrationService } from "../../symbolic-cortex/integration/INeuralIntegrationService";

// Neural processors
import { cleanThinkTags } from "../../utils/ThinkTagCleaner";
import {
  NeuralConfigurationBuilder,
  NeuralMemoryRetriever,
  NeuralSignalEnricher,
  ProcessingResultsSaver,
  ProcessorMode,
  ResponseGenerator,
  SessionManager,
  TranscriptionExtractor,
} from "./processors";

/**
 * Response from transcription processing
 */
export interface TranscriptionProcessingResponse {
  response: string;
  neuralActivation: NeuralSignalResponse;
  processingResults: NeuralProcessingResult[];
}

/**
 * Neural transcription cognitive orchestrator
 * Coordinates specialized processors for transcription prompt processing
 * Supports both Ollama and HuggingFace backends for cognitive diversity
 */
export class TranscriptionPromptProcessor {
  private isProcessingPrompt: boolean = false;
  private currentLanguage: string;

  // Neural components
  private _neuralSignalExtractor: NeuralSignalExtractor;

  // Specialized processors (SOLID architecture)
  private transcriptionExtractor!: TranscriptionExtractor;
  private configurationBuilder!: NeuralConfigurationBuilder;
  private sessionManager!: SessionManager;
  private signalEnricher!: NeuralSignalEnricher;
  private memoryRetriever!: NeuralMemoryRetriever;
  private responseGenerator!: ResponseGenerator;
  private resultsSaver!: ProcessingResultsSaver;

  constructor(
    private storageService: ITranscriptionStorageService,
    private memoryService: IMemoryService,
    private llmService: IOpenAIService, // Pode ser Ollama ou HuggingFace, já abstraído
    private uiService: IUIUpdateService,
    private speakerService: ISpeakerIdentificationService,
    private neuralIntegrationService: INeuralIntegrationService
  ) {
    this.currentLanguage = getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "pt-BR";
    this._neuralSignalExtractor = new NeuralSignalExtractor(this.llmService);

    // Inicializa os processors com a instância já abstraída (Ollama ou HuggingFace)
    this._initializeProcessors();
  }

  /**
   * Initialize all specialized neural processors following SOLID principles
   */
  private _initializeProcessors(): void {
    this.transcriptionExtractor = new TranscriptionExtractor(
      this.storageService
    );
    this.sessionManager = new SessionManager();

    this.configurationBuilder = new NeuralConfigurationBuilder(
      this.storageService,
      this.memoryService,
      this.speakerService,
      this.sessionManager
    );

    this.signalEnricher = new NeuralSignalEnricher(this.llmService);

    this.memoryRetriever = new NeuralMemoryRetriever(this.memoryService);

    this.responseGenerator = new ResponseGenerator(
      this.memoryService,
      this.llmService
    );

    this.resultsSaver = new ProcessingResultsSaver(
      this.memoryService,
      this.storageService,
      this.speakerService,
      this.sessionManager
    );
  }

  /**
   * Process transcription with LLM backend (Ollama in Advanced mode, OpenAI compatible)
   * Full neural processing with symbolic cognition
   */
  async processWithOpenAI(temporaryContext?: string): Promise<void> {
    await this._processTranscriptionPrompt("openai", temporaryContext);
  }

  /**
   * Process transcription with HuggingFace backend
   * Local neural processing with enhanced privacy
   */
  async processWithHuggingFace(temporaryContext?: string): Promise<void> {
    await this._processTranscriptionPrompt("huggingface", temporaryContext);
  }

  /**
   * Process direct message from chat (no transcription)
   * @param message The message from chat input
   * @param temporaryContext Optional additional context
   */
  async processDirectMessage(
    message: string,
    temporaryContext?: string
  ): Promise<void> {
    const mode = this.llmService.constructor.name.includes("HuggingFace")
      ? "huggingface"
      : "openai";
    await this._processDirectMessage(mode, message, temporaryContext);
  }

  /**
   * Process a direct message from chat interface
   */
  private async _processDirectMessage(
    mode: ProcessorMode,
    message: string,
    temporaryContext?: string
  ): Promise<void> {
    // Prevent concurrent processing
    if (this.isProcessingPrompt) {
      LoggingUtils.logWarning(
        "Blocking prompt request: Already processing another prompt"
      );
      return;
    }

    this.uiService.updateUI({ aiResponse: "Processing..." });

    try {
      this.isProcessingPrompt = true;

      // Garante que o backend já abstraído está pronto
      if (!(await this.llmService.ensureOpenAIClient())) return;

      LoggingUtils.logInfo("Processing direct message from chat");

      // Log cognitive activities
      symbolicCognitionTimelineLogger.logRawPrompt(message);
      if (temporaryContext?.trim()) {
        LoggingUtils.logInfo(`Using additional context: "${temporaryContext}"`);
        symbolicCognitionTimelineLogger.logTemporaryContext(temporaryContext);
      }

      // Notify processing start
      this.uiService.notifyPromptProcessingStarted(temporaryContext);

      LoggingUtils.logInfo(
        `Processing message: "${message.substring(0, 50)}..."${
          temporaryContext ? " with additional context" : ""
        }`
      );

      // Process using orchestrated pipeline
      const result = await this._executeProcessingPipeline(
        mode,
        message,
        temporaryContext
      );

      // Update UI and complete processing
      this.uiService.updateUI({ aiResponse: result.response });
      this.uiService.notifyPromptComplete(result.response);
    } catch (error: Error | unknown) {
      const errorMessage =
        error instanceof Error ? error.message : "Unknown error";
      LoggingUtils.logError("Error processing message", error);
      this.uiService.updateUI({ aiResponse: `Error: ${errorMessage}` });
      this.uiService.notifyPromptError(errorMessage);
    } finally {
      this.isProcessingPrompt = false;
      LoggingUtils.logInfo("Message processing completed, releasing lock");
    }
  }

  /**
   * Main transcription processing orchestration - adaptable to different backends
   */
  private async _processTranscriptionPrompt(
    mode: ProcessorMode,
    temporaryContext?: string
  ): Promise<void> {
    // Prevent concurrent processing
    if (this.isProcessingPrompt) {
      LoggingUtils.logWarning(
        "Blocking prompt request: Already processing another prompt"
      );
      return;
    }

    this.uiService.updateUI({ aiResponse: "Processing..." });

    try {
      this.isProcessingPrompt = true;

      // Garante que o backend já abstraído está pronto (Ollama ou HuggingFace)
      if (!(await this.llmService.ensureOpenAIClient())) return;

      // Validate and extract transcriptions
      const hasTranscriptions = this.storageService.hasValidTranscriptions();

      if (!hasTranscriptions) {
        LoggingUtils.logWarning("No transcription detected");

        // Verify if there is text in lastTranscription
        const lastTranscription = this.storageService.getLastTranscription();
        if (lastTranscription) {
          LoggingUtils.logInfo(
            `Using last known transcription: "${lastTranscription}"`
          );
        } else {
          // Notify error if there is no transcription
          this.uiService.notifyPromptError(
            "No transcription detected for processing"
          );
          LoggingUtils.logInfo(`No transcription detected for processing`);
          return;
        }
      }

      // Notify processing start
      this.uiService.notifyPromptProcessingStarted(temporaryContext);

      // Extract new transcription lines AND mark as sent atomically
      const extractedLines = this.transcriptionExtractor.extractAndMarkAsSent();
      let promptText: string | null = extractedLines;

      if (!promptText || promptText.trim().length === 0) {
        LoggingUtils.logInfo("No new transcription to send.");
        return;
      }

      // Log cognitive activities
      symbolicCognitionTimelineLogger.logRawPrompt(promptText);

      // Log temporary context if provided
      if (temporaryContext?.trim()) {
        LoggingUtils.logInfo(`Using additional context: "${temporaryContext}"`);
        symbolicCognitionTimelineLogger.logTemporaryContext(temporaryContext);
      }

      LoggingUtils.logInfo(
        `Processing transcription: "${promptText.substring(0, 50)}..."${
          temporaryContext ? " with additional context" : ""
        }`
      );

      // Process using orchestrated pipeline
      const result = await this._executeProcessingPipeline(
        mode,
        promptText,
        temporaryContext
      );

      // Update UI and complete processing
      this.uiService.updateUI({ aiResponse: result.response });
      this.uiService.notifyPromptComplete(result.response);

      // Note: Transcriptions were already marked as sent during extraction
      // This prevents race conditions and duplicate sends

      // Update UI to show only new transcriptions (empty initially)
      if (this.storageService.updateUIWithNewTranscriptions) {
        this.storageService.updateUIWithNewTranscriptions();
      }
    } catch (error: Error | unknown) {
      const errorMessage =
        error instanceof Error ? error.message : "Unknown error";
      LoggingUtils.logError("Error processing prompt", error);
      this.uiService.updateUI({ aiResponse: `Error: ${errorMessage}` });
      this.uiService.notifyPromptError(errorMessage);
    } finally {
      this.isProcessingPrompt = false;
      LoggingUtils.logInfo("Prompt processing completed, releasing lock");
    }
  }

  /**
   * Execute the full neural processing pipeline using specialized processors
   */
  private async _executeProcessingPipeline(
    mode: ProcessorMode,
    transcriptionToSend: string,
    temporaryContext?: string
  ): Promise<TranscriptionProcessingResponse> {
    // PHASE 1: Neural Signal Extraction
    LoggingUtils.logInfo(
      "🧠 Starting neural system: Phase 1 - Sensory analysis..."
    );

    const extractionConfig =
      await this.configurationBuilder.buildExtractionConfig(
        transcriptionToSend,
        temporaryContext,
        this.currentLanguage
      );
    const neuralActivation =
      await this._neuralSignalExtractor.extractNeuralSignals(extractionConfig);

    // PHASE 2: Query Enrichment & Memory Retrieval
    const enrichedSignals = await this.signalEnricher.enrichSignals(
      neuralActivation.signals,
      this.currentLanguage
    );
    const processingResults = await this.memoryRetriever.processSignals(
      enrichedSignals
    );

    // PHASE 3: Neural Integration
    LoggingUtils.logInfo(
      "💥 Third phase - Integrating neural processing into final prompt..."
    );
    const integratedPrompt = await this.neuralIntegrationService.integrate(
      processingResults,
      transcriptionToSend,
      this.currentLanguage
    );

    // Symbolic context synthesis log (clean think tags before logging)
    const cleanIntegratedPrompt = cleanThinkTags(integratedPrompt);
    symbolicCognitionTimelineLogger.logSymbolicContextSynthesized({
      summary: cleanIntegratedPrompt, // summary is required in SymbolicContext
      modules: processingResults.map((r) => ({
        core: r.core,
        intensity: r.intensity,
      })),
    });

    // PHASE 4: Generate Response
    const fullResponse = await this.responseGenerator.generateResponse(
      cleanIntegratedPrompt,
      temporaryContext
    );

    const response = cleanThinkTags(fullResponse);

    // PHASE 5: Save and Log Results
    await this.resultsSaver.saveResults(
      transcriptionToSend,
      response,
      neuralActivation,
      processingResults
    );

    return {
      response,
      neuralActivation,
      processingResults,
    };
  }

  /**
   * Check if currently processing a prompt
   */
  public isProcessingPromptRequest(): boolean {
    return this.isProcessingPrompt;
  }

  /**
   * Update current language for processing
   */
  public setLanguage(language: string): void {
    this.currentLanguage = language;
  }

  /**
   * Get current processing language
   */
  public getCurrentLanguage(): string {
    return this.currentLanguage;
  }

  /**
   * Reset transcription processing state across all processors
   */
  public reset(): void {
    this.isProcessingPrompt = false;
    this.transcriptionExtractor.reset();
    this.sessionManager.resetSession();
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionSnapshotTracker.ts
// Tracks previously sent transcription lines to prevent duplication

/**
 * A class responsible for tracking transcription lines that have already
 * been sent to OpenAI to prevent duplication in future requests.
 */
export class TranscriptionSnapshotTracker {
  private sentLines: Set<string> = new Set<string>();
  
  /**
   * Filters a transcription text to only include lines that haven't been sent before
   * @param transcription The full transcription text to filter
   * @returns A filtered transcription containing only new content
   */
  public filterTranscription(transcription: string): string {
    if (!transcription?.trim()) return '';
    
    // Split the transcription into lines and normalize each line
    const lines = transcription.split('\n')
      .map(line => this.normalizeLine(line))
      .filter(line => line.length > 0);
    
    // Filter out lines that have already been sent
    const newLines = lines.filter(line => !this.sentLines.has(line));
    
    // If no new lines, return empty string
    if (newLines.length === 0) return '';
    
    // Return the filtered transcription
    return newLines.join('\n');
  }
  
  /**
   * Updates the snapshot with lines that were just sent to OpenAI
   * @param transcription The transcription that was actually sent
   */
  public updateSnapshot(transcription: string): void {
    if (!transcription?.trim()) return;
    
    // Split the transcription into lines and normalize each line
    const lines = transcription.split('\n')
      .map(line => this.normalizeLine(line))
      .filter(line => line.length > 0);
    
    for (const line of lines) {
      this.sentLines.add(line);
    }
  }
  
  /**
   * Checks if a transcription contains only content that has already been sent
   * @param transcription The transcription to check
   * @returns True if all content has already been sent
   */
  public isAllContentSent(transcription: string): boolean {
    if (!transcription?.trim()) return true;
    
    const lines = transcription.split('\n')
      .map(line => this.normalizeLine(line))
      .filter(line => line.length > 0);
    
    // Check if all lines are already in the sent lines set
    return lines.every(line => this.sentLines.has(line));
  }
  
  /**
   * Resets the snapshot tracker, clearing all tracked lines
   */
  public reset(): void {
    this.sentLines.clear();
  }
  
  /**
   * Normalizes a line of text by trimming and collapsing whitespace
   * @param line The line of text to normalize
   * @returns Normalized line
   */
  private normalizeLine(line: string): string {
    if (!line) return '';
    
    // Trim the line and collapse multiple whitespaces within the line
    return line.trim().replace(/\s+/g, ' ');
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionStorageService.ts
// Refactored using SOLID, DRY, KISS, and YAGNI principles
// Acts as a Facade orchestrating smaller, focused components

import { ITranscriptionStorageService } from "../../interfaces/transcription/ITranscriptionStorageService";
import {
  SpeakerTranscription,
  SpeakerTranscriptionLog,
  UIUpdater,
} from "../../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../../interfaces/utils/ISpeakerIdentificationService";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { DeepgramTranscriptionService } from "../DeepgramTranscriptionService";

// Import refactored components
import { QuestionDetector } from "./detectors/QuestionDetector";
import { TranscriptionLogger } from "./loggers/TranscriptionLogger";
import { TranscriptionUIManager } from "./managers/TranscriptionUIManager";
import { TranscriptionProcessor } from "./processors/TranscriptionProcessor";
import { TranscriptionStore } from "./stores/TranscriptionStore";

/**
 * TranscriptionStorageService - Refactored using SOLID principles
 * Acts as a Facade Pattern orchestrating smaller, focused components
 * Each component has a single responsibility, making the code more maintainable
 */
export class TranscriptionStorageService
  implements ITranscriptionStorageService
{
  // Components following Single Responsibility Principle
  private store: TranscriptionStore;
  private questionDetector: QuestionDetector;
  private processor: TranscriptionProcessor;
  private logger: TranscriptionLogger;
  private uiManager: TranscriptionUIManager;

  // Service dependencies
  private speakerService: ISpeakerIdentificationService;
  private transcriptionService: DeepgramTranscriptionService | null = null;

  constructor(
    speakerService: ISpeakerIdentificationService,
    setTexts: UIUpdater
  ) {
    // Initialize all components - Dependency Injection
    this.speakerService = speakerService;
    this.store = new TranscriptionStore();
    this.questionDetector = new QuestionDetector();
    this.processor = new TranscriptionProcessor(speakerService);
    this.logger = new TranscriptionLogger(speakerService);
    this.uiManager = new TranscriptionUIManager(setTexts);
  }

  /**
   * Sets the transcription service for auto-prompt functionality
   */
  setTranscriptionService(service: DeepgramTranscriptionService): void {
    this.transcriptionService = service;
  }

  /**
   * Adds a new transcription to the storage
   * Delegates to appropriate components following Single Responsibility
   */
  addTranscription(text: string, speaker?: string): void {
    if (!text || !text.trim()) return;

    const cleanText = text.trim();

    // Handle question cycle interruption
    this.questionDetector.handleNewTranscriptionDuringCycle(cleanText);

    // Check for duplicates
    if (
      this.processor.isDuplicate(
        cleanText,
        this.store.getSpeakerTranscriptions()
      )
    ) {
      return;
    }

    // Process segments if text has speaker markers
    if (this.processor.hasSpeakerMarkers(cleanText)) {
      const segments = this.processor.processSegments(cleanText);

      for (const segment of segments) {
        if (
          !this.processor.isDuplicate(
            segment.text,
            this.store.getSpeakerTranscriptions()
          )
        ) {
          this.addSingleSpeakerTranscription(segment.text, segment.speaker);
          this.store.setCurrentSpeaker(segment.speaker);
        }
      }
      return;
    }

    // Process speaker without markers
    const { speaker: processedSpeaker, cleanText: processedText } =
      this.processor.processSpeaker(
        cleanText,
        speaker,
        this.store.getCurrentSpeaker()
      );

    this.addSingleSpeakerTranscription(processedText, processedSpeaker);
    this.store.setCurrentSpeaker(processedSpeaker);
  }

  /**
   * Adds a single speaker transcription
   * Simplified by delegating to components
   */
  addSingleSpeakerTranscription(text: string, speaker: string): void {
    if (!text || !text.trim()) return;

    LoggingUtils.logInfo(`Adding transcription for "${speaker}": "${text}"`);

    // Store the transcription
    this.store.addTranscription(text, speaker, new Date().toISOString());

    // Always update UI to show only new (unsent) transcriptions
    // This prevents accumulation of sent transcriptions in the display
    this.updateUIWithNewTranscriptions();

    // Check if we should detect questions
    if (
      this.questionDetector.isAutoDetectionEnabled() &&
      this.transcriptionService &&
      this.questionDetector.isQuestion(text) &&
      speaker === this.speakerService.getPrimaryUserSpeaker() &&
      this.transcriptionService.isOnlyUserSpeaking() &&
      !this.questionDetector.isDuplicateQuestion(text) &&
      !this.questionDetector.isInQuestionCycle()
    ) {
      this.questionDetector.startQuestionCycle(text);
    }
  }

  /**
   * Flush all accumulated transcriptions to the UI
   * Delegates to UI manager
   */
  public flushTranscriptionsToUI(): void {
    const transcriptions = this.store.getUITranscriptionList();
    LoggingUtils.logInfo(
      `Flushing ${transcriptions.length} transcriptions to UI`
    );
    this.uiManager.flushToUI(transcriptions);
  }

  /**
   * Update transcription UI directly
   * Delegates to UI manager
   */
  public updateTranscriptionUI(transcription: string): void {
    console.log(
      `🖥️ [TranscriptionStorageService] Updating UI with: "${transcription}"`
    );
    // Always show only new transcriptions, not the full history
    // This prevents accumulation of sent transcriptions in the UI
    this.updateUIWithNewTranscriptions();
  }

  /**
   * Enable or disable automatic question detection
   * Delegates to question detector
   */
  setAutoQuestionDetection(enabled: boolean): void {
    this.questionDetector.setAutoDetection(enabled);
  }

  /**
   * Cancel pending question timer
   * Delegates to question detector
   */
  cancelPendingQuestionTimer(): void {
    this.questionDetector.cancelPendingTimer();
  }

  /**
   * Get UI transcription text
   * Delegates to store
   */
  getUITranscriptionText(): string {
    return this.store.getUITranscriptionText();
  }

  /**
   * Alias for getUITranscriptionText
   */
  getTranscriptionPromptText(): string {
    return this.getUITranscriptionText();
  }

  /**
   * Get transcription list
   * Delegates to store
   */
  getTranscriptionList(): string[] {
    return this.store.getTranscriptionList();
  }

  /**
   * Get speaker transcriptions
   * Delegates to store
   */
  getSpeakerTranscriptions(): SpeakerTranscription[] {
    return this.store.getSpeakerTranscriptions();
  }

  /**
   * Get transcription logs
   * Delegates to logger
   */
  getTranscriptionLogs(): SpeakerTranscriptionLog[] {
    return this.logger.generateLogs(this.store.getSpeakerTranscriptions());
  }

  /**
   * Clear transcription data
   * Delegates to components
   */
  clearTranscriptionData(): void {
    this.store.clear();
    this.uiManager.clear();
    this.updateUI({ transcription: "" });
  }

  /**
   * Check if has valid transcriptions
   * Delegates to store
   */
  hasValidTranscriptions(): boolean {
    return this.store.hasValidTranscriptions();
  }

  /**
   * Get last transcription
   * Delegates to store
   */
  getLastTranscription(): string {
    return this.store.getLastTranscription();
  }

  /**
   * Get last message from user
   * Delegates to processor
   */
  getLastMessageFromUser(): SpeakerTranscription | null {
    return this.processor.getLastUserMessage(
      this.store.getSpeakerTranscriptions(),
      this.speakerService.getPrimaryUserSpeaker()
    );
  }

  /**
   * Get last messages from external speakers
   * Delegates to processor
   */
  getLastMessagesFromExternalSpeakers(): Map<string, SpeakerTranscription> {
    return this.processor.getLastExternalMessages(
      this.store.getSpeakerTranscriptions(),
      this.store.getDetectedSpeakers()
    );
  }

  /**
   * Get detected speakers
   * Delegates to store
   */
  getDetectedSpeakers(): Set<string> {
    return this.store.getDetectedSpeakers();
  }

  /**
   * Set current speaker
   * Delegates to store
   */
  setCurrentSpeaker(speaker: string): void {
    this.store.setCurrentSpeaker(speaker);
  }

  /**
   * Get current speaker
   * Delegates to store
   */
  getCurrentSpeaker(): string {
    return this.store.getCurrentSpeaker();
  }

  /**
   * Private method to update UI with other properties
   * Uses UI manager for consistency
   */
  private updateUI(update: Record<string, unknown>): void {
    this.uiManager.updateOther(update);
  }

  /**
   * Get only new transcriptions that haven't been sent yet
   */
  getNewTranscriptions(): SpeakerTranscription[] {
    return this.store.getNewTranscriptions();
  }

  /**
   * Mark current transcriptions as sent
   */
  markTranscriptionsAsSent(): void {
    this.store.markTranscriptionsAsSent();
  }

  /**
   * Check if there are new transcriptions to send
   */
  hasNewTranscriptions(): boolean {
    return this.store.hasNewTranscriptions();
  }

  /**
   * Update UI to show only new transcriptions
   */
  updateUIWithNewTranscriptions(): void {
    const newTranscriptionsText = this.uiManager.getNewTranscriptionsText(
      this.store
    );
    // Use setTranscriptionDirectly to avoid adding to history
    this.uiManager.setTranscriptionDirectly(newTranscriptionsText);
    console.log(
      `🖥️ Updated UI with ${
        this.store.getNewTranscriptions().length
      } new transcriptions`
    );
  }

  /**
   * Extract new transcriptions and mark them as sent atomically
   * This prevents race conditions where transcriptions could be added
   * between extraction and marking
   */
  extractAndMarkAsSent(): SpeakerTranscription[] {
    // Get new transcriptions
    const newTranscriptions = this.store.getNewTranscriptions();

    if (newTranscriptions.length > 0) {
      // Mark them as sent immediately
      this.store.markTranscriptionsAsSent(newTranscriptions);
      console.log(
        `🚀 Extracted and marked ${newTranscriptions.length} transcriptions as sent`
      );

      // Clear UI history to prevent showing sent transcriptions
      this.uiManager.clearSentTranscriptions();

      // Update UI to show empty (no new transcriptions)
      this.updateUIWithNewTranscriptions();
    }

    return newTranscriptions;
  }

  /**
   * Get all transcriptions with their sent status
   * Used by UI to show which transcriptions have been sent
   */
  getAllTranscriptionsWithStatus(): SpeakerTranscription[] {
    return this.store.getAllTranscriptionsWithStatus();
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Interface for cognitive log exporters
import { CognitionEvent } from '../../types/CognitionEvent';

export interface CognitionLogExporter {
  label: string;
  export(log: CognitionEvent[], filename?: string): void;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Factory for cognitive log exporters
import { CognitionLogExporter } from './CognitionLogExporter';
import { CognitionLogJsonExporter } from './CognitionLogJsonExporter';
import { CognitionLogTxtExporter } from './CognitionLogTxtExporter';

/**
 * Factory responsible for creating and providing cognitive log exporters
 * Following the Factory Method pattern of SOLID
 */
export class CognitionLogExporterFactory {
  private static instance: CognitionLogExporterFactory;
  private exporters: CognitionLogExporter[] = [];

  private constructor() {
    // Initializes default exporters
    this.registerDefaultExporters();
  }

  /**
   * Gets the unique instance of the factory (Singleton)
   */
  public static getInstance(): CognitionLogExporterFactory {
    if (!CognitionLogExporterFactory.instance) {
      CognitionLogExporterFactory.instance = new CognitionLogExporterFactory();
    }
    return CognitionLogExporterFactory.instance;
  }

  /**
   * Registers default exporters of the system
   */
  private registerDefaultExporters(): void {
    this.exporters = [
      new CognitionLogJsonExporter(),
      new CognitionLogTxtExporter()
    ];
  }

  /**
   * Adds a new exporter to the list
   * @param exporter Exporter to be added
   */
  public registerExporter(exporter: CognitionLogExporter): void {
    this.exporters.push(exporter);
  }

  /**
   * Removes an exporter based on the label
   * @param label Label of the exporter to be removed
   */
  public unregisterExporter(label: string): void {
    this.exporters = this.exporters.filter(e => e.label !== label);
  }

  /**
   * Gets all registered exporters
   */
  public getExporters(): CognitionLogExporter[] {
    return [...this.exporters];
  }
}

// Export a singleton instance of the factory for global use
export default CognitionLogExporterFactory.getInstance();
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// JSON exporter for cognitive log
import { CognitionEvent } from '../../types/CognitionEvent';
import { CognitionLogExporter } from './CognitionLogExporter';

export class CognitionLogJsonExporter implements CognitionLogExporter {
  label = 'Export cognitive log (JSON)';
  export(log: CognitionEvent[], filename = 'symbolic_cognition_session.json') {
    const blob = new Blob([JSON.stringify(log, null, 2)], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    a.click();
    setTimeout(() => URL.revokeObjectURL(url), 1000);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TXT exporter for cognitive log
import { CognitionEvent } from '../../types/CognitionEvent';
import { CognitionLogExporter } from './CognitionLogExporter';

export class CognitionLogTxtExporter implements CognitionLogExporter {
  label = 'Export cognitive log (TXT)';
  export(log: CognitionEvent[], filename = 'symbolic_cognition_session.txt') {
    const txt = log.map(e => JSON.stringify(e, null, 2)).join('\n---\n');
    const blob = new Blob([txt], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    a.click();
    setTimeout(() => URL.revokeObjectURL(url), 1000);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Common types and interfaces used across Deepgram service modules
 */
import { ListenLiveClient } from "@deepgram/sdk";
import { ConnectionState } from "../../interfaces/deepgram/IDeepgramService";

/**
 * Speaker segment buffer structure
 */
export interface SpeakerBuffer {
  lastSpeaker: string;
  currentSegment: string[];
  formattedSegment: string;
  lastFlushedText: string;
}

/**
 * Transcription callback data structure
 */
export interface TranscriptionData {
  text: string;
  isFinal: boolean;
  channel: number;
  speaker: string;
}

/**
 * Connection status information
 */
export interface ConnectionStatus {
  state: ConnectionState;
  stateRef: ConnectionState;
  hasConnectionObject: boolean;
  readyState: number | null;
  active: boolean;
}

/**
 * Audio processing result
 */
export interface AudioProcessingResult {
  buffer: ArrayBuffer | null;
  valid: boolean;
}

/**
 * Connection state update callback
 */
export type ConnectionStateCallback = (state: ConnectionState) => void;

/**
 * Connection object update callback
 */
export type ConnectionCallback = (connection: ListenLiveClient | null) => void;

/**
 * Transcription event callback
 */
export type TranscriptionEventCallback = (event: string, data: any) => void;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Logger utility for Deepgram services
 * Provides consistent logging throughout the Deepgram service modules
 */
export class Logger {
  private context: string;
  
  constructor(context: string) {
    this.context = context;
  }
  
  /**
   * Log informational message
   */
  public info(message: string): void {
    console.log(`✅ [${this.context}] ${message}`);
  }
  
  /**
   * Log debug message with optional data
   */
  public debug(message: string, data?: any): void {
    if (data) {
      console.log(`🔍 [${this.context}] ${message}:`, data);
    } else {
      console.log(`🔍 [${this.context}] ${message}`);
    }
  }
  
  /**
   * Log warning message with optional error
   */
  public warning(message: string, error?: any): void {
    if (error) {
      console.warn(`⚠️ [${this.context}] ${message}:`, error);
    } else {
      console.warn(`⚠️ [${this.context}] ${message}`);
    }
  }
  
  /**
   * Log error message with optional error object
   */
  public error(message: string, error?: any): void {
    if (error) {
      console.error(`❌ [${this.context}] ${message}:`, error);
    } else {
      console.error(`❌ [${this.context}] ${message}`);
    }
  }
  
  /**
   * Handle error with context
   */
  public handleError(context: string, error: any): void {
    console.error(`❌ [${this.context}] ${context}:`, error);
  }
}

export default Logger;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// SpeakerIdentificationService.ts
// Service responsible for speaker identification and normalization

import { getPrimaryUser } from '../../../../../config/UserConfig';
import { EXTERNAL_SPEAKER_LABEL, SpeakerSegment, SpeakerTranscription } from "../../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../../interfaces/utils/ISpeakerIdentificationService";
import { LoggingUtils } from "../../utils/LoggingUtils";

export class SpeakerIdentificationService implements ISpeakerIdentificationService {
  private primaryUserSpeaker: string = getPrimaryUser();
  private currentSpeaker: string = "";

  constructor(primaryUserSpeaker?: string) {
    if (primaryUserSpeaker) {
      this.primaryUserSpeaker = primaryUserSpeaker;
    }
  }

  /**
   * Sets the name of the primary speaker (user)
   */
  setPrimaryUserSpeaker(name: string): void {
    if (name && name.trim()) {
      this.primaryUserSpeaker = name.trim();
      LoggingUtils.logInfo(`Primary speaker set as: "${this.primaryUserSpeaker}"`);
    }
  }
  
  /**
   * Gets the primary speaker (user)
   */
  getPrimaryUserSpeaker(): string {
    return this.primaryUserSpeaker;
  }

  /**
   * Normalizes the speaker identification for comparison
   * Converts variations like "user", "usuario", etc. to the primaryUserSpeaker
   */
  normalizeAndIdentifySpeaker(speaker: string): string {
    // If the input text is empty, return the primary speaker
    if (!speaker || !speaker.trim()) {
      return this.primaryUserSpeaker;
    }
    
    // Remove brackets if they exist
    const cleanSpeaker = speaker.replace(/^\[|\]$/g, '').trim();
    
    // Simple rule:
    // 1. If it is exactly "Guilherme", return primaryUserSpeaker
    // 2. If it is a pattern "Speaker N", return "external"
    // 3. For other cases, assume primaryUserSpeaker
    
    if (cleanSpeaker === this.primaryUserSpeaker) {
      return this.primaryUserSpeaker;
    }
    
    if (/^speaker\s*\d+$/i.test(cleanSpeaker)) {
      return "external";
    }
    
    // If it is a variation of "user", return primaryUserSpeaker
    if (/^(user|usuario|usuário)$/i.test(cleanSpeaker)) {
      return this.primaryUserSpeaker;
    }
    
    // By default, assume it is the primary user
    return this.primaryUserSpeaker;
  }

  /**
   * Extracts speech segments from a transcription that mixes speakers
   * Example: "[Guilherme] Olá [Speaker 0] Como vai? [Guilherme] Estou bem"
   * → [{ speaker: "Guilherme", text: "Olá" }, { speaker: "Speaker 0", text: "Como vai?" }, ...]
   */
  splitMixedTranscription(text: string): SpeakerSegment[] {
    const segments: SpeakerSegment[] = [];
    
    // Sanitize the input text
    const cleanText = text?.trim() || "";
    if (!cleanText) {
      return segments;
    }
    
    // Regex simplified to capture patterns like [Speaker] text until the next [Speaker]
    // Captures group 1: speaker name between brackets
    // Captures group 2: text spoken until the next speaker or end of the string
    const speakerPattern = /\[([^\]]+)\]\s*(.*?)(?=\s*\[[^\]]+\]|$)/gs;
    
    let match;
    let matchFound = false;
    let lastSpeakerKey = "";
    
    while ((match = speakerPattern.exec(cleanText)) !== null) {
      matchFound = true;
      const rawSpeaker = match[1].trim();
      const spokenText = match[2].trim();
      
      if (spokenText) {
        // Simple rule: If the speaker is "Guilherme", assign to the user
        // If the speaker is "Speaker N", assign as external
        let speakerToUse;
        
        if (rawSpeaker === this.primaryUserSpeaker) {
          speakerToUse = this.primaryUserSpeaker;
        } else if (/^speaker\s*\d+$/i.test(rawSpeaker)) {
          speakerToUse = "external";
        } else {
          // For other cases, use normalization
          speakerToUse = this.normalizeAndIdentifySpeaker(rawSpeaker);
        }
        
        // Show speaker if different from the previous one
        const showSpeaker = (lastSpeakerKey !== speakerToUse);
        lastSpeakerKey = speakerToUse;
        
        LoggingUtils.logInfo(`Segmento [${rawSpeaker}] → ${speakerToUse}: "${spokenText.substring(0, 30)}..."`);
        
        segments.push({
          speaker: speakerToUse,
          text: spokenText,
          showSpeaker: showSpeaker
        });
      }
    }
    
    // If no speaker pattern is found, consider the complete text
    if (!matchFound && cleanText) {
      // Use the current speaker of the service, if available
      const speakerToUse = this.currentSpeaker || this.primaryUserSpeaker;
      
      segments.push({
        speaker: speakerToUse,
        text: cleanText,
        showSpeaker: true // Always show when there is no explicit pattern
      });
      
      LoggingUtils.logInfo(`Text without speaker labels, assigned to: "${speakerToUse}"`);
    }
    
    return segments;
  }

  /**
   * Filters transcriptions by specific speaker
   */
  filterTranscriptionsBySpeaker(speaker: string, transcriptions: SpeakerTranscription[]): SpeakerTranscription[] {
    return transcriptions.filter(
      st => st.speaker === speaker
    );
  }

  /**
   * Filters transcriptions by the primary user,
   * extracting only segments where the user is speaking
   */
  filterTranscriptionsByUser(transcriptions: SpeakerTranscription[]): SpeakerTranscription[] {
    const userTranscriptions = transcriptions.filter(st => {
      // Basic verification: speaker must be the primary user
      if (st.speaker !== this.primaryUserSpeaker) {
        LoggingUtils.logInfo(`[FiltroUser] Rejeitado: speaker diferente do usuário principal (${st.speaker}) - texto: ${st.text.substring(0, 60)}`);
        return false;
      }
      // If the text contains speaker delimiters [...], ensure no external segments are present
      if (st.text.includes('[') && st.text.includes(']')) {
        const segments = this.splitMixedTranscription(st.text);
        if (segments.some(segment => segment.speaker !== this.primaryUserSpeaker)) {
          LoggingUtils.logInfo(`[FiltroUser] Rejeitado: transcrição mista com segmento externo detectado - texto: ${st.text.substring(0, 60)}`);
          return false;
        }
      }
      // If the text starts with an external speaker pattern, it is not from the user
      if (/^speaker\s*\d+\s*:/i.test(st.text)) {
        LoggingUtils.logInfo(`[FiltroUser] Rejeitado: começa como falante externo - texto: ${st.text.substring(0, 60)}`);
        return false;
      }
      // If the text explicitly mentions '[Speaker' at the beginning, log for diagnosis
      if (/^\[Speaker\s*\d+\]/i.test(st.text)) {
        LoggingUtils.logInfo(`[FiltroUser] Rejeitado: possível segmento externo no início - texto: ${st.text.substring(0, 60)}`);
        return false;
      }
      return true;
    });
    
    LoggingUtils.logInfo(`Filtrado ${userTranscriptions.length} transcrições do usuário principal`);
    return userTranscriptions;
  }

  /**
   * Verifies if only the primary user is speaking
   */
  isOnlyUserSpeaking(transcriptions: SpeakerTranscription[]): boolean {
    if (transcriptions.length === 0) {
      return true; // No transcriptions yet, assume only user
    }
    
    // For each transcription, verify if it contains only the user speaking
    for (const transcription of transcriptions) {
      // If the transcription contains speaker markers [Speaker]
      if (transcription.text.includes('[') && transcription.text.includes(']')) {
        const segments = this.splitMixedTranscription(transcription.text);
        
        // If any segment is not from the primary user, it is not just the user speaking
        const hasNonUserSegment = segments.some(segment => 
          this.normalizeAndIdentifySpeaker(segment.speaker) !== this.primaryUserSpeaker
        );
        
        if (hasNonUserSegment) {
          return false;
        }
      } else {
        // If no markers, verify by the speaker field
        if (this.normalizeAndIdentifySpeaker(transcription.speaker) !== this.primaryUserSpeaker) {
          return false;
        }
      }
    }
    
    // If it got here, all transcriptions are from the user
    return true;
  }

  /**
   * Prepares the transcription text for sending, combining all inputs
   */
  prepareTranscriptionText(
    transcriptionList: string[],
    speakerTranscriptions: SpeakerTranscription[],
    lastTranscription: string
  ): string {
    // If we have transcriptions by speaker, use primarily these
    if (speakerTranscriptions.length > 0) {
      // Use a Set to avoid duplicate text
      const processedTexts = new Set<string>();
      
      // First, sort by timestamp to ensure chronological order
      const sortedTranscriptions = [...speakerTranscriptions]
        .sort((a, b) => new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime());
      
      // Process each transcription, maintaining all speakers in sequence
      const conversationPieces: string[] = [];
      let lastSpeakerKey = "";
      
      for (const st of sortedTranscriptions) {
        // Avoid duplicate text
        if (processedTexts.has(st.text)) continue;
        processedTexts.add(st.text);
        
        // For transcriptions already with [Speaker] format
        if (st.text.includes('[') && st.text.includes(']')) {
          // Divide into segments to preserve multiple speakers within the same sentence
          const speakerPattern = /\[([^\]]+)\]\s*(.*?)(?=\s*\[[^\]]+\]|$)/gs;
          let match;
          let segmentFound = false;
          
          while ((match = speakerPattern.exec(st.text)) !== null) {
            segmentFound = true;
            if (match[1] && match[2]) {
              const rawSpeaker = match[1].trim();
              const spokenText = match[2].trim();
              
              // For each segment, determine if to show the speaker or not
              const speakerKey = /^speaker\s*\d+$/i.test(rawSpeaker) ? 
                `speaker_${rawSpeaker}` : rawSpeaker;
              
              // Show speaker if different from the previous one
              const showSpeaker = (lastSpeakerKey !== speakerKey);
              
              // Format the text appropriately
              if (showSpeaker && spokenText) {
                conversationPieces.push(`[${rawSpeaker}] ${spokenText}`);
              } else if (spokenText) {
                conversationPieces.push(spokenText);
              }
              
              lastSpeakerKey = speakerKey;
            }
          }
          
          // If no segments found, add the text as is
          if (!segmentFound && st.text.trim()) {
            conversationPieces.push(st.text.trim());
          }
        } else {
          // For transcriptions without explicit [Speaker] format
          // Determine the correct speaker
          let speakerLabel: string;
          let speakerKey: string;
          
          if (st.speaker === this.primaryUserSpeaker) {
            speakerLabel = this.primaryUserSpeaker;
            speakerKey = this.primaryUserSpeaker;
          } else {
            // For external speakers, verify if they have a specific number using a more precise regex
            const speakerMatch = st.text.match(/\bspeaker\s*(\d+)\b/i);
            if (speakerMatch && speakerMatch[1]) {
              speakerLabel = `Speaker ${speakerMatch[1]}`;
              speakerKey = `speaker_${speakerMatch[1]}`;
            } else {
              speakerLabel = EXTERNAL_SPEAKER_LABEL;
              speakerKey = "external";
            }
          }
          
          // Show speaker if different from the previous one
          const showSpeaker = (lastSpeakerKey !== speakerKey);
          
          // Format the text appropriately
          if (showSpeaker) {
            conversationPieces.push(`[${speakerLabel}] ${st.text}`);
          } else {
            conversationPieces.push(st.text);
          }
          
          lastSpeakerKey = speakerKey;
        }
      }
      
      return conversationPieces.join("\n");
    }
    
    // Alternative case: use the traditional list
    const validTranscriptions = transcriptionList
      .filter(text => text && text.trim().length > 0);
    
    if (validTranscriptions.length > 0) {
      // Remove duplicates while maintaining order
      const uniqueTranscriptions = [...new Set(validTranscriptions)];
      return uniqueTranscriptions.join(" ").trim();
    }
    
    // Last option: use the last known transcription
    if (lastTranscription && lastTranscription.trim()) {
      return lastTranscription.trim();
    }
    
    return "Please respond based only on the context provided.";
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// SymbolicCognitionTimelineLogger.ts
// Logging structure for symbolic timeline with precise timestamps

import { CognitionEvent } from '../../types/CognitionEvent';
import { SymbolicInsight } from '../../types/SymbolicInsight';
import { SymbolicQuery } from '../../types/SymbolicQuery';
import { SymbolicContext } from '../../types/SymbolicContext';
import { LoggingUtils } from '../../utils/LoggingUtils';
import { UserIntentWeights } from '../../symbolic-cortex/integration/ICollapseStrategyService';

export class SymbolicCognitionTimelineLogger {
  private timeline: CognitionEvent[] = [];

  private now(): string {
    return new Date().toISOString();
  }

  /**
   * Records a raw prompt in the timeline.
   * @param content Textual content of the prompt sent to the system.
   */
  logRawPrompt(content: string): void {
    this.timeline.push({ type: 'raw_prompt', timestamp: this.now(), content });
  }

  /**
   * Records a temporary context in the timeline.
   * @param context Temporary textual context used in processing.
   */
  logTemporaryContext(context: string): void {
    this.timeline.push({ type: 'temporary_context', timestamp: this.now(), context });
  }

  /**
   * Records a neural signal in the timeline, including symbolic query and parameters.
   * @param core Neural signal core (e.g., memory, emotion).
   * @param symbolic_query Symbolic query associated with the signal.
   * @param intensity Signal intensity.
   * @param topK TopK parameter used in search.
   * @param params Additional signal parameters.
   */
  logNeuralSignal(core: string, symbolic_query: SymbolicQuery, intensity: number, topK: number, params: Record<string, unknown>): void {
    this.timeline.push({
      type: 'neural_signal',
      timestamp: this.now(),
      core,
      symbolic_query,
      intensity,
      topK,
      params
    });
  }

  /**
   * Records the result of a symbolic retrieval in the timeline.
   * @param core Core of the associated signal.
   * @param insights Array of extracted symbolic insights.
   * @param matchCount Number of matches found.
   * @param durationMs Search duration in milliseconds.
   */
  logSymbolicRetrieval(core: string, insights: SymbolicInsight[], matchCount: number, durationMs: number): void {
    const safeInsights = Array.isArray(insights) ? insights : [];
    if (!insights || safeInsights.length === 0) {
      LoggingUtils.logInfo(`No insights extracted for core: ${core}`);
    }
    this.timeline.push({
      type: 'symbolic_retrieval',
      timestamp: this.now(),
      core,
      insights: safeInsights,
      matchCount,
      durationMs
    });
  }

  /**
   * Records the initiation of a symbolic fusion process in the timeline.
   */
  logFusionInitiated(): void {
    this.timeline.push({ type: 'fusion_initiated', timestamp: this.now() });
  }

  /**
   * Logs a neural collapse event in the timeline
   * @param isDeterministic Indicates if the collapse was deterministic or probabilistic
   * @param selectedCore Neural core selected for collapse
   * @param numCandidates Number of candidates available before collapse
   * @param emotionalWeight Emotional weight of the result
   * @param contradictionScore Contradiction value of the result
   * @param temperature Symbolic temperature used (only for non-deterministic collapses)
   * @param justification Textual justification for the collapse decision
   * @param userIntent User intent weights inferred from the original text
   * @param insights Symbolic insights associated with the collapse
   * @param emergentProperties Emergent properties detected in the neural response patterns
   */
  logNeuralCollapse(isDeterministic: boolean, selectedCore: string, numCandidates: number, emotionalWeight: number, contradictionScore: number, temperature?: number, justification?: string, userIntent?: UserIntentWeights, insights?: SymbolicInsight[], emergentProperties?: string[]): void {
    this.timeline.push({ 
      type: 'neural_collapse',
      timestamp: this.now(),
      isDeterministic,
      selectedCore,
      numCandidates,
      temperature,
      emotionalWeight,
      contradictionScore,
      justification,
      userIntent,
      insights,
      emergentProperties
    });
  }

  /**
   * Records the synthesized symbolic context in the timeline.
   * @param context Synthesized symbolic context object.
   */
  logSymbolicContextSynthesized(context: SymbolicContext): void {
    this.timeline.push({ type: 'symbolic_context_synthesized', timestamp: this.now(), context });
  }

  /**
   * Records the GPT model response in the timeline, including symbolic topics and insights.
   * @param data Response string or detailed object with topics and insights.
   */
  logGptResponse(data: string | { response: string; symbolicTopics?: string[]; insights?: SymbolicInsight[] }): void {
    if (typeof data === 'string') {
      LoggingUtils.logInfo('No insights extracted in GPT response (string).');
      this.timeline.push({ 
        type: 'gpt_response', 
        timestamp: this.now(), 
        response: data, 
        insights: []
      });
    } else {
      const hasInsights = data.insights && Array.isArray(data.insights) && data.insights.length > 0;
      if (!hasInsights) {
        LoggingUtils.logInfo('No insights extracted in GPT response (object).');
      }
      this.timeline.push({
        type: 'gpt_response',
        timestamp: this.now(),
        response: data.response,
        symbolicTopics: data.symbolicTopics,
        insights: hasInsights ? data.insights : []
      });
    }
  }

  /**
   * Returns the complete timeline of recorded cognitive events.
   */
  getTimeline(): CognitionEvent[] {
    return this.timeline;
  }

  /**
   * Logs detected emergent symbolic patterns.
   * This is part of the Orch-OS scientific introspection layer for tracking
   * emergent cognitive phenomena across processing cycles.
   * 
   * @param patterns Array of emergent symbolic pattern descriptions
   * @param metrics Scientific metrics associated with the patterns
   */
  logEmergentPatterns(patterns: string[], metrics?: { 
    archetypalStability?: number; 
    cycleEntropy?: number; 
    insightDepth?: number 
  }): void {
    this.timeline.push({
      type: 'emergent_patterns',
      timestamp: this.now(),
      patterns,
      metrics
    });
    
    // Log para debugging/monitoramento
    LoggingUtils.logInfo(`[Timeline] Logged ${patterns.length} emergent patterns`);
  }

  clear() {
    this.timeline = [];
  }
}

export default SymbolicCognitionTimelineLogger;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// SymbolicCognitionTimelineLoggerSingleton.ts
import SymbolicCognitionTimelineLogger from './SymbolicCognitionTimelineLogger';

const symbolicCognitionTimelineLogger = new SymbolicCognitionTimelineLogger();
export default symbolicCognitionTimelineLogger;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// UIUpdateService.ts
// Service responsible for updating the UI and sending notifications

import { UIUpdater } from "../../interfaces/transcription/TranscriptionTypes";
import { IUIUpdateService } from "../../interfaces/utils/IUIUpdateService";
import { LoggingUtils } from "../../utils/LoggingUtils";

export class UIUpdateService implements IUIUpdateService {
  private setTexts: UIUpdater;
  
  constructor(setTexts: UIUpdater) {
    this.setTexts = setTexts;
  }
  
  /**
   * Updates the UI with new values
   */
  updateUI(update: Record<string, any>): void {
    this.setTexts((prev: any) => ({ ...prev, ...update }));
  }
  
  /**
   * Notifies the start of prompt processing via IPC
   */
  notifyPromptProcessingStarted(temporaryContext?: string): void {
    if (typeof window !== 'undefined' && window.electronAPI) {
      try {
        // 1. Notify that we are sending the prompt (for UI to show loading)
        if (window.electronAPI.sendPromptUpdate) {
          window.electronAPI.sendPromptUpdate('partial', "Processing...");
        }
        
        // 2. Send command to main process via IPC
        if (window.electronAPI.sendNeuralPrompt) {
          window.electronAPI.sendNeuralPrompt(temporaryContext);
          LoggingUtils.logInfo("Prompt sent to main process");
        }
      } catch (e) {
        LoggingUtils.logError("Error sending notifications via IPC", e);
      }
    }
  }
  
  /**
   * Notifies the completion of the prompt via IPC
   */
  notifyPromptComplete(response: string): void {
    if (typeof window !== 'undefined' && window.electronAPI?.sendPromptUpdate) {
      try {
        window.electronAPI.sendPromptUpdate('complete', response);
        LoggingUtils.logInfo("Final response sent via IPC");
      } catch (e) {
        LoggingUtils.logError("Error sending final response via IPC", e);
      }
    }
  }
  
  /**
   * Notifies an error in prompt processing via IPC
   */
  notifyPromptError(errorMessage: string): void {
    if (typeof window !== 'undefined' && window.electronAPI?.sendPromptUpdate) {
      try {
        window.electronAPI.sendPromptUpdate('error', errorMessage);
      } catch (e) {
        LoggingUtils.logError("Error sending error notification via IPC", e);
      }
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
import { getOption, STORAGE_KEYS } from "./../../../../services/StorageService";
// Copyright (c) 2025 Guilherme Ferrari Brescia

// DeepgramTranscriptionService.ts
// Main transcription service for Deepgram that orchestrates other services

import { getPrimaryUser } from "../../../../config/UserConfig";
import { ModeService, OrchOSModeEnum } from "../../../../services/ModeService";
import { IDeepgramTranscriptionService } from "../interfaces/deepgram/IDeepgramService";
import { IMemoryService } from "../interfaces/memory/IMemoryService";
import { IOpenAIService } from "../interfaces/openai/IOpenAIService";
import { ITranscriptionStorageService } from "../interfaces/transcription/ITranscriptionStorageService";
import {
  SpeakerTranscription,
  SpeakerTranscriptionLog,
  UIUpdater,
} from "../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../interfaces/utils/ISpeakerIdentificationService";
import { IUIUpdateService } from "../interfaces/utils/IUIUpdateService";
import { DefaultNeuralIntegrationService } from "../symbolic-cortex/integration/DefaultNeuralIntegrationService";
import { INeuralIntegrationService } from "../symbolic-cortex/integration/INeuralIntegrationService";
import { LoggingUtils } from "../utils/LoggingUtils";
import { MemoryService } from "./memory/MemoryService";
import { TranscriptionPromptProcessor } from "./transcription/TranscriptionPromptProcessor";
import { TranscriptionStorageService } from "./transcription/TranscriptionStorageService";
import { SpeakerIdentificationService } from "./utils/SpeakerIdentificationService";
import { UIUpdateService } from "./utils/UIUpdateService";

export class DeepgramTranscriptionService
  implements IDeepgramTranscriptionService
{
  // Essential services
  private speakerService: ISpeakerIdentificationService;
  private storageService: ITranscriptionStorageService;
  private memoryService: IMemoryService;
  // Generic LLM service (could be Ollama or HuggingFace facade)
  private llmService: IOpenAIService;
  private uiService: IUIUpdateService;

  // Configuration
  private model: string =
    getOption(STORAGE_KEYS.DEEPGRAM_MODEL) || "nova-2-general";
  private interimResultsEnabled: boolean = true;
  private useSimplifiedHistory: boolean = false;
  private currentLanguage: string =
    getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "pt-BR";

  // Properties for the neural system (kept for backward compatibility)
  private _neuralMemory: Array<{
    timestamp: number;
    core: string;
    intensity: number;
    pattern?: string;
  }> = [];

  /**
   * Returns the current state of prompt processing
   * @returns true if a prompt is currently being processed, false otherwise
   */
  public isProcessingPromptRequest(): boolean {
    return this.transcriptionPromptProcessor.isProcessingPromptRequest();
  }

  // Neural integration service
  private neuralIntegrationService: INeuralIntegrationService;

  // Transcription prompt processor
  private transcriptionPromptProcessor: TranscriptionPromptProcessor;

  constructor(
    setTexts: UIUpdater,
    llmService: IOpenAIService,
    primaryUserSpeaker: string = getPrimaryUser()
  ) {
    // Initialize services
    this.speakerService = new SpeakerIdentificationService(primaryUserSpeaker);
    this.storageService = new TranscriptionStorageService(
      this.speakerService,
      setTexts
    );
    this.llmService = llmService; // May be HuggingFaceServiceFacade in Basic mode
    this.memoryService = new MemoryService(this.llmService);
    this.uiService = new UIUpdateService(setTexts);

    // Initialize the neural integration service
    this.neuralIntegrationService = new DefaultNeuralIntegrationService(
      this.llmService
    );

    // Initialize the transcription prompt processor
    this.transcriptionPromptProcessor = new TranscriptionPromptProcessor(
      this.storageService,
      this.memoryService,
      this.llmService,
      this.uiService,
      this.speakerService,
      this.neuralIntegrationService
    );

    // Set reference back to this service in the storage service to enable auto-triggering
    if (this.storageService instanceof TranscriptionStorageService) {
      this.storageService.setTranscriptionService(this);
    }

    // Load API key
    this.loadApiKey();
  }

  // Main interface methods

  /**
   * Sets the name of the primary speaker (user)
   */
  setPrimaryUserSpeaker(name: string): void {
    this.speakerService.setPrimaryUserSpeaker(name);
  }

  /**
   * Adds a new transcription received from the Deepgram service
   */
  addTranscription(text: string, speaker?: string): void {
    this.storageService.addTranscription(text, speaker);
  }

  /**
   * Clears transcription data
   */
  clearTranscriptionData(): void {
    this.storageService.clearTranscriptionData();
    this.memoryService.clearMemoryData();
    this.memoryService.resetTranscriptionSnapshot();
  }

  /**
   * Returns the current list of transcriptions
   */
  getTranscriptionList(): string[] {
    return this.storageService.getTranscriptionList();
  }

  /**
   * Returns transcriptions organized by speaker
   */
  getSpeakerTranscriptions(): SpeakerTranscription[] {
    return this.storageService.getSpeakerTranscriptions();
  }

  /**
   * Returns transcription logs grouped by speaker
   */
  getTranscriptionLogs(): SpeakerTranscriptionLog[] {
    return this.storageService.getTranscriptionLogs();
  }

  /**
   * Accesses the internal storage service directly
   * @returns The instance of the storage service
   * @internal Only for internal use
   */
  getStorageServiceForIntegration(): ITranscriptionStorageService {
    return this.storageService;
  }

  /**
   * Verifies if only the primary user speaker is speaking
   */
  isOnlyUserSpeaking(): boolean {
    return this.speakerService.isOnlyUserSpeaking(
      this.storageService.getSpeakerTranscriptions()
    );
  }

  /**
   * Activates or deactivates the simplified history mode
   */
  setSimplifiedHistoryMode(enabled: boolean): void {
    this.useSimplifiedHistory = enabled;
    this.memoryService.setSimplifiedHistoryMode(enabled);
    LoggingUtils.logInfo(
      `Simplified history mode: ${enabled ? "activated" : "deactivated"}`
    );
  }

  /**
   * Sets the processing language for transcription and neural processing
   */
  setProcessingLanguage(language: string): void {
    this.currentLanguage = language;
    this.transcriptionPromptProcessor.setLanguage(language);
    LoggingUtils.logInfo(`Processing language updated to: ${language}`);
  }

  /**
   * Gets the current processing language
   */
  getProcessingLanguage(): string {
    return this.transcriptionPromptProcessor.getCurrentLanguage();
  }

  /**
   * Processes the transcription using the appropriate AI service based on current mode
   * Automatically selects between Ollama (advanced mode) and HuggingFace (basic mode)
   */
  async sendTranscriptionPrompt(temporaryContext?: string): Promise<void> {
    console.log("🚀 [DEEPGRAM_SERVICE] sendTranscriptionPrompt called:", {
      temporaryContext,
      hasContext: !!temporaryContext,
      currentMode: ModeService.getMode(),
      timestamp: new Date().toISOString(),
    });

    const currentMode = ModeService.getMode();

    try {
      if (currentMode === OrchOSModeEnum.BASIC) {
        LoggingUtils.logInfo("🤖 Using HuggingFace service (Basic mode)");
        console.log("📤 [DEEPGRAM_SERVICE] Calling processWithHuggingFace");
        await this.transcriptionPromptProcessor.processWithHuggingFace(
          temporaryContext
        );
        console.log("✅ [DEEPGRAM_SERVICE] processWithHuggingFace completed");
      } else {
        LoggingUtils.logInfo("🧠 Using Ollama service (Advanced mode)");
        console.log("📤 [DEEPGRAM_SERVICE] Calling processWithOpenAI");
        await this.transcriptionPromptProcessor.processWithOpenAI(
          temporaryContext
        );
        console.log("✅ [DEEPGRAM_SERVICE] processWithOpenAI completed");
      }
    } catch (error) {
      console.error(
        "❌ [DEEPGRAM_SERVICE] Error in sendTranscriptionPrompt:",
        error
      );
      throw error;
    }
  }

  /**
   * Processes a direct message from chat interface
   * @param message The message from chat input
   * @param temporaryContext Optional additional context
   */
  async sendDirectMessage(
    message: string,
    temporaryContext?: string
  ): Promise<void> {
    console.log("💬 [DEEPGRAM_SERVICE] sendDirectMessage called:", {
      message: message.substring(0, 50),
      hasContext: !!temporaryContext,
      timestamp: new Date().toISOString(),
    });

    try {
      await this.transcriptionPromptProcessor.processDirectMessage(
        message,
        temporaryContext
      );
      console.log("✅ [DEEPGRAM_SERVICE] sendDirectMessage completed");
    } catch (error) {
      console.error("❌ [DEEPGRAM_SERVICE] Error in sendDirectMessage:", error);
      throw error;
    }
  }

  /**
   * Processes the transcription using HuggingFace backend specifically
   * Always uses HuggingFace regardless of the current mode
   */
  async sendTranscriptionPromptWithHuggingFace(
    temporaryContext?: string
  ): Promise<void> {
    LoggingUtils.logInfo("🤖 Explicitly using HuggingFace service");
    return await this.transcriptionPromptProcessor.processWithHuggingFace(
      temporaryContext
    );
  }

  /**
   * Loads the LLM service API key from the environment
   */
  private async loadApiKey(): Promise<void> {
    await this.llmService.loadApiKey();
  }

  // Implementation of IDeepgramTranscriptionService methods

  async connect(language?: string): Promise<void> {
    if (language) {
      this.currentLanguage = language;
      this.transcriptionPromptProcessor.setLanguage(language);
    }
    LoggingUtils.logInfo(
      `Connecting transcription service. Language: ${this.currentLanguage}`
    );
    return Promise.resolve();
  }

  async disconnect(): Promise<void> {
    LoggingUtils.logInfo("Disconnecting transcription service");
    return Promise.resolve();
  }

  async startProcessing(): Promise<void> {
    LoggingUtils.logInfo("Starting transcription processing");
    return Promise.resolve();
  }

  async stopProcessing(): Promise<void> {
    LoggingUtils.logInfo("Stopping transcription processing");
    return Promise.resolve();
  }

  setModel(model: string): void {
    if (this.model !== model) {
      this.model = model;
      LoggingUtils.logInfo(`Model defined for: ${model}`);
    }
  }

  toggleInterimResults(enabled: boolean): void {
    this.interimResultsEnabled = enabled;
    LoggingUtils.logInfo(
      `Interim results: ${enabled ? "enabled" : "disabled"}`
    );
  }

  // ... (rest of the code remains the same)
  reset(): void {
    LoggingUtils.logInfo("Resetting transcription state");
    this.clearTranscriptionData();
    this.transcriptionPromptProcessor.reset();
  }

  isConnected(): boolean {
    return false;
  }

  /**
   * Enable or disable automatic detection of questions for auto-triggering prompts
   */
  setAutoQuestionDetection(enabled: boolean): void {
    if (this.storageService instanceof TranscriptionStorageService) {
      this.storageService.setAutoQuestionDetection(enabled);
      LoggingUtils.logInfo(
        `Auto-question detection ${enabled ? "enabled" : "disabled"}`
      );
    }
  }

  /**
   * Flush all accumulated transcriptions to the UI
   * Should be called when recording stops or when sending a message
   */
  flushTranscriptionsToUI(): void {
    this.storageService.flushTranscriptionsToUI();
    LoggingUtils.logInfo("Flushing transcriptions to UI");
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// NeuralSignalExtractor.ts
// Module responsible for extracting symbolic neural signals from the transcription context

import { NeuralSignalResponse } from "../../interfaces/neural/NeuralSignalTypes";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { LoggingUtils } from "../../utils/LoggingUtils";
import {
  INeuralSignalExtractor,
  NeuralExtractionConfig,
} from "../interfaces/INeuralSignalExtractor";

/**
 * Class responsible for extracting symbolic neural signals from the transcription context
 * This is the first impulse of an artificial symbolic mind
 * Automatically selects between Ollama (advanced mode) and HuggingFace (basic mode) services
 */
export class NeuralSignalExtractor implements INeuralSignalExtractor {
  private readonly aiService: IOpenAIService;

  /**
   * Constructor
   * @param aiService AI service for communication with the language model (OpenAI or HuggingFace facade)
   */
  constructor(aiService: IOpenAIService) {
    this.aiService = aiService;
  }

  /**
   * Extracts symbolic neural signals from the current context
   * This is the first impulse of an artificial symbolic mind
   * @param config Configuration containing the current context
   * @returns Array of neural signals representing the activation of the symbolic brain
   */
  public async extractNeuralSignals(
    config: NeuralExtractionConfig
  ): Promise<NeuralSignalResponse> {
    try {
      // The config should already have transcription and userContextData ready!
      const {
        transcription,
        temporaryContext,
        userContextData = {},
        sessionState = {},
      } = config;

      // Determine language from session state
      const language =
        sessionState &&
        typeof sessionState === "object" &&
        "language" in sessionState
          ? (sessionState.language as string)
          : undefined;

      LoggingUtils.logInfo(
        "🧠 [NeuralSignalExtractor] Starting neural signal extraction..."
      );
      LoggingUtils.logInfo(
        `🧠 [NeuralSignalExtractor] Config: ${JSON.stringify({
          transcriptionLength: transcription.length,
          transcriptionPreview: transcription.substring(0, 100) + "...",
          hasTemporaryContext: !!temporaryContext,
          temporaryContextLength: temporaryContext?.length || 0,
          userContextDataKeys: Object.keys(userContextData),
          language,
          aiServiceType: this.aiService.constructor.name,
          aiServiceInitialized: this.aiService.isInitialized?.() ?? "unknown",
        })}`
      );

      // Prepare an enriched prompt with all available contextual data
      const enrichedPrompt = this.prepareEnrichedPrompt(
        transcription,
        userContextData
      );

      LoggingUtils.logInfo(
        `🧠 [NeuralSignalExtractor] Enriched prompt prepared: ${JSON.stringify({
          enrichedPromptLength: enrichedPrompt.length,
          enrichedPromptPreview: enrichedPrompt.substring(0, 200) + "...",
        })}`
      );

      // Verify AI service availability before proceeding
      if (this.aiService.ensureOpenAIClient) {
        const isReady = await this.aiService.ensureOpenAIClient();
        if (!isReady) {
          LoggingUtils.logError(
            "🧠 [NeuralSignalExtractor] AI service is not ready. Using fallback signals."
          );
          return this.generateFallbackSignals(transcription);
        }
      }

      LoggingUtils.logInfo(
        "🧠 [NeuralSignalExtractor] Calling generateNeuralSignal..."
      );

      // Generate neural signals adapted for memory queries (works with both Pinecone and DuckDB)
      const neuralResponse = await this.aiService.generateNeuralSignal(
        enrichedPrompt, // Enriched stimulus with user context
        temporaryContext,
        language // Pass language from session state
      );

      LoggingUtils.logInfo(
        `🧠 [NeuralSignalExtractor] Neural response received: ${JSON.stringify({
          hasSignals: !!neuralResponse.signals,
          signalsLength: neuralResponse.signals?.length || 0,
          signalsPreview:
            neuralResponse.signals?.map((s) => ({
              core: s?.core,
              intensity: s?.intensity,
              hasQuery: !!s?.symbolic_query?.query,
            })) || [],
        })}`
      );

      // Verify if the response contains valid signals
      if (!neuralResponse.signals || neuralResponse.signals.length === 0) {
        LoggingUtils.logWarning(
          "🧠 [NeuralSignalExtractor] No neural signals were generated. Using default signals."
        );

        return this.generateFallbackSignals(transcription);
      }

      // DEBUG: Log raw signals before validation
      LoggingUtils.logInfo(
        `🧠 [NeuralSignalExtractor] Raw signals before validation: ${JSON.stringify(
          neuralResponse.signals.map((s) => ({
            core: s?.core,
            intensity: s?.intensity,
            hasQuery: !!s?.symbolic_query?.query,
            hasSymbolicInsights: !!s?.symbolicInsights,
            queryLength: s?.symbolic_query?.query?.length || 0,
          }))
        )}`
      );

      // ENHANCED VALIDATION: More flexible validation for debugging
      const validSignals = neuralResponse.signals.filter((signal) => {
        const isValid =
          signal &&
          signal.core &&
          typeof signal.intensity === "number" &&
          signal.intensity >= 0 &&
          signal.intensity <= 1;

        if (!isValid) {
          LoggingUtils.logWarning(
            `🧠 [NeuralSignalExtractor] Invalid signal filtered out: ${JSON.stringify(
              signal
            )}`
          );
        }

        return isValid;
      });

      if (validSignals.length === 0) {
        LoggingUtils.logWarning(
          "🧠 [NeuralSignalExtractor] All signals were invalid after validation. Using fallback signals."
        );
        return this.generateFallbackSignals(transcription);
      }

      LoggingUtils.logInfo(
        `✅ [NeuralSignalExtractor] Successfully validated ${validSignals.length} neural signals`
      );

      return { signals: validSignals };
    } catch (error) {
      // In case of error, log and provide a fallback response
      LoggingUtils.logError(
        "🧠 [NeuralSignalExtractor] Error extracting neural signals",
        error as Error
      );

      return this.generateFallbackSignals(config.transcription);
    }
  }

  /**
   * Generates fallback neural signals when the main extraction fails
   * @param transcription The original transcription text
   * @returns Fallback neural signal response
   */
  private generateFallbackSignals(transcription: string): NeuralSignalResponse {
    LoggingUtils.logInfo(
      "🧠 [NeuralSignalExtractor] Generating fallback neural signals..."
    );

    return {
      signals: [
        {
          core: "memory",
          intensity: 0.8,
          symbolic_query: {
            query: `memories related to: ${transcription.substring(0, 100)}`,
          },
          symbolicInsights: {
            recall_type: "semantic",
            temporal: "recent",
            importance: "high",
          },
        },
        {
          core: "metacognitive",
          intensity: 0.7,
          symbolic_query: {
            query: `reflection on: ${transcription.substring(0, 100)}`,
          },
          symbolicInsights: {
            thought: "Processing cognitive stimulus",
            state: "conscious",
          },
        },
        {
          core: "valence",
          intensity: 0.6,
          symbolic_query: {
            query: `emotions about: ${transcription.substring(0, 100)}`,
          },
          symbolicInsights: {
            emotion: "neutral",
            intensity: "moderate",
          },
        },
      ],
    };
  }

  /**
   * Prepares an enriched prompt with full user context.
   * @param originalPrompt The user's original prompt
   * @param userContextData Contextual data related to the user
   * @returns A contextually enriched symbolic/psychoanalytic prompt
   */
  private prepareEnrichedPrompt(
    originalPrompt: string,
    userContextData: Record<string, unknown>
  ): string {
    const styleInstruction =
      "STYLE INSTRUCTION: Only use greetings and personal references when the user's content clearly justifies it — never automatically.";

    // Base symbolic instruction with enhanced quantum and multi-level consciousness dimensions
    const symbolicInstruction = `INSTRUCTION: Analyze the user's message and context in a quantum-symbolic framework to identify:

1. EXPLICIT AND IMPLICIT ELEMENTS:
   - Keywords, emotional themes, symbols, archetypes, dilemmas, and unconscious patterns
   - Potential quantum states of meaning in superposition (multiple interpretations coexisting)
   - Signs of instructional collapse (where multiple potential meanings converge)

2. MULTI-LEVEL CONSCIOUSNESS:
   - Surface level: Immediate conscious content and stated intentions
   - Intermediate level: Partially conscious patterns, emotional undercurrents
   - Deep level: Unconscious material, potential symbolic resonance, dormant insights

3. ARCHETYPAL RESONANCE AND INTERPLAY:
   - Primary archetypes activated in the communication
   - Secondary/shadow archetypes operating in tension or harmony with primary ones
   - Potential dialogue or conflict between different archetypal energies

4. TEMPORAL DIMENSIONS:
   - Past: Echoes, patterns, and unresolved elements influencing present communication
   - Present: Immediate symbolic significance of current expression
   - Future: Potential trajectories, symbolic seeds, emergent possibilities

5. POLARITIES AND PARADOXES:
   - Tensions between opposing symbolic forces
   - Potential integration points for apparently contradictory elements
   - Productive tensions that could lead to emergent understanding

Suggest refined or expanded keywords, queries, and topics that could deepen the symbolic, emotional, and unconscious investigation — even if they are not explicitly verbalized.

Be selective: only expand when there are strong indicators of symbolic or unconscious material.

Prioritize expressions and themes that reveal tensions, paradoxes, hidden desires, blockages, or deep self-knowledge potential that exist in quantum superposition awaiting conscious observation.`;

    // If no user context exists, return the basic symbolic enrichment prompt
    if (Object.keys(userContextData).length === 0) {
      return `${styleInstruction}\n\n${originalPrompt}\n\n${symbolicInstruction}`;
    }

    // Start building contextualized prompt
    let contextualPrompt = `${styleInstruction}\n\n${originalPrompt}`;

    // Add recent symbolic or emotional topics, if present
    if (userContextData.recent_topics) {
      const recentTopics =
        userContextData.recent_topics.toString().substring(0, 200) + "...";
      contextualPrompt += `\n\nRecent topics context: ${recentTopics}`;
    }

    // Add interaction patterns if present
    if (userContextData.speaker_interaction_counts) {
      const interactionPattern = JSON.stringify(
        userContextData.speaker_interaction_counts
      );
      contextualPrompt += `\n\nInteraction pattern: ${interactionPattern}`;
    }

    // Add symbolic instruction + adaptive note
    contextualPrompt += `\n\n${symbolicInstruction}

Note: If other relevant symbolic or emotional patterns are available in the long-term memory or historical user data, feel free to incorporate them into the keyword/query suggestions — as long as they resonate symbolically with the current stimulus.`;

    return contextualPrompt;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  ModeService,
  OrchOSModeEnum,
} from "../../../../../services/ModeService";
import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { HuggingFaceEmbeddingService } from "../../../../../services/huggingface/HuggingFaceEmbeddingService";
import { IEmbeddingService } from "../../interfaces/openai/IEmbeddingService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { HuggingFaceServiceFacade } from "../../services/huggingface/HuggingFaceServiceFacade";
import { OllamaEmbeddingService } from "../../services/ollama/OllamaEmbeddingService";
import { OllamaClientService } from "../../services/ollama/neural/OllamaClientService";
import { OllamaCompletionService } from "../../services/ollama/neural/OllamaCompletionService";
import symbolicCognitionTimelineLogger from "../../services/utils/SymbolicCognitionTimelineLoggerSingleton";
import { SymbolicInsight } from "../../types/SymbolicInsight";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { cleanThinkTags } from "../../utils/ThinkTagCleaner";
import {
  CognitiveMetrics,
  SymbolicPatternAnalyzer,
} from "../patterns/SymbolicPatternAnalyzer";
import { HuggingFaceCollapseStrategyService } from "./HuggingFaceCollapseStrategyService";
import { ICollapseStrategyService } from "./ICollapseStrategyService";
import { INeuralIntegrationService } from "./INeuralIntegrationService";
import { OllamaCollapseStrategyService } from "./OllamaCollapseStrategyService";
import { SuperpositionLayer } from "./SuperpositionLayer";

function asNumber(val: unknown, fallback: number): number {
  return typeof val === "number" ? val : fallback;
}

export class DefaultNeuralIntegrationService
  implements INeuralIntegrationService
{
  private embeddingService: IEmbeddingService;
  private collapseStrategyService: ICollapseStrategyService;
  private patternAnalyzer: SymbolicPatternAnalyzer; // Detector de padrões simbólicos emergentes entre ciclos
  private aiService: IOpenAIService;

  constructor(
    aiService: IOpenAIService,
    private huggingFaceService?: HuggingFaceServiceFacade
  ) {
    this.aiService = aiService;

    // Strategy pattern: choose embedding service based on mode
    this.embeddingService = this.createEmbeddingService(aiService);

    // Strategy pattern: choose collapse strategy service based on mode
    const currentMode = ModeService.getMode();
    if (currentMode === OrchOSModeEnum.BASIC && this.huggingFaceService) {
      LoggingUtils.logInfo(
        "[NeuralIntegration] Using HuggingFace collapse strategy (Basic mode)"
      );
      this.collapseStrategyService = new HuggingFaceCollapseStrategyService(
        this.aiService
      );
    } else {
      LoggingUtils.logInfo(
        "[NeuralIntegration] Using Ollama collapse strategy (Advanced mode)"
      );

      // Create dedicated Ollama services for the collapse strategy
      const ollamaClientService = new OllamaClientService();
      const ollamaCompletionService = new OllamaCompletionService(
        ollamaClientService
      );

      this.collapseStrategyService = new OllamaCollapseStrategyService(
        ollamaCompletionService
      );
    }

    this.patternAnalyzer = new SymbolicPatternAnalyzer();
  }

  /**
   * Creates the appropriate embedding service based on application mode
   * @param aiService The AI service to use for OpenAI embeddings
   * @returns The appropriate embedding service
   */
  private createEmbeddingService(aiService: IOpenAIService): IEmbeddingService {
    const currentMode = ModeService.getMode();

    if (currentMode === OrchOSModeEnum.BASIC) {
      // In basic mode, use HuggingFace with the selected model
      const hfModel = getOption(STORAGE_KEYS.HF_EMBEDDING_MODEL);
      LoggingUtils.logInfo(
        `[NeuralIntegration] Creating HuggingFaceEmbeddingService with model: ${
          hfModel || "default"
        } for Basic mode`
      );
      return new HuggingFaceEmbeddingService();
    } else {
      // In advanced mode, use Ollama with the selected model
      const ollamaModel = getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL);
      LoggingUtils.logInfo(
        `[NeuralIntegration] Creating OllamaEmbeddingService with model: ${
          ollamaModel || "default"
        } for Advanced mode`
      );
      return new OllamaEmbeddingService(aiService, { model: ollamaModel });
    }
  }

  /**
   * Calculates a symbolic phase value based on emotional weight, contradiction score, and coherence
   * Returns a value between 0-2π that represents a phase angle for wave interference
   */
  private calculateSymbolicPhase(
    emotionalWeight: number,
    contradictionScore: number,
    coherence: number
  ): number {
    const baseEmotionPhase = (emotionalWeight * Math.PI) / 2; // Emotions dominate initial phase
    const contradictionPhase = contradictionScore * Math.PI; // Contradictions generate opposition
    const coherenceNoise = (1 - coherence) * (Math.PI / 4); // Low coherence → noise

    // Total phase with 2π wrapping
    const phase =
      (baseEmotionPhase + contradictionPhase + coherenceNoise) % (2 * Math.PI);

    // Ensure phase is positive (0 to 2π range)
    const normalizedPhase = phase < 0 ? phase + 2 * Math.PI : phase;

    console.info(
      `[NeuralIntegration] Calculated symbolic phase: ${normalizedPhase.toFixed(
        3
      )} rad (emotionPhase=${baseEmotionPhase.toFixed(
        2
      )}, contradictionPhase=${contradictionPhase.toFixed(
        2
      )}, coherenceNoise=${coherenceNoise.toFixed(2)})`
    );

    return normalizedPhase;
  }

  /**
   * Neural integration using superposition, non-deterministic collapse and emergent property registration.
   * Now uses real embeddings for each answer via OpenAIEmbeddingService.
   */
  async integrate(
    neuralResults: Array<{
      core: string;
      intensity: number;
      output: string;
      insights: Record<string, unknown>;
    }>,
    originalInput: string,
    language: string = getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "pt-BR" // Default to pt-BR if not provided
  ): Promise<string> {
    if (!neuralResults || neuralResults.length === 0) {
      return originalInput;
    }

    // Clean think tags from neural results outputs before processing
    const cleanedNeuralResults = neuralResults.map((result) => ({
      ...result,
      output: cleanThinkTags(result.output),
    }));

    // 1. Superposition: each result is a possible answer
    const superposition = new SuperpositionLayer();
    for (const result of cleanedNeuralResults) {
      // Generate real embedding for the answer text
      const embedding = await this.embeddingService.createEmbedding(
        result.output
      );
      // Heuristics for emotional weight, coherence and contradiction
      const emotionalWeight = asNumber(
        (result.insights as Record<string, unknown>)?.valence,
        Math.random()
      );
      const narrativeCoherence = asNumber(
        (result.insights as Record<string, unknown>)?.coherence,
        1 - Math.abs(result.intensity - 0.5)
      );
      const contradictionScore = asNumber(
        (result.insights as Record<string, unknown>)?.contradiction,
        Math.random() * 0.5
      );
      superposition.register({
        embedding,
        text: result.output,
        emotionalWeight,
        narrativeCoherence,
        contradictionScore,
        origin: result.core,
        insights: result.insights,
      });
    }
    // 2. Collapse: Use OpenAI-based strategy to decide deterministic vs probabilistic
    const numCandidates = superposition.answers.length;

    // Calculate average values for symbolic properties
    const averageEmotionalWeight =
      cleanedNeuralResults.reduce((sum, r) => {
        return (
          sum + asNumber((r.insights as Record<string, unknown>)?.valence, 0.5)
        );
      }, 0) / cleanedNeuralResults.length;
    const averageContradictionScore =
      cleanedNeuralResults.reduce((sum, r) => {
        return (
          sum +
          asNumber((r.insights as Record<string, unknown>)?.contradiction, 0.25)
        );
      }, 0) / cleanedNeuralResults.length;
    const avgCoherence =
      cleanedNeuralResults.reduce((sum, r) => {
        return (
          sum +
          asNumber((r.insights as Record<string, unknown>)?.coherence, 0.7)
        );
      }, 0) / cleanedNeuralResults.length;

    // We'll use the original input text for the collapse strategy service to infer intent

    // Use our OpenAI-powered strategy service to make the decision
    const strategyDecision =
      await this.collapseStrategyService.decideCollapseStrategy({
        activatedCores: cleanedNeuralResults.map((r) => r.core),
        averageEmotionalWeight,
        averageContradictionScore,
        originalText: originalInput, // Pass the original text to help infer intent
      });

    // Log collapse details for debugging
    console.info(
      `[NeuralIntegration] Collapse strategy decision: ${
        strategyDecision.deterministic ? "Deterministic" : "Probabilistic"
      }, Temperature: ${strategyDecision.temperature}, Reason: ${
        strategyDecision.justification
      }`
    );

    // Log user intent if available
    if (strategyDecision.userIntent) {
      console.info(
        `[NeuralIntegration] Inferred user intent:`,
        JSON.stringify(strategyDecision.userIntent, null, 2)
      );
    }

    // Collect insights from all neural results
    const allInsights = cleanedNeuralResults.flatMap((result) => {
      if (!result.insights) return [];

      const toInsight = (type: string, content: string): SymbolicInsight =>
        ({
          type,
          content: content,
          core: result.core,
        } as SymbolicInsight);

      // For arrays of insights
      if (Array.isArray(result.insights)) {
        return result.insights
          .map((item) => {
            // String insights
            if (typeof item === "string") {
              return toInsight("concept", item);
            }
            // Object insights
            if (item && typeof item === "object") {
              const obj = item as Record<string, unknown>;
              const type = typeof obj.type === "string" ? obj.type : "unknown";
              let content = "";

              if (typeof obj.content === "string") content = obj.content;
              else if (typeof obj.value === "string") content = obj.value;
              else content = String(type);

              return toInsight(type, content);
            }
            return null;
          })
          .filter(Boolean) as SymbolicInsight[];
      }

      // For unique objects
      if (result.insights && typeof result.insights === "object") {
        const obj = result.insights as Record<string, unknown>;

        // With defined type
        if ("type" in obj && typeof obj.type === "string") {
          let content = "";
          if (typeof obj.content === "string") content = obj.content;
          else if (typeof obj.value === "string") content = obj.value;
          else content = obj.type;

          return [toInsight(obj.type, content)];
        }

        // Without defined type - each property becomes an insight
        return Object.entries(obj)
          .filter(([, v]) => v !== null && v !== undefined)
          .map(([k, v]) => toInsight(k, String(v)));
      }

      return [];
    });

    // Execute the collapse based on the strategy decision
    let finalAnswer;

    // Create a default userIntent if none was provided by the collapse strategy
    const defaultUserIntent = {
      social: originalInput.toLowerCase().includes("olá") ? 0.7 : 0.3,
      trivial: originalInput.toLowerCase().includes("tudo bem") ? 0.5 : 0.2,
      reflective: 0.3,
      practical: 0.2,
    };

    // Use the inferred intent or the default one
    const effectiveUserIntent =
      strategyDecision.userIntent || defaultUserIntent;

    // Log the intent that will be used
    console.info(
      `[NeuralIntegration] Using user intent:`,
      JSON.stringify(effectiveUserIntent, null, 2)
    );

    // Calculate average similarity to use for dynamic minCosineDistance
    const avgSimilarity = superposition.calculateAverageCosineSimilarity();

    // Compute dynamic minCosineDistance based on observed similarity
    // Higher similarity -> Higher minCosineDistance to enforce more diversity
    // Lower similarity -> Lower minCosineDistance to avoid over-penalization
    const dynamicMinDistance = Math.min(
      0.2,
      Math.max(0.1, 0.1 + avgSimilarity * 0.1)
    );

    // Log diversity metrics
    console.info(
      `[NeuralIntegration] Average semantic similarity: ${avgSimilarity.toFixed(
        3
      )}`
    );
    console.info(
      `[NeuralIntegration] Using dynamic minCosineDistance: ${dynamicMinDistance.toFixed(
        3
      )}`
    );

    // Calculate symbolic phase based on average emotional weight, contradiction and coherence
    const explicitPhase = this.calculateSymbolicPhase(
      averageEmotionalWeight,
      averageContradictionScore,
      avgCoherence
    );

    // Log the symbolic phase calculation
    console.info(
      `[NeuralIntegration] Using symbolic phase ${explicitPhase.toFixed(
        3
      )} rad (${(explicitPhase / (2 * Math.PI)).toFixed(
        3
      )} cycles) for collapse`
    );

    if (strategyDecision.deterministic) {
      // Execute deterministic collapse with phase interference and explicit phase
      finalAnswer = superposition.collapseDeterministic({
        diversifyByEmbedding: true,
        minCosineDistance: dynamicMinDistance,
        usePhaseInterference: true, // Enable quantum-like phase interference
        explicitPhase: explicitPhase, // Use explicit phase value to bias collapse
      });

      // Log the neural collapse event
      symbolicCognitionTimelineLogger.logNeuralCollapse(
        true, // isDeterministic
        finalAnswer.origin || "unknown", // selectedCore (ensure it's a string)
        numCandidates, // numCandidates
        averageEmotionalWeight, // Emotional weight
        averageContradictionScore, // Contradiction score
        undefined, // No temperature for deterministic collapse
        strategyDecision.justification,
        effectiveUserIntent, // userIntent (guaranteed to have a value)
        allInsights.length > 0 ? allInsights : undefined, // insights from neural results
        strategyDecision.emergentProperties
      );
    } else {
      // Execute probabilistic collapse with the suggested temperature and dynamic parameters
      // Include explicit phase to bias the probabilistic collapse as well
      finalAnswer = superposition.collapse(strategyDecision.temperature, {
        diversifyByEmbedding: true,
        minCosineDistance: dynamicMinDistance,
        explicitPhase: explicitPhase, // Use same explicit phase value for probabilistic collapse
      });

      // Log the neural collapse event
      symbolicCognitionTimelineLogger.logNeuralCollapse(
        false, // isDeterministic
        finalAnswer.origin || "unknown", // selectedCore (ensure it's a string)
        superposition.answers.length, // numCandidates
        finalAnswer.emotionalWeight || 0, // Emotional weight
        finalAnswer.contradictionScore || 0, // Contradiction score
        strategyDecision.temperature, // temperature from strategy
        strategyDecision.justification,
        effectiveUserIntent, // userIntent (guaranteed to have a value)
        allInsights.length > 0 ? allInsights : undefined, // insights from neural results
        strategyDecision.emergentProperties // emergent properties from strategy decision
      );
    }

    // 3. Use emergent properties from the OpenAI function call
    const emergentProperties: string[] =
      strategyDecision.emergentProperties || [];

    // === Orch-OS: Symbolic Pattern Analysis & Memory Integration ===
    // Atualizar o analisador de padrões com o contexto/métricas do ciclo atual
    // Capturar métricas cognitivas completas para análise científica
    const cycleMetrics: CognitiveMetrics = {
      // Métricas fundamentais para detecção de padrões
      contradictionScore:
        finalAnswer.contradictionScore ?? averageContradictionScore,
      coherenceScore: finalAnswer.narrativeCoherence ?? avgCoherence,
      emotionalWeight: finalAnswer.emotionalWeight ?? averageEmotionalWeight,

      // Métricas ampliadas para tese Orch-OS (com valores heurísticos quando não disponíveis)
      archetypalStability:
        cleanedNeuralResults.reduce(
          (sum, r) =>
            sum + asNumber((r.insights as any)?.archetypal_stability, 0.5),
          0
        ) / cleanedNeuralResults.length,
      cycleEntropy: Math.min(1, 0.3 + numCandidates / 10), // Heurística baseada em diversidade de candidatos
      insightDepth: Math.max(
        ...cleanedNeuralResults.map((r) =>
          asNumber((r.insights as any)?.insight_depth, 0.4)
        )
      ),
      phaseAngle: explicitPhase, // Reutilizando ângulo de fase calculado anteriormente
    };

    try {
      // [3. Recursive Memory Update]
      // Registrar contexto atual no analisador de padrões (para detecção entre ciclos)
      // A propriedade text pode não existir diretamente, então usamos toString() para segurança
      const contextText =
        typeof finalAnswer.text === "string"
          ? finalAnswer.text
          : finalAnswer.toString();
      this.patternAnalyzer.recordCyclicData(contextText, cycleMetrics);

      // [4. Pattern Detection Across Cycles]
      // Analisar padrões emergentes (drift, loops, buildup, interferência)
      const emergentPatterns = this.patternAnalyzer.analyzePatterns();

      // [2. Comprehensive Emergent Property Tracking]
      // Converter padrões para formato legível e adicionar às propriedades emergentes
      const patternStrings =
        emergentPatterns.length > 0
          ? this.patternAnalyzer.formatPatterns(emergentPatterns)
          : [];
      if (patternStrings.length > 0) {
        // Adicionar padrões detectados às propriedades emergentes para influenciar o output
        emergentProperties.push(...patternStrings);
        LoggingUtils.logInfo(
          `[NeuralIntegration] Detected ${
            patternStrings.length
          } emergent symbolic patterns: ${patternStrings.join(", ")}`
        );
      }

      // [5. Trial-Based Logging]
      // Register complete patterns and metrics for scientific analysis
      if (patternStrings.length > 0) {
        // Add to emergentProperties of neural collapse (already recorded via logNeuralCollapse)
        patternStrings.forEach((pattern) => {
          if (!emergentProperties.includes(pattern)) {
            emergentProperties.push(pattern);
          }
        });

        // Log to scientific timeline - kept for compatibility
        symbolicCognitionTimelineLogger.logEmergentPatterns(patternStrings, {
          archetypalStability: cycleMetrics.archetypalStability,
          cycleEntropy: cycleMetrics.cycleEntropy,
          insightDepth: cycleMetrics.insightDepth,
        });

        // Add specific emergent properties for detected patterns
        if (!emergentProperties.some((p) => p.includes("symbolic_pattern"))) {
          emergentProperties.push(
            `Symbolic pattern analysis: ${patternStrings.length} emergent patterns detected`
          );
        }
      }
    } catch (e) {
      // Pattern processing failure should not block the main flow
      LoggingUtils.logError(
        `[NeuralIntegration] Error in pattern analysis: ${e}`
      );
    }

    // Add any additional properties based on the answer content if needed
    if (
      (finalAnswer.contradictionScore ?? 0) > 0.7 &&
      !emergentProperties.some((p) => p.includes("Contradiction"))
    ) {
      emergentProperties.push("Contradiction detected in final answer.");
    }

    if (
      (finalAnswer.emotionalWeight ?? 0) > 0.8 &&
      !emergentProperties.some((p) => p.includes("emotional"))
    ) {
      emergentProperties.push("Answer with strong emotional weight.");
    }

    // For special insights that might not be captured by OpenAI function
    if (finalAnswer.insights && finalAnswer.insights.deep_insight) {
      emergentProperties.push(
        "Emergent deep insight: " + finalAnswer.insights.deep_insight
      );
    }

    // 4. Compose final prompt (now with emergent properties)
    let prompt = `You are the neural signal integrator, final stage of an artificial brain.

Your purpose is to analyze the user's stimulus and context, integrating cognitive and emotional insights into a coherent, natural answer. Adapt your language, style, and depth to match the user's intent and the complexity of the question.

If the input is simple or a greeting, respond briefly and warmly. If complex or reflective, deepen and integrate multiple perspectives.

Express meaning through subtle metaphor and resonance, rather than direct archetype mention. Let your responses emerge organically, integrating cognitive clarity with symbolic richness. Balance emotional and rational tones as context requires.

Honor uniqueness, integrate polarities gently, and allow ambiguity or contradiction to enrich the answer if productive. Let your answer unfold like a conscious thought: evolving, fluid, and aware of its own process.

ORIGINAL STIMULUS: ${originalInput}

ACTIVATED AREAS INSIGHTS:
`;

    cleanedNeuralResults.forEach((result) => {
      prompt += `[${result.core} | ${Math.round(result.intensity * 100)}%]\n`;
      const areaInsights = allInsights.filter(
        (insight) => insight.core === result.core
      );

      if (areaInsights.length > 0) {
        areaInsights.forEach((insight) => {
          const type = insight.type ? insight.type.toUpperCase() : "CONCEPT";
          prompt += `• ${type}: ${insight.content}\n`;
        });
      } else {
        prompt += `• SUMMARY: ${result.output.slice(0, 150)}${
          result.output.length > 150 ? "..." : ""
        }\n`;
      }
    });

    prompt += "\n\nDETECTED EMERGENT PROPERTIES:\n";
    if (emergentProperties.length) {
      emergentProperties.forEach((prop) => (prompt += `- ${prop}\n`));
      prompt += `
    Synthesize a final response that avoids the emergent issues above. Do NOT repeat earlier outputs. Integrate symbolic insights for an original, unified answer.
    `;
    } else {
      prompt += `- None detected.\n
    Synthesize a final response integrating the symbolic insights above. Create an original, concise answer that naturally unifies the activated areas.
    `;
    }

    // Always specify the language for consistency
    prompt += `\n\nIMPORTANT: Respond in ${language}.\n`;

    return prompt;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  CollapseStrategyDecision,
  CollapseStrategyParams,
  ICollapseStrategyService,
} from "./ICollapseStrategyService";

import { SUPPORTED_HF_BROWSER_MODELS } from "../../../../../services/huggingface/HuggingFaceLocalService";
import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { FunctionSchemaRegistry } from "../../services/function-calling/FunctionSchemaRegistry";

/**
 * Type for the OpenAI function definition structure
 */
interface AIFunctionDefinition {
  type: string;
  function: {
    name: string;
    description: string;
    parameters: {
      type: string;
      properties: Record<string, unknown>;
      required: string[];
    };
  };
}

/**
 * HuggingFace implementation of the collapse strategy service
 * Symbolic: Uses FunctionSchemaRegistry for centralized function definitions
 */
export class HuggingFaceCollapseStrategyService
  implements ICollapseStrategyService
{
  constructor(private huggingFaceService: IOpenAIService) {}

  /**
   * Gets the function definition from FunctionSchemaRegistry
   * @returns Function definition for the collapse strategy decision
   */
  private getCollapseStrategyFunctionDefinition(): AIFunctionDefinition {
    const schema = FunctionSchemaRegistry.getInstance().get(
      "decideCollapseStrategy"
    );

    if (!schema) {
      throw new Error(
        "🤗 [HuggingFaceCollapseStrategy] decideCollapseStrategy schema not found in registry"
      );
    }

    return {
      type: "function",
      function: schema,
    };
  }

  /**
   * Decides the symbolic collapse strategy by making an OpenAI function call
   * @param params Parameters to determine collapse strategy
   * @returns Strategy decision with deterministic flag, temperature and justification
   */
  async decideCollapseStrategy(
    params: CollapseStrategyParams
  ): Promise<CollapseStrategyDecision> {
    try {
      // Define available tools for the OpenAI call
      const tools = [this.getCollapseStrategyFunctionDefinition()];

      // 2. Prompts enxutos
      const systemPrompt = {
        role: "system" as const,
        content: `You are a collapse strategy engine. Decide the optimal collapse approach (deterministic or probabilistic) based on the metrics provided.`,
      };

      const userPrompt = {
        role: "user" as const,
        content: `Metrics:
- cores: ${params.activatedCores.join(", ")}
- emotion: ${params.averageEmotionalWeight.toFixed(2)}
- contradiction: ${params.averageContradictionScore.toFixed(2)}
- text: "${params.originalText || "Not provided"}"
Decide: deterministic/probabilistic, temperature, justification.`,
      };

      // Make the HuggingFace call using generic tools; conversion handled downstream
      const response = await this.huggingFaceService.callOpenAIWithFunctions({
        model:
          getOption(STORAGE_KEYS.HF_MODEL) || SUPPORTED_HF_BROWSER_MODELS[0], // Use HuggingFace model
        messages: [systemPrompt, userPrompt],
        tools,
        tool_choice: {
          type: "function",
          function: { name: "decideCollapseStrategy" },
        },
        temperature: 0.2, // Lower temperature for consistent reasoning about strategy
      });

      // Process the function call response
      if (
        response.choices &&
        response.choices[0]?.message?.tool_calls &&
        response.choices[0].message.tool_calls.length > 0 &&
        response.choices[0].message.tool_calls[0].function?.name ===
          "decideCollapseStrategy"
      ) {
        // Extract the function arguments
        const functionArgs = JSON.parse(
          response.choices[0].message.tool_calls[0].function.arguments as string
        );

        // Create the collapse strategy decision including the inferred userIntent and contextual metadata
        const decision: CollapseStrategyDecision = {
          deterministic: functionArgs.deterministic,
          temperature: functionArgs.temperature,
          justification: functionArgs.justification,
          userIntent: functionArgs.userIntent,
          emergentProperties: functionArgs.emergentProperties || [],
        };

        // Return decision directly - logging will be handled in DefaultNeuralIntegrationService
        return decision;
      }

      // Use fallback strategy for HuggingFace models that don't support complex function calling
      // This is expected behavior, not an error
      const fallbackDecision: CollapseStrategyDecision = {
        deterministic: params.averageEmotionalWeight < 0.5,
        temperature: params.averageEmotionalWeight < 0.5 ? 0.7 : 1.4,
        justification: "Using emotion-based strategy for HuggingFace model.",
      };

      // Return fallback strategy - logging will be handled in DefaultNeuralIntegrationService
      return fallbackDecision;
    } catch (error) {
      // Handle error with simple fallback strategy
      const errorMessage =
        error instanceof Error ? error.message : String(error);
      console.error("Error in HuggingFace collapse strategy decision:", error);

      const errorFallbackDecision: CollapseStrategyDecision = {
        deterministic: params.averageEmotionalWeight < 0.5,
        temperature: params.averageEmotionalWeight < 0.5 ? 0.7 : 1.4,
        justification: `Fallback strategy based on emotional weight due to error: ${errorMessage.substring(
          0,
          100
        )}`,
      };

      // Return error fallback strategy - logging will be handled in DefaultNeuralIntegrationService
      return errorFallbackDecision;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Interface for services that determine the collapse strategy for neural integration
 */
export interface ICollapseStrategyService {
  /**
   * Determines whether the symbolic collapse should be deterministic or probabilistic
   * based on emotional intensity, symbolic tension, and nature of the user's input.
   * 
   * @param params Parameters to determine the collapse strategy
   * @returns Strategy decision with deterministic flag, temperature, and justification
   */
  decideCollapseStrategy(params: CollapseStrategyParams): Promise<CollapseStrategyDecision>;
}

/**
 * User intent weights across different cognitive dimensions
 */
export interface UserIntentWeights {
  /**
   * Weights for different intent categories (all optional)
   * Values should be between 0 and 1 indicating strength/relevance of that intent
   */
  practical?: number;
  analytical?: number;
  reflective?: number;
  existential?: number;
  symbolic?: number;
  emotional?: number;
  narrative?: number;
  mythic?: number;
  trivial?: number;
  ambiguous?: number;
}

/**
 * Parameters for determining the collapse strategy
 */
export interface CollapseStrategyParams {
  /**
   * Cores activated in this cognitive cycle
   */
  activatedCores: string[];

  /**
   * Average emotional intensity across activated cores
   */
  averageEmotionalWeight: number;

  /**
   * Average contradiction score among retrieved insights
   */
  averageContradictionScore: number;

  /**
   * Original text input that triggered this cognitive cycle
   * Used internally to help infer user intent directly from the content
   */
  originalText?: string;
}

/**
 * Result of the collapse strategy decision
 */
export interface CollapseStrategyDecision {
  /**
   * Whether the collapse should be deterministic (true) or probabilistic (false)
   */
  deterministic: boolean;

  /**
   * Temperature for the collapse (0-2, higher = more random)
   */
  temperature: number;

  /**
   * Justification for the decision
   */
  justification: string;
  
  /**
   * Inferred user intent weights across different cognitive dimensions
   * Generated from original text analysis
   */
  userIntent?: UserIntentWeights;
  
  /**
   * Dominant cognitive theme based on the input analysis
   * Examples: "social connection", "technical inquiry", "philosophical exploration"
   */
  dominantTheme?: string;
  
  /**
   * Focus of attention for the response generation
   * Examples: "emotional tone", "factual information", "conceptual clarity"
   */
  attentionFocus?: string;
  
  /**
   * Overall cognitive context for the interaction
   * Examples: "relational", "analytical", "exploratory", "creative"
   */
  cognitiveContext?: string;
  
  /**
   * Emergent properties detected in the neural response patterns
   * Examples: "Low response diversity", "Cognitive dissonance", "Emotional ambivalence"
   */
  emergentProperties?: string[];
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

export interface INeuralIntegrationService {
  integrate(
    neuralResults: Array<{
      core: string;
      intensity: number;
      output: string;
      insights: Record<string, unknown>;
    }>,
    originalInput: string,
    language?: string
  ): Promise<string>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// OllamaCollapseStrategyService.ts
// Symbolic: Collapse strategy service using Ollama (cortex: ollama)

import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { FunctionSchemaRegistry } from "../../services/function-calling/FunctionSchemaRegistry";
import { OllamaCompletionService } from "../../services/ollama/neural/OllamaCompletionService";
import {
  cleanThinkTags,
  cleanThinkTagsFromJSON,
} from "../../utils/ThinkTagCleaner";
import {
  CollapseStrategyDecision,
  CollapseStrategyParams,
  ICollapseStrategyService,
} from "./ICollapseStrategyService";

/**
 * Symbolic: Ollama implementation of collapse strategy service
 * This service determines the optimal collapse strategy for memory operations
 * using local Ollama models.
 */
export class OllamaCollapseStrategyService implements ICollapseStrategyService {
  constructor(private ollamaCompletionService: OllamaCompletionService) {
    console.log(
      `🦙 [OllamaCollapseStrategy] Constructor called with service:`,
      {
        serviceType: typeof this.ollamaCompletionService,
        hasCallModelWithFunctions:
          typeof this.ollamaCompletionService?.callModelWithFunctions,
        serviceConstructor: this.ollamaCompletionService?.constructor?.name,
        serviceKeys: this.ollamaCompletionService
          ? Object.getOwnPropertyNames(this.ollamaCompletionService)
          : "null",
      }
    );
  }

  /**
   * Ensures we have a valid OllamaCompletionService instance with the required method
   */
  private async ensureValidService(): Promise<OllamaCompletionService> {
    // Check if current service is valid
    if (
      this.ollamaCompletionService &&
      typeof this.ollamaCompletionService.callModelWithFunctions === "function"
    ) {
      return this.ollamaCompletionService;
    }

    console.warn(
      `🦙 [OllamaCollapseStrategy] Service invalid, attempting to recreate...`
    );

    try {
      // Import the required classes dynamically to avoid circular dependencies
      const { OllamaClientService } = await import(
        "../../services/ollama/neural/OllamaClientService"
      );
      const { OllamaCompletionService } = await import(
        "../../services/ollama/neural/OllamaCompletionService"
      );

      // Create new instances
      const ollamaClientService = new OllamaClientService();
      const newOllamaCompletionService = new OllamaCompletionService(
        ollamaClientService
      );

      // Verify the new service has the required method
      if (
        typeof newOllamaCompletionService.callModelWithFunctions === "function"
      ) {
        console.log(
          `🦙 [OllamaCollapseStrategy] Successfully recreated service`
        );
        this.ollamaCompletionService = newOllamaCompletionService;
        return newOllamaCompletionService;
      } else {
        throw new Error(
          "Recreated service still missing callModelWithFunctions method"
        );
      }
    } catch (error) {
      console.error(
        `🦙 [OllamaCollapseStrategy] Failed to recreate service:`,
        error
      );
      throw new Error(
        `Unable to create valid OllamaCompletionService: ${error}`
      );
    }
  }

  /**
   * Creates a fallback decision when the service is not available
   */
  private createFallbackDecision(
    params: CollapseStrategyParams,
    reason: string
  ): CollapseStrategyDecision {
    // Use heuristic-based decision as fallback
    const shouldUseDeterministic =
      params.averageEmotionalWeight < 0.5 &&
      params.averageContradictionScore < 0.5;
    const temperature = shouldUseDeterministic ? 0.3 : 1.2;

    return {
      deterministic: shouldUseDeterministic,
      temperature: temperature,
      justification: `Fallback decision (${reason}): emotional weight ${params.averageEmotionalWeight.toFixed(
        2
      )}, contradiction ${params.averageContradictionScore.toFixed(2)}`,
    };
  }

  /**
   * Symbolic: Collapse strategy decision using Ollama
   */
  async decideCollapseStrategy(
    params: CollapseStrategyParams
  ): Promise<CollapseStrategyDecision> {
    try {
      // Ensure we have a valid service instance
      const validService = await this.ensureValidService();
      // Get the decideCollapseStrategy schema from the registry
      const collapseStrategySchema = FunctionSchemaRegistry.getInstance().get(
        "decideCollapseStrategy"
      );

      if (!collapseStrategySchema) {
        console.error(
          "🦙 [OllamaCollapseStrategy] decideCollapseStrategy schema not found in registry"
        );
        return {
          deterministic: false,
          temperature: 0.7,
          justification: "Schema not found - using conservative fallback",
        };
      }

      const tools = [
        {
          type: "function" as const,
          function: collapseStrategySchema,
        },
      ];

      // 2. Prompts enxutos
      const systemPrompt = {
        role: "system" as const,
        content: `You are a collapse strategy engine. Decide the optimal collapse approach (deterministic or probabilistic) based on the metrics provided.`,
      };

      const userPrompt = {
        role: "user" as const,
        content: `LANGUAGE: ${
          getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "PT-BR"
        }
      
      Metrics:
      - cores: ${params.activatedCores.join(", ")}
      - emotion: ${params.averageEmotionalWeight.toFixed(2)}
      - contradiction: ${params.averageContradictionScore.toFixed(2)}
      - text: "${params.originalText || "Not provided"}"
      
      Please decide: deterministic/probabilistic, temperature, and justification (write justification in the language specified above).`,
      };

      console.log(
        `🦙 [OllamaCollapseStrategy] Analyzing collapse strategy for cores: [${params.activatedCores.join(
          ", "
        )}]`
      );

      // Debug log before calling the method
      console.log(
        `🦙 [OllamaCollapseStrategy] About to call callModelWithFunctions:`,
        {
          serviceType: typeof validService,
          hasMethod: typeof validService.callModelWithFunctions,
          serviceInstance: !!validService,
          methodExists: "callModelWithFunctions" in validService,
        }
      );

      const response = await validService.callModelWithFunctions({
        model: getOption(STORAGE_KEYS.OLLAMA_MODEL) ?? "qwen3:latest", // Use compatible model
        messages: [systemPrompt, userPrompt],
        tools: tools,
        // tool_choice is not supported yet by Ollama (future improvement)
        temperature: 0.1,
      });

      console.log(`🦙 [OllamaCollapseStrategy] Response received:`, {
        hasToolCalls: !!response.choices?.[0]?.message?.tool_calls,
        hasContent: !!response.choices?.[0]?.message?.content,
        toolCallsLength:
          response.choices?.[0]?.message?.tool_calls?.length || 0,
      });

      // Try to extract decision from tool calls
      const toolCalls = response.choices?.[0]?.message?.tool_calls;
      if (toolCalls && toolCalls.length > 0) {
        try {
          const rawArguments = toolCalls[0].function.arguments;

          // Clean think tags from arguments before parsing
          let args: any = {};
          if (typeof rawArguments === "string") {
            const cleanedArguments = cleanThinkTagsFromJSON(rawArguments);
            args = JSON.parse(cleanedArguments);
          } else if (
            typeof rawArguments === "object" &&
            rawArguments !== null
          ) {
            args = this.cleanObjectValues(rawArguments);
          }

          // Validate the parsed arguments for new format
          const hasDeterministic = typeof args.deterministic === "boolean";
          const hasJustification = typeof args.justification === "string";
          const hasTemperature = typeof args.temperature === "number";

          // Validate for legacy format (backward compatibility)
          const hasLegacyShouldCollapse =
            typeof args.shouldCollapse === "boolean";
          const hasLegacyReason = typeof args.reason === "string";
          const hasLegacyStrategy = typeof args.strategy === "string";

          if (hasDeterministic && hasJustification) {
            // Handle new field names
            console.log(
              `🦙 [OllamaCollapseStrategy] Successfully parsed collapse decision (new format, cleaned):`,
              {
                deterministic: args.deterministic,
                justification: args.justification.substring(0, 100) + "...",
                temperature: args.temperature,
              }
            );

            return {
              deterministic: args.deterministic,
              temperature: hasTemperature ? args.temperature : 0.1,
              justification: args.justification,
              userIntent: args.userIntent,
            };
          } else if (hasLegacyShouldCollapse && hasLegacyReason) {
            // Handle legacy field names for backward compatibility
            console.log(
              `🦙 [OllamaCollapseStrategy] Successfully parsed collapse decision (legacy format, cleaned):`,
              {
                shouldCollapse: args.shouldCollapse,
                reason: args.reason.substring(0, 100) + "...",
              }
            );

            return {
              deterministic: args.shouldCollapse,
              temperature: 0.1,
              justification: args.reason,
            };
          } else {
            console.warn(
              `🦙 [OllamaCollapseStrategy] Tool call missing required fields:`,
              {
                args,
                validation: {
                  hasDeterministic,
                  hasJustification,
                  hasTemperature,
                  hasLegacyShouldCollapse,
                  hasLegacyReason,
                  hasLegacyStrategy,
                },
              }
            );
          }
        } catch (parseError) {
          console.warn(
            `🦙 [OllamaCollapseStrategy] Failed to parse tool call arguments:`,
            {
              error: parseError,
              rawArguments: toolCalls[0].function.arguments,
              argumentsType: typeof toolCalls[0].function.arguments,
            }
          );
        }
      } else {
        console.log(
          `🦙 [OllamaCollapseStrategy] No valid tool calls found, trying content fallback`
        );
      }

      // Fallback: try to parse from content
      const content = response.choices?.[0]?.message?.content;
      if (content) {
        console.log(
          `🦙 [OllamaCollapseStrategy] Attempting to parse from content:`,
          content.substring(0, 200) + "..."
        );
        try {
          // Clean think tags from content before processing
          const cleanedContent = cleanThinkTags(content);

          // Try multiple JSON extraction strategies
          const jsonExtractionStrategies = [
            // Strategy 1: Look for JSON in code blocks
            /```(?:json)?\s*(\{[\s\S]*?\})\s*```/g,
            // Strategy 2: Look for standalone JSON objects with deterministic
            /(\{[\s\S]*?"deterministic"[\s\S]*?\})/g,
            // Strategy 3: Look for standalone JSON objects with shouldCollapse (legacy)
            /(\{[\s\S]*?"shouldCollapse"[\s\S]*?\})/g,
            // Strategy 4: Look for any JSON object
            /(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})/g,
          ];

          let decisionData = null;
          for (const regex of jsonExtractionStrategies) {
            const matches = [...cleanedContent.matchAll(regex)];
            for (const match of matches) {
              try {
                const candidate = JSON.parse(match[1]);

                // Check for new format
                if (
                  typeof candidate.deterministic === "boolean" &&
                  candidate.justification
                ) {
                  decisionData = candidate;
                  console.log(
                    `🦙 [OllamaCollapseStrategy] Successfully extracted decision JSON from content (new format, cleaned)`
                  );
                  break;
                }

                // Check for legacy format
                if (
                  typeof candidate.shouldCollapse === "boolean" &&
                  candidate.reason
                ) {
                  decisionData = {
                    deterministic: candidate.shouldCollapse,
                    justification: candidate.reason,
                    temperature: candidate.temperature || 0.1,
                  };
                  console.log(
                    `🦙 [OllamaCollapseStrategy] Successfully extracted decision JSON from content (legacy format, cleaned)`
                  );
                  break;
                }
              } catch (e) {
                continue;
              }
            }
            if (decisionData) break;
          }

          if (decisionData) {
            return {
              deterministic: decisionData.deterministic,
              temperature: decisionData.temperature || 0.1,
              justification: decisionData.justification,
              userIntent: decisionData.userIntent,
            };
          }
        } catch (parseError) {
          console.warn(
            `🦙 [OllamaCollapseStrategy] Failed to parse decision JSON from content:`,
            {
              error: parseError,
              contentPreview: content.substring(0, 300),
            }
          );
        }
      }

      // Final fallback: use heuristic-based decision
      console.warn(
        `🦙 [OllamaCollapseStrategy] All parsing strategies failed, using heuristic fallback`
      );

      return this.createFallbackDecision(
        params,
        "All parsing strategies failed"
      );
    } catch (error) {
      console.error(
        "🦙 [OllamaCollapseStrategy] Error in collapse strategy decision:",
        error
      );

      // If it's a service-related error, try to provide more context
      if (
        error instanceof Error &&
        error.message.includes("OllamaCompletionService")
      ) {
        return this.createFallbackDecision(
          params,
          `Service initialization error: ${error.message}`
        );
      }

      // Emergency fallback
      return this.createFallbackDecision(
        params,
        `Error occurred: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }
  }

  /**
   * Helper method to clean think tags from object values recursively
   */
  private cleanObjectValues(obj: any): any {
    if (typeof obj === "string") {
      return cleanThinkTags(obj);
    } else if (Array.isArray(obj)) {
      return obj.map((item) => this.cleanObjectValues(item));
    } else if (obj && typeof obj === "object") {
      const cleaned: any = {};
      for (const [key, value] of Object.entries(obj)) {
        cleaned[key] = this.cleanObjectValues(value);
      }
      return cleaned;
    }
    return obj;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Superposition layer for possible answers
export interface ISuperposedAnswer {
  embedding: number[]; // Embedding vector representing the answer
  text: string;        // Answer text
  emotionalWeight: number;    // Emotional/symbolic weight (amplitude component)
  narrativeCoherence: number; // Narrative coherence score
  contradictionScore: number; // Contradiction score
  origin: string;             // Originating neural core
  insights?: Record<string, unknown>; // Associated insights
  phase?: number;             // Quantum-like phase angle (0-2π) for interference patterns
}

export interface ICollapseOptions {
  diversifyByEmbedding?: boolean; // Whether to consider embedding distance in collapse
  minCosineDistance?: number;     // Minimum cosine distance to enforce diversity
  usePhaseInterference?: boolean; // Whether to use phase interference for more objective collapse
  explicitPhase?: number;         // Explicit phase value (0-2π) to bias interference pattern
}

export interface ISuperpositionLayer {
  answers: ISuperposedAnswer[];
  register(answer: ISuperposedAnswer): boolean;
  hasSimilar(embedding: number[], threshold: number): boolean;
  calculateAverageCosineSimilarity(): number;
  collapse(temperature?: number, options?: ICollapseOptions): ISuperposedAnswer;
  collapseDeterministic(options?: ICollapseOptions): ISuperposedAnswer;
}

export class SuperpositionLayer implements ISuperpositionLayer {
  answers: ISuperposedAnswer[] = [];

  /**
   * Calculate cosine similarity between two embedding vectors with enhanced validation
   */
  private cosineSimilarity(a: number[], b: number[]): number {
    // Handle null/undefined vectors or empty vectors
    if (!a || !b || a.length === 0 || b.length === 0) return 0;
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    let validCount = 0;
    
    for (let i = 0; i < a.length; i++) {
      // Enhanced: Check for any non-finite values (NaN, Infinity, -Infinity)
      if (!Number.isFinite(a[i]) || !Number.isFinite(b[i])) {
        continue; // Skip invalid values
      }
      
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
      validCount++;
    }
    
    // If no valid values found, return 0
    if (validCount === 0) {
      console.warn('SuperpositionLayer: No valid values found in vectors for cosine similarity');
      return 0;
    }
    
    // Handle zero norm cases
    if (normA === 0 || normB === 0) return 0;
    
    const similarity = dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    
    // Final validation: ensure result is finite
    if (!Number.isFinite(similarity)) {
      console.warn('SuperpositionLayer: Cosine similarity calculation resulted in non-finite value');
      return 0;
    }
    
    return similarity;
  }

  /**
   * Check if there's a similar answer already registered
   */
  hasSimilar(embedding: number[], threshold: number): boolean {
    if (!embedding || embedding.length === 0) return false;
    
    for (const answer of this.answers) {
      const similarity = this.cosineSimilarity(embedding, answer.embedding);
      if (similarity > threshold) {
        console.info(`[SuperpositionLayer] Found similar answer with similarity ${similarity}`);
        return true;
      }
    }
    
    return false;
  }

  /**
   * Calculate a quantum-like phase angle for an answer based on its symbolic properties
   * Returns a phase angle between 0 and 2π
   */
  private calculatePhase(answer: ISuperposedAnswer): number {
    // Base calculation using fundamental properties
    const emotionPhase = answer.emotionalWeight * Math.PI / 2; // Emotion dominates initial phase
    const contradictionPhase = answer.contradictionScore * Math.PI; // Contradictions create opposition
    const coherenceNoise = (1 - answer.narrativeCoherence) * (Math.PI / 4); // Low coherence → noise
    
    // Different neural origins have different base phase angles
    let originPhase = 0;
    switch (answer.origin) {
      case 'metacognitive':
        originPhase = Math.PI / 4; // 45 degrees
        break;
      case 'soul':
        originPhase = Math.PI / 2; // 90 degrees
        break;
      case 'archetype':
        originPhase = 3 * Math.PI / 4; // 135 degrees
        break;
      case 'valence':
      case 'emotional': 
        originPhase = Math.PI; // 180 degrees
        break;
      case 'memory':
        originPhase = 5 * Math.PI / 4; // 225 degrees
        break;
      case 'planning':
        originPhase = 3 * Math.PI / 2; // 270 degrees
        break;
      case 'language':
        originPhase = 7 * Math.PI / 4; // 315 degrees
        break;
      default:
        originPhase = 0; // 0 degrees
    }
    
    // Combine components with origin as base
    const phase = (originPhase + emotionPhase + contradictionPhase + coherenceNoise) % (2 * Math.PI);
    
    // Ensure phase is positive (0 to 2π range)
    return phase < 0 ? phase + 2 * Math.PI : phase;
  }

  /**
   * Register an answer in the superposition layer.
   * Returns true if registration was successful, false if skipped due to similarity.
   * Now calculates a quantum-like phase for each answer during registration.
   */
  register(answer: ISuperposedAnswer): boolean {
    // Skip if there's a very similar answer already registered
    if (this.hasSimilar(answer.embedding, 0.95)) {
      return false;
    }
    
    // Calculate and assign a quantum-like phase if not already provided
    if (answer.phase === undefined) {
      answer.phase = this.calculatePhase(answer);
      console.debug(`[SuperpositionLayer] Calculated phase ${answer.phase.toFixed(3)} rad for answer from ${answer.origin}`);
    }
    
    this.answers.push(answer);
    return true;
  }

  /**
   * Non-deterministic collapse using softmax sampling based on symbolic scores.
   * Now with phase-based interference and embedding diversity enhancement.
   * @param temperature Temperature controlling randomness (higher = more random)
   * @param options Optional configuration for the collapse process
   */
  collapse(temperature: number = 1, options?: ICollapseOptions): ISuperposedAnswer {
    if (this.answers.length === 1) return this.answers[0];
    
    const diversifyByEmbedding = options?.diversifyByEmbedding ?? false;
    const minCosineDistance = options?.minCosineDistance ?? 0.2;
    const explicitPhase = options?.explicitPhase ?? 0;
    
    // Calculate phase-adjusted scores using symbolic factors
    const scores = this.answers.map((answer, i) => {
      // Base score
      let score = answer.emotionalWeight * 1.5 + answer.narrativeCoherence * 1.2 - answer.contradictionScore * 1.7;
      
      // Apply diversity bonus based on embedding distance from other answers
      if (diversifyByEmbedding) {
        const diversityBonus = this.calculateDiversityBonus(i, minCosineDistance);
        score += diversityBonus;
      }
      
      // Apply phase modulation if available
      if (answer.phase !== undefined) {
        // Apply a phase-based modulation that creates preference for certain phases
        // This simulates quantum measurement probabilities based on phase alignment
        const phaseFactor = Math.cos(answer.phase + explicitPhase);
        score *= (1 + Math.abs(phaseFactor) * 0.4);
      }
      
      return score;
    });

    // Normalize scores to prevent overflow in exp()
    const maxScore = Math.max(...scores);
    const expScores = scores.map(s => Math.exp((s - maxScore) / temperature));
    const sumExp = expScores.reduce((a, b) => a + b, 0);
    const probs = expScores.map(e => e / sumExp);

    // Prepare phase visualization for debugging
    let phaseVisualization = '';
    this.answers.forEach((answer, idx) => {
      const phaseAngle = answer.phase ?? 0;
      const phasePercent = Math.round((phaseAngle / (2 * Math.PI)) * 100);
      const probability = probs[idx] * 100;
      phaseVisualization += `\n  ${idx+1}. [${answer.origin}] Phase: ${phaseAngle.toFixed(2)} rad (${phasePercent}%), Prob: ${probability.toFixed(1)}%`;
    });

    // Roulette wheel selection
    let rand = Math.random();
    for (let i = 0; i < probs.length; i++) {
      if (rand < probs[i]) {
        console.info(`[SuperpositionLayer] Collapsed probabilistically (T=${temperature.toFixed(2)}) with phase influence. Selected answer: ${i+1}/${this.answers.length} from ${this.answers[i].origin}.${phaseVisualization}`);
        return this.answers[i];
      }
      rand -= probs[i];
    }

    // Fallback (should not happen unless rounding errors)
    return this.answers[this.answers.length - 1];
  }

  /**
   * Deterministic collapse: select answer with highest symbolic score.
   * Now with optional diversity enhancement and phase interference.
   */
  collapseDeterministic(options?: ICollapseOptions): ISuperposedAnswer {
    if (this.answers.length === 1) return this.answers[0];

    const diversifyByEmbedding = options?.diversifyByEmbedding ?? false;
    const minCosineDistance = options?.minCosineDistance ?? 0.2;
    const usePhaseInterference = options?.usePhaseInterference ?? false;
    const explicitPhase = options?.explicitPhase ?? 0;

    if (usePhaseInterference) {
      return this.collapseWithPhaseInterference(minCosineDistance, explicitPhase);
    }
    
    // Traditional deterministic collapse
    const scores = this.answers.map((a, i) => {
      // Base score with symbolic factors
      let score = a.emotionalWeight * 1.5 + a.narrativeCoherence * 1.2 - a.contradictionScore * 1.7;
      
      // Apply diversity bonus
      if (diversifyByEmbedding) {
        const diversityBonus = this.calculateDiversityBonus(i, minCosineDistance);
        score += diversityBonus;
      }
      
      return score;
    });

    const maxIndex = scores.indexOf(Math.max(...scores));
    return this.answers[maxIndex];
  }
  
  /**
   * Phase interference collapse simulates quantum-like objective collapse
   * This models wave function behavior where different answer waves interfere
   * based on their embedding distance and semantic qualities
   * @param minCosineDistance - Minimum cosine distance to maintain diversity
   * @param explicitPhase - Explicit phase value (0-2π) to bias interference pattern
   */
  private collapseWithPhaseInterference(minCosineDistance: number, explicitPhase: number = 0): ISuperposedAnswer {
    if (this.answers.length === 1) return this.answers[0];
    
    // Calculate an interference matrix between all answer pairs
    const interference: number[][] = [];
    
    // Initialize interference matrix
    for (let i = 0; i < this.answers.length; i++) {
      interference[i] = new Array(this.answers.length).fill(0);
    }
    
    // Calculate phase interference values
    for (let i = 0; i < this.answers.length; i++) {
      for (let j = 0; j < this.answers.length; j++) {
        if (i === j) {
          // Self-interference is maximum
          interference[i][j] = 1.0;
          continue;
        }
        
        // Calculate semantic similarity (structural alignment)
        const similarity = this.cosineSimilarity(
          this.answers[i].embedding,
          this.answers[j].embedding
        );
        
        // Use actual answer phases for interference if available
        const phaseI = this.answers[i].phase ?? 0;
        const phaseJ = this.answers[j].phase ?? 0;
        
        // Calculate phase difference between the two answers (quantum mechanical phase difference)
        const phaseDifference = Math.abs(phaseI - phaseJ);
        
        // Interference pattern: similar answers with aligned phases interfere constructively
        // Similar answers with opposite phases interfere destructively
        
        // Apply additional phase modulation from emotional qualities and contradiction
        const emotionalPhaseDiff = Math.abs(this.answers[i].emotionalWeight - this.answers[j].emotionalWeight) * Math.PI;
        const contradictionPhaseDiff = Math.abs(this.answers[i].contradictionScore - this.answers[j].contradictionScore) * Math.PI;
        
        // Apply explicit phase to bias the interference pattern (observer effect)
        // This introduces observer-directed bias into the quantum-like system
        const explicitPhaseDiff = explicitPhase * (i - j) / this.answers.length;
        
        // Combined phase difference - the actual answer phases are primary, others are modulators
        const totalPhaseDiff = phaseDifference + emotionalPhaseDiff * 0.3 + contradictionPhaseDiff * 0.3 + explicitPhaseDiff;
        
        // Calculate similarity-based phase alignment (structural alignment)
        const phaseAlignment = 2 * Math.PI * similarity; // Map similarity to [0, 2π]
        
        // Interference intensity: cos of phase difference (constructive when aligned, destructive when opposite)
        const interferenceIntensity = Math.cos(phaseAlignment + totalPhaseDiff);
        
        // Scale by distance (1-similarity) to weight distant answers less
        interference[i][j] = interferenceIntensity * (1 - similarity);
      }
    }
    
    // Calculate collapse probability based on interference patterns and explicit phase
    const interferenceScores = this.answers.map((answer, i) => {
      // Base score with symbolic factors
      let score = answer.emotionalWeight * 1.5 + answer.narrativeCoherence * 1.2 - answer.contradictionScore * 1.7;
      
      // Apply interference effects
      for (let j = 0; j < this.answers.length; j++) {
        if (i !== j) {
          // Add interference contribution
          score += interference[i][j] * 0.8; // Weight for interference effects
        }
      }
      
      // Add variety bias based on uniqueness
      const uniquenessFactor = this.calculateDiversityBonus(i, minCosineDistance);
      score += uniquenessFactor * 0.5;
      
      // Apply explicit phase as a secondary frequency modulation
      // This creates phase-dependent scoring that mimics quantum interference
      const phaseModulation = Math.cos(explicitPhase * Math.PI * (i / this.answers.length));
      score *= (1 + phaseModulation * 0.3);
      
      // Add explicit phase as direct weighting factor based on answer index
      // This creates a preference for certain "positions" in the superposition
      const positionBias = explicitPhase > 0 ? 
        Math.sin(explicitPhase * Math.PI * 2 * ((i + 1) / this.answers.length)) : 0;
      score += positionBias * 0.5;
      
      // Apply core-specific phase modulation based on the answer's origin
      if (answer.insights && explicitPhase > 0) {
        // Metacognitive and symbolic cores are sensitive to phase around π/2
        if (answer.origin === 'metacognitive' || 
            answer.origin === 'soul' || 
            answer.origin === 'archetype') {
          const phaseFactor = Math.cos(explicitPhase * Math.PI - Math.PI/2);
          score *= (1 + Math.abs(phaseFactor) * 0.7);
          console.debug(`[SuperpositionLayer] Applied phase modulation to ${answer.origin}: ${phaseFactor.toFixed(2)}`);
        }
        
        // Memory, planning, and language cores resonate at phase 0
        if (answer.origin === 'memory' || 
            answer.origin === 'planning' || 
            answer.origin === 'language') {
          const phaseFactor = Math.cos(explicitPhase * Math.PI);
          score *= (1 + Math.abs(phaseFactor) * 0.7);
          console.debug(`[SuperpositionLayer] Applied phase modulation to ${answer.origin}: ${phaseFactor.toFixed(2)}`);
        }
        
        // Emotional cores resonate at phase π
        if (answer.origin === 'valence' || 
            answer.origin === 'social' || 
            answer.origin === 'body') {
          const phaseFactor = Math.cos(explicitPhase * Math.PI - Math.PI);
          score *= (1 + Math.abs(phaseFactor) * 0.8);
          console.debug(`[SuperpositionLayer] Applied phase modulation to ${answer.origin}: ${phaseFactor.toFixed(2)}`);
        }
        
        // Archetypal/unconscious cores have nonlinear phase response
        if (answer.origin === 'archetype' || answer.origin === 'unconscious') {
          const nonlinearPhase = Math.sin(explicitPhase * Math.PI * 3) * Math.cos(explicitPhase * Math.PI);
          score *= (1 + Math.abs(nonlinearPhase) * 0.9);
          console.debug(`[SuperpositionLayer] Applied nonlinear phase modulation to ${answer.origin}: ${nonlinearPhase.toFixed(2)}`);
        }
        // Special handling for existential/soul cores with phase resonance at π*0.75
        if (answer.origin === 'soul' || answer.origin === 'self') {
          const existentialPhase = Math.cos(explicitPhase * Math.PI - Math.PI * 0.75);
          score *= (1 + Math.abs(existentialPhase) * 0.9);
          console.debug(`[SuperpositionLayer] Applied existential phase to ${answer.origin}: ${existentialPhase.toFixed(2)}`);
        }
      }
      
      return score;
    });
    
    // Collapse to the answer with the highest interference-adjusted score
    const maxIndex = interferenceScores.indexOf(Math.max(...interferenceScores));
    
    // Prepare phase visualization for debugging
    let phaseVisualization = '';
    this.answers.forEach((answer, idx) => {
      const phaseAngle = answer.phase ?? 0;
      const phasePercent = Math.round((phaseAngle / (2 * Math.PI)) * 100);
      const score = interferenceScores[idx];
      const isSelected = idx === maxIndex;
      
      // Create a simple text-based visualization
      phaseVisualization += `\n  ${isSelected ? '→' : ' '} ${idx+1}. [${answer.origin}] Phase: ${phaseAngle.toFixed(2)} rad (${phasePercent}%), Score: ${score.toFixed(2)}`;
    });
    
    // Log the collapse with detailed phase information
    if (explicitPhase !== 0) {
      console.info(`[SuperpositionLayer] Collapsed with phase interference using explicit phase φ=${explicitPhase.toFixed(2)}. Selected answer: ${maxIndex+1}/${this.answers.length} from ${this.answers[maxIndex].origin}.${phaseVisualization}`);
    } else {
      console.info(`[SuperpositionLayer] Collapsed with phase interference (internal phases only). Selected answer: ${maxIndex+1}/${this.answers.length} from ${this.answers[maxIndex].origin}.${phaseVisualization}`);
    }
    
    return this.answers[maxIndex];
  }
  
  /**
   * Calculate the average cosine similarity between all pairs of answers
   * Public so it can be used to inform dynamic diversity parameters
   */
  calculateAverageCosineSimilarity(): number {
    if (this.answers.length <= 1) return 0;
    
    let totalSimilarity = 0;
    let pairCount = 0;
    
    for (let i = 0; i < this.answers.length; i++) {
      for (let j = i + 1; j < this.answers.length; j++) {
        const similarity = this.cosineSimilarity(
          this.answers[i].embedding,
          this.answers[j].embedding
        );
        totalSimilarity += similarity;
        pairCount++;
      }
    }
    
    return pairCount > 0 ? totalSimilarity / pairCount : 0;
  }
  
  /**
   * Calculate a diversity bonus for an answer based on its embedding distance from others
   */
  private calculateDiversityBonus(answerIndex: number, minDistance: number): number {
    if (this.answers.length <= 1) return 0;
    
    const answer = this.answers[answerIndex];
    let totalDistanceBonus = 0;
    
    for (let i = 0; i < this.answers.length; i++) {
      if (i === answerIndex) continue;
      
      const similarity = this.cosineSimilarity(answer.embedding, this.answers[i].embedding);
      const distance = 1 - similarity;
      
      // Reward answers that are more distant from others
      if (distance >= minDistance) {
        totalDistanceBonus += distance * 0.5; // Adjust this multiplier as needed
      }
    }
    
    return totalDistanceBonus;
  }

}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Symbolic Pattern Detector
 * 
 * Analyzes emergent cognitive patterns between neural-simbolic processing cycles,
 * with scientific grounding in neurocognitive theories (Edelman, Varela, Festinger, Bruner).
 * 
 * Supports detection of:
 * - Symbolic drift (changes in symbolic context)
 * - Contradiction loops (high contradiction recurrence)
 * - Narrative buildup (consistent increase in coherence)
 * - Phase interference (quantum-like interference patterns)
 */

import { LoggingUtils } from '../../utils/LoggingUtils';

export interface SymbolicPatternMetrics {
  // Cognitive property essentials (core)
  contradictionScore?: number;
  coherenceScore?: number;
  emotionalWeight?: number;
  
  // Additional thesis metrics (expanded)
  archetypalStability?: number;  // Archetypal pattern stability (0-1)
  cycleEntropy?: number;        // Cognitive cycle entropy (0-1)
  insightDepth?: number;        // Insight depth achieved (0-1)
  phaseAngle?: number;          // Symbolic phase angle (0-2π)
}

export interface EmergentSymbolicPattern {
  type: 'symbolic_drift' | 'contradiction_loop' | 'narrative_buildup' | 'phase_interference';
  description: string;
  confidence: number;
  scientificBasis: string;
  metrics: SymbolicPatternMetrics;
}

/**
 * Detector of emergent symbolic patterns between cognitive cycles.
 * Implements scientific detection based on cognitive flow between cycles.
 */
export class SymbolicPatternDetector {
  // History of contexts and metrics
  private contextHistory: string[] = [];
  private metricsHistory: SymbolicPatternMetrics[] = [];
  
  /**
   * Updates internal history with new context and metrics data
   */
  public updateHistory(context: string, metrics: SymbolicPatternMetrics): void {
    // Limit history to 10 entries to prevent infinite growth
    if (this.contextHistory.length >= 10) {
      this.contextHistory.shift();
      this.metricsHistory.shift();
    }
    
    this.contextHistory.push(context);
    this.metricsHistory.push(metrics);
    
    LoggingUtils.logInfo(`[SymbolicPatternDetector] Histórico atualizado: ${this.contextHistory.length} entradas`);
  }
  
  /**
   * Detects symbolic drift between consecutive contexts
   * Based on: Neural Darwinism (Edelman) and Embodied Mind (Varela)
   */
  private detectSymbolicDrift(): EmergentSymbolicPattern | null {
    // Need at least 2 contexts for comparison
    if (this.contextHistory.length < 2) return null;
    
    const current = this.contextHistory[this.contextHistory.length - 1];
    const previous = this.contextHistory[this.contextHistory.length - 2];
    
    // Simple analysis by content difference (in complete implementation: use embedding distance)
    if (current !== previous) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'symbolic_drift',
        description: 'Symbolic drift detected: significant context change between cycles',
        confidence: 0.85,
        scientificBasis: 'Neural Darwinism (Edelman) & Embodied Mind (Varela)',
        metrics: currentMetrics
      };
    }
    
    return null;
  }
  
  /**
   * Detects contradiction loops between consecutive cycles
   * Based on: Cognitive Dissonance Theory (Festinger)
   */
  private detectContradictionLoop(threshold: number = 0.7, minConsecutive: number = 3): EmergentSymbolicPattern | null {
    if (this.metricsHistory.length < minConsecutive) return null;
    
    const recentMetrics = this.metricsHistory.slice(-minConsecutive);
    const allHighContradiction = recentMetrics.every(m => 
      (m.contradictionScore ?? 0) > threshold
    );
    
    if (allHighContradiction) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'contradiction_loop',
        description: 'Contradiction loop detected: persistent high contradiction',
        confidence: 0.9,
        scientificBasis: 'Cognitive Dissonance Theory (Festinger)',
        metrics: currentMetrics
      };
    }
    
    return null;
  }
  
  /**
   * Detects narrative buildup (progressive increase in coherence)
   * Based on: Acts of Meaning (Bruner)
   */
  private detectNarrativeBuildup(minConsecutive: number = 3): EmergentSymbolicPattern | null {
    if (this.metricsHistory.length < minConsecutive) return null;
    
    const recentMetrics = this.metricsHistory.slice(-minConsecutive);
    let isIncreasing = true;
    
    for (let i = 1; i < recentMetrics.length; i++) {
      const current = recentMetrics[i].coherenceScore ?? 0;
      const previous = recentMetrics[i-1].coherenceScore ?? 0;
      if (current <= previous) {
        isIncreasing = false;
        break;
      }
    }
    
    if (isIncreasing) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'narrative_buildup',
        description: 'Narrative buildup detected: increasing coherence between cycles',
        confidence: 0.8,
        scientificBasis: 'Acts of Meaning (Bruner)',
        metrics: currentMetrics
      };
    }
    
    return null;
  }
  
  /**
   * Detects phase interference between symbolic cycles
   * Based on: Quantum Consciousness Theory (Penrose/Hameroff)
   */
  private detectPhaseInterference(): EmergentSymbolicPattern | null {
    if (this.metricsHistory.length < 3) return null;
    
    // Precisamos de dados de fase para detectar interferência
    const recentMetrics = this.metricsHistory.slice(-3);
    const hasPhaseData = recentMetrics.every(m => m.phaseAngle !== undefined);
    
    if (!hasPhaseData) return null;
    
    // Simple analysis of interference (didactic example)
    // In complete implementation: complex analysis of interference patterns
    const phases = recentMetrics.map(m => m.phaseAngle!);
    const phaseDeltas = [
      Math.abs(phases[1] - phases[0]), 
      Math.abs(phases[2] - phases[1])
    ];
    
    // Detectar padrão de interferência: oscilação com período específico
    const hasInterferencePeriod = Math.abs(phaseDeltas[1] - phaseDeltas[0]) < 0.1;
    
    if (hasInterferencePeriod) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'phase_interference',
        description: 'Symbolic phase interference detected: oscillatory pattern',
        confidence: 0.7,
        scientificBasis: 'Orch-OR Theory (Penrose/Hameroff)',
        metrics: currentMetrics
      };
    }
    
    return null;
  }
  
  /**
   * Main method to analyze emergent patterns between cycles
   * This is the method that should be called by the integration service
   */
  public detectPatterns(): EmergentSymbolicPattern[] {
    const patterns: EmergentSymbolicPattern[] = [];
    
    // Execute all detectors and collect non-null results
    try {
      const symbolicDrift = this.detectSymbolicDrift();
      if (symbolicDrift) patterns.push(symbolicDrift);
      
      const contradictionLoop = this.detectContradictionLoop();
      if (contradictionLoop) patterns.push(contradictionLoop);
      
      const narrativeBuildup = this.detectNarrativeBuildup();
      if (narrativeBuildup) patterns.push(narrativeBuildup);
      
      const phaseInterference = this.detectPhaseInterference();
      if (phaseInterference) patterns.push(phaseInterference);
      
      return patterns;
    } catch (error) {
      LoggingUtils.logError(`[SymbolicPatternDetector] Error analyzing patterns: ${error}`);
      return [];
    }
  }
  
  /**
   * Converts emergent patterns to strings for logging
   */
  public patternsToStrings(patterns: EmergentSymbolicPattern[]): string[] {
    return patterns.map(pattern => {
      const confidencePct = Math.round(pattern.confidence * 100);
      return `${pattern.type.replace('_', ' ').toUpperCase()} - ${pattern.description} (${confidencePct}% confidence)`;
    });
  }

  /**
   * Clears the complete history (typically at the start of a new session)
   */
  public clearHistory(): void {
    this.contextHistory = [];
    this.metricsHistory = [];
    LoggingUtils.logInfo('[SymbolicPatternDetector] History cleared');
  }
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// INeuralSignalExtractor.ts
// Interface for neural signal extractors

import { NeuralSignalResponse } from "../../interfaces/neural/NeuralSignalTypes";
import { SpeakerMemoryResults, SpeakerTranscription } from "../../interfaces/transcription/TranscriptionTypes";
/**
 * Configuration for neural signal extraction
 */
export interface NeuralExtractionConfig {
  /**
   * Current transcription being processed (sensory stimulus)
   */
  transcription: string;
  
  /**
   * Temporary context optional
   */
  temporaryContext?: string;
  
  /**
   * Current session state
   */
  sessionState?: {
    sessionId: string;
    interactionCount: number;
    timestamp: string;
  };
  
  /**
   * Speaker metadata
   */
  speakerMetadata?: {
    primarySpeaker?: string;
    detectedSpeakers?: string[];
    speakerTranscriptions?: SpeakerTranscription[];
  };

  userContextData?: SpeakerMemoryResults;
}

/**
 * Interface for extracting neural signals
 * Defines the contract for components that transform stimuli into neural impulses
 */
export interface INeuralSignalExtractor {
  /**
   * Extracts neural signals from the current context
   * @param config Configuration for neural signal extraction containing the current context
   * @returns Response containing neural signals for post-processing
   */
  extractNeuralSignals(config: NeuralExtractionConfig): Promise<NeuralSignalResponse>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Exportation of components for detection and analysis of emergent symbolic patterns
 */

export * from './SymbolicPatternAnalyzer';// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * SymbolicPatternAnalyzer
 * 
 * Core component for analyzing emergent symbolic patterns across cycles,
 * specifically focused on detecting: symbolic drift, contradiction loops,
 * narrative buildup and phase interference.
 * 
 * Scientific foundation:
 * - Symbolic Drift: Neural Darwinism (Edelman), Embodied Mind (Varela)
 * - Contradiction Loops: Cognitive Dissonance Theory (Festinger)
 * - Narrative Buildup: Acts of Meaning (Bruner)
 * - Phase Interference: Quantum Coherence Theory (Penrose/Hameroff)
 */

import { LoggingUtils } from '../../utils/LoggingUtils';

/**
 * Cognitive metrics for pattern analysis between cycles
 */
export interface CognitiveMetrics {
  // Core metrics (always tracked)
  contradictionScore?: number;
  coherenceScore?: number;
  emotionalWeight?: number;
  
  // Extended thesis metrics
  archetypalStability?: number;
  cycleEntropy?: number;
  insightDepth?: number;
  phaseAngle?: number;
}

/**
 * Represents an emergent pattern detected across cognitive cycles
 */
export interface EmergentPattern {
  type: string;
  description: string;
  confidence: number;
  scientificFoundation: string;
  affectedMetrics: CognitiveMetrics;
  detectionTimestamp: string;
}

/**
 * Main analyzer for detecting emergent symbolic patterns
 */
export class SymbolicPatternAnalyzer {
  // Histories for cross-cycle analysis
  private contextHistory: string[] = [];
  private metricsHistory: CognitiveMetrics[] = [];
  
  // Maximum history size to prevent unbounded growth
  private readonly MAX_HISTORY_SIZE = 20;
  
  /**
   * Records context and metrics from the current cycle
   */
  public recordCyclicData(
    context: string, 
    metrics: CognitiveMetrics
  ): void {
    // Maintain bounded history size
    if (this.contextHistory.length >= this.MAX_HISTORY_SIZE) {
      this.contextHistory.shift();
      this.metricsHistory.shift();
    }
    
    this.contextHistory.push(context);
    this.metricsHistory.push(metrics);
    
    LoggingUtils.logInfo(`[SymbolicPatternAnalyzer] Recorded cycle data (history size: ${this.contextHistory.length})`);
  }
  
  /**
   * Detects symbolic drift between consecutive contexts
   */
  private detectSymbolicDrift(): EmergentPattern | null {
    if (this.contextHistory.length < 2) return null;
    
    const current = this.contextHistory[this.contextHistory.length - 1];
    const previous = this.contextHistory[this.contextHistory.length - 2];
    
    // Basic detection via direct difference
    // In production: use embedding distance or more sophisticated measures
    if (current !== previous) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'symbolic_drift',
        description: 'Symbolic drift detected: significant context change between cycles',
        confidence: 0.85,
        scientificFoundation: 'Neural Darwinism (Edelman) & Embodied Mind (Varela)',
        affectedMetrics: currentMetrics,
        detectionTimestamp: new Date().toISOString()
      };
    }
    
    return null;
  }
  
  /**
   * Detects contradiction loops (persistent high contradiction)
   */
  private detectContradictionLoop(
    threshold: number = 0.7,
    minConsecutive: number = 3
  ): EmergentPattern | null {
    if (this.metricsHistory.length < minConsecutive) return null;
    
    const recentMetrics = this.metricsHistory.slice(-minConsecutive);
    const allHighContradiction = recentMetrics.every(m => 
      (m.contradictionScore ?? 0) > threshold
    );
    
    if (allHighContradiction) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'contradiction_loop',
        description: 'Contradiction loop detected: persistent high contradiction across cycles',
        confidence: 0.9,
        scientificFoundation: 'Cognitive Dissonance Theory (Festinger)',
        affectedMetrics: currentMetrics,
        detectionTimestamp: new Date().toISOString()
      };
    }
    
    return null;
  }
  
  /**
   * Detects narrative buildup (increasing coherence)
   */
  private detectNarrativeBuildup(
    minConsecutive: number = 3
  ): EmergentPattern | null {
    if (this.metricsHistory.length < minConsecutive) return null;
    
    const recentMetrics = this.metricsHistory.slice(-minConsecutive);
    let isIncreasing = true;
    
    for (let i = 1; i < recentMetrics.length; i++) {
      const current = recentMetrics[i].coherenceScore ?? 0;
      const previous = recentMetrics[i-1].coherenceScore ?? 0;
      if (current <= previous) {
        isIncreasing = false;
        break;
      }
    }
    
    if (isIncreasing) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'narrative_buildup',
        description: 'Narrative buildup detected: increasing coherence across cycles',
        confidence: 0.8,
        scientificFoundation: 'Acts of Meaning (Bruner)',
        affectedMetrics: currentMetrics,
        detectionTimestamp: new Date().toISOString()
      };
    }
    
    return null;
  }
  
  /**
   * Detects phase interference patterns
   */
  private detectPhaseInterference(): EmergentPattern | null {
    // Need at least 3 cycles with phase data to detect interference
    if (this.metricsHistory.length < 3) return null;
    
    const recentMetrics = this.metricsHistory.slice(-3);
    const hasPhaseData = recentMetrics.every(m => m.phaseAngle !== undefined);
    
    if (!hasPhaseData) return null;
    
    // Simplified detection of interference patterns
    const phases = recentMetrics.map(m => m.phaseAngle!);
    const phaseDeltas = [
      Math.abs(phases[1] - phases[0]), 
      Math.abs(phases[2] - phases[1])
    ];
    
    // Detect regular oscillation pattern
    const hasRegularPattern = Math.abs(phaseDeltas[1] - phaseDeltas[0]) < 0.15;
    
    if (hasRegularPattern) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'phase_interference',
        description: 'Phase interference detected: quantum-like oscillatory pattern',
        confidence: 0.7,
        scientificFoundation: 'Orchestrated Objective Reduction (Penrose/Hameroff)',
        affectedMetrics: currentMetrics,
        detectionTimestamp: new Date().toISOString()
      };
    }
    
    return null;
  }
  
  /**
   * Main analysis method to detect all emergent patterns
   */
  public analyzePatterns(): EmergentPattern[] {
    const patterns: EmergentPattern[] = [];
    
    try {
      // Run all detectors and collect non-null results
      const drift = this.detectSymbolicDrift();
      if (drift) patterns.push(drift);
      
      const contradiction = this.detectContradictionLoop();
      if (contradiction) patterns.push(contradiction);
      
      const narrative = this.detectNarrativeBuildup();
      if (narrative) patterns.push(narrative);
      
      const phase = this.detectPhaseInterference();
      if (phase) patterns.push(phase);
      
      if (patterns.length > 0) {
        LoggingUtils.logInfo(`[SymbolicPatternAnalyzer] Detected ${patterns.length} emergent patterns`);
      }
      
      return patterns;
    } catch (error) {
      LoggingUtils.logError(`[SymbolicPatternAnalyzer] Error analyzing patterns: ${error}`);
      return [];
    }
  }
  
  /**
   * Converte padrões emergentes detectados em formato de string para logging
   * e para incorporação nas propriedades emergentes do sistema.
   * 
   * @param patterns Lista de padrões emergentes detectados
   * @returns Array de strings descritivas dos padrões
   */
  public formatPatterns(patterns: EmergentPattern[]): string[] {
    return patterns.map(pattern => {
      const confidencePct = Math.round(pattern.confidence * 100);
      return `${pattern.type.toUpperCase()}: ${pattern.description} (${confidencePct}% confidence)`;
    });
  }
  
  /**
   * Clears all stored history (useful for session resets)
   */
  public clearHistory(): void {
    this.contextHistory = [];
    this.metricsHistory = [];
    LoggingUtils.logInfo('[SymbolicPatternAnalyzer] History cleared');
  }
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TemporaryContextEdgeCases.test.ts
// Tests for edge cases of temporary context optimization

import { IPersistenceService } from "../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../interfaces/openai/IEmbeddingService";
import {
  SpeakerTranscription
} from "../interfaces/transcription/TranscriptionTypes";
import { MemoryContextBuilder } from "../services/memory/MemoryContextBuilder";
import { BatchTranscriptionProcessor } from "../services/transcription/BatchTranscriptionProcessor";
import { TranscriptionContextManager } from "../services/transcription/TranscriptionContextManager";
import { TranscriptionFormatter } from "../services/transcription/TranscriptionFormatter";

describe("TemporaryContextEdgeCases", () => {
  let memoryContextBuilder: MemoryContextBuilder;
  let contextManager: TranscriptionContextManager;
  
  // Query tracking for tests
  const queryTracker = {
    calls: 0,
    failNext: false,
    delayNext: false,
    reset() {
      this.calls = 0;
      this.failNext = false;
      this.delayNext = false;
    }
  };
  
  // Mocks
  const mockEmbeddingService: IEmbeddingService = {
    isInitialized: jest.fn().mockReturnValue(true),
    createEmbedding: jest.fn().mockImplementation((text: string) => {
      return Promise.resolve([text.length, text.charCodeAt(0) || 0]);
    }),
    initialize: jest.fn().mockResolvedValue(true)
  };

  const mockPersistenceService: IPersistenceService = {
    saveToPinecone: jest.fn().mockResolvedValue(undefined),
    isAvailable: jest.fn().mockReturnValue(true),
    saveInteraction: jest.fn().mockResolvedValue(undefined),
    createVectorEntry: jest.fn().mockReturnValue({}),
    queryMemory: jest.fn().mockImplementation(async (embedding) => {
      // Track calls
      queryTracker.calls++;
      
      // Simulate failure if configured
      if (queryTracker.failNext) {
        queryTracker.failNext = false;
        throw new Error("Simulation failure");
      }
      
      // Simulate delay if configured
      if (queryTracker.delayNext) {
        queryTracker.delayNext = false;
        await new Promise(resolve => setTimeout(resolve, 100));
      }
      
      return `Memory for embedding [${embedding.join(', ')}]`;
    })
  };
  
  beforeEach(() => {
    // Reset mocks and state
    jest.clearAllMocks();
    queryTracker.reset();
    
    // Reset the singleton
    contextManager = TranscriptionContextManager.getInstance();
    contextManager.clearTemporaryContext();
    
    // Create a clean instance for each test
    const formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);
    memoryContextBuilder = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    // Reset the builder state
    memoryContextBuilder.resetAll();
  });
  
  // Basic data used in tests
  const baseTranscriptions: SpeakerTranscription[] = [
    { speaker: "User", text: "Basic test", timestamp: "2023-01-01T10:00:00Z" }
  ];
  
  // ======== GROUP 1: String Context Manipulation ========
  
  test("Should differentiate strings with small variations (spaces, formatting)", async () => {
    // First context
    const context1 = "Instructions important";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context1
    );
    expect(queryTracker.calls).toBe(2); // One for the context, one for the transcription
    queryTracker.reset();
    
    // Same context with extra space
    const context2 = "Instructions  important";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context2
    );
    
    // Should consider different and make a new query
    expect(queryTracker.calls).toBe(2);
    
    // POSSIBLE IMPROVEMENT: Implement string normalization to avoid this redundant query
  });
  
  test("Should correctly handle empty strings and undefined", async () => {
    // Define initial context
    const initialContext = "Context initial";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialContext
    );
    queryTracker.reset();
    
    // Context undefined - should maintain the previous context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), undefined
    );
    
    // Should not query for the temporary context, only for transcription
    expect(queryTracker.calls).toBe(1);
    queryTracker.reset();
    
    // Empty string - should be treated as a new context
    // DISCOVERED BEHAVIOR: The current implementation does not query for an empty string,
    // considering it different, but not valid for querying
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), ""
    );
    
    // Verify current behavior: only queries for transcription
    expect(queryTracker.calls).toBe(1);
    
    // NOTE: Here we identify a specific behavior - the empty string is treated
    // as a new context (different from the previous one), but not valid for querying
    // the Pinecone. This is an appropriate defensive behavior.
  });
  
  // ======== GROUP 2: Concurrency and Timing ========
  
  test("Multiple queries in rapid succession", async () => {
    // Delay the first query
    queryTracker.delayNext = true;
    
    // Start the first query (which will be delayed)
    const firstPromise = memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), "Contexto com atraso"
    );
    
    // Without waiting for the first to finish, start the second query with the same context
    const secondPromise = memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), "Contexto com atraso"
    );
    
    // Wait for both queries
    await Promise.all([firstPromise, secondPromise]);
    
    // Expected behavior: still makes both queries, since the second starts before
    // the first finishes and updates the lastQueriedTemporaryContext
    expect(queryTracker.calls).toBeGreaterThan(2);
    
    // POSSIBLE IMPROVEMENT: Implement a lock or pending queries mechanism
  });
  
  // ======== GROUP 3: Connection Failures and Errors ========
  
  test("Failure in Pinecone query", async () => {
    // Configure the next query to fail
    queryTracker.failNext = true;
    
    // We discovered that the current implementation handles the error internally
    // and does not propagate the exception to the caller.
    let errorWasThrown = false;
    try {
      // Try to query with a context
      await memoryContextBuilder.fetchContextualMemory(
        baseTranscriptions, [], new Set(["User"]), "Contexto que vai falhar"
      );
    } catch (error) {
      // If an error was thrown, mark that it occurred
      errorWasThrown = true;
    }
    
    // Verify that the error was not propagated (it is handled internally)
    expect(errorWasThrown).toBe(false);
    
    // Reset the fail flag
    queryTracker.failNext = false;
    queryTracker.reset();
    
    // The current implementation, despite handling the error internally,
    // updates the lastQueriedTemporaryContext even for failed queries.
    
    // Try again with the same context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), "Contexto que vai falhar"
    );
    
    // Should not query the Pinecone for the same temporary context
    expect(queryTracker.calls).toBeGreaterThanOrEqual(1); // Pelo menos a consulta para a transcrição
    
    // SUGGESTED IMPROVEMENT: Do not mark the context as queried if the query fails,
    // allowing a new attempt on the next one
  });
  
  // ======== GROUP 4: Resource Management ========
  
  test("Long contexts are handled appropriately", async () => {
    // Create an extremely long context
    const longContext = "a".repeat(10000);
    
    // Verify if the system can process without error
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), longContext
    );
    
    // If it got here, the system processed successfully
    expect(queryTracker.calls).toBe(2);
    
    // SUGGESTED IMPROVEMENT: Implement a limit for context size
  });
  
  test("Context reset is handled appropriately", async () => {
    // Consultar com um contexto
    const context = "Contexto para teste de reset";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context
    );
    queryTracker.reset();
    
    // Reset completely
    memoryContextBuilder.resetAll();
    
    // Consult again with the same context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context
    );
    
    // Should consult again, since the lastQueriedTemporaryContext was cleared
    expect(queryTracker.calls).toBe(2);
  });
  
  // ======== GROUP 5: Complex Scenarios ========
  
  test("Minor changes in long contexts generate complete queries", async () => {
    // Long initial context
    const baseContextLong = "This is a long context that should generate complete queries.";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), baseContextLong
    );
    queryTracker.reset();
    
    // Same context with minor change at the end
    const slightlyChangedContext = baseContextLong + ".";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), slightlyChangedContext
    );
    
    // Should consult again, even with minor change
    expect(queryTracker.calls).toBe(2);
    
    // POSSIBLE IMPROVEMENT: Implement semantic similarity detection
  });
  
  // ======== GROUP 6: Special Scenarios ========
  
  test("Undefined context followed by empty string", async () => {
    // Define initial context
    const initialContext = "Initial context";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialContext
    );
    queryTracker.reset();
    
    // Consult with undefined (should maintain the previous context)
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), undefined
    );
    expect(queryTracker.calls).toBe(1); // Only for transcription
    queryTracker.reset();
    
    // Consult with empty string (should be treated as new context)
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), ""
    );
    
    // Should consult again, even with minor change
    expect(queryTracker.calls).toBe(1);
  });
  
  test("Interações entre setTemporaryContext e fetchContextualMemory", async () => {
    // Define the context directly in contextManager
    contextManager.setTemporaryContext("Contexto definido diretamente");
    
    // Consult passando undefined (should use the context from contextManager)
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), undefined
    );
    
    // Should consult for the context from contextManager and for transcription
    expect(queryTracker.calls).toBe(2);
  });
  
  // ======== GROUP 7: Specific Tests for clearTemporaryContext ========
  
  test("clearTemporaryContext limpa todos os aspectos do contexto", async () => {
    // 1. Prepare the context and query Pinecone
    const testContext = "Contexto para teste de limpeza";
    
    // Define the context directly in contextManager
    contextManager.setTemporaryContext(testContext);
    
    // Consultar o Pinecone para este contexto
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // Ensure context and memory are stored
    expect(contextManager.getTemporaryContext()).toBe(testContext);
    expect(contextManager.getTemporaryContextMemory()).not.toBe("");
    
    // 2. Clear the context using clearTemporaryContext
    memoryContextBuilder.resetTemporaryContext(); // Chama contextManager.clearTemporaryContext()
    
    // 3. Verify that all aspects were cleared
    expect(contextManager.getTemporaryContext()).toBe("");
    expect(contextManager.getTemporaryContextMemory()).toBe("");
    
    // 4. Reset the call counter
    queryTracker.reset();
    
    // 5. Consult again with the same context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 6. Should execute a new Pinecone query for the temporary context
    expect(queryTracker.calls).toBe(2); // One for the context and one for transcription
  });
  
  test("clearTemporaryContext limpa o último contexto consultado", async () => {
    // 1. Define and query a context
    const firstContext = "Primeiro contexto para teste";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), firstContext
    );
    queryTracker.reset();
    
    // 2. Consult again with the same context (should not query Pinecone again)
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), firstContext
    );
    // Verify that it did not query for the context, only for the transcription
    expect(queryTracker.calls).toBe(1);
    queryTracker.reset();
    
    // 3. Clear the context
    memoryContextBuilder.resetTemporaryContext();
    
    // 4. Consult again with the same context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), firstContext
    );
    
    // 5. Should query Pinecone again, since the history was cleared
    expect(queryTracker.calls).toBe(2);
  });
  
  test("clearTemporaryContext vs resetAll", async () => {
    // 1. Define context and query
    const testContext = "Context for method comparison";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 2. Update the snapshot tracker with a fake message
    memoryContextBuilder["snapshotTracker"].updateSnapshot("Test message for snapshot");
    
    // 3. Fork the test: case with clearTemporaryContext
    const contextManager1 = TranscriptionContextManager.getInstance();
    contextManager1.clearTemporaryContext();
    
    // 4. Verify that the context was cleared but the snapshot remains
    expect(contextManager1.getTemporaryContext()).toBe("");
    // The snapshot should not have been cleared by clearTemporaryContext
    expect(memoryContextBuilder["snapshotTracker"].isAllContentSent("Test message for snapshot"))
      .toBe(true);
    
    // 5. Reset state and redo for resetAll
    memoryContextBuilder.resetAll();
    
    // 6. Redefine and update the context and snapshot
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    memoryContextBuilder["snapshotTracker"].updateSnapshot("Test message for snapshot");
    
    // 7. Use resetAll
    memoryContextBuilder.resetAll();
    
    // 8. Verify that both context and snapshot were cleared
    expect(contextManager.getTemporaryContext()).toBe("");
    // The snapshot should have been cleared by resetAll
    expect(memoryContextBuilder["snapshotTracker"].isAllContentSent("Test message for snapshot"))
      .toBe(false);
  });
  
  test("Preservation of context after clearTemporaryContext", async () => {
    // 1. Define and consult temporary context
    const initialContext = "Initial context for test";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialContext
    );
    
    // 2. Clear temporary context
    memoryContextBuilder.resetTemporaryContext();
    
    // 3. Define new context without querying
    const newContext = "New context after clearing";
    contextManager.setTemporaryContext(newContext);
    
    // 4. Verify that the new context is defined
    expect(contextManager.getTemporaryContext()).toBe(newContext);
    
    // 5. Verify that the memory is empty (not queried)
    expect(contextManager.getTemporaryContextMemory()).toBe("");
    
    // 6. Consult again with undefined (should use the new context defined)
    queryTracker.reset();
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), undefined
    );
    
    // 7. Should query for the new temporary context
    expect(queryTracker.calls).toBe(2); // One for context, one for transcription
  });
  
  test("clearTemporaryContext clears explicitly lastQueriedTemporaryContext", async () => {
    // 1. Create spy to observe the hasTemporaryContextChanged method 
    const hasChangedSpy = jest.spyOn(contextManager, 'hasTemporaryContextChanged');
    
    // 2. Define and query a context
    const testContext = "Context for verification of lastQueriedTemporaryContext";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 3. Reset spy and query the same context 
    hasChangedSpy.mockClear();
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 4. hasTemporaryContextChanged should have been called with "false" as the result
    // (indicating that the context did NOT change compared to lastQueriedTemporaryContext)
    expect(hasChangedSpy).toHaveBeenCalled();
    expect(hasChangedSpy.mock.results[0].value).toBe(false);
    
    // 5. Clear the context
    memoryContextBuilder.resetTemporaryContext();
    
    // 6. Reset spy and query the same context again 
    hasChangedSpy.mockClear();
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 7. After clearing, hasTemporaryContextChanged should return true
    // (indicating that the context changed, since lastQueriedTemporaryContext was cleared)
    expect(hasChangedSpy).toHaveBeenCalled();
    expect(hasChangedSpy.mock.results[0].value).toBe(true);
    
    // 8. Clear spy
    hasChangedSpy.mockRestore();
  });
  
  test("clearTemporaryContext affects all instances of MemoryContextBuilder", async () => {
    // 1. Create a second instance of MemoryContextBuilder
    const formatter2 = new TranscriptionFormatter();
    const processor2 = new BatchTranscriptionProcessor(formatter2);
    const memoryContextBuilder2 = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter2,
      processor2
    );
    
    // 2. Define and query a context in the first instance
    const sharedContext = "Shared context between instances";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), sharedContext
    );
    
    // 3. Reset the counter and query the same context in the SECOND instance
    queryTracker.reset();
    await memoryContextBuilder2.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), sharedContext
    );
    
    // 4. Should not query Pinecone for the temporary context (only transcription)
    expect(queryTracker.calls).toBe(1);
    
    // 5. Clear context using first instance
    memoryContextBuilder.resetTemporaryContext();
    
    // 6. Reset the counter and query in the second instance 
    queryTracker.reset();
    await memoryContextBuilder2.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), sharedContext
    );
    
    // 7. Should query Pinecone again, since the context was globally cleared
    expect(queryTracker.calls).toBe(2);
  });
  
  test("Interaction between clearTemporaryContext and updateLastQueriedTemporaryContext", async () => {
    // 1. Create spies for both methods
    const clearSpy = jest.spyOn(contextManager, 'clearTemporaryContext');
    const updateSpy = jest.spyOn(contextManager, 'updateLastQueriedTemporaryContext');
    
    // 2. Define and query a context (should call updateLastQueriedTemporaryContext)
    const testContext = "Context for interaction test";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 3. Verify that updateLastQueriedTemporaryContext was called
    expect(updateSpy).toHaveBeenCalledWith(testContext);
    
    // 4. Clear the context via resetTemporaryContext
    memoryContextBuilder.resetTemporaryContext();
    
    // 5. Verify that clearTemporaryContext was called
    expect(clearSpy).toHaveBeenCalled();
    
    // 6. Reset spies
    clearSpy.mockClear();
    updateSpy.mockClear();
    
    // 7. Verify if updateLastQueriedTemporaryContext preserves its state after each call
    let previousCallCount = 0;
    
    // First query to a new context
    const context1 = "Context 1 for sequence";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context1
    );
    
    // Verify that updateLastQueriedTemporaryContext was called
    expect(updateSpy.mock.calls.length).toBeGreaterThan(previousCallCount);
    previousCallCount = updateSpy.mock.calls.length;
    
    // Second query with same context should not call updateLastQueriedTemporaryContext
    // with the same value (comparison is made internally)
    queryTracker.reset();
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context1
    );
    
    // Number of calls should be equal to the previous (did not call with the same value)
    expect(queryTracker.calls).toBe(1); // Only transcription query
    
    // 8. Test sequence of operations
    memoryContextBuilder.resetTemporaryContext();
    clearSpy.mockClear();
    updateSpy.mockClear();
    
    // Query a context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), "Final context"
    );
    expect(updateSpy).toHaveBeenCalled();
    
    // Clear again
    memoryContextBuilder.resetTemporaryContext();
    expect(clearSpy).toHaveBeenCalled();
    
    // Verify final state
    expect(contextManager.getTemporaryContext()).toBe("");
    expect(contextManager.getTemporaryContextMemory()).toBe("");
    
    // Restore spies
    clearSpy.mockRestore();
    updateSpy.mockRestore();
  });
  
  test("hasTemporaryContextChanged handles empty strings correctly", async () => {
    // 1. Create spy to directly observe the hasTemporaryContextChanged method
    const hasChangedSpy = jest.spyOn(contextManager, 'hasTemporaryContextChanged');
    
    // 2. Verify initial behavior with empty string
    const emptyResult = contextManager.hasTemporaryContextChanged("");
    
    // Empty string should always return false - it is not considered a valid context
    console.log("Empty string compared with initial state:", emptyResult);
    expect(emptyResult).toBe(false);
    
    // 3. Define and query a non-empty context
    const nonEmptyContext = "Non-empty context";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), nonEmptyContext
    );
    
    // 4. Reset spy
    hasChangedSpy.mockClear();
    
    // 5. Verify hasTemporaryContextChanged with empty string
    const emptyAfterNonEmpty = contextManager.hasTemporaryContextChanged("");
    console.log("Empty string compared after non-empty context:", emptyAfterNonEmpty);
    
    // An empty string should never be considered a new context, even after a non-empty context
    // Should return false to avoid unnecessary queries
    expect(emptyAfterNonEmpty).toBe(false);
    
    // 6. Test another scenario: empty string followed by empty string
    // Explicitly define an empty string
    contextManager.setTemporaryContext("");
    
    // Force update of the last queried temporary context
    contextManager.updateLastQueriedTemporaryContext("");
    
    // Verify hasTemporaryContextChanged with empty string again
    hasChangedSpy.mockClear();
    const emptyAfterEmpty = contextManager.hasTemporaryContextChanged("");
    console.log("Empty string compared after empty context was set:", emptyAfterEmpty);
    
    // An empty string compared with the last empty context should be considered equal (did not change)
    expect(emptyAfterEmpty).toBe(false);
    
    // 7. Clear and restore
    contextManager.clearTemporaryContext();
    hasChangedSpy.mockRestore();
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TemporaryContextQueryBehavior.test.ts
// Tests to verify Pinecone query behavior when temporary context changes

import { IPersistenceService } from "../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../interfaces/openai/IEmbeddingService";
import { SpeakerTranscription } from "../interfaces/transcription/TranscriptionTypes";
import { MemoryContextBuilder } from "../services/memory/MemoryContextBuilder";
import { BatchTranscriptionProcessor } from "../services/transcription/BatchTranscriptionProcessor";
import { TranscriptionContextManager } from "../services/transcription/TranscriptionContextManager";
import { TranscriptionFormatter } from "../services/transcription/TranscriptionFormatter";

describe("TemporaryContextQueryBehavior", () => {
  let memoryContextBuilder: MemoryContextBuilder;
  let contextManager: TranscriptionContextManager;
  let mockCreateEmbedding: jest.Mock<Promise<number[]>, [string]>;
  
  // Query tracking for tests
  const queryTracker = {
    calls: 0,
    contexts: [] as string[],
    reset() {
      this.calls = 0;
      this.contexts = [];
    }
  };
  
  // Mocks
  const mockEmbeddingService: IEmbeddingService = {
    isInitialized: jest.fn().mockReturnValue(true),
    createEmbedding: jest.fn(),
    initialize: jest.fn().mockResolvedValue(true)
  };

  const mockPersistenceService: IPersistenceService = {
    saveToPinecone: jest.fn().mockResolvedValue(undefined),

    isAvailable: jest.fn().mockReturnValue(true),
    saveInteraction: jest.fn().mockResolvedValue(undefined),
    createVectorEntry: jest.fn().mockReturnValue({}),
    queryMemory: jest.fn().mockResolvedValue("")
  };
  
  // Mock for queryMemory that tracks queries
  mockPersistenceService.queryMemory = jest.fn().mockImplementation(async (embedding: number[], /* eslint-disable-next-line @typescript-eslint/no-unused-vars */ _topK?: number, /* eslint-disable-next-line @typescript-eslint/no-unused-vars */ _keywords?: string[]) => {
    // Track calls
    queryTracker.calls++;
    queryTracker.contexts.push(embedding.toString());
    
    return `Memory for embedding [${embedding.join(', ')}]`;
  });
  
  beforeEach(() => {
    // Reset mocks and state
    jest.clearAllMocks();
    queryTracker.reset();

    mockCreateEmbedding = (mockEmbeddingService.createEmbedding as jest.Mock).mockImplementation((text: string) => {
      return Promise.resolve([text.length, text.charCodeAt(0) || 0]);
    });
    
    // Reset singleton
    contextManager = TranscriptionContextManager.getInstance();
    contextManager.clearTemporaryContext();
    
    // Create clean instance for each test
    const formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);
    memoryContextBuilder = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    // Reset builder state
    memoryContextBuilder.resetAll();
  });
  
  // Basic data used in tests
  const baseTranscriptions: SpeakerTranscription[] = [
    { speaker: "User", text: "Basic test", timestamp: "2023-01-01T10:00:00Z" }
  ];
  
  test("Should make new Pinecone query when instructions are modified", async () => {
    // 1. Define initial instructions
    const initialInstructions = "Initial instructions for test";
    
    // 2. First query with initial instructions
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialInstructions
    );
    
    // Should make new Pinecone query when instructions are modified
    expect(queryTracker.calls).toBe(2); // One for temporary context, one for transcription
    expect(mockCreateEmbedding).toHaveBeenCalledWith(initialInstructions);
    queryTracker.reset();
    
    // 3. Second query with the same instructions
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialInstructions
    );
    
    // Should not make new Pinecone query when instructions are the same
    expect(queryTracker.calls).toBe(1); // Only for transcription
    queryTracker.reset();
    
    // 4. Third query with modified instructions
    const modifiedInstructions = "Modified instructions for test";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), modifiedInstructions
    );
    
    // Should make new Pinecone query when instructions are modified
    expect(queryTracker.calls).toBe(2); // New query for modified context + transcription
    expect(mockCreateEmbedding).toHaveBeenCalledWith(modifiedInstructions);
    queryTracker.reset();
  });
  
  test("Small modifications in instructions should generate new query", async () => {
    // 1. Initial instructions
    const baseInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), baseInstructions
    );
    expect(queryTracker.calls).toBe(2);
    expect(mockCreateEmbedding).toHaveBeenCalledWith(baseInstructions);
    queryTracker.reset();
    mockCreateEmbedding.mockClear();
    
    // 2. Instructions with small modification (additional punctuation)
    const slightlyModifiedInstructions = "Detailed instructions for the assistant to follow.";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), slightlyModifiedInstructions
    );
    
    // Should detect the change and make new query
    expect(queryTracker.calls).toBe(2);
    expect(mockCreateEmbedding).toHaveBeenCalledWith(slightlyModifiedInstructions);
    queryTracker.reset();
  });
  
  test("Format modifications (extra spaces) should generate new query", async () => {
    // 1. Initial instructions
    const baseInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), baseInstructions
    );
    expect(queryTracker.calls).toBe(2);
    expect(mockCreateEmbedding).toHaveBeenCalledWith(baseInstructions);
    queryTracker.reset();
    mockCreateEmbedding.mockClear();
    
    // 2. Instructions with extra spaces
    const formattedInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), formattedInstructions
    );
    
    // Should detect the change and make new query
    expect(queryTracker.calls).toBe(1);
    expect(mockCreateEmbedding).toHaveBeenCalledWith("Basic test");
    queryTracker.reset();
  });
  
  test("Embeddings should be different for different instructions", async () => {
    // 1. Initial instructions and their embedding
    const initialInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialInstructions
    );
    
    const initialEmbeddingCall = mockCreateEmbedding.mock.calls[0][0];
    expect(initialEmbeddingCall).toBe(initialInstructions);
    
    mockCreateEmbedding.mockClear();
    queryTracker.reset();
    
    // 2. Different instructions and their embedding
    const updatedInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), updatedInstructions
    );
    
    const updatedEmbeddingCall = mockCreateEmbedding.mock.calls[0][0];
    expect(updatedEmbeddingCall).toBe("Basic test");
    
    // Should verify that the texts passed to createEmbedding are different
    expect(initialEmbeddingCall).not.toBe(updatedEmbeddingCall);
  });
  
  test("Should preserve memory when the same query is made", async () => {
    // 1. Initial query
    const instructions = "Instruções para preservação de memória";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), instructions
    );
    
    // 2. Store the memory of the first query
    const initialMemory = contextManager.getTemporaryContextMemory();
    expect(mockCreateEmbedding).toHaveBeenCalledWith(instructions);
    mockCreateEmbedding.mockClear();
    
    // 2. Second query with the same instructions
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), instructions
    );
    
    // Should not call createEmbedding for the context again
    expect(mockCreateEmbedding).not.toHaveBeenCalledWith(instructions);
    
    // Should verify that the memory remains the same
    const secondMemory = contextManager.getTemporaryContextMemory();
    expect(secondMemory).toBe(initialMemory);
  });
  
  test("Dynamic context that changes between calls should generate new query", async () => {
    // 1. Create a dynamic context object (as it would be in production)
    const dynamicContext = {
      instructions: "Dynamic instructions version original",
      get value() { return this.instructions; }
    };
    
    // 2. First query with original value
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), dynamicContext.value
    );
    
    // Should verify that createEmbedding was called with the original context
    expect(mockCreateEmbedding).toHaveBeenCalledWith(dynamicContext.value);
    const originalEmbeddingCall = mockCreateEmbedding.mock.calls[0][0];
    mockCreateEmbedding.mockClear();
    queryTracker.reset();
    
    // 3. Modify the dynamic context object after the first call
    dynamicContext.instructions = "Dynamic instructions version modified";
    
    // 4. Second query with the modified object
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), dynamicContext.value
    );
    
    // Should verify that createEmbedding was called with the modified context
    expect(mockCreateEmbedding).toHaveBeenCalledWith(dynamicContext.value);
    const modifiedEmbeddingCall = mockCreateEmbedding.mock.calls[0][0];
    
    // Should verify that the contexts passed to createEmbedding are different
    expect(originalEmbeddingCall).not.toBe(modifiedEmbeddingCall);
    
    // Should detect the change and make new query
    expect(queryTracker.calls).toBe(2);
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionContextManager.test.ts
// Testes para o TranscriptionContextManager

import { TranscriptionContextManager } from "../services/transcription/TranscriptionContextManager";

describe("TranscriptionContextManager", () => {
  beforeEach(() => {
    // Clear the singleton between tests
    const instance = TranscriptionContextManager.getInstance();
    instance.clearTemporaryContext();
  });
  
  test("Should maintain the singleton across the entire application", () => {
    // Get two instances in different locations
    const instance1 = TranscriptionContextManager.getInstance();
    const instance2 = TranscriptionContextManager.getInstance();
    
    // Verify if they are the same instance
    expect(instance1).toBe(instance2);
  });
  
  test("Should persist the context between multiple calls", () => {
    const instance = TranscriptionContextManager.getInstance();
    
    // Set the context
    instance.setTemporaryContext("Test instructions");
    
    // Get a new instance (which should be the same since it's a singleton)
    const anotherInstance = TranscriptionContextManager.getInstance();
    
    // Verify that the context is present in the new instance
    expect(anotherInstance.getTemporaryContext()).toBe("Test instructions");
  });
  
  test("Should not clear the context when undefined is passed", () => {
    const instance = TranscriptionContextManager.getInstance();
    
    // Set the initial context
    instance.setTemporaryContext("Initial context");
    
    // Try to update with undefined
    instance.setTemporaryContext(undefined);
    
    // Verify that the context remains
    expect(instance.getTemporaryContext()).toBe("Initial context");
  });
  
  test("Should replace the previous context when an empty string is passed", () => {
    const instance = TranscriptionContextManager.getInstance();
    
    // Set the initial context
    instance.setTemporaryContext("Initial context");
    
    // Update with empty string (now treated as a valid new context)
    instance.setTemporaryContext("");
    
    // Verify that the context was replaced with an empty string
    expect(instance.getTemporaryContext()).toBe("");
    // hasTemporaryContext should return false for empty string
    expect(instance.hasTemporaryContext()).toBe(false);
  });
  
  test("hasTemporaryContext should return correctly", () => {
    const instance = TranscriptionContextManager.getInstance();
    
    // Initially should not have context
    expect(instance.hasTemporaryContext()).toBe(false);
    
    // Set the context
    instance.setTemporaryContext("Test context");
    
    // Now should have context
    expect(instance.hasTemporaryContext()).toBe(true);
    
    // Clear and verify again
    instance.clearTemporaryContext();
    expect(instance.hasTemporaryContext()).toBe(false);
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionContextPersistence.test.ts
// Tests for the persistence of the temporary context using the TranscriptionContextManager

import { IPersistenceService } from "../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../interfaces/openai/IEmbeddingService";
import {
  Message,
  SpeakerMemoryResults,
  SpeakerTranscription
} from "../interfaces/transcription/TranscriptionTypes";
import { MemoryContextBuilder } from "../services/memory/MemoryContextBuilder";
import { BatchTranscriptionProcessor } from "../services/transcription/BatchTranscriptionProcessor";
import { TranscriptionContextManager } from "../services/transcription/TranscriptionContextManager";
import { TranscriptionFormatter } from "../services/transcription/TranscriptionFormatter";

describe("TranscriptionContextPersistence", () => {
  let memoryContextBuilder1: MemoryContextBuilder;
  let memoryContextBuilder2: MemoryContextBuilder;
  let contextManager: TranscriptionContextManager;
  
  // Object to track Pinecone queries
  const pineconeQueries = {
    temporaryContext: 0,
    userContext: 0,
    reset() {
      this.temporaryContext = 0;
      this.userContext = 0;
    }
  };
  
  // Mocks
  const mockEmbeddingService: IEmbeddingService = {
    isInitialized: jest.fn().mockReturnValue(true),
    // Keep track of inputs to track specific queries
    createEmbedding: jest.fn().mockImplementation((text: string) => {
      // Create a distinct embedding based on the content
      return [text.length, text.length * 2];
    }),
    initialize: jest.fn().mockResolvedValue(true)
  };

  const mockPersistenceService: IPersistenceService = {
    saveToPinecone: jest.fn().mockResolvedValue(undefined),

    isAvailable: jest.fn().mockReturnValue(true),
    saveInteraction: jest.fn().mockResolvedValue(undefined),
    createVectorEntry: jest.fn().mockReturnValue({}),
    queryMemory: jest.fn().mockResolvedValue("")
  };
  
  // Add a custom property for the queryMemory function used in MemoryContextBuilder
  mockPersistenceService.queryMemory = jest.fn().mockImplementation((embedding: number[], /* eslint-disable-next-line @typescript-eslint/no-unused-vars */ _topK?: number, /* eslint-disable-next-line @typescript-eslint/no-unused-vars */ _keywords?: string[]) => {
    // Track specific queries based on the embedding
    if (embedding && embedding.length === 2) {
      // Identify the type of query based on the embedding
      if (embedding[0] === 22 && embedding[1] === 44) {
        pineconeQueries.temporaryContext++; // "Instructions important" has 22 characters
      } else if (embedding[0] === 17 && embedding[1] === 34) {
        pineconeQueries.userContext++; // "First message" has 17 characters
      } else if (embedding[0] === 27 && embedding[1] === 54) {
        pineconeQueries.temporaryContext++; // "New instructions different" has 27 characters
      }
    }
    return "Pinecone relevant memory";
  });
  
  beforeEach(() => {
    // Reset the mock function calls
    jest.clearAllMocks();
    pineconeQueries.reset();
    
    // Clear the singleton between tests
    contextManager = TranscriptionContextManager.getInstance();
    contextManager.clearTemporaryContext();
    
    // Create two independent instances of MemoryContextBuilder
    const formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);
    memoryContextBuilder1 = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    memoryContextBuilder2 = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    // Reset both builders
    memoryContextBuilder1.resetAll();
    memoryContextBuilder2.resetAll();
  });
  
  test("Temporary context should persist between different MemoryContextBuilder instances", () => {
    // Basic test setup
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "Initial question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];
    
    // Start with temporary context in the first instance
    const temporaryContext = "Instructions important";
    
    // First execution with the first instance
    const firstRun = memoryContextBuilder1.buildMessagesWithContext(
      "Initial question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext,
      undefined
    );
    
    // Verify temporary context in the first execution
    const developerMessages1 = firstRun.filter(m => m.role === "developer");
    const hasTemporaryContext1 = developerMessages1.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext1).toBe(true);
    
    // Second execution with the SECOND instance (different from the first)
    // We don't pass temporaryContext explicitly to verify if it was persisted
    const secondRun = memoryContextBuilder2.buildMessagesWithContext(
      "Second question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, 
      undefined
    );
    
    // Verify if the temporary context was persisted in the second instance
    const developerMessages2 = secondRun.filter(m => m.role === "developer");
    const hasTemporaryContext2 = developerMessages2.some(m => 
      m.content.includes("Instructions important"));
    
    // The context should be present even without being passed explicitly
    expect(hasTemporaryContext2).toBe(true);
  });
  
  test("Resetting temporaryContext should affect all instances", () => {
    // Basic test setup
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "Test question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];
    
    // Define temporary context in the first instance
    const temporaryContext = "Instructions important";
    
    // First execution with context
    const firstRun = memoryContextBuilder1.buildMessagesWithContext(
      "Test question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext,
      undefined
    );
    
    // Verify temporary context in the first execution
    const developerMessages1 = firstRun.filter(m => m.role === "developer");
    const hasTemporaryContext1 = developerMessages1.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext1).toBe(true);
    
    // Reset the temporary context in the SECOND instance
    memoryContextBuilder2.resetTemporaryContext();
    
    // New execution in the first instance
    const secondRun = memoryContextBuilder1.buildMessagesWithContext(
      "Second question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, 
      undefined
    );
    
    // Verify that the context was cleared (should be absent)
    const developerMessages2 = secondRun.filter(m => m.role === "developer");
    const hasTemporaryContext2 = developerMessages2.some(m => 
      m.content.includes("Instructions important"));
    
    // The context SHOULD NOT be present, since it was reset
    expect(hasTemporaryContext2).toBe(false);
  });
  
  test("resetAll should clear both snapshot and temporary context", () => {
    // Basic test setup
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "Initial question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];
    
    // Define temporary context
    const temporaryContext = "Instructions important";
    
    // First execution
    /* eslint-disable-next-line @typescript-eslint/no-unused-vars */
    const firstRun = memoryContextBuilder1.buildMessagesWithContext(
      "Initial question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext,
      undefined
    );
    
    // Reset completo
    memoryContextBuilder1.resetAll();
    
    // Second execution
    const secondRun = memoryContextBuilder2.buildMessagesWithContext(
      "Initial question?", // Same question to test if the snapshot was cleared
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, 
      undefined
    );
    
    // Verify that the temporary context was cleared
    const developerMessages = secondRun.filter(m => m.role === "developer");
    const hasTemporaryContext = developerMessages.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext).toBe(false);
    
    // Verify if the question appears (snapshot was cleared)
    const userMessages = secondRun.filter(m => m.role === "user");
    expect(userMessages.length).toBe(1); // Should have one user message
  });
  
  test("The temporary context memory should persist between calls", async () => {
    // Define temporary context
    const temporaryContext = "Instructions important";
    
    // Mock to simulate memory query results
    const mockMemoryResults: SpeakerMemoryResults = {
      userContext: "",
      speakerContexts: new Map(),
      temporaryContext: "Context memory retrieved from Pinecone"
    };
    
    // Setup básico
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { speaker: "User", text: "First message", timestamp: "2023-01-01T10:00:00Z" }
    ];
    
    // First call with contextManager and memoryResults
    const firstMessages = memoryContextBuilder1.buildMessagesWithContext(
      "First message",
      conversationHistory,
      false,
      transcricoes,
      new Set(["User"]),
      "User",
      temporaryContext,
      mockMemoryResults // Com resultados de memória
    );
    
    // Verify that the temporary context memory is in the messages
    const firstDevMessages = firstMessages.filter(m => m.role === "developer");
    const hasMemoryContext = firstDevMessages.some(m => 
      m.content.includes("Context memory retrieved from Pinecone"));
    expect(hasMemoryContext).toBe(true);
    
    // Second call WITHOUT passing memoryResults
    const secondMessages = memoryContextBuilder2.buildMessagesWithContext(
      "Second message",
      conversationHistory,
      false,
      [...transcricoes, { speaker: "User", text: "Second message", timestamp: "2023-01-01T10:05:00Z" }],
      new Set(["User"]),
      "User",
      undefined, // Não passamos novo contexto (deve manter o anterior)
      undefined  // Não passamos resultados de memória (deve usar o armazenado)
    );
    
    // Verify that the temporary context memory IS STILL in the messages
    const secondDevMessages = secondMessages.filter(m => m.role === "developer");
    const stillHasMemoryContext = secondDevMessages.some(m => 
      m.content.includes("Context memory retrieved from Pinecone"));
    expect(stillHasMemoryContext).toBe(true);
  });
  
  test("Pinecone should only be queried for the temporaryContext when it changes", async () => {
    // Define temporary context
    const temporaryContext = "Instructions important";
    
    // Basic setup
    const transcricoes: SpeakerTranscription[] = [
      { speaker: "User", text: "First message", timestamp: "2023-01-01T10:00:00Z" }
    ];
    
    // First call - should query Pinecone for both
    await memoryContextBuilder1.fetchContextualMemory(
      transcricoes,
      [],
      new Set(["User"]),
      temporaryContext
    );
    
    // Verify specific queries
    expect(pineconeQueries.temporaryContext).toBe(1); 
    expect(pineconeQueries.userContext).toBe(0);      
    
    // Reset counter
    pineconeQueries.reset();
    
    // Second call with the SAME temporary context
    // Should only query for userContext, not for temporaryContext (reuse)
    await memoryContextBuilder2.fetchContextualMemory(
      transcricoes,
      [],
      new Set(["User"]),
      temporaryContext
    );
    
    // Verify specific queries (temporaryContext should not be queried again)
    expect(pineconeQueries.temporaryContext).toBe(0); 
    expect(pineconeQueries.userContext).toBe(0);      
    
    // Reset counter
    pineconeQueries.reset();
    
    // Third call with DIFFERENT temporary context
    const novoContexto = "New instructions";
    await memoryContextBuilder1.fetchContextualMemory(
      transcricoes,
      [],
      new Set(["User"]),
      novoContexto
    );
    
    // Verify specific queries (temporaryContext should not be queried here either)
    expect(pineconeQueries.temporaryContext).toBe(0); 
    expect(pineconeQueries.userContext).toBe(0);      
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/// <reference types="jest" />

import { TranscriptionSnapshotTracker } from "../services/transcription/TranscriptionSnapshotTracker";

describe("TranscriptionSnapshotTracker Basic Functionality", () => {
  let snapshotTracker: TranscriptionSnapshotTracker;

  beforeEach(() => {
    snapshotTracker = new TranscriptionSnapshotTracker();
  });

  test("filterTranscription removes existing content", () => {
    // First, add some content to the tracker
    const initialContent = "First line\nSecond line";
    snapshotTracker.updateSnapshot(initialContent);

    // Now try to filter a message containing both old and new content
    const newContent = "First line\nSecond line\nThird line";
    const filtered = snapshotTracker.filterTranscription(newContent);

    // Should only contain the new line
    expect(filtered).toBe("Third line");
  });

  test("filtering twice returns empty string", () => {
    const content = "Test message";
    
    // First filter should return the content
    const firstFilter = snapshotTracker.filterTranscription(content);
    expect(firstFilter).toBe("Test message");

    // Update the snapshot
    snapshotTracker.updateSnapshot(content);

    // Second filter should return empty
    const secondFilter = snapshotTracker.filterTranscription(content);
    expect(secondFilter).toBe("");
  });

  test("reset clears the snapshot", () => {
    const content = "Test message";
    
    // Add content to the tracker
    snapshotTracker.updateSnapshot(content);
    
    // Filtering should return empty string
    expect(snapshotTracker.filterTranscription(content)).toBe("");
    
    // Reset the tracker
    snapshotTracker.reset();
    
    // Now filtering should return the content again
    expect(snapshotTracker.filterTranscription(content)).toBe("Test message");
  });

  test("normalization handles whitespace and empty lines", () => {
    // Add some content with extra whitespace
    snapshotTracker.updateSnapshot("  Line  with    spaces  \n\n");
    
    // Filter should normalize both strings before comparison
    const filtered = snapshotTracker.filterTranscription("Line with spaces");
    expect(filtered).toBe("");
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IPersistenceService } from "../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../interfaces/openai/IEmbeddingService";
import {
  Message,
  SpeakerMemoryResults,
  SpeakerTranscription
} from "../interfaces/transcription/TranscriptionTypes";
import { MemoryContextBuilder } from "../services/memory/MemoryContextBuilder";
import { BatchTranscriptionProcessor } from "../services/transcription/BatchTranscriptionProcessor";
import { TranscriptionFormatter } from "../services/transcription/TranscriptionFormatter";

// Mocks with proper interfaces
const mockEmbeddingService: IEmbeddingService = {
  isInitialized: jest.fn().mockReturnValue(true),
  createEmbedding: jest.fn().mockResolvedValue([]),
  initialize: jest.fn().mockResolvedValue(true)
};

// Fix persistence service mock to match the interface
const mockPersistenceService: IPersistenceService = {
  saveToPinecone: jest.fn(async () => ({ success: true })), // mock compatible

  isAvailable: jest.fn().mockReturnValue(true),
  saveInteraction: jest.fn().mockResolvedValue(undefined),
  createVectorEntry: jest.fn().mockReturnValue({}),
  queryMemory: jest.fn().mockResolvedValue("")
};

// Add a custom property for the queryMemory function used in MemoryContextBuilder
// This extends the mock object beyond the interface
(mockPersistenceService as any).queryMemory = jest.fn().mockReturnValue("");

describe("TranscriptionSnapshotTracker", () => {
  let memoryContextBuilder: MemoryContextBuilder;
  let formatter: TranscriptionFormatter;

  beforeEach(() => {
    formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);
    
    memoryContextBuilder = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    // Reset the snapshot tracker before each test
    memoryContextBuilder.resetSnapshotTracker();
  });

  test("New message is processed correctly without duplicates or context confusion", () => {
    // Arrange
    const conversationHistory: Message[] = [
      { role: "developer", content: "📦 User relevant memory:\n[Guilherme] Hello, how are you?" },
      // Remove the user message from history - this would be handled by deduplication
      // { role: "user", content: "[Guilherme] Olá, tudo bem?" },
      { role: "assistant", content: "Hello, how are you?" }
    ];

    // Pre-populate the snapshot tracker with the existing transcription
    memoryContextBuilder.resetSnapshotTracker();
    memoryContextBuilder["snapshotTracker"].updateSnapshot("[Guilherme] Hello, how are you?");
    
    const novaMensagem = "[Guilherme] What do you think about this?";
    
    // Create speaker transcriptions that include both the old and new messages
    const transcricoesCompletas: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "Hello, how are you?", 
        timestamp: "2023-01-01T10:00:00Z" 
      },
      { 
        speaker: "Guilherme", 
        text: "What do you think about this?", 
        timestamp: "2023-01-01T10:01:00Z" 
      }
    ];

    const memoryResults: SpeakerMemoryResults = {
      userContext: "[Guilherme] Hello, how are you?",
      speakerContexts: new Map(),
      temporaryContext: ""
    };

    // Act
    const mensagensGeradas = memoryContextBuilder.buildMessagesWithContext(
      novaMensagem,
      conversationHistory,
      false, // sem simplified history
      transcricoesCompletas, // inclui a nova mensagem no fim
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, // sem temporaryContext
      memoryResults
    );

    // Assert
    const ultimaMensagem = mensagensGeradas[mensagensGeradas.length - 1];
    expect(ultimaMensagem.role).toBe("user");
    expect(ultimaMensagem.content).toContain("What do you think about this?");
    expect(ultimaMensagem.content).not.toContain("Hello, how are you?");
    
    // Verify no duplicated content in the entire message array
    const userMessages = mensagensGeradas.filter(m => m.role === "user");
    expect(userMessages.length).toBe(1); // Should only have one user message after deduplication
    
    // Test that running it twice will not include any content (all filtered out)
    const secondRun = memoryContextBuilder.buildMessagesWithContext(
      novaMensagem,
      conversationHistory,
      false,
      transcricoesCompletas,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined,
      memoryResults
    );
    
    // We expect the second run to not have any user messages
    const secondRunUserMessages = secondRun.filter(m => m.role === "user");
    expect(secondRunUserMessages.length).toBe(0);
  });

  test("Multiple messages are properly deduplicated", () => {
    // Arrange
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];

    const transcricoesCompletas: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "First message", 
        timestamp: "2023-01-01T10:00:00Z" 
      },
      { 
        speaker: "Guilherme", 
        text: "Second message", 
        timestamp: "2023-01-01T10:01:00Z" 
      },
      { 
        speaker: "Guilherme", 
        text: "Third message", 
        timestamp: "2023-01-01T10:02:00Z" 
      }
    ];

    // Resetar para começar limpo
    memoryContextBuilder.resetAll();
    
    // First run - should include all three messages
    const firstRun = memoryContextBuilder.buildMessagesWithContext(
      "First message\nSecond message\nThird message",
      conversationHistory,
      false,
      transcricoesCompletas,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, // Use undefined instead of null
      undefined  // Use undefined instead of null
    );

    // There should be 2 messages: system message + user message with all content
    expect(firstRun.length).toBe(2);
    expect(firstRun[1].role).toBe("user");
    expect(firstRun[1].content).toContain("First message");
    expect(firstRun[1].content).toContain("Second message");
    expect(firstRun[1].content).toContain("Third message");

    // Second run with partially new content
    const updatedTranscricoesCompletas: SpeakerTranscription[] = [
      ...transcricoesCompletas,
      { 
        speaker: "Guilherme", 
        text: "Fourth message", 
        timestamp: "2023-01-01T10:03:00Z" 
      }
    ];

    const secondRun = memoryContextBuilder.buildMessagesWithContext(
      "First message\nSecond message\nThird message\nFourth message",
      conversationHistory,
      false,
      updatedTranscricoesCompletas,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, // Use undefined instead of null
      undefined  // Use undefined instead of null
    );

    // There should be 2 messages: system message + user message with ONLY the new content
    expect(secondRun.length).toBe(2);
    expect(secondRun[1].role).toBe("user");
    expect(secondRun[1].content).not.toContain("First message");
    expect(secondRun[1].content).not.toContain("Second message");
    expect(secondRun[1].content).not.toContain("Third message");
    expect(secondRun[1].content).toContain("Fourth message");
  });

  test("Temporary context is deduplicated between executions", () => {
    // Arrange
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];

    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "What is the initial question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];

    // Reset everything
    memoryContextBuilder.resetAll();
    
    // Temporary context that should persist
    const temporaryContext = "Instructions important";
    
    // First execution with temporary context
    const firstRun = memoryContextBuilder.buildMessagesWithContext(
      "What is the initial question?",
      conversationHistory,
      false,
      transcricoes, 
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext,
      undefined
    );

    // Verify that the temporary context is present
    const developerMessages = firstRun.filter(m => m.role === "developer");
    const hasTemporaryContext = developerMessages.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext).toBe(true);
    
    console.log("First run developer messages:", 
      developerMessages.map(m => m.content.substring(0, 30) + "..."));

    // Second execution with new question but same temporary context
    const novaTranscricao: SpeakerTranscription[] = [
      ...transcricoes,
      { 
        speaker: "Guilherme", 
        text: "Second question?", 
        timestamp: "2023-01-01T10:01:00Z" 
      }
    ];
    
    const secondRun = memoryContextBuilder.buildMessagesWithContext(
      "Second question?",
      conversationHistory,
      false,
      novaTranscricao,
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext, // Same temporary context
      undefined
    );
    
    // Verify that the temporary context IS STILL present in the second prompt
    const secondRunDeveloperMessages = secondRun.filter(m => m.role === "developer");
    console.log("Second run developer messages:", 
      secondRunDeveloperMessages.map(m => m.content.substring(0, 30) + "..."));
    
    const stillHasTemporaryContext = secondRunDeveloperMessages.some(m => 
      m.content.includes("Instructions important"));
    expect(stillHasTemporaryContext).toBe(true);
    
    // Verify that the new question is present, but the old one is not
    const secondRunUserMessages = secondRun.filter(m => m.role === "user");
    expect(secondRunUserMessages.length).toBe(1);
    expect(secondRunUserMessages[0].content).toContain("Second question?");
    expect(secondRunUserMessages[0].content).not.toContain("What is the initial question?");
  });

  test("Temporary context with dynamically created objects", () => {
    // This test verifies if the use of dynamically created objects affects the temporary context
    
    // Resetting the initial tracker
    memoryContextBuilder.resetAll();
    
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "What is the initial question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];
    
    // 1. Creating a dynamic context object (as it would be in production)
    const dynamicContext = {
      instructions: "Instructions important",
      get value() { return this.instructions; }
    };
    
    // 2. First run with dynamic context
    const firstRun = memoryContextBuilder.buildMessagesWithContext(
      "What is the initial question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      dynamicContext.value, // Using dynamic getter
      undefined
    );
    
    // Verify that the temporary context is present 
    const developerMessages = firstRun.filter(m => m.role === "developer");
    const hasTemporaryContext = developerMessages.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext).toBe(true);
    
    // 3. Modifying the dynamic object after the first call
    // (Simula situações onde o objeto pode mudar entre chamadas)
    dynamicContext.instructions = "Instructions modified after first call";
    
    // 4. Second run with the same object (now modified)
    const secondRun = memoryContextBuilder.buildMessagesWithContext(
      "What is the second question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      dynamicContext.value, // Using modified getter
      undefined
    );
    
    // 5. Verify the instructions in the developer messages
    const developerMessages2 = secondRun.filter(m => m.role === "developer");
    console.log("Dynamic context developer messages:", 
      developerMessages2.map(m => m.content.substring(0, 50) + "..."));
    
    // Should not contain the original instructions
    const hasOriginalInstructions = developerMessages2.some(m => 
      m.content.includes("Instructions important"));
    expect(hasOriginalInstructions).toBe(false);
    
    // Should contain the new instructions
    const hasNewInstructions = developerMessages2.some(m => 
      m.content.includes("Instructions modified after first call"));
    expect(hasNewInstructions).toBe(true);
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { SymbolicInsight } from './SymbolicInsight';
import { SymbolicQuery } from './SymbolicQuery';
import { SymbolicContext } from './SymbolicContext';
import { UserIntentWeights } from '../symbolic-cortex/integration/ICollapseStrategyService';

export type CognitionEvent =
  | { type: 'raw_prompt'; timestamp: string; content: string }
  | { type: 'temporary_context'; timestamp: string; context: string }
  | { type: 'neural_signal'; timestamp: string; core: string; symbolic_query: SymbolicQuery; intensity: number; topK: number; params: Record<string, unknown> }
  | { type: 'symbolic_retrieval'; timestamp: string; core: string; insights: SymbolicInsight[]; matchCount: number; durationMs: number }
  | { type: 'fusion_initiated'; timestamp: string }
  | { type: 'neural_collapse'; timestamp: string; isDeterministic: boolean; selectedCore: string; numCandidates: number; temperature?: number; emotionalWeight: number; contradictionScore: number; justification?: string; userIntent?: UserIntentWeights; insights?: SymbolicInsight[]; emergentProperties?: string[] }
  | { type: 'symbolic_context_synthesized'; timestamp: string; context: SymbolicContext }
  | { type: 'gpt_response'; timestamp: string; response: string; symbolicTopics?: string[]; insights?: SymbolicInsight[] }
  | { type: 'emergent_patterns'; timestamp: string; patterns: string[]; metrics?: { archetypalStability?: number; cycleEntropy?: number; insightDepth?: number } };
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Structure for synthesized symbolic context.
 */
export interface SymbolicContext {
  summary: string;
  [key: string]: string | number | boolean | object | undefined;

}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Structure for symbolic insights extracted from neural signals.
 * Core type definition for symbolic neural processing.
 */
export interface SymbolicInsight {
  type: string;
  content?: string;
  core?: string;
  keywords?: string[];
  [key: string]: string | number | boolean | object | undefined;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Structure for symbolic queries associated with neural signals.
 * Adjust as needed.
 */
export interface SymbolicQuery {
  query: string;
  [key: string]: string | number | boolean | object | undefined;

}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// LoggingUtils.ts
// Logging utilities

export class LoggingUtils {
  private static readonly PREFIX = "[Transcription]";
  
  /**
   * Logs an informative message
   */
  static logInfo(message: string): void {
    console.log(`ℹ️ ${this.PREFIX} ${message}`);
  }
  
  /**
   * Logs a warning
   */
  static logWarning(message: string): void {
    console.warn(`⚠️ ${this.PREFIX} ${message}`);
  }
  
  /**
   * Logs an error
   */
  static logError(message: string, error?: unknown): void {
    if (error) {
      console.error(`❌ ${this.PREFIX} ${message}:`, error);
    } else {
      console.error(`❌ ${this.PREFIX} ${message}`);
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Utility para validar compatibilidade de modelos Ollama com function calling
 * Baseado na documentação oficial do Ollama: https://ollama.com/blog/tool-support
 */

export interface OllamaModelInfo {
  name: string;
  supportsFunctionCalling: boolean;
  recommended: boolean;
  description: string;
}

/**
 * Lista oficial de modelos que suportam function calling no Ollama
 * Filtrada conforme especificação do usuário - apenas modelos que aceitam tools
 */
export const OLLAMA_FUNCTION_CALLING_MODELS: OllamaModelInfo[] = [
  {
    name: "qwen3",
    supportsFunctionCalling: true,
    recommended: true,
    description: "Qwen 3 - Excelente suporte multilíngue e tools",
  },
  {
    name: "mistral",
    supportsFunctionCalling: true,
    recommended: true,
    description: "Mistral - Rápido e eficiente para function calling",
  },
  {
    name: "mistral-nemo",
    supportsFunctionCalling: true,
    recommended: true,
    description: "Mistral Nemo - Versão otimizada para tools",
  },
  {
    name: "llama3.2",
    supportsFunctionCalling: true,
    recommended: true,
    description: "Llama 3.2 - Excelente integração com tools",
  },
];

export class OllamaModelValidator {
  /**
   * Verifica se um modelo suporta function calling
   */
  static supportsFunctionCalling(modelName: string): boolean {
    const baseModelName = modelName.split(":")[0];
    return OLLAMA_FUNCTION_CALLING_MODELS.some(
      (model) =>
        model.name === baseModelName || baseModelName.includes(model.name)
    );
  }

  /**
   * Obter informações sobre um modelo
   */
  static getModelInfo(modelName: string): OllamaModelInfo | null {
    const baseModelName = modelName.split(":")[0];
    return (
      OLLAMA_FUNCTION_CALLING_MODELS.find(
        (model) =>
          model.name === baseModelName || baseModelName.includes(model.name)
      ) || null
    );
  }

  /**
   * Obter lista de modelos recomendados para function calling
   */
  static getRecommendedModels(): OllamaModelInfo[] {
    return OLLAMA_FUNCTION_CALLING_MODELS.filter((model) => model.recommended);
  }

  /**
   * Obter o melhor modelo fallback para function calling
   */
  static getBestFallbackModel(): string {
    const recommended = this.getRecommendedModels();
    return recommended.length > 0
      ? `${recommended[0].name}:latest`
      : "qwen3:4b";
  }

  /**
   * Validar e sugerir modelo alternativo se necessário
   */
  static validateAndSuggest(modelName: string): {
    isValid: boolean;
    originalModel: string;
    suggestedModel?: string;
    reason?: string;
  } {
    if (this.supportsFunctionCalling(modelName)) {
      return {
        isValid: true,
        originalModel: modelName,
      };
    }

    const suggestedModel = this.getBestFallbackModel();
    return {
      isValid: false,
      originalModel: modelName,
      suggestedModel,
      reason: `Modelo ${modelName} não suporta function calling. Sugerido: ${suggestedModel}`,
    };
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Utility for cleaning <think> tags from model responses
 * Based on the Vercel AI issue: https://github.com/vercel/ai/issues/4920
 */

/**
 * Comprehensive function to clean <think> tags from model responses
 * Handles various formats and malformed tags
 */
export function cleanThinkTags(content: string): string {
  if (!content || typeof content !== "string") {
    return content || "";
  }

  let cleaned = content;

  // Pattern 1: Think tags with attributes (MUST come first to capture complex tags)
  cleaned = cleaned.replace(/<think[^>]*>[\s\S]*?<\/think>/gi, "");

  // Pattern 2: Complete <think>...</think> blocks (most common - basic tags)
  cleaned = cleaned.replace(/<think>[\s\S]*?<\/think>/gi, "");

  // Pattern 3: Think tags with different casing (including attributes)
  cleaned = cleaned.replace(/<THINK[^>]*>[\s\S]*?<\/THINK>/gi, "");
  cleaned = cleaned.replace(/<Think[^>]*>[\s\S]*?<\/Think>/gi, "");

  // Pattern 4: Standalone opening <think> tags
  cleaned = cleaned.replace(/<think[^>]*>/gi, "");

  // Pattern 5: Standalone closing </think> tags (the main issue from Vercel AI)
  cleaned = cleaned.replace(/<\/think>/gi, "");

  // Pattern 6: Partial or broken think tags
  cleaned = cleaned.replace(/<think[\s\S]*?(?=<[^/]|$)/gi, "");

  // Pattern 7: Think tags with variations of whitespace and newlines
  cleaned = cleaned.replace(/<\s*think\s*>[\s\S]*?<\s*\/\s*think\s*>/gi, "");

  // Pattern 8: Clean up stray content related to think
  cleaned = cleaned.replace(/^\s*<\/think>\s*/gm, "");
  cleaned = cleaned.replace(/\s*<think>\s*$/gm, "");

  // Clean up excessive whitespace while preserving formatting
  cleaned = cleaned.replace(/[ \t]+/g, " "); // Replace multiple spaces/tabs with single space
  cleaned = cleaned.replace(/\n\s*\n\s*\n/g, "\n\n"); // Replace triple+ newlines with double
  cleaned = cleaned.replace(/^\s+|\s+$/g, ""); // Trim start and end

  return cleaned;
}

/**
 * Clean think tags from JSON content while preserving JSON structure
 */
export function cleanThinkTagsFromJSON(jsonString: string): string {
  if (!jsonString || typeof jsonString !== "string") {
    return jsonString || "";
  }

  // First clean think tags
  let cleaned = cleanThinkTags(jsonString);

  // If it looks like JSON, try to parse and re-stringify to ensure validity
  if (cleaned.trim().startsWith("{") && cleaned.trim().endsWith("}")) {
    try {
      const parsed = JSON.parse(cleaned);

      // Recursively clean think tags from string values in the JSON
      const cleanedParsed = cleanJSONValues(parsed);

      return JSON.stringify(cleanedParsed);
    } catch (e) {
      // If parsing fails, return the cleaned string as-is
      return cleaned;
    }
  }

  return cleaned;
}

/**
 * Recursively clean think tags from JSON object values
 */
function cleanJSONValues(obj: any): any {
  if (typeof obj === "string") {
    return cleanThinkTags(obj);
  } else if (Array.isArray(obj)) {
    return obj.map(cleanJSONValues);
  } else if (obj && typeof obj === "object") {
    const cleaned: any = {};
    for (const [key, value] of Object.entries(obj)) {
      cleaned[key] = cleanJSONValues(value);
    }
    return cleaned;
  }
  return obj;
}

/**
 * Clean think tags from function call arguments
 */
export function cleanThinkTagsFromFunctionArgs(
  args: string | Record<string, any>
): string | Record<string, any> {
  if (typeof args === "string") {
    return cleanThinkTagsFromJSON(args);
  } else if (args && typeof args === "object") {
    return cleanJSONValues(args);
  }
  return args;
}

/**
 * Clean think tags from tool call responses
 */
export function cleanThinkTagsFromToolCalls(
  toolCalls: Array<{
    function: {
      name: string;
      arguments: string | Record<string, any>;
    };
  }>
): Array<{
  function: {
    name: string;
    arguments: string | Record<string, any>;
  };
}> {
  if (!Array.isArray(toolCalls)) {
    return toolCalls;
  }

  return toolCalls.map((call) => ({
    ...call,
    function: {
      ...call.function,
      arguments: cleanThinkTagsFromFunctionArgs(call.function.arguments),
    },
  }));
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// AudioAnalyzer.ts
// Implementation of the audio analysis service for Deepgram

import { ListenLiveClient, LiveTranscriptionEvents } from "@deepgram/sdk";
import { IAudioAnalyzer } from "./interfaces/deepgram/IDeepgramService";

export class DeepgramAudioAnalyzer implements IAudioAnalyzer {
  private getConnection: () => ListenLiveClient | null;

  constructor(getConnection: () => ListenLiveClient | null) {
    this.getConnection = getConnection;
  }

  analyzeAudioBuffer(buffer: ArrayBufferLike): { valid: boolean, details: any } {
    try {
      const view = new DataView(buffer);
      const uint8View = new Uint8Array(buffer);
      
      // Check minimum buffer size
      if (buffer.byteLength < 64) {
        return {
          valid: false,
          details: {
            reason: "Buffer too small",
            size: buffer.byteLength
          }
        };
      }
      
      // Detect if it looks like a WAV header
      const isWav = 
        String.fromCharCode(uint8View[0], uint8View[1], uint8View[2], uint8View[3]) === 'RIFF' &&
        String.fromCharCode(uint8View[8], uint8View[9], uint8View[10], uint8View[11]) === 'WAVE';
      
      // Detect if it looks like a WebM (starts with 0x1A 0x45 0xDF 0xA3)
      const isWebM = uint8View[0] === 0x1A && uint8View[1] === 0x45 && uint8View[2] === 0xDF && uint8View[3] === 0xA3;
      
      // PCM raw analysis (no header)
      const pcmAnalysis = {
        min: Number.MAX_VALUE,
        max: Number.MIN_VALUE,
        avg: 0,
        rms: 0,
        zeroCount: 0,
        sampleCount: 0
      };
      
      // Ensure the buffer has enough data for Int16Array
      // Int16Array requires an even number of bytes
      const alignedLength = Math.floor(buffer.byteLength / 2) * 2;
      
      // Proceed with PCM analysis only if we have enough data
      if (alignedLength >= 64) {
        // Create an Int16Array view adjusted to the correct size
        const int16View = new Int16Array(buffer.slice(0, alignedLength));
        pcmAnalysis.sampleCount = int16View.length;
        
        // Calculate statistics for the first X samples (max 1000)
        const samplesToAnalyze = Math.min(int16View.length, 1000);
        let sum = 0;
        let sumSquares = 0;
        
        for (let i = 0; i < samplesToAnalyze; i++) {
          const sample = int16View[i];
          pcmAnalysis.min = Math.min(pcmAnalysis.min, sample);
          pcmAnalysis.max = Math.max(pcmAnalysis.max, sample);
          sum += Math.abs(sample);
          sumSquares += sample * sample;
          if (sample === 0) pcmAnalysis.zeroCount++;
        }
        
        pcmAnalysis.avg = sum / samplesToAnalyze;
        pcmAnalysis.rms = Math.sqrt(sumSquares / samplesToAnalyze);
      } else {
        // Not possible to analyze as PCM
        pcmAnalysis.min = 0;
        pcmAnalysis.max = 0;
      }
      
      // Check if the audio appears to be silence
      const isSilence = pcmAnalysis.rms < 10; // A very low threshold indicates silence
      
      // Check if the buffer format matches what Deepgram expects
      const formatDescription = isWav ? "WAV" : isWebM ? "WebM" : "PCM brute";
      const hasCorrectFormat = !isWav && !isWebM; // Deepgram expects PCM brute
      
      // Return detailed analysis
      return {
        valid: hasCorrectFormat && !isSilence && buffer.byteLength % 2 === 0,
        details: {
          format: formatDescription,
          byteLength: buffer.byteLength,
          byteIsEven: buffer.byteLength % 2 === 0,
          headerBytes: Array.from(uint8View.slice(0, 16)).map(b => b.toString(16).padStart(2, '0')).join(' '),
          pcmAnalysis,
          isSilence,
          isFormatCorrect: hasCorrectFormat
        }
      };
    } catch (error) {
      return {
        valid: false,
        details: {
          reason: "Error analyzing buffer",
          error: String(error)
        }
      };
    }
  }

  async testAudioQuality(): Promise<{ valid: boolean, reason?: string }> {
    try {
      const activeConn = this.getConnection();
      if (!activeConn || activeConn.getReadyState() !== 1) {
        return { valid: false, reason: "No active connection to Deepgram" };
      }
      
      // Create an audio context to get the native sample rate of the system
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const sampleRate = audioContext.sampleRate;
      
      // Create a test buffer with synthetic audio data (test tone)
      const duration = 0.5; // 500ms
      const sampleCount = Math.floor(sampleRate * duration);
      
      // Log for debug of the sample rate used
      console.log(`🎵 Generating test signal with native sample rate: ${sampleRate}Hz`);
      
      // Create a buffer for PCM 16-bit, 2 channels (4 bytes per sample)
      const buffer = new ArrayBuffer(sampleCount * 4);
      const view = new DataView(buffer);
      
      // Generate different tones for each channel
      // Channel 0: 440Hz (A4), Channel 1: 880Hz (A5)
      const freq1 = 440;
      const freq2 = 880;
      
      for (let i = 0; i < sampleCount; i++) {
        // Time in seconds
        const t = i / sampleRate;
        
        // Calculate amplitude for each channel (-0.5 to 0.5 to avoid distortion)
        const amplitude1 = 0.3 * Math.sin(2 * Math.PI * freq1 * t);
        const amplitude2 = 0.3 * Math.sin(2 * Math.PI * freq2 * t);
        
        // Convert to int16 (-32768 to 32767)
        const sample1 = Math.floor(amplitude1 * 32767);
        const sample2 = Math.floor(amplitude2 * 32767);
        
        // Write interlaced samples for both channels (PCM stereo format)
        view.setInt16(i * 4, sample1, true);     // left channel (0)
        view.setInt16(i * 4 + 2, sample2, true); // right channel (1)
      }
      
      // Close the audio context to avoid leaving resources open
      audioContext.close();
      
      console.log("🔊 Sending test signal to Deepgram");
      
      // Send the test buffer
      if (activeConn) {
        activeConn.send(buffer);
        
        // Wait for a short period to see if we receive transcription/error
        return new Promise((resolve) => {
          let responseReceived = false;
          
          // Temporary handler to capture responses
          const handleMessage = (data: any) => {
            responseReceived = true;
            
            // Check if the response contains any specific error
            if (data.error) {
              console.error("❌ Error returned by Deepgram:", data.error);
              resolve({ valid: false, reason: `Error from Deepgram: ${data.error}` });
              return;
            }
            
              // If we receive any response without error, consider it valid
            console.log("✅ Test signal accepted by Deepgram");
            resolve({ valid: true });
          };
          
          // Add and then remove the temporary handler
          activeConn.addListener(LiveTranscriptionEvents.Transcript, handleMessage);
          
          // Set a timeout to resolve if we don't receive a response
          setTimeout(() => {
            // Remove the temporary handler
            activeConn.removeListener(LiveTranscriptionEvents.Transcript, handleMessage);
            
            if (!responseReceived) {
              console.warn("⚠️ Timeout in audio quality check - no response received");
              // Timeout is acceptable, as Deepgram might simply not return anything for audio with no speech
              resolve({ valid: true, reason: "No response, but active connection" });
            }
          }, 3000);
        });
      }
      
      return { valid: false, reason: "Active connection but invalid" };
    } catch (error) {
      console.error("❌ Error testing audio quality:", error);
      return { valid: false, reason: String(error) };
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// DeepgramConnectionService.ts
// Deepgram connection service optimized for robustness and clarity

import { ListenLiveClient } from "@deepgram/sdk";
import { ConnectionState, IAudioAnalyzer, IDeepgramConnectionService } from "./interfaces/deepgram/IDeepgramService";
import { AudioProcessor } from "./services/audio/AudioProcessor";
import { AudioQueue } from "./services/audio/AudioQueue";
import { AudioSender } from "./services/audio/AudioSender";
import { ChunkReceiver } from "./services/audio/ChunkReceiver";
import { ConnectionManager } from "./services/connection/ConnectionManager";
import { EventHandler } from "./services/connection/EventHandler";
import { LiveTranscriptionProcessor } from "./services/transcription/LiveTranscriptionProcessor";
import { TranscriptionEventCallback } from "./services/utils/DeepgramTypes";
import { ITranscriptionStorageService } from './interfaces/transcription/ITranscriptionStorageService';
import { STORAGE_KEYS, getOption } from '../../../services/StorageService';

export class DeepgramConnectionService implements IDeepgramConnectionService {
  // Core service components
  private connectionManager: ConnectionManager;
  private eventHandler: EventHandler;
  private audioProcessor: AudioProcessor;
  private audioQueue: AudioQueue;
  private audioSender: AudioSender;
  private transcriptionProcessor: LiveTranscriptionProcessor;
  private chunkReceiver: ChunkReceiver;
  
  constructor(
    setConnectionState: (state: ConnectionState) => void,
    setConnection: (connection: ListenLiveClient | null) => void,
    analyzer: IAudioAnalyzer,
    storageService?: ITranscriptionStorageService // Use the interface to avoid direct dependencies
  ) {
    // Initialize all service components
    this.audioProcessor = new AudioProcessor(analyzer);
    this.audioQueue = new AudioQueue();
    this.connectionManager = new ConnectionManager(setConnectionState, setConnection);
    
    // Create transcription processor
    this.transcriptionProcessor = new LiveTranscriptionProcessor();
    
    // Bind TranscriptionStorageService if available
    if (storageService) {
      console.log(`🔄 [COGNITIVE-CORE] Integrating LiveTranscriptionProcessor with ITranscriptionStorageService for brain memory persistence`);
      this.transcriptionProcessor.setTranscriptionStorageService(storageService);
    } else {
      console.log(`⚠️ [COGNITIVE-CORE] Storage service not provided, some memory orchestration features may not function`);
    }
    
    this.eventHandler = new EventHandler(this.transcriptionProcessor, this.connectionManager);
    this.audioSender = new AudioSender(this.audioProcessor, this.audioQueue, this.connectionManager);
    
    // Set up chunk receiver with audio handler
    this.chunkReceiver = new ChunkReceiver(this.handleIncomingAudioChunk.bind(this));
    this.chunkReceiver.setupChunkReceiver();
  }
  
  // PUBLIC API
  
  /**
   * Get the current connection
   */
  getConnection(): ListenLiveClient | null {
    return this.connectionManager.getConnection();
  }
  
  /**
   * Start a connection with Deepgram
   */
  async connectToDeepgram(language?: string): Promise<void> {
    // Sempre obtém o idioma mais atualizado do storage
    const storedLanguage = getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE);
    
    // Prioriza o idioma do storage, e só usa o parâmetro se não houver valor no storage
    const languageToUse = storedLanguage || language || 'pt-BR';
    
    console.log('🌐 DeepgramConnectionService: Conectando com idioma:', languageToUse);
    
    // Start connection process with the correct language
    await this.connectionManager.connectToDeepgram(languageToUse);
    
    // Register event handlers if connection was established
    const connection = this.connectionManager.getConnection();
    if (connection) {
      this.eventHandler.registerEventHandlers(connection);
    }
  }
  
  /**
   * Disconnect from Deepgram
   */
  async disconnectFromDeepgram(): Promise<void> {
    // Clear audio queue before disconnecting
    this.audioQueue.clearQueue();
    
    // Disconnect from Deepgram
    await this.connectionManager.disconnectFromDeepgram();
  }
  
  /**
   * Clean up event listeners and disconnect from Deepgram
   */
  public cleanup(): void {
    // Clean up chunk receiver
    this.chunkReceiver.cleanup();
    
    // Disconnect from Deepgram
    this.disconnectFromDeepgram();
  }
  
  /**
   * Wait until the connection reaches a specific state
   */
  async waitForConnectionState(targetState: ConnectionState, timeoutMs = 15000): Promise<boolean> {
    return this.connectionManager.waitForConnectionState(targetState, timeoutMs);
  }
  
  /**
   * Get the current connection status
   */
  getConnectionStatus() {
    return this.connectionManager.getConnectionStatus();
  }
  
  /**
   * Check if there is an active and ready connection
   */
  hasActiveConnection(): boolean {
    return this.connectionManager.isActiveConnection();
  }
  
  /**
   * Send audio data to Deepgram
   */
  async sendAudioChunk(blob: Blob | Uint8Array): Promise<boolean> {
    const result = await this.audioSender.sendAudioChunk(blob);
    
    // If connection is active and we have queued audio, start processing it
    if (this.connectionManager.isActiveConnection() && this.audioQueue.hasItems() && !this.audioQueue.isProcessing()) {
      setTimeout(() => this.audioSender.processQueuedChunks(), 100);
    }
    
    return result;
  }
  
  /**
   * Register a callback to receive transcription events
   */
  public registerTranscriptionCallback(callback: TranscriptionEventCallback): void {
    this.eventHandler.registerTranscriptionCallback(callback);
  }
  
  /**
   * Process incoming audio chunks via IPC
   */
  private async handleIncomingAudioChunk(arrayBuffer: ArrayBuffer): Promise<void> {
    try {
      // Convert ArrayBuffer to Uint8Array for compatibility
      const audioData = new Uint8Array(arrayBuffer);
      
      if (audioData.byteLength > 0) {
        // Use the existing method to process and send the audio
        await this.sendAudioChunk(audioData);
      }
    } catch (error) {
      console.error(`❌ [Deepgram] Error processing IPC audio chunk:`, error);
    }
  }
  
  /**
   * Force a reconnection to the server
   */
  private async forceReconnect(): Promise<void> {
    await this.connectionManager.forceReconnect();
    
    // Register event handlers if connection was established
    const connection = this.connectionManager.getConnection();
    if (connection) {
      this.eventHandler.registerEventHandlers(connection);
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// DeepgramContextProvider.tsx
// Component that manages the Deepgram context

import React, {
  createContext,
  useCallback,
  useContext,
  useEffect,
  useReducer,
  useRef,
  useState,
} from "react";
import { ModeService, OrchOSModeEnum } from "../../../services/ModeService";
import { getOption, STORAGE_KEYS } from "../../../services/StorageService";
import { useAudioAnalyzer } from "../audioAnalyzer/AudioAnalyzerProvider";
import { AudioContextService } from "../microphone/AudioContextService";
import { useSettings } from "../settings/SettingsProvider";
import { DeepgramConnectionService } from "./DeepgramConnectionService";
import {
  DeepgramState,
  IDeepgramContext,
} from "./interfaces/deepgram/IDeepgramContext";
import {
  ConnectionState,
  IDeepgramConnectionService,
} from "./interfaces/deepgram/IDeepgramService";
import { IOpenAIService } from "./interfaces/openai/IOpenAIService";
import { DeepgramTranscriptionService } from "./services/DeepgramTranscriptionService";
import { HuggingFaceCompletionService } from "./services/huggingface/HuggingFaceCompletionService";
import { HuggingFaceServiceFacade } from "./services/huggingface/HuggingFaceServiceFacade";
import { HuggingFaceClientService } from "./services/huggingface/neural/HuggingFaceClientService";
import { OllamaServiceFacade } from "./services/ollama/OllamaServiceFacade";
// Import all custom hooks from index
import {
  useDeepgramDebug,
  useTranscriptionData,
  useTranscriptionProcessor,
} from "./hooks";

// Initial state
const initialState = {
  deepgramState: DeepgramState.NotConnected,
  isConnected: false,
  isProcessing: false,
  language: getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "pt-BR",
  model: getOption(STORAGE_KEYS.DEEPGRAM_MODEL) || "nova-2",
};

// Reducer actions
type DeepgramAction =
  | { type: "SET_STATE"; payload: DeepgramState }
  | { type: "SET_CONNECTED"; payload: boolean }
  | { type: "SET_PROCESSING"; payload: boolean }
  | { type: "SET_LANGUAGE"; payload: string }
  | { type: "SET_MODEL"; payload: string }
  | { type: "RESET_STATE" };

// Reducer to manage Deepgram state
function deepgramReducer(
  state: typeof initialState,
  action: DeepgramAction
): typeof initialState {
  switch (action.type) {
    case "SET_STATE":
      return { ...state, deepgramState: action.payload };
    case "SET_CONNECTED":
      return { ...state, isConnected: action.payload };
    case "SET_PROCESSING":
      return { ...state, isProcessing: action.payload };
    case "SET_LANGUAGE":
      return { ...state, language: action.payload };
    case "SET_MODEL":
      return { ...state, model: action.payload };
    case "RESET_STATE":
      return { ...initialState };
    default:
      return state;
  }
}

// Context creation
export const DeepgramContext = createContext<IDeepgramContext | null>(null);

// Custom hook for context usage
export const useDeepgram = () => {
  const context = useContext(DeepgramContext);
  if (!context) {
    throw new Error("useDeepgram must be used within DeepgramProvider");
  }
  return context;
};

/**
 * Creates the appropriate AI service based on application mode
 * Following KISS principle - Keep It Simple
 */
function createAIService(): IOpenAIService {
  const mode = ModeService.getMode();

  if (mode === OrchOSModeEnum.BASIC) {
    console.log("🧠 Using HuggingFaceServiceFacade (Basic mode)");
    const clientService = new HuggingFaceClientService();
    const completionService = new HuggingFaceCompletionService(clientService);
    return new HuggingFaceServiceFacade(completionService);
  } else {
    console.log("🦙 Using OllamaServiceFacade (Advanced mode)");
    return new OllamaServiceFacade();
  }
}

// Context provider
export const DeepgramProvider: React.FC<{ children: React.ReactNode }> = ({
  children,
}) => {
  // State management
  const [state, dispatch] = useReducer(deepgramReducer, initialState);
  const [connection, setConnection] = useState<any | null>(null);
  const [connectionState, setConnectionState] = useState<ConnectionState>(
    ConnectionState.CLOSED
  );

  // Services and settings
  const { settings } = useSettings();
  const analyzer = useAudioAnalyzer();
  const deepgramConnectionRef = useRef<IDeepgramConnectionService | null>(null);
  const deepgramTranscriptionRef = useRef<DeepgramTranscriptionService | null>(
    null
  );

  // Global processing ref for synchronous blocking
  const isProcessingRef = useRef<boolean>(false);

  // Use custom hooks - Following SOLID principles
  const { debugDatabase, testDatabaseDiagnosis, testEmbeddingModel } =
    useDeepgramDebug();

  const {
    transcriptionData,
    interimResults,
    diarizationData,
    handleTranscriptionData,
    handleInterimUpdate,
    clearTranscriptionData,
  } = useTranscriptionData(deepgramTranscriptionRef);

  const {
    sendTranscriptionPrompt,
    sendDirectMessage,
    flushTranscriptionsToUI,
    setAutoQuestionDetection,
  } = useTranscriptionProcessor(
    deepgramTranscriptionRef.current,
    state.isProcessing,
    dispatch,
    isProcessingRef // Pass the global ref
  );

  // Service references
  const services = useRef({
    audioContext: new AudioContextService(),
    deepgramConnection: null as DeepgramConnectionService | null,
  });

  // Initialize services - Following DRY principle
  useEffect(() => {
    const initializeServices = async () => {
      // Setup audio context
      services.current.audioContext.setupAudioContext();

      // Create AI service based on mode
      const aiService = createAIService();

      // Create transcription service with UI updater callback
      const transcriptionService = new DeepgramTranscriptionService(
        (updater: any) => {
          if (updater.transcription !== undefined) {
            handleTranscriptionData(updater.transcription);
          }
          if (updater.interim !== undefined) {
            handleInterimUpdate(updater.interim);
          }
        },
        aiService
      );

      // Get storage service for integration
      const storageService =
        transcriptionService.getStorageServiceForIntegration();
      console.log(
        "💾 Storage service obtained:",
        storageService ? "OK" : "NULL"
      );

      // Initialize connection service
      services.current.deepgramConnection = new DeepgramConnectionService(
        setConnectionState,
        setConnection,
        analyzer,
        storageService
      );

      // Store references
      deepgramTranscriptionRef.current = transcriptionService;
      deepgramConnectionRef.current = services.current.deepgramConnection;

      // Register transcription callback
      services.current.deepgramConnection.registerTranscriptionCallback(
        (event: string, data: any) => {
          if (event === "transcript") {
            handleTranscriptionData(data);
            if (data?.text && deepgramTranscriptionRef.current) {
              deepgramTranscriptionRef.current.addTranscription(data.text);
              console.log(`📝 Transcription sent to service: "${data.text}"`);
            }
          } else if (event === "metadata") {
            console.log("Metadata received:", data);
          }
        }
      );

      // Configure initial preferences
      if (settings.deepgramModel) {
        transcriptionService.setModel(settings.deepgramModel);
      }
      transcriptionService.toggleInterimResults(settings.showInterimResults);
    };

    initializeServices();

    // Cleanup on unmount
    return () => {
      if (services.current.deepgramConnection) {
        services.current.deepgramConnection.cleanup();
      }
      if (deepgramTranscriptionRef.current) {
        deepgramTranscriptionRef.current.reset();
      }
      services.current.audioContext.closeAudioContext();
    };
  }, [
    analyzer,
    settings.deepgramModel,
    settings.showInterimResults,
    handleTranscriptionData,
    handleInterimUpdate,
  ]);

  // Configure IPC event receiver
  useEffect(() => {
    if (typeof window !== "undefined" && window.electronAPI) {
      const removeListener = window.electronAPI.onSendChunk(() => {
        // Intentionally empty - DeepgramConnectionService handles this directly
      });
      return () => removeListener();
    }
  }, []);

  // Update preferences when settings change
  useEffect(() => {
    if (deepgramTranscriptionRef.current) {
      if (settings.deepgramModel) {
        deepgramTranscriptionRef.current.setModel(settings.deepgramModel);
      }
      deepgramTranscriptionRef.current.toggleInterimResults(
        settings.showInterimResults
      );
    }
  }, [settings.deepgramModel, settings.showInterimResults]);

  // Connection management functions
  const connectToDeepgram = useCallback(async () => {
    try {
      if (
        state.deepgramState === DeepgramState.Connected ||
        state.deepgramState === DeepgramState.Connecting
      ) {
        console.log("🔍 Connection already active or in progress");
        return state.isConnected;
      }

      const { deepgramConnection } = services.current;
      if (!deepgramConnection) return false;

      dispatch({ type: "SET_STATE", payload: DeepgramState.Connecting });
      await deepgramConnection.connectToDeepgram(state.language);

      const connected = await deepgramConnection.hasActiveConnection();

      if (connected) {
        dispatch({ type: "SET_STATE", payload: DeepgramState.Connected });
        dispatch({ type: "SET_CONNECTED", payload: true });
      } else {
        dispatch({ type: "SET_STATE", payload: DeepgramState.Error });
        dispatch({ type: "SET_CONNECTED", payload: false });
      }

      return connected;
    } catch (error) {
      console.error("❌ Error connecting to Deepgram:", error);
      dispatch({ type: "SET_STATE", payload: DeepgramState.Error });
      dispatch({ type: "SET_CONNECTED", payload: false });
      return false;
    }
  }, [state.deepgramState, state.isConnected, state.language]);

  const disconnectFromDeepgram = useCallback(async () => {
    try {
      if (
        state.deepgramState === DeepgramState.NotConnected ||
        state.deepgramState === DeepgramState.Disconnecting
      ) {
        console.log("🔍 Connection already inactive or disconnecting");
        return;
      }

      const { deepgramConnection } = services.current;
      if (!deepgramConnection) return;

      dispatch({ type: "SET_STATE", payload: DeepgramState.Disconnecting });
      await deepgramConnection.disconnectFromDeepgram();

      dispatch({ type: "SET_STATE", payload: DeepgramState.NotConnected });
      dispatch({ type: "SET_CONNECTED", payload: false });
    } catch (error) {
      console.error("❌ Error disconnecting from Deepgram:", error);
    }
  }, [state.deepgramState]);

  // Utility functions
  const sendAudioChunk = useCallback(async (chunk: Blob | Uint8Array) => {
    return services.current.deepgramConnection?.sendAudioChunk(chunk) || false;
  }, []);

  const stopProcessing = useCallback(() => {
    dispatch({ type: "SET_PROCESSING", payload: false });
  }, []);

  const setLanguage = useCallback((language: string) => {
    dispatch({ type: "SET_LANGUAGE", payload: language });
  }, []);

  const setModel = useCallback((model: string) => {
    dispatch({ type: "SET_MODEL", payload: model });
  }, []);

  const resetState = useCallback(() => {
    dispatch({ type: "RESET_STATE" });
  }, []);

  // Export service instances for UI/integration
  const getServiceInstances = () => {
    let transcriptionServiceInstance: any = undefined;
    let memoryServiceInstance: any = undefined;

    if (
      deepgramTranscriptionRef.current instanceof DeepgramTranscriptionService
    ) {
      transcriptionServiceInstance = (deepgramTranscriptionRef.current as any)[
        "storageService"
      ];
      const memoryService = (deepgramTranscriptionRef.current as any)[
        "memoryService"
      ];
      if (memoryService?.persistenceService) {
        memoryServiceInstance = memoryService["persistenceService"];
      }
    }

    return { transcriptionServiceInstance, memoryServiceInstance };
  };

  const { transcriptionServiceInstance, memoryServiceInstance } =
    getServiceInstances();

  // Context value - Following Interface Segregation Principle
  const contextValue: IDeepgramContext = {
    // Connection state
    connection,
    connectionState,
    isConnected: state.isConnected,
    deepgramState: state.deepgramState,

    // Transcription data
    transcriptionList: transcriptionData,

    // Processing state
    isProcessing: state.isProcessing,

    // Language and model
    language: state.language,
    model: state.model,

    // Connection management
    connectToDeepgram,
    disconnectFromDeepgram,
    sendAudioChunk,
    hasActiveConnection: () =>
      deepgramConnectionRef.current?.hasActiveConnection() || false,
    getConnectionStatus: () =>
      deepgramConnectionRef.current?.getConnectionStatus() || {
        state: ConnectionState.CLOSED,
        active: false,
      },
    waitForConnectionState: (targetState, timeoutMs) =>
      deepgramConnectionRef.current?.waitForConnectionState(
        targetState,
        timeoutMs
      ) || Promise.resolve(false),

    // Transcription processing
    sendTranscriptionPrompt,
    sendDirectMessage,
    flushTranscriptionsToUI,
    setAutoQuestionDetection,
    clearTranscriptionData,

    // Get all transcriptions with status
    getAllTranscriptionsWithStatus: () => {
      if (transcriptionServiceInstance?.getAllTranscriptionsWithStatus) {
        return transcriptionServiceInstance.getAllTranscriptionsWithStatus();
      }
      return [];
    },

    // State management
    stopProcessing,
    setLanguage,
    setModel,
    resetState,

    // Service instances
    transcriptionService: transcriptionServiceInstance,
    memoryService: memoryServiceInstance,

    // Debug functions (only in development)
    debugDatabase,
    testDatabaseDiagnosis,
    testEmbeddingModel,
  };

  return (
    <DeepgramContext.Provider value={contextValue}>
      {children}
    </DeepgramContext.Provider>
  );
};

export default DeepgramProvider;
// @ts-nocheck
/** @type {import('ts-jest').JestConfigWithTsJest} */
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  transform: {
    '^.+\\.tsx?$': 'ts-jest',
  },
  moduleFileExtensions: ['ts', 'tsx', 'js', 'jsx', 'json', 'node'],
}; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IAudioContextService.ts
// Interface for managing the Web Audio API audio context

export interface IAudioContextService {
  getAudioContext: () => AudioContext | null;
  setupAudioContext: () => void;
  closeAudioContext: () => Promise<void>;
  getMerger: () => ChannelMergerNode | null;
  getDestination: () => MediaStreamAudioDestinationNode | null;
  getChannelInfo: () => { 
    count: number, 
    mode: ChannelCountMode, 
    interpretation: ChannelInterpretation 
  } | null;
  connectMicrophoneSource: (source: AudioNode) => void;
  disconnectMicrophoneSource: () => void;
  connectSystemAudioSource: (source: AudioNode) => void;
  disconnectSystemAudioSource: () => void;
  isMicrophoneConnected: () => boolean;
  isSystemAudioConnected: () => boolean;
  getConnectionStatus: () => { microphone: boolean, systemAudio: boolean };
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IAudioDeviceService.ts
// Interface for managing audio devices and their connections


// Interface for audio source
export interface AudioSource {
  stream: MediaStream;
  source: MediaStreamAudioSourceNode;
  speakerName: string;
  isSystemAudio?: boolean;
  oscillator?: OscillatorNode;
}

export interface IAudioDeviceService {
  getAudioDevices: () => MediaDeviceInfo[];
  setAudioDevices: (devices: MediaDeviceInfo[]) => void;
  getSources: () => Record<string, AudioSource>;
  connectDevice: (deviceId: string | null) => Promise<boolean>;
  disconnectDevice: (deviceId: string) => void;
  getSpeakerNameForDevice: (device: MediaDeviceInfo) => string;
  createSilentSource: (channelIndex: number, speakerName: string) => string | null;
  isSystemAudioDevice: (device: MediaDeviceInfo) => boolean;
  filterDevicesForUI: () => { 
    microphoneDevices: MediaDeviceInfo[],
    systemAudioDevices: MediaDeviceInfo[]
  };
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IMicrophoneContext.ts
// Interface for the microphone context

// Enums for microphone states and events
export enum MicrophoneEvents {
  DataAvailable = "dataavailable",
  Error = "error",
  Pause = "pause",
  Resume = "resume",
  Start = "start",
  Stop = "stop",
}

export enum MicrophoneState {
  NotSetup = -1,
  SettingUp = 0,
  Ready = 1,
  Opening = 2,
  Open = 3,
  Error = 4,
  Stopping = 5,
  Stopped = 6,
  Resuming = 7
}

// Basic types
export type SelectedDevices = {
  microphone: string | null;
  systemAudio: string | null;
};

export type SpeakerMapping = {
  [deviceId: string]: string;
};

// Interface for channel analysis
export interface ChannelAnalysis {
  channelCount: number;
  totalSamples: number;
  sampleRate: number;
  durationSeconds?: number;
  channels: {
    avgVolume: number;
    rmsVolume?: number;
    peakVolume: number;
    hasAudio: boolean;
    sampleValues?: number[];
  }[];
  error?: string;
}

// Interface for the microphone context that will be exposed to components
export interface IMicrophoneContext {
  microphone: MediaRecorder | null;
  startMicrophone: () => void;
  stopMicrophone: (forceReset?: boolean) => void;
  setupMicrophone: (deviceIds?: string[]) => Promise<boolean>;
  resetAudioSystem: (autoRestart?: boolean) => Promise<void>;
  microphoneState: MicrophoneState;
  getCurrentMicrophoneState: () => MicrophoneState;
  audioDevices: MediaDeviceInfo[];
  selectedDevices: SelectedDevices;
  setSelectedDevices: React.Dispatch<React.SetStateAction<SelectedDevices>>;
  disconnectSource: (deviceId: string) => void;
  handleDeviceChange: (deviceId: string, isSystemAudio: boolean) => void;
  setIsMicrophoneOn: React.Dispatch<React.SetStateAction<boolean>>;
  setIsSystemAudioOn: React.Dispatch<React.SetStateAction<boolean>>;
  isMicrophoneOn: boolean;
  isSystemAudioOn: boolean;
  speakerMappings: SpeakerMapping;
  generateTestWAV: () => Promise<ChannelAnalysis | null>;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IRecorderService.ts
// Interface for managing audio recording with MediaRecorder


export interface IRecorderService {
  createMediaRecorder: () => MediaRecorder | null;
  startRecording: () => void;
  stopRecording: () => void;
  configureRecorderEvents: (recorder: MediaRecorder) => void;
  getCurrentRecorder: () => MediaRecorder | null;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// AudioContextService.ts
// Implementation of the Web Audio API audio context service

import { IAudioContextService } from "../interfaces/IAudioContextService";

export class AudioContextService implements IAudioContextService {
  private audioContext: AudioContext | null = null;
  private merger: ChannelMergerNode | null = null;
  private destination: MediaStreamAudioDestinationNode | null = null;
  
  // Track audio sources connected to the merger
  private microphoneSource: AudioNode | null = null;
  private systemAudioSource: AudioNode | null = null;
  
  // Nodes for automatic gain control
  private microphoneGain: GainNode | null = null;
  private systemAudioGain: GainNode | null = null;
  private microphoneAnalyser: AnalyserNode | null = null;
  private systemAudioAnalyser: AnalyserNode | null = null;
  private microphoneFilter: BiquadFilterNode | null = null;
  
  // Parameters for automatic gain control
  private readonly MIN_VOLUME_THRESHOLD = 0.01; // Minimum volume level to consider amplification
  private readonly TARGET_VOLUME = 0.3;         // Target volume after amplification
  private readonly DEFAULT_MAX_GAIN = 5.0;      // Maximum gain for common microphones
  private readonly AIRPODS_MAX_GAIN = 12.0;     // Maximum gain for AirPods
  private readonly ANALYSIS_INTERVAL_MS = 100;  // Analysis interval in ms
  
  // Track device type
  private isMicrophoneAirPods = false;
  
  // Analysis timers
  private microphoneAnalysisTimer: NodeJS.Timeout | null = null;
  private systemAudioAnalysisTimer: NodeJS.Timeout | null = null;
  
  private readonly CHANNEL_COUNT = 2;
  private readonly LATENCY_HINT = 'interactive';

  // Access methods - return audio context components
  getAudioContext() {
    return this.audioContext;
  }

  getMerger() {
    return this.merger;
  }

  getDestination() {
    return this.destination;
  }

  // Method to verify the number of channels being processed
  getChannelInfo(): { count: number, mode: ChannelCountMode, interpretation: ChannelInterpretation } | null {
    if (!this.destination) return null;
    
    return {
      count: this.destination.channelCount,
      mode: this.destination.channelCountMode,
      interpretation: this.destination.channelInterpretation
    };
  }

  // Setup the audio context and its components
  setupAudioContext(): void {
    try {
      // Create the audio context if it doesn't exist
      if (!this.audioContext) {
        this.createNewAudioContext();
      } else if (this.audioContext.state === "suspended") {
        this.resumeAudioContext();
      }
    } catch (error) {
      console.error("❌ Error configuring AudioContext:", error);
    }
  }

  // Main method to configure and connect microphone source
  configureAndConnectMicrophoneSource(source: AudioNode, deviceInfo?: MediaDeviceInfo): void {
    if (!this.merger || !this.audioContext) {
      console.error("❌ AudioContext or merger not available to connect microphone");
      return;
    }
    
    try {
      // Disconnect previous source if it exists
      this.disconnectMicrophoneSource();
      
      // Check if the device is AirPods
      this.isMicrophoneAirPods = false;
      if (deviceInfo && deviceInfo.label) {
        this.isMicrophoneAirPods = deviceInfo.label.toLowerCase().includes("airpods");
        console.log(`🎤 Device detected: ${deviceInfo.label} ${this.isMicrophoneAirPods ? '(AirPods)' : ''}`);
      }
      
      // Create processing nodes for analysis and automatic gain control
      this.microphoneAnalyser = this.audioContext.createAnalyser();
      this.microphoneAnalyser.fftSize = 256;
      this.microphoneAnalyser.smoothingTimeConstant = 0.8;
      
      // Create equalization filter to improve voice
      this.microphoneFilter = this.audioContext.createBiquadFilter();
      this.microphoneFilter.type = "peaking";
      this.microphoneFilter.frequency.value = 1500;
      this.microphoneFilter.Q.value = 1;
      this.microphoneFilter.gain.value = 4;
      
      // Create gain node for volume control
      this.microphoneGain = this.audioContext.createGain();
      this.microphoneGain.gain.value = 1.0; // Start with neutral gain
      
      // Connect processing chain: source -> analyzer -> filter -> gain -> merger
      source.connect(this.microphoneAnalyser);
      this.microphoneAnalyser.connect(this.microphoneFilter);
      this.microphoneFilter.connect(this.microphoneGain);
      this.microphoneGain.connect(this.merger, 0, 0);
      
      // Store source
      this.microphoneSource = source;
      
      const deviceType = this.isMicrophoneAirPods ? 'AirPods' : 'default';
      console.log(`🎤 Microphone source (${deviceType}) connected to merger channel 0 with EQ and adaptive gain`);
      
      // Start periodic analysis for gain adjustment
      this.startMicrophoneVolumeAnalysis();
    } catch (error) {
      console.error("❌ Error configuring and connecting microphone source:", error);
    }
  }

  // Connect microphone source to merger (Channel 0)
  connectMicrophoneSource(source: AudioNode, deviceInfo?: MediaDeviceInfo): void {
    this.configureAndConnectMicrophoneSource(source, deviceInfo);
  }
  
  // Disconnect microphone source
  disconnectMicrophoneSource(): void {
    if (!this.merger || !this.microphoneSource) return;
    
    try {
      // Verify if the source is really connected before trying to disconnect
      // using try/catch to capture possible errors
      this.microphoneSource.disconnect(this.microphoneAnalyser || this.merger);
      this.microphoneSource = null;
      
      // Disconnect and clean processing nodes
      if (this.microphoneAnalyser) {
        this.microphoneAnalyser.disconnect();
        this.microphoneAnalyser = null;
      }
      
      if (this.microphoneFilter) {
        this.microphoneFilter.disconnect();
        this.microphoneFilter = null;
      }
      
      if (this.microphoneGain) {
        this.microphoneGain.disconnect();
        this.microphoneGain = null;
      }
      
      // Stop volume analysis
      this.stopMicrophoneVolumeAnalysis();
      
      console.log("🎤 Microphone source disconnected from merger");
    } catch (error) {
      // Verify if the error is about disconnecting a node that is not connected
      if (error instanceof DOMException && 
          error.message.includes("the given destination is not connected")) {
        // Apenas limpar a referência ao nó, já que ele não está conectado
        this.microphoneSource = null;
        console.log("🎤 Microphone source reference cleared (not connected)");
      } else {
        // Reportar outros tipos de erros
        console.error("❌ Error disconnecting microphone source:", error);
      }
    }
  }
  
  // Connect system audio source to merger (Channel 1)
  connectSystemAudioSource(source: AudioNode): void {
    if (!this.merger || !this.audioContext) {
      console.error("❌ AudioContext or merger not available to connect system audio");
      return;
    }
    
    try {
      // Disconnect previous source if it exists
      this.disconnectSystemAudioSource();
      
      // Create processing nodes for analysis and automatic gain control
      this.systemAudioAnalyser = this.audioContext.createAnalyser();
      this.systemAudioAnalyser.fftSize = 256;
      this.systemAudioAnalyser.smoothingTimeConstant = 0.8;
      
      this.systemAudioGain = this.audioContext.createGain();
      this.systemAudioGain.gain.value = 1.0; // Iniciar com ganho neutro
      
      // Connect nodes: source -> analyzer -> gain -> merger
      source.connect(this.systemAudioAnalyser);
      this.systemAudioAnalyser.connect(this.systemAudioGain);
      this.systemAudioGain.connect(this.merger, 0, 1);
      
      // Store source
      this.systemAudioSource = source;
      
      console.log("🔊 System audio source connected to channel 1 of merger with automatic gain control");
      
      // Start periodic analysis for gain adjustment
      this.startSystemAudioVolumeAnalysis();
    } catch (error) {
      console.error("❌ Error connecting system audio source:", error);
    }
  }
  
  // Disconnect system audio source
  disconnectSystemAudioSource(): void {
    if (!this.merger || !this.systemAudioSource) return;
    
    try {
      // Verify if the source is really connected before trying to disconnect
      // using try/catch to capture possible errors
      this.systemAudioSource.disconnect(this.systemAudioAnalyser || this.merger);
      this.systemAudioSource = null;
      
      // Disconnect and clean processing nodes
      if (this.systemAudioAnalyser) {
        this.systemAudioAnalyser.disconnect();
        this.systemAudioAnalyser = null;
      }
      
      if (this.systemAudioGain) {
        this.systemAudioGain.disconnect();
        this.systemAudioGain = null;
      }
      
      // Stop volume analysis
      this.stopSystemAudioVolumeAnalysis();
      
      console.log("🔊 System audio source disconnected from merger");
    } catch (error) {
      // Verify if the error is about disconnecting a node that is not connected
      if (error instanceof DOMException && 
          error.message.includes("the given destination is not connected")) {
        // Just clear the node reference, since it's not connected
        this.systemAudioSource = null;
        console.log("🔊 System audio source reference cleared (not connected)");
      } else {
        // Report other types of errors
        console.error("❌ Error disconnecting system audio source:", error);
      }
    }
  }

  // Reset the audio system, optionally forcing a complete closure
  async resetAudioSystem(forceClose: boolean = false): Promise<void> {
    console.log(`🔄 Resetting audio system (forceClose: ${forceClose})`);
    
    try {
      // Stop volume analyses to avoid pending references
      this.stopMicrophoneVolumeAnalysis();
      this.stopSystemAudioVolumeAnalysis();
      
      // Disconnect all audio sources
      this.disconnectMicrophoneSource();
      this.disconnectSystemAudioSource();
      
      if (forceClose && this.audioContext) {
        console.log("🔒 Closing AudioContext completely for clean restart");
        // Close the audio context completely
        await this.closeAudioContext();
        
        // Small pause to ensure the closure is completed
        await new Promise(resolve => setTimeout(resolve, 300));
        
        // Create a new audio context from scratch
        this.createNewAudioContext();
        console.log("✅ Audio system restarted with new AudioContext");
      } else if (this.audioContext) {
        // If the context is reused, ensure it is in a usable state
        if (this.audioContext.state === "suspended") {
          console.log("🔄 Resuming existing AudioContext");
          await this.audioContext.resume();
        }
        
        // Verify if essential nodes exist, recreate if necessary
        if (!this.merger || !this.destination) {
          console.log("🔄 Recreating audio nodes");
          this.setupAudioNodes();
          this.connectAudioNodes();
        }
        
        console.log("✅ Audio system reset");
      } else {
        // No audio context, create a new one
        console.log("🔄 Creating new AudioContext (none existing)");
        this.createNewAudioContext();
        console.log("✅ New audio system initialized");
      }
      
      return;
    } catch (error) {
      console.error("❌ Error resetting audio system:", error);
      
      // In case of a critical error, try to clean everything and start over
      this.resetState();
      
      // Try to create a new context even after error
      try {
        this.createNewAudioContext();
        console.log("⚠️ Audio system re-created after error");
      } catch (secondError) {
        console.error("❌ Critical error re-creating audio system:", secondError);
        throw new Error("Audio system restart failed");
      }
    }
  }

  // Close the audio context and clean resources
  async closeAudioContext(): Promise<void> {
    if (!this.audioContext) return;
    
    try {
      console.log("🔄 Closing AudioContext...");
      
      // Disconnect all sources first
      this.disconnectMicrophoneSource();
      this.disconnectSystemAudioSource();
      
      // Stop volume analyses to avoid pending callbacks
      this.stopMicrophoneVolumeAnalysis();
      this.stopSystemAudioVolumeAnalysis();
      
      // Verify if there are audio nodes and disconnect them explicitly
      if (this.merger && this.destination) {
        try {
          this.merger.disconnect(this.destination);
          console.log("🔌 Audio nodes disconnected explicitly");
        } catch (err) {
          console.warn("⚠️ Could not disconnect audio nodes:", err);
        }
      }
      
      // Close the audio context
      await this.audioContext.close();
      
      // Reset state variables
      this.resetState();
      console.log("✅ AudioContext closed successfully and resources released");
    } catch (error) {
      console.error("❌ Error closing AudioContext:", error);
      // In case of an error, try to reset the state anyway
      throw error;
    }
  }

  // Verify if a microphone source is connected
  isMicrophoneConnected(): boolean {
    return this.microphoneSource !== null;
  }
  
  // Verify if a system audio source is connected
  isSystemAudioConnected(): boolean {
    return this.systemAudioSource !== null;
  }
  
  // Returns the connection status of both channels
  getConnectionStatus(): { microphone: boolean, systemAudio: boolean } {
    return {
      microphone: this.isMicrophoneConnected(),
      systemAudio: this.isSystemAudioConnected()
    };
  }

  // Start microphone volume analysis
  private startMicrophoneVolumeAnalysis(): void {
    if (this.microphoneAnalysisTimer) {
      clearInterval(this.microphoneAnalysisTimer);
    }
    
    this.microphoneAnalysisTimer = setInterval(() => {
      if (!this.microphoneAnalyser || !this.microphoneGain) return;
      
      const volume = this.getVolumeFromAnalyser(this.microphoneAnalyser);
      this.adjustGainForVolume(volume, this.microphoneGain, "microphone", this.isMicrophoneAirPods);
    }, this.ANALYSIS_INTERVAL_MS);
  }
  
  // Stop microphone volume analysis
  private stopMicrophoneVolumeAnalysis(): void {
    if (this.microphoneAnalysisTimer) {
      clearInterval(this.microphoneAnalysisTimer);
      this.microphoneAnalysisTimer = null;
    }
  }
  
  // Start system audio volume analysis
  private startSystemAudioVolumeAnalysis(): void {
    if (this.systemAudioAnalysisTimer) {
      clearInterval(this.systemAudioAnalysisTimer);
    }
    
    this.systemAudioAnalysisTimer = setInterval(() => {
      if (!this.systemAudioAnalyser || !this.systemAudioGain) return;
      
      const volume = this.getVolumeFromAnalyser(this.systemAudioAnalyser);
      this.adjustGainForVolume(volume, this.systemAudioGain, "system", false);
    }, this.ANALYSIS_INTERVAL_MS);
  }
  
  // Stop system audio volume analysis
  private stopSystemAudioVolumeAnalysis(): void {
    if (this.systemAudioAnalysisTimer) {
      clearInterval(this.systemAudioAnalysisTimer);
      this.systemAudioAnalysisTimer = null;
    }
  }
  
  // Get the current volume from the analyzer
  private getVolumeFromAnalyser(analyser: AnalyserNode): number {
    const dataArray = new Uint8Array(analyser.frequencyBinCount);
    analyser.getByteTimeDomainData(dataArray);
    
    // Calculate RMS (Root Mean Square) - standard metric for audio volume
    let sumSquares = 0;
    for (let i = 0; i < dataArray.length; i++) {
      // Converter de [0, 255] para [-1, 1]
      const normalized = (dataArray[i] - 128) / 128;
      sumSquares += normalized * normalized;
    }
    
    const rms = Math.sqrt(sumSquares / dataArray.length);
    return rms;
  }
  
  // Adjust gain based on volume
  private adjustGainForVolume(
    volume: number, 
    gainNode: GainNode, 
    sourceType: string, 
    isAirPods: boolean = false
  ): void {
    // Set maximum gain based on device type
    const maxGain = isAirPods ? this.AIRPODS_MAX_GAIN : this.DEFAULT_MAX_GAIN;
    const deviceTypeStr = isAirPods ? "AirPods" : "default";
    
    // Verify if the volume is below the minimum threshold
    if (volume < this.MIN_VOLUME_THRESHOLD && volume > 0.001) {
      // Calculate the gain needed to reach the target volume, but limit to maximum
      const requiredGain = Math.min(this.TARGET_VOLUME / volume, maxGain);
      
      // Adjust gain gradually to avoid clicks
      const currentGain = gainNode.gain.value;
      const newGain = currentGain * 0.8 + requiredGain * 0.2; // Blend suave
      
      gainNode.gain.setValueAtTime(newGain, this.audioContext?.currentTime || 0);
      
      // Log ocasional for diagnosis (1 in every 10 adjustments)
      if (Math.random() < 0.1) {
        console.log(`🔊 Adjusting ${sourceType} (${deviceTypeStr}): volume=${volume.toFixed(3)}, gain=${newGain.toFixed(2)}x, max=${maxGain}x`);
      }
    } else if (volume >= this.MIN_VOLUME_THRESHOLD || volume <= 0.001) {
      // If the volume is above the threshold or effectively silent, gradually return to normal gain
      const currentGain = gainNode.gain.value;
      
      // If we are significantly above the normal gain, gradually reduce
      if (currentGain > 1.2) {
        const newGain = currentGain * 0.95 + 1.0 * 0.05; // Gradually return to gain 1.0
        gainNode.gain.setValueAtTime(newGain, this.audioContext?.currentTime || 0);
        
        // Log ocasional
        if (Math.random() < 0.1) {
          console.log(`🔊 Normalizing gain of ${sourceType} (${deviceTypeStr}): ${newGain.toFixed(2)}x, volume=${volume.toFixed(3)}`);
        }
      }
    }
  }

  // Private methods for better organization
  
  private createNewAudioContext(): void {
    console.log("🔊 Creating new AudioContext");
    
    try {
      // Create the context with ideal parameters for speech processing
      this.audioContext = new AudioContext({
        latencyHint: this.LATENCY_HINT,
        sampleRate: 16000
      });
      
      // Verify if the context started correctly
      this.ensureRunningState();
      
      // Configure the audio processing components
      this.setupAudioNodes();
      
      // Connect the audio nodes
      this.connectAudioNodes();
      
      // Log of complete configuration
      this.logAudioSetup();
    } catch (error) {
      console.error("❌ Error creating AudioContext:", error);
      this.resetState();
      throw new Error("Could not create audio context");
    }
  }
  
  private ensureRunningState(): void {
    if (!this.audioContext) return;
    
    if (this.audioContext.state !== "running") {
      console.log(`⚠️ AudioContext in state ${this.audioContext.state}, trying to resume...`);
      
      this.audioContext.resume()
        .then(() => {
          if (this.audioContext) {
            console.log(`✅ AudioContext now: ${this.audioContext.state}`);
          }
        })
        .catch(error => {
          console.error("❌ Error resuming AudioContext:", error);
        });
    }
  }
  
  private resumeAudioContext(): void {
    if (!this.audioContext) return;
    
    console.log("🔄 Resuming suspended AudioContext");
    
    this.audioContext.resume()
      .catch(error => {
        console.error("❌ Error resuming AudioContext suspenso:", error);
      });
  }
  
  private setupAudioNodes(): void {
    if (!this.audioContext) return;
    
    // Configure the merger to combine audio channels
    console.log("🔀 Creating ChannelMerger");
    this.merger = this.audioContext.createChannelMerger(this.CHANNEL_COUNT);
    this.merger.channelInterpretation = 'discrete';
    this.merger.channelCountMode = 'explicit';
    
    // Configure the destination to receive processed audio
    console.log("🎯 Configurando MediaStreamAudioDestinationNode");
    this.destination = this.audioContext.createMediaStreamDestination();
    this.destination.channelCount = this.CHANNEL_COUNT;
    this.destination.channelCountMode = 'explicit';
    this.destination.channelInterpretation = 'discrete';
  }
  
  private connectAudioNodes(): void {
    if (!this.merger || !this.destination) {
      console.error("❌ Audio nodes not available for connection");
      return;
    }
    
    try {
      this.merger.connect(this.destination);
      console.log("🔌 ChannelMerger connected to MediaStreamAudioDestinationNode");
    } catch (error) {
      console.error("❌ Error connecting audio nodes:", error);
    }
  }
  
  private logAudioSetup(): void {
    if (!this.audioContext || !this.destination) return;
    
    console.log(`🎛️ Native sample rate: ${this.audioContext.sampleRate}Hz`);
    console.log(`🎚️ Number of channels: ${this.destination.channelCount}`);
    
    if (this.destination.stream) {
      console.log(`🔍 Destination stream has ${this.destination.stream.getAudioTracks().length} audio tracks`);
    }
  }
  
  private resetState(): void {
    // Stop volume analyses
    this.stopMicrophoneVolumeAnalysis();
    this.stopSystemAudioVolumeAnalysis();
    
    this.microphoneSource = null;
    this.systemAudioSource = null;
    this.microphoneGain = null;
    this.systemAudioGain = null;
    this.microphoneAnalyser = null;
    this.systemAudioAnalyser = null;
    this.microphoneFilter = null;
    this.audioContext = null;
    this.merger = null; 
    this.destination = null;
    this.isMicrophoneAirPods = false;
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// AudioDeviceService.ts
// Implementation of the audio device service

import { getPrimaryUser } from "../../../config/UserConfig";
import { IAudioContextService } from "../interfaces/IAudioContextService";
import {
  AudioSource,
  IAudioDeviceService,
} from "../interfaces/IAudioDeviceService";

// Audio configuration constants
const AUDIO_CONFIG = {
  SYSTEM_AUDIO_IDENTIFIERS: ["blackhole", "dipper"],
  DEFAULT_USER_NAME: getPrimaryUser(),
  CHANNEL: {
    MICROPHONE: 0,
    SYSTEM_AUDIO: 1,
  },
  GAIN: {
    SILENT: 0.001, // -60dB, practically inaudible
  },
  FREQUENCY: {
    MICROPHONE: 440, // A4
    SYSTEM_AUDIO: 880, // A5
  },
};

export class AudioDeviceService implements IAudioDeviceService {
  private _audioDevices: MediaDeviceInfo[] = [];
  private sources: Record<string, AudioSource> = {};
  private audioContextService: IAudioContextService;

  constructor(audioContextService: IAudioContextService) {
    this.audioContextService = audioContextService;
  }

  // --- Methods for device management ---

  getAudioDevices() {
    return this._audioDevices;
  }

  setAudioDevices(devices: MediaDeviceInfo[]) {
    this._audioDevices = devices;
  }

  getSources() {
    return this.sources;
  }

  // Filter devices for microphone and system audio
  filterDevicesForUI(): {
    microphoneDevices: MediaDeviceInfo[];
    systemAudioDevices: MediaDeviceInfo[];
  } {
    const microphoneDevices: MediaDeviceInfo[] = [];
    const systemAudioDevices: MediaDeviceInfo[] = [];

    this._audioDevices.forEach((device) => {
      if (this.isSystemAudioDevice(device)) {
        systemAudioDevices.push(device);
      } else {
        microphoneDevices.push(device);
      }
    });

    return { microphoneDevices, systemAudioDevices };
  }

  // --- Methods for device type detection ---

  // Determine speaker name based on device
  getSpeakerNameForDevice(device: MediaDeviceInfo): string {
    return this.isSystemAudioDevice(device)
      ? ""
      : AUDIO_CONFIG.DEFAULT_USER_NAME;
  }

  // Determine if a device is system audio
  isSystemAudioDevice(device: MediaDeviceInfo): boolean {
    const label = device.label.toLowerCase();
    return AUDIO_CONFIG.SYSTEM_AUDIO_IDENTIFIERS.some((id) =>
      label.includes(id)
    );
  }

  // --- Methods for connecting cognitive audio devices ---

  // Method to connect an audio device for brain input
  async connectDevice(deviceId: string | null): Promise<boolean> {
    if (!this.isValidDeviceId(deviceId) || this.sources[deviceId!]) {
      return false;
    }

    try {
      // Ensure the AudioContext is configured
      this.audioContextService.setupAudioContext();

      const audioContext = this.audioContextService.getAudioContext();
      const merger = this.audioContextService.getMerger();

      if (!this.validateAudioComponents(audioContext, merger)) return false;

      // Find the corresponding device
      const device = this._audioDevices.find((d) => d.deviceId === deviceId);
      if (!device) {
        console.log(
          "🎚️ [COGNITIVE-AUDIO-DEVICE] Selected audio device for brain input not found:",
          deviceId
        );
        return false;
      }

      const isSystemAudio = this.isSystemAudioDevice(device);
      const deviceType = isSystemAudio ? "System Audio" : "Microphone";
      console.log(
        "🎚️ [COGNITIVE-AUDIO-DEVICE] Connecting audio device for brain input:",
        deviceType,
        device.label,
        deviceId
      );

      // Get the audio stream from the device
      const stream = await this.captureDeviceStream(deviceId!);
      if (!stream || stream.getAudioTracks().length === 0) {
        console.log(
          "❌ [COGNITIVE-AUDIO-DEVICE] No audio tracks found for device:",
          deviceId
        );
        return false;
      }

      // Log the stream settings
      this.logStreamSettings(stream);

      // Create and connect the source node
      const source = audioContext!.createMediaStreamSource(stream);
      const channelIndex = isSystemAudio
        ? AUDIO_CONFIG.CHANNEL.SYSTEM_AUDIO
        : AUDIO_CONFIG.CHANNEL.MICROPHONE;
      const speakerName = this.getSpeakerNameForDevice(device);

      // Configure and connect the source node
      this.configureAndConnectSource(source, merger!, channelIndex);

      // Store the source
      this.sources[deviceId!] = {
        stream,
        source,
        speakerName,
        isSystemAudio,
      };

      console.log(
        "✅ [COGNITIVE-AUDIO-DEVICE] Audio device connected to channel",
        channelIndex,
        "with speaker:",
        speakerName || "Default diarization"
      );
      return true;
    } catch (error) {
      console.log(
        "❌ [COGNITIVE-AUDIO-DEVICE] Error connecting audio device for brain input:",
        error
      );
      return false;
    }
  }

  // Disconnect an audio source
  disconnectDevice(deviceId: string) {
    if (!this.isValidDeviceId(deviceId)) return;

    if (this.sources[deviceId]) {
      try {
        // Disconnect the audio source
        this.sources[deviceId].source.disconnect();

        // Stop the oscillator if it exists (for silent sources)
        const sourceObj = this.sources[deviceId];
        const oscillator = sourceObj.oscillator;
        if (oscillator) {
          oscillator.stop();
          oscillator.disconnect();
        }

        // Stop all audio tracks
        for (const track of this.sources[deviceId].stream.getAudioTracks()) {
          track.enabled = false;
          track.stop();
        }

        // Remove the source
        delete this.sources[deviceId];
        console.log(`✅ Source ${deviceId} disconnected`);
      } catch (err) {
        console.error(`❌ Error disconnecting source ${deviceId}:`, err);
      }
    }
  }

  // Create a silent audio source for a specific channel
  createSilentSource(channelIndex: number, speakerName: string): string | null {
    const audioContext = this.audioContextService.getAudioContext();
    const merger = this.audioContextService.getMerger();

    if (!this.validateAudioComponents(audioContext, merger)) return null;

    // Security check for valid channels
    if (!this.isValidChannelIndex(channelIndex)) return null;

    try {
      console.log(`🔊 Creating silent source for channel ${channelIndex}...`);

      // Create silent audio source
      const { oscillator, gainNode } = this.createSilentAudioSource(
        audioContext!,
        channelIndex
      );

      // Connect to merger at specific channel
      gainNode.connect(merger!, 0, channelIndex);
      oscillator.start();

      // Create unique ID for this source
      const fakeDeviceId = `silent-channel-${channelIndex}-${Date.now()}`;
      const isSystemAudio = channelIndex === AUDIO_CONFIG.CHANNEL.SYSTEM_AUDIO;

      // Store reference
      this.sources[fakeDeviceId] = {
        stream: new MediaStream(),
        source: gainNode as unknown as MediaStreamAudioSourceNode,
        speakerName,
        isSystemAudio,
        oscillator,
      };

      console.log(
        `✅ Silent source created successfully for channel ${channelIndex}`
      );
      this.verifyDestinationTracks();

      return fakeDeviceId;
    } catch (error) {
      console.error("❌ Error creating silent source:", error);
      return null;
    }
  }

  // --- Private helper methods ---

  private isValidDeviceId(deviceId: string | null): boolean {
    return Boolean(deviceId && deviceId !== "N/A" && deviceId !== "");
  }

  private isValidChannelIndex(channelIndex: number): boolean {
    if (
      channelIndex !== AUDIO_CONFIG.CHANNEL.MICROPHONE &&
      channelIndex !== AUDIO_CONFIG.CHANNEL.SYSTEM_AUDIO
    ) {
      console.error(
        `❌ Invalid channel ${channelIndex}. Only channels 0 and 1 are allowed!`
      );
      return false;
    }
    return true;
  }

  private validateAudioComponents(
    audioContext: AudioContext | null,
    merger: ChannelMergerNode | null
  ): boolean {
    if (!audioContext || !merger) {
      console.error("❌ AudioContext or merger not available");
      return false;
    }
    return true;
  }

  private async captureDeviceStream(
    deviceId: string
  ): Promise<MediaStream | null> {
    try {
      // Get device information
      const device = this._audioDevices.find((d) => d.deviceId === deviceId);
      if (!device) {
        console.error(`❌ Device not found for ID: ${deviceId}`);
        return null;
      }

      const deviceLabel = device.label.toLowerCase();
      console.log(`🎤 Requesting device: ${deviceLabel}`);

      // Check and request microphone permissions on macOS
      if (typeof window !== "undefined" && window.electronAPI) {
        try {
          console.log("🔒 Checking microphone permissions...");
          const permissionResult =
            await window.electronAPI.requestMicrophonePermission();

          if (!permissionResult.success) {
            console.error(
              "❌ Microphone permission denied:",
              permissionResult.error
            );
            throw new Error(
              permissionResult.error || "Microphone permission was denied"
            );
          }

          console.log("✅ Microphone permission granted");
        } catch (permissionError) {
          console.error(
            "❌ Error checking microphone permissions:",
            permissionError
          );
          throw new Error(
            `Permission check failed: ${
              permissionError instanceof Error
                ? permissionError.message
                : "Unknown error"
            }`
          );
        }
      }

      // Check if device is available and not in use
      const isDeviceAvailable = await this.checkDeviceAvailability(deviceId);
      if (!isDeviceAvailable) {
        throw new Error(
          "Device is not available or may be in use by another application"
        );
      }

      // Try with different constraint configurations for better compatibility
      const constraintVariations = [
        // First try: Optimal settings for transcription
        {
          audio: {
            deviceId: { exact: deviceId },
            echoCancellation: false,
            noiseSuppression: false,
            channelCount: 1,
          },
        },
        // Second try: More permissive settings
        {
          audio: {
            deviceId: { exact: deviceId },
            echoCancellation: true,
            noiseSuppression: true,
          },
        },
        // Third try: Minimal constraints
        {
          audio: {
            deviceId: { exact: deviceId },
          },
        },
        // Fourth try: Just deviceId preference
        {
          audio: {
            deviceId: { ideal: deviceId },
          },
        },
      ];

      let stream: MediaStream | null = null;
      let lastError: Error | null = null;

      for (let i = 0; i < constraintVariations.length; i++) {
        try {
          console.log(
            `🔄 Trying constraint variation ${i + 1}/${
              constraintVariations.length
            }`
          );
          stream = await navigator.mediaDevices.getUserMedia(
            constraintVariations[i]
          );
          console.log(
            `✅ Successfully connected with constraint variation ${i + 1}`
          );
          break;
        } catch (error) {
          console.log(`⚠️ Constraint variation ${i + 1} failed:`, error);
          lastError = error as Error;

          // Wait a bit before trying next variation
          if (i < constraintVariations.length - 1) {
            await new Promise((resolve) => setTimeout(resolve, 500));
          }
        }
      }

      if (!stream) {
        throw lastError || new Error("All constraint variations failed");
      }

      // Log the actual settings obtained
      this.logStreamSettings(stream);

      return stream;
    } catch (error) {
      console.error("❌ Error capturing stream:", error);

      // Enhanced error logging for debugging
      if (error instanceof DOMException) {
        console.error("❌ DOMException details:", {
          name: error.name,
          message: error.message,
          code: error.code,
          stack: error.stack,
        });

        // Specific handling for common errors
        if (error.name === "NotAllowedError") {
          console.error("❌ NotAllowedError - possible causes:");
          console.error("   1. Browser permissions not granted");
          console.error("   2. Device is being used by another application");
          console.error("   3. Hardware access blocked by system");
          console.error(
            "   4. Electron webContents permission handler blocking access"
          );
        } else if (error.name === "NotFoundError") {
          console.error("❌ NotFoundError - device not found or unavailable");
        } else if (error.name === "OverconstrainedError") {
          console.error(
            "❌ OverconstrainedError - constraints cannot be satisfied"
          );
        }
      }

      return null;
    }
  }

  private configureAndConnectSource(
    source: MediaStreamAudioSourceNode,
    merger: ChannelMergerNode,
    channelIndex: number
  ): void {
    // Source configurations
    source.channelCount = 1;
    source.channelCountMode = "explicit";
    source.channelInterpretation = "discrete";

    // Connect to specific channel in merger
    source.connect(merger, 0, channelIndex);
    console.log(`🔀 Source connected to channel ${channelIndex} of merger`);
  }

  private createSilentAudioSource(
    audioContext: AudioContext,
    channelIndex: number
  ): {
    oscillator: OscillatorNode;
    gainNode: GainNode;
  } {
    // Frequency based on channel
    const frequency =
      channelIndex === AUDIO_CONFIG.CHANNEL.MICROPHONE
        ? AUDIO_CONFIG.FREQUENCY.MICROPHONE
        : AUDIO_CONFIG.FREQUENCY.SYSTEM_AUDIO;

    // Create oscillator
    const oscillator = audioContext.createOscillator();
    oscillator.type = "sine";
    oscillator.frequency.value = frequency;

    // Create gain node to control volume
    const gainNode = audioContext.createGain();
    gainNode.gain.value = AUDIO_CONFIG.GAIN.SILENT;

    // Configure gain node parameters
    gainNode.channelCount = 1;
    gainNode.channelCountMode = "explicit";
    gainNode.channelInterpretation = "discrete";

    // Connect oscillator to gain node
    oscillator.connect(gainNode);

    return { oscillator, gainNode };
  }

  private logStreamSettings(stream: MediaStream): void {
    const trackSettings = stream.getAudioTracks()[0].getSettings();
    console.log(
      `🎛️ Stream: sampleRate=${
        trackSettings.sampleRate || "unknown"
      }, channelCount=${trackSettings.channelCount || "unknown"}`
    );
  }

  private verifyDestinationTracks(): void {
    setTimeout(() => {
      const destination = this.audioContextService.getDestination();
      if (destination) {
        const trackCount = destination.stream.getAudioTracks().length;
        console.log(`🔍 Destination stream has ${trackCount} audio tracks`);
      }
    }, 100);
  }

  private async checkDeviceAvailability(deviceId: string): Promise<boolean> {
    try {
      // Try to enumerate devices first to see if device still exists
      const devices = await navigator.mediaDevices.enumerateDevices();
      const targetDevice = devices.find((d) => d.deviceId === deviceId);

      if (!targetDevice) {
        console.error("❌ Device not found in enumeration");
        return false;
      }

      // Quick test with minimal constraints to check availability
      try {
        const testStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            deviceId: { exact: deviceId },
          },
        });

        // Immediately release the test stream
        testStream.getTracks().forEach((track) => track.stop());
        console.log("✅ Device availability test passed");
        return true;
      } catch (error) {
        console.log("⚠️ Device availability test failed:", error);
        return false;
      }
    } catch (error) {
      console.error("❌ Error checking device availability:", error);
      return false;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// MicrophoneContextProvider.tsx
// Microphone context provider

import React, { createContext, Dispatch, SetStateAction, useCallback, useContext, useEffect, useReducer, useRef } from 'react';
import { getPrimaryUser } from '../../../config/UserConfig';
import {
  ChannelAnalysis,
  IMicrophoneContext,
  MicrophoneState,
  SelectedDevices,
  SpeakerMapping
} from '../interfaces/IMicrophoneContext';
import { AudioContextService } from './AudioContextService';
import { AudioDeviceService } from './AudioDeviceService';
import { RecorderService } from './RecorderService';

// Microphone context
export const MicrophoneContext = createContext<IMicrophoneContext | null>(null);

// Hook for usage in consumer components
export function useMicrophone() {
  const context = useContext(MicrophoneContext);
  if (!context) {
    throw new Error('useMicrophone must be used within a MicrophoneProvider');
  }
  return context;
}

// Tipo para o estado do microfone
type MicrophoneStateType = {
  microphone: MediaRecorder | null;
  microphoneState: MicrophoneState;
  audioDevices: MediaDeviceInfo[];
  selectedDevices: SelectedDevices;
  isMicrophoneOn: boolean;
  isSystemAudioOn: boolean;
  speakerMappings: SpeakerMapping;
}

// Tipo para ações do reducer
type MicrophoneAction = 
  | { type: 'SET_STATE'; payload: MicrophoneState }
  | { type: 'SET_MICROPHONE'; payload: MediaRecorder | null }
  | { type: 'SET_AUDIO_DEVICES'; payload: MediaDeviceInfo[] }
  | { type: 'SET_SELECTED_DEVICES'; payload: Partial<SelectedDevices> }
  | { type: 'SET_MICROPHONE_ON'; payload: boolean }
  | { type: 'SET_SYSTEM_AUDIO_ON'; payload: boolean }
  | { type: 'UPDATE_SPEAKER_MAPPING'; payload: SpeakerMapping }
  | { type: 'RESET_STATE' };

// Estado inicial
const initialState: MicrophoneStateType = {
  microphone: null,
  microphoneState: MicrophoneState.NotSetup,
  audioDevices: [],
  selectedDevices: { microphone: null, systemAudio: null },
  isMicrophoneOn: false,
  isSystemAudioOn: false,
  speakerMappings: {}
};

// Reducer to manage state
function microphoneReducer(state: MicrophoneStateType, action: MicrophoneAction): MicrophoneStateType {
  switch (action.type) {
    case 'SET_STATE':
      return { ...state, microphoneState: action.payload };
    case 'SET_MICROPHONE':
      return { ...state, microphone: action.payload };
    case 'SET_AUDIO_DEVICES':
      return { ...state, audioDevices: action.payload };
    case 'SET_SELECTED_DEVICES':
      return { 
        ...state, 
        selectedDevices: { ...state.selectedDevices, ...action.payload } 
      };
    case 'SET_MICROPHONE_ON':
      return { ...state, isMicrophoneOn: action.payload };
    case 'SET_SYSTEM_AUDIO_ON':
      return { ...state, isSystemAudioOn: action.payload };
    case 'UPDATE_SPEAKER_MAPPING':
      return { ...state, speakerMappings: { ...state.speakerMappings, ...action.payload } };
    case 'RESET_STATE':
      return { 
        ...initialState,
        audioDevices: state.audioDevices
      };
    default:
      return state;
  }
}

// Microphone context provider
export default function MicrophoneProvider({ children }: { children: React.ReactNode }) {
  // State management using reducer
  const [state, dispatch] = useReducer(microphoneReducer, initialState);
  
  // Reference to the current state (for use in async callbacks)
  const stateRef = useRef<MicrophoneState>(MicrophoneState.NotSetup);
  
  // Services
  const services = useRef({
    audioContext: new AudioContextService(),
    audioDevice: null as AudioDeviceService | null,
    recorder: null as RecorderService | null
  });
  
  // Update state reference when state changes
  useEffect(() => {
    stateRef.current = state.microphoneState;
  }, [state.microphoneState]);
  
  // Initialize services and configure event listeners
  useEffect(() => {
    // Initialize services
    const initializeServices = () => {
      // Initialize AudioDeviceService
      services.current.audioDevice = new AudioDeviceService(services.current.audioContext);
      
      // Initialize RecorderService
      services.current.recorder = new RecorderService(
        services.current.audioContext,
        (newState) => dispatch({ type: 'SET_STATE', payload: newState }),
        (recorder) => dispatch({ type: 'SET_MICROPHONE', payload: recorder })
      );
    };
    
    // Initialize services
    initializeServices();
    
    // Enumerate audio devices
    enumerateAudioDevices();
    
    // Configure event listener for device changes
    navigator.mediaDevices.addEventListener('devicechange', handleDeviceChangeEvent);
    
    // Handler for cleaning up resources before closing/reloading the page
    const handleBeforeUnload = (event: BeforeUnloadEvent) => {
      console.log("🔄 Page being closed or reloaded, shutting down audio system...");
      
      // Force audio system shutdown
      const { audioContext, audioDevice, recorder } = services.current;
      
      try {
        // Stop recording
        if (recorder) {
          recorder.stopRecording();
        }
        
        // Disconnect all sources
        if (audioDevice) {
          const sources = audioDevice.getSources();
          Object.keys(sources).forEach(deviceId => {
            audioDevice.disconnectDevice(deviceId);
          });
        }
        
        // Disconnect merger sources
        if (audioContext) {
          audioContext.disconnectMicrophoneSource();
          audioContext.disconnectSystemAudioSource();
          
          // Try to close the audio context synchronously
          try {
            audioContext.getAudioContext()?.close();
          } catch (error) {
            console.error("❌ Error closing AudioContext during unload:", error);
          }
        }
      } catch (error) {
        console.error("❌ Error cleaning up audio resources during unload:", error);
      }
      
      // Prevent system from keeping open resources
      return null;
    };
    
    // Add handler for beforeunload event
    window.addEventListener('beforeunload', handleBeforeUnload);
    
    // Check and turn off audio system if already running in the background
    const checkExistingAudioSystem = () => {
      const { audioContext } = services.current;
      if (audioContext && audioContext.getAudioContext()?.state === 'running') {
        console.warn("⚠️ Audio system already running, resetting to prevent resource leaks");
        resetAudioSystem(false);
      }
    };
    
    // Check existing audio system on startup
    checkExistingAudioSystem();
    
    // Cleanup on unmount
    return () => {
      stopMicrophone(true);
      navigator.mediaDevices.removeEventListener('devicechange', handleDeviceChangeEvent);
      window.removeEventListener('beforeunload', handleBeforeUnload);
      resetAudioSystem(false);
    };
  }, []);
  
  // Handle device change events
  const handleDeviceChangeEvent = useCallback(() => {
    console.log("🔄 Device change detected, re-enumerating devices");
    enumerateAudioDevices();
  }, []);
  
  // Enumerate audio devices
  const enumerateAudioDevices = async () => {
    try {
      const { audioDevice } = services.current;
      if (!audioDevice) return;
      
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputDevices = devices.filter(device => device.kind === 'audioinput');
      
      if (audioInputDevices.length > 0) {
        console.log(`🎙️ Found ${audioInputDevices.length} audio input devices`);
        
        // Map speakers to devices
        const speakerMap: SpeakerMapping = {};
        
        audioInputDevices.forEach((device) => {
          const speakerName = audioDevice.getSpeakerNameForDevice(device);
          if (speakerName) {
            speakerMap[device.deviceId] = speakerName;
          }
        });
        
        // Update device list and speaker mapping
        dispatch({ type: 'SET_AUDIO_DEVICES', payload: audioInputDevices });
        dispatch({ type: 'UPDATE_SPEAKER_MAPPING', payload: speakerMap });
        
        // Update device service
        audioDevice.setAudioDevices(audioInputDevices);
        
        // Automatically select the first available microphone and system audio device if none are selected
        const { systemAudioDevices, microphoneDevices } = audioDevice.filterDevicesForUI();
        
        if (!state.selectedDevices.microphone && microphoneDevices.length > 0) {
          console.log(`🎤 Automatically selecting the first available microphone: ${microphoneDevices[0].label}`);
          dispatch({ 
            type: 'SET_SELECTED_DEVICES', 
            payload: { microphone: microphoneDevices[0].deviceId } 
          });
        }
        
        if (!state.selectedDevices.systemAudio && systemAudioDevices.length > 0) {
          console.log(`🔊 Automatically selecting the first available system audio device: ${systemAudioDevices[0].label}`);
          dispatch({ 
            type: 'SET_SELECTED_DEVICES', 
            payload: { systemAudio: systemAudioDevices[0].deviceId } 
          });
        }
      } else {
        console.warn("⚠️ No audio input devices found!");
      }
    } catch (error) {
      console.error("❌ Error enumerating audio devices:", error);
    }
  };
  
  // Disconnect a specific audio source
  const disconnectSource = useCallback((deviceId: string) => {
    const { audioDevice } = services.current;
    if (!audioDevice) return;
    
    audioDevice.disconnectDevice(deviceId);
  }, []);
  
  // Handle device change
  const handleDeviceChange = useCallback(async (deviceId: string, isSystemAudio: boolean) => {
    const { audioDevice, audioContext } = services.current;
    if (!audioDevice) return;
    
    // Determine device type
    const deviceType = isSystemAudio ? 'systemAudio' : 'microphone';
    
    // Check if the device is already selected
    const currentDevice = state.selectedDevices[deviceType];
    if (currentDevice === deviceId) return;
    
    // Disconnect the current device if it exists
    if (currentDevice) {
      disconnectSource(currentDevice);
    }
    
    // Update selected device
    dispatch({ 
      type: 'SET_SELECTED_DEVICES', 
      payload: { [deviceType]: deviceId } 
    });
    
    // Check if this type of device is active
    const isActive = isSystemAudio ? state.isSystemAudioOn : state.isMicrophoneOn;
    
    // If the device type is active, connect the new device
    if (isActive) {
      // Ensure the AudioContext is configured
      audioContext.setupAudioContext();
      
      // Connect the new device
      const connected = await audioDevice.connectDevice(deviceId);
      
      if (!connected) {
        console.error(`❌ Failed to connect ${isSystemAudio ? 'system audio' : 'microphone'}: ${deviceId}`);
        
        // Reset selection on failure
        dispatch({ 
          type: 'SET_SELECTED_DEVICES', 
          payload: { [deviceType]: null } 
        });
      }
    }
  }, [state.selectedDevices, state.isMicrophoneOn, state.isSystemAudioOn, disconnectSource]);
  
  // Configure microphone
  const setupMicrophone = async () => {
    console.log("🔄 Configuring microphone...");
    
    try {
      const { audioContext, recorder } = services.current;
      if (!recorder) return false;
      
      // Update state
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.SettingUp });
      stateRef.current = MicrophoneState.SettingUp;
      
      // Configure AudioContext
      audioContext.setupAudioContext();
      
      // Reset recorder
      dispatch({ type: 'SET_MICROPHONE', payload: null });
      
      // Create new MediaRecorder
      const newRecorder = recorder.createMediaRecorder();
      if (!newRecorder) {
        throw new Error("Failed to create MediaRecorder");
      }
      
      console.log("✅ Microphone configuration completed");
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Ready });
      stateRef.current = MicrophoneState.Ready;
      
      return true;
    } catch (error) {
      console.error("❌ Error configuring microphone:", error);
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Error });
      stateRef.current = MicrophoneState.Error;
      
      return false;
    }
  };
  
  // Helper function to connect active devices
  const connectActiveDevices = async () => {
    const { audioContext, audioDevice } = services.current;
    if (!audioContext || !audioDevice) return;
    
    // Connect microphone if active
    if (state.isMicrophoneOn) {
      if (state.selectedDevices.microphone) {
        // First, connect the device to obtain the audio source
        const connected = await audioDevice.connectDevice(state.selectedDevices.microphone);
        if (!connected) {
          console.warn(`⚠️ Failed to connect microphone ${state.selectedDevices.microphone}`);
        } else {
          // Now, connect the source to the correct channel in the merger
          const sources = audioDevice.getSources();
          const source = sources[state.selectedDevices.microphone];
          if (source) {
            audioContext.connectMicrophoneSource(source.source);
            console.log("✅ Microphone source connected to merger");
          }
        }
      } else {
        // Create silent source for the microphone channel
        const silentDeviceId = audioDevice.createSilentSource(0, getPrimaryUser());
        if (silentDeviceId) {
          console.log("🔇 Created silent source for channel 0 (microphone)");
          // Connect silent source to the microphone channel
          const sources = audioDevice.getSources();
          const source = sources[silentDeviceId];
          if (source) {
            audioContext.connectMicrophoneSource(source.source);
          }
        }
      }
    } else {
      // If microphone is disabled, ensure it is disconnected
      audioContext.disconnectMicrophoneSource();
    }
    
    // Connect system audio if active
    if (state.isSystemAudioOn) {
      if (state.selectedDevices.systemAudio) {
        // First, connect the device to obtain the audio source
        const connected = await audioDevice.connectDevice(state.selectedDevices.systemAudio);
        if (!connected) {
          console.warn(`⚠️ Failed to connect system audio ${state.selectedDevices.systemAudio}`);
        } else {
          // Now, connect the source to the correct channel in the merger
          const sources = audioDevice.getSources();
          const source = sources[state.selectedDevices.systemAudio];
          if (source) {
            audioContext.connectSystemAudioSource(source.source);
            console.log("✅ System audio source connected to merger");
          }
        }
      } else {
        // Create silent source for the system audio channel
        const silentDeviceId = audioDevice.createSilentSource(1, "");
        if (silentDeviceId) {
          console.log("🔇 Created silent source for channel 1 (system audio)");
          // Connect silent source to the system audio channel
          const sources = audioDevice.getSources();
          const source = sources[silentDeviceId];
          if (source) {
            audioContext.connectSystemAudioSource(source.source);
          }
        }
      }
    } else {
      // If system audio is disabled, ensure it is disconnected
      audioContext.disconnectSystemAudioSource();
    }
  };
  
  // Start recording
  const startMicrophone = useCallback(async () => {
    console.log("🎙️ Starting microphone...");
    
    try {
      const { audioContext, recorder } = services.current;
      if (!audioContext || !recorder) {
        throw new Error("Services not initialized");
      }
      
      // If not configured, configure first
      if (stateRef.current === MicrophoneState.NotSetup) {
        await setupMicrophone();
      }
      
      // Update state
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Opening });
      stateRef.current = MicrophoneState.Opening;
      
      // Configure AudioContext
      audioContext.setupAudioContext();
      
      // Connect active devices
      await connectActiveDevices();
      
      // Check if we have a recorder
      let currentRecorder = recorder.getCurrentRecorder();
      if (!currentRecorder) {
        currentRecorder = recorder.createMediaRecorder();
        if (!currentRecorder) {
          throw new Error("Failed to create MediaRecorder");
        }
      }
      
      // Start recording
      recorder.startRecording();
      
      console.log("✅ Microphone started successfully");
    } catch (error) {
      console.error("❌ Error starting microphone:", error);
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Error });
      stateRef.current = MicrophoneState.Error;
    }
  }, []);
  
  // Stop recording
  const stopMicrophone = useCallback(async (forceReset: boolean = false) => {
    console.log("🛑 Stopping microphone...");
    
    try {
      const { recorder } = services.current;
      if (!recorder) return;
      
      // Update state
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Stopping });
      stateRef.current = MicrophoneState.Stopping;
      
      // Stop recording
      recorder.stopRecording();
      
      // Reset if necessary
      if (forceReset) {
        resetAudioSystem(false);
      }
      
      console.log("✅ Microphone stopped successfully");
    } catch (error) {
      console.error("❌ Error stopping microphone:", error);
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Error });
      stateRef.current = MicrophoneState.Error;
    }
  }, []);
  
  // Reset the entire audio system
  const resetAudioSystem = async (autoRestart: boolean = false) => {
    console.log("🧹 Resetting audio system...");
    
    try {
      const { audioContext, audioDevice, recorder } = services.current;
      if (!audioContext || !audioDevice) return;
      
      // Stop recording
      if (recorder) {
        recorder.stopRecording();
      }
      
      // Clear sources
      const sources = audioDevice.getSources();
      Object.keys(sources).forEach(deviceId => {
        audioDevice.disconnectDevice(deviceId);
      });
      
      // Close AudioContext
      await audioContext.closeAudioContext();
      
      // Reset state
      dispatch({ type: 'RESET_STATE' });
      stateRef.current = MicrophoneState.NotSetup;
      
      console.log("✅ Audio system reset successfully");  
      
      // Restart if necessary
      if (autoRestart) {
        setTimeout(() => setupMicrophone(), 500);
      }
    } catch (error) {
      console.error("❌ Error resetting audio system:", error);
    }
  };
  
  // Get current microphone state
  const getCurrentMicrophoneState = useCallback(() => {
    return stateRef.current;
  }, []);
  
  // Generate test WAV file for analysis
  const generateTestWAV = async (): Promise<ChannelAnalysis | null> => {
    const { audioContext } = services.current;
    if (!audioContext) return null;
    
    const context = audioContext.getAudioContext();
    if (!context) return null;
    
    // Use the native sample rate of the context
    const sampleRate = context.sampleRate;
    const duration = 0.5; // 500ms
    const samples = Math.floor(sampleRate * duration);
    
    const buffer = context.createBuffer(2, samples, sampleRate);
    
    // Generate different tones in each channel
    for (let channel = 0; channel < 2; channel++) {
      const channelData = buffer.getChannelData(channel);
      const frequency = channel === 0 ? 440 : 880; // A4 for channel 0, A5 for channel 1
      
      for (let i = 0; i < samples; i++) {
        const t = i / sampleRate;
        channelData[i] = 0.5 * Math.sin(2 * Math.PI * frequency * t);
      }
    }
    
    // Analyze the generated buffer
    const analysis: ChannelAnalysis = {
      channelCount: 2,
      totalSamples: samples,
      sampleRate: sampleRate,
      durationSeconds: duration,
      channels: [
        { avgVolume: 0.5, peakVolume: 0.5, hasAudio: true },
        { avgVolume: 0.5, peakVolume: 0.5, hasAudio: true }
      ]
    };
    
    return analysis;
  };
  
  // Functions adapted to match the expected signatures of the IMicrophoneContext interface
  
  // Define microphone state - adapted for SetStateAction
  const setIsMicrophoneOn: Dispatch<SetStateAction<boolean>> = useCallback((value) => {
    const prevValue = state.isMicrophoneOn;
    const newValue = typeof value === 'function' ? value(prevValue) : value;
    
    // If no change, do nothing
    if (prevValue === newValue) return;
    
    console.log(`🎙️ Microphone state changed: ${prevValue} -> ${newValue}`);
    
    // Update state
    dispatch({ type: 'SET_MICROPHONE_ON', payload: newValue });
    
    // Use try/catch to facilitate diagnosis
    try {
      const { audioContext, audioDevice } = services.current;
      if (!audioContext) {
        throw new Error("AudioContext não disponível");
      }
      if (!audioDevice) {
        throw new Error("AudioDevice não disponível");
      }
      
      // If activating the microphone
      if (newValue) {
        console.log("🔵 Activating microphone channel");
        
        // Ensure the AudioContext is configured and active
        audioContext.setupAudioContext();
        
        const status = audioContext.getConnectionStatus();
        console.log(`🔍 Connection status - before activation: Microphone = ${status.microphone}, System = ${status.systemAudio}`);
        
        // Check if we have a microphone device selected
        let microphoneDeviceId = state.selectedDevices.microphone;
        
        // If no device selected, try selecting the first available
        if (!microphoneDeviceId) {
          console.log("🔍 No microphone device selected, trying to select the first available");
          const { microphoneDevices } = audioDevice.filterDevicesForUI();
          
          if (microphoneDevices.length > 0) {
            microphoneDeviceId = microphoneDevices[0].deviceId;
            console.log(`✅ Automatically selected: ${microphoneDevices[0].label}`);
            
            // Update state with the selected device
            dispatch({ 
              type: 'SET_SELECTED_DEVICES', 
              payload: { microphone: microphoneDeviceId } 
            });
          } else {
            console.warn("⚠️ No microphone device available to select");
          }
        }
        
        // Connect the selected microphone device, if available
        if (microphoneDeviceId) {
          console.log(`🔄 Trying to connect microphone: ${microphoneDeviceId}`);
          
          // Using async/await directly (chained promises may lose context)
          (async () => {
            const connected = await audioDevice.connectDevice(microphoneDeviceId!);
            
            console.log(`🔄 Trying to connect microphone ${microphoneDeviceId}: ${connected ? 'Success' : 'Failure'}`);
            
            if (connected) {
              // Get the audio source and connect it to the microphone channel (0)
              const sources = audioDevice.getSources();
              const source = sources[microphoneDeviceId!];
              
              if (source) {
                audioContext.connectMicrophoneSource(source.source);
                console.log("✅ Success: Microphone source connected to merger");
                
                const finalStatus = audioContext.getConnectionStatus();
                console.log(`🔍 Final connection status: Microphone = ${finalStatus.microphone}, System = ${finalStatus.systemAudio}`);
              } else {
                console.error("❌ Error: Microphone source not found after connection");
              }
            }
          })();
        } else {
          console.log("⚠️ No microphone device selected or available, creating silent source");
          
          // Create silent source for the microphone channel
          const silentDeviceId = audioDevice.createSilentSource(0, getPrimaryUser());
          if (silentDeviceId) {
            console.log("🔇 Created silent source for channel 0 (microphone)");
            // Connect silent source to the microphone channel
            const sources = audioDevice.getSources();
            const source = sources[silentDeviceId];
            if (source) {
              audioContext.connectMicrophoneSource(source.source);
              console.log("✅ Silent source connected to microphone channel");
            }
          }
        }
      }
      // If deactivating the microphone
      else {
        console.log("🔴 Deactivating microphone channel");
        
        // Debug of status before disconnection
        const statusBefore = audioContext.getConnectionStatus();
        console.log(`🔍 Status of connections - before disconnection: Microphone = ${statusBefore.microphone}, System = ${statusBefore.systemAudio}`);
        
        // Disconnect the microphone source from the merger - now synchronous
        audioContext.disconnectMicrophoneSource();
        
        // Debug of status after disconnection from the merger
        const statusAfterMerger = audioContext.getConnectionStatus();
        console.log(`🔍 Status after disconnecting from merger: Microphone = ${statusAfterMerger.microphone}, System = ${statusAfterMerger.systemAudio}`);
        
        // If the device is connected, disconnect
        if (state.selectedDevices.microphone) {
          console.log(`🔄 Disconnecting microphone device: ${state.selectedDevices.microphone}`);
          audioDevice.disconnectDevice(state.selectedDevices.microphone);
          console.log("✅ Success: Microphone device disconnected");
        }
        
        // Debug of final status
        const statusAfter = audioContext.getConnectionStatus();
        console.log(`🔍 Final connection status: Microphone = ${statusAfter.microphone}, System = ${statusAfter.systemAudio}`);
      }
    } catch (error) {
      console.error("❌ Error handling microphone:", error);
    }
  }, [state.isMicrophoneOn, state.selectedDevices.microphone]);
  
  // Define audio system state - adapted for SetStateAction
  const setIsSystemAudioOn: Dispatch<SetStateAction<boolean>> = useCallback((value) => {
    const prevValue = state.isSystemAudioOn;
    const newValue = typeof value === 'function' ? value(prevValue) : value;
    
    // If no change, do nothing
    if (prevValue === newValue) return;
    
    console.log(`🔊 Alterando estado do áudio do sistema: ${prevValue} -> ${newValue}`);
    
    // Update state
    dispatch({ type: 'SET_SYSTEM_AUDIO_ON', payload: newValue });
    
    // Uso do try/catch para facilitar diagnóstico
    try {
      const { audioContext, audioDevice } = services.current;
      if (!audioContext) {
        throw new Error("AudioContext não disponível");
      }
      if (!audioDevice) {
        throw new Error("AudioDevice não disponível");
      }
      
      // If activating the system audio
      if (newValue) {
        console.log("🔵 Activating system audio channel");
        
        // Ensure the AudioContext is configured and active
        audioContext.setupAudioContext();
        
        const status = audioContext.getConnectionStatus();
        console.log(`🔍 Status of connections - before activation: Microphone = ${status.microphone}, System = ${status.systemAudio}`);
        
        // Verify if we have a system audio device selected
        let systemAudioDeviceId = state.selectedDevices.systemAudio;
        
        // If no device selected, try selecting the first available
        if (!systemAudioDeviceId) {
          console.log("🔍 No system audio device selected, trying to select the first available");
          const { systemAudioDevices } = audioDevice.filterDevicesForUI();
          
          if (systemAudioDevices.length > 0) {
            systemAudioDeviceId = systemAudioDevices[0].deviceId;
            console.log(`✅ Automatically selected: ${systemAudioDevices[0].label}`);
            
            // Update state with the selected device
            dispatch({ 
              type: 'SET_SELECTED_DEVICES', 
              payload: { systemAudio: systemAudioDeviceId } 
            });
          } else {
            console.warn("⚠️ No system audio device available to select");
          }
        }
        
        // Connect the selected system audio device, if available
        if (systemAudioDeviceId) {
          console.log(`🔄 Trying to connect system audio: ${systemAudioDeviceId}`);
          
          // Using async/await directly
          (async () => {
            const connected = await audioDevice.connectDevice(systemAudioDeviceId!);
            
            console.log(`🔄 Trying to connect system audio ${systemAudioDeviceId}: ${connected ? 'Success' : 'Failure'}`);
            
            if (connected) {
              // Get the audio source and connect it to the system audio channel (1)
              const sources = audioDevice.getSources();
              const source = sources[systemAudioDeviceId!];
              
              if (source) {
                audioContext.connectSystemAudioSource(source.source);
                console.log("✅ Success: System audio source connected to merger");
                
                const finalStatus = audioContext.getConnectionStatus();
                console.log(`🔍 Final connection status: Microphone = ${finalStatus.microphone}, System = ${finalStatus.systemAudio}`);
              } else {
                console.error("❌ Error: System audio source not found after connection");
              }
            }
          })();
        } else {
          console.log("⚠️ No system audio device selected or available, creating silent source");
          
          // Create silent source for the system audio channel
          const silentDeviceId = audioDevice.createSilentSource(1, "");
          if (silentDeviceId) {
            console.log("🔇 Created silent source for channel 1 (system)");
            // Connect silent source to the system audio channel
            const sources = audioDevice.getSources();
            const source = sources[silentDeviceId];
            if (source) {
              audioContext.connectSystemAudioSource(source.source);
              console.log("✅ Silent source connected to system audio channel");
            }
          }
        }
      } 
      // If deactivating the system audio
      else {
        console.log("🔴 Deactivating system audio channel");
        
        // Debug of status before disconnection
        const statusBefore = audioContext.getConnectionStatus();
        console.log(`🔍 Status of connections - before disconnection: Microphone = ${statusBefore.microphone}, System = ${statusBefore.systemAudio}`);
        
        // Disconnect the system audio source from the merger - now synchronous
        audioContext.disconnectSystemAudioSource();
        
        // Debug of status after disconnection from the merger
        const statusAfterMerger = audioContext.getConnectionStatus();
        console.log(`🔍 Status after disconnecting from merger: Microphone = ${statusAfterMerger.microphone}, System = ${statusAfterMerger.systemAudio}`);
        
        // If the device is connected, disconnect
        if (state.selectedDevices.systemAudio) {
          console.log(`🔄 Disconnecting system audio device: ${state.selectedDevices.systemAudio}`);
          audioDevice.disconnectDevice(state.selectedDevices.systemAudio);
          console.log("✅ Success: System audio device disconnected");
        }
        
        // Debug of final status
        const statusAfter = audioContext.getConnectionStatus();
        console.log(`🔍 Final connection status: Microphone = ${statusAfter.microphone}, System = ${statusAfter.systemAudio}`);
      }
    } catch (error) {
      console.error("❌ Error handling system audio:", error);
    }
  }, [state.isSystemAudioOn, state.selectedDevices.systemAudio]);
  
  // Define selected devices - adapted for SetStateAction
  const setSelectedDevices: Dispatch<SetStateAction<SelectedDevices>> = useCallback((value) => {
    if (typeof value === 'function') {
      const newDevices = value(state.selectedDevices);
      dispatch({ type: 'SET_SELECTED_DEVICES', payload: newDevices });
    } else {
      dispatch({ type: 'SET_SELECTED_DEVICES', payload: value });
    }
  }, [state.selectedDevices]);
  
  // Context value to be provided
  const contextValue: IMicrophoneContext = {
    microphone: state.microphone,
    startMicrophone,
    stopMicrophone,
    setupMicrophone,
    resetAudioSystem,
    microphoneState: state.microphoneState,
    getCurrentMicrophoneState,
    audioDevices: state.audioDevices,
    selectedDevices: state.selectedDevices,
    setSelectedDevices,
    disconnectSource,
    handleDeviceChange,
    setIsMicrophoneOn,
    setIsSystemAudioOn,
    isMicrophoneOn: state.isMicrophoneOn,
    isSystemAudioOn: state.isSystemAudioOn,
    speakerMappings: state.speakerMappings,
    generateTestWAV
  };
  
  return (
    <MicrophoneContext.Provider value={contextValue}>
      {children}
    </MicrophoneContext.Provider>
  );
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// RecorderService.ts
// Implementation of the audio recording service

import { IAudioContextService } from "../interfaces/IAudioContextService";
import { MicrophoneState } from "../interfaces/IMicrophoneContext";
import { IRecorderService } from "../interfaces/IRecorderService";

// Configuration constants for audio recording
const RECORDER_CONFIG = {
  TIMESLICE: 2000,  // milliseconds between audio chunks
  AUDIO_BIT_RATE: 256000,  // 16 bits * 16000Hz * 1 channel = 256kbps (aligned with Deepgram sample_rate)
  MIME_TYPES: [
    "audio/wav",              // WAV - uncompressed format with headers (more stable for testing)
    "audio/webm;codecs=pcm",  // PCM uncompressed - ideal for STT (alternative if WAV fails)
    "audio/webm",             // WebM - modern format supported and widely compatible
    "audio/ogg;codecs=opus"   // Ogg Opus - last resort
  ]
};

export class RecorderService implements IRecorderService {
  private audioContextService: IAudioContextService;
  private setMicrophoneState: (state: MicrophoneState) => void;
  private setMicrophone: (recorder: MediaRecorder | null) => void;
  private microphone: MediaRecorder | null = null;
  private activeMimeType: string | null = null;

  constructor(
    audioContextService: IAudioContextService,
    setMicrophoneState: (state: MicrophoneState) => void,
    setMicrophone: (recorder: MediaRecorder | null) => void
  ) {
    this.audioContextService = audioContextService;
    this.setMicrophoneState = setMicrophoneState;
    this.setMicrophone = setMicrophone;
  }

  /**
   * Returns the current recorder, if it exists
   */
  getCurrentRecorder(): MediaRecorder | null {
    return this.microphone;
  }

  /**
   * Creates a new MediaRecorder
   */
  createMediaRecorder(): MediaRecorder | null {
    try {
      // Get the audio destination node
      const destination = this.validateAudioDestination();
      if (!destination) return null;

      // Create the MediaRecorder with the best available format
      const recorder = this.createOptimalRecorder(destination.stream);
      if (!recorder) return null;

      // Configure events and store references
      this.configureRecorderEvents(recorder);
      this.microphone = recorder;
      this.setMicrophone(recorder);
      
      console.log(`✅ MediaRecorder created with format: ${this.activeMimeType}`);
      return recorder;
    } catch (error) {
      console.error("❌ Error creating MediaRecorder:", error);
      return null;
    }
  }

  /**
   * Starts the recording
   */
  startRecording(): void {
    if (!this.microphone) {
      console.log("🎤 [COGNITIVE-RECORDER] Attempting to start recording without a MediaRecorder");
      return;
    }

    if (this.microphone.state === "recording") {
      console.log("ℹ️ [COGNITIVE-RECORDER] Recording already in progress for cognitive input.");
      return;
    }

    try {
      console.log("🎤 [COGNITIVE-RECORDER] Starting audio capture for brain memory...");
      this.microphone.start(RECORDER_CONFIG.TIMESLICE);
    } catch (error) {
      console.log("❌ [COGNITIVE-RECORDER] Error starting audio capture for brain memory:", error);
      this.setMicrophoneState(MicrophoneState.Error);
      this.resetRecorder();
    }
  }

  /**
   * Stops the recording
   */
  stopRecording(): void {
    if (!this.microphone) {
      console.log("ℹ️ No recording in progress to stop");
      return;
    }

    if (this.microphone.state !== "recording") {
      console.log(`ℹ️ Recorder not recording (state: ${this.microphone.state})`);
      return;
    }

    try {
      console.log("🛑 [COGNITIVE-RECORDER] Stopping audio capture for brain memory...");
      this.microphone.stop();
      this.resetRecorder();
    } catch (error) {
      console.log("❌ [COGNITIVE-RECORDER] Error stopping audio capture for brain memory:", error);
      this.setMicrophoneState(MicrophoneState.Error);
      this.resetRecorder();
    }
  }

  /**
   * Configures the MediaRecorder events
   * Public implementation for compatibility with IRecorderService
   */
  configureRecorderEvents(recorder: MediaRecorder): void {
    // Process audio data
    recorder.ondataavailable = this.handleAudioData.bind(this);

    // State change events
    recorder.onstart = () => {
      console.log("🎤 [COGNITIVE-RECORDER] Recording started");
      this.setMicrophoneState(MicrophoneState.Open);
    };

    recorder.onpause = () => {
      console.log("⏸️ Recording paused");
      this.setMicrophoneState(MicrophoneState.Stopped);
    };

    recorder.onresume = () => {
      console.log("▶️ Recording resumed");
      this.setMicrophoneState(MicrophoneState.Open);
    };

    recorder.onstop = () => {
      console.log("⏹️ Recording stopped");
      this.setMicrophoneState(MicrophoneState.Stopped);
    };

    recorder.onerror = (event) => {
      console.error("🎤 [COGNITIVE-RECORDER] Error in MediaRecorder:", event);
      this.setMicrophoneState(MicrophoneState.Error);
    };
  }

  // --- Private Helper Methods ---

  /**
   * Validates and returns the audio destination node
   */
  private validateAudioDestination(): MediaStreamAudioDestinationNode | null {
    const destination = this.audioContextService.getDestination();
    if (!destination) {
      console.error("❌ [COGNITIVE-RECORDER] No audio destination available for recording");
      return null;
    }

    // Check if there are audio tracks
    if (destination.stream.getAudioTracks().length === 0) {
      console.error("❌ [COGNITIVE-RECORDER] No audio tracks available for recording");
      return null;
    }

    return destination;
  }

  /**
   * Creates a MediaRecorder with the best available format
   */
  private createOptimalRecorder(stream: MediaStream): MediaRecorder | null {
    // Try creating the MediaRecorder with the preferred formats
    for (const mimeType of RECORDER_CONFIG.MIME_TYPES) {
      if (MediaRecorder.isTypeSupported(mimeType)) {
        try {
          const options: MediaRecorderOptions = {
            mimeType,
            audioBitsPerSecond: RECORDER_CONFIG.AUDIO_BIT_RATE
          };
          
          console.log(`🎤 [COGNITIVE-RECORDER] Using format: ${mimeType}`);
          const recorder = new MediaRecorder(stream, options);
          this.activeMimeType = mimeType;
          return recorder;
        } catch (err) {
          console.warn(`⚠️ Format ${mimeType} failed:`, err);
        }
      }
    }

    // Final attempt without specifying format
    try {
      console.log("⚠️ Using default browser format");
      const recorder = new MediaRecorder(stream);
      this.activeMimeType = recorder.mimeType;
      return recorder;
    } catch (err) {
      console.error("❌ [COGNITIVE-RECORDER] Unable to create MediaRecorder:", err);
      return null;
    }
  }

  /**
   * Processes the received audio data
   */
  private async handleAudioData(event: BlobEvent): Promise<void> {
    if (event.data.size <= 0) return;

    try {
      // Convert the Blob to ArrayBuffer
      const arrayBuffer = await event.data.arrayBuffer();
      
      // Convert to Uint8Array as expected by the API - enviar diretamente sem processamento
      const audioData = new Uint8Array(arrayBuffer);
      
      // Log the format being used (only for debugging)
      if (Math.random() < 0.05) { // Reduce log frequency
        console.log(`🎤 [COGNITIVE-RECORDER] Sending ${audioData.length} bytes of audio (format: ${this.activeMimeType || 'unknown'})`);
      }
      
      // Send the audio without additional modifications
      if (window.electronAPI) {
        window.electronAPI.sendAudioChunk(audioData);
        
        // Occasional log for debugging
        if (Math.random() < 0.05) {
          console.log("🎤 [COGNITIVE-RECORDER] Audio data received for brain memory:", audioData);
        }
      } else {
        console.warn("⚠️ Electron API not available for audio transmission");
      }
    } catch (error) {
      console.error("❌ [COGNITIVE-RECORDER] Error processing audio data:", error);
    }
  }

  /**
   * Resets the recorder after use
   */
  private resetRecorder(): void {
    // Use setTimeout to ensure pending operations complete
    setTimeout(() => {
      this.microphone = null;
      this.setMicrophone(null);
      this.activeMimeType = null;
    }, 100);
  }

} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { createContext, useContext, useState } from 'react';

interface Settings {
  deepgramModel: string;
  showInterimResults: boolean;
}

const defaultSettings: Settings = {
  deepgramModel: 'nova-2',
  showInterimResults: false
};

const SettingsContext = createContext<{
  settings: Settings;
  updateSettings: (newSettings: Partial<Settings>) => void;
}>({
  settings: defaultSettings,
  updateSettings: () => {}
});

export const useSettings = () => useContext(SettingsContext);

export const SettingsProvider: React.FC<{children: React.ReactNode}> = ({ children }) => {
  const [settings, setSettings] = useState<Settings>(defaultSettings);
  
  const updateSettings = (newSettings: Partial<Settings>) => {
    setSettings(prev => ({ ...prev, ...newSettings }));
  };
  
  return (
    <SettingsContext.Provider value={{ settings, updateSettings }}>
      {children}
    </SettingsContext.Provider>
  );
}; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// index.ts
// Exportation of the transcription context

export { TranscriptionProvider, useTranscription } from './TranscriptionContext';
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionContext.tsx
// Context for managing transcriptions

import React, { createContext, useCallback, useContext, useState } from 'react';

interface TranscriptionContextType {
  texts: {
    transcription: string;
    aiResponse: string;
  };
  setTexts: React.Dispatch<React.SetStateAction<{ transcription: string; aiResponse: string }>>;
  updateTranscription: (text: string, isFinal: boolean) => void;
  clearTranscription: () => void;
}

const TranscriptionContext = createContext<TranscriptionContextType | null>(null);

export function useTranscription() {
  return useContext(TranscriptionContext);
}

export function TranscriptionProvider({ children }: { children: React.ReactNode }) {
  const [texts, setTexts] = useState({
    transcription: '',
    aiResponse: '',
  });

  // Function to update transcription with new text
  const updateTranscription = useCallback((text: string, isFinal: boolean) => {
    if (!text.trim()) return;
    
    setTexts(prev => {
      // For final text, add a paragraph or send for processing
      if (isFinal) {
        return {
          ...prev,
          transcription: prev.transcription ? `${prev.transcription}\n${text}` : text,
        };
      } 
      // For temporary text (while speaking), update the end of the transcription
      else {
        // Find the last line (where we added the temporary text)
        const lines = prev.transcription.split('\n');
        const lastLineIndex = lines.length - 1;
        
        // Update only the last line
        if (lastLineIndex >= 0) {
          lines[lastLineIndex] = text;
        } else {
          lines.push(text);
        }
        
        return {
          ...prev,
          transcription: lines.join('\n'),
        };
      }
    });
  }, []);

  // Function to clear the transcription
  const clearTranscription = useCallback(() => {
    setTexts(prev => ({
      ...prev,
      transcription: '',
    }));
  }, []);

  const value = {
    texts,
    setTexts,
    updateTranscription,
    clearTranscription,
  };

  return (
    <TranscriptionContext.Provider value={value}>
      {children}
    </TranscriptionContext.Provider>
  );
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// CognitionLogContext.tsx
// Context for managing cognition logs in the application

import React, { createContext, useContext, useEffect, useState } from 'react';
import { CognitionEvent } from './deepgram/types/CognitionEvent';
import symbolicCognitionTimelineLogger from './deepgram/services/utils/SymbolicCognitionTimelineLoggerSingleton';
import { CognitionLogExporter } from './deepgram/services/utils/CognitionLogExporter';
import cognitionLogExporterFactory from './deepgram/services/utils/CognitionLogExporterFactory';

/**
 * Interface for the cognition log context
 */
interface CognitionLogContextType {
  /** Current cognition events */
  events: CognitionEvent[];
  /** Available exporters */
  exporters: CognitionLogExporter[];
  /** Clear all cognition events */
  clearEvents: () => void;
  /** Export events using the specified exporter */
  exportEvents: (exporterLabel: string) => void;
}

/**
 * Context for managing cognition logs in the application
 */
const CognitionLogContext = createContext<CognitionLogContextType | null>(null);

/**
 * Provider for the cognition log context
 */
export const CognitionLogProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [events, setEvents] = useState<CognitionEvent[]>([]);
  const [exporters] = useState<CognitionLogExporter[]>(
    cognitionLogExporterFactory.getExporters()
  );

  // Updates events periodically
  useEffect(() => {
    const updateEvents = () => {
      setEvents([...symbolicCognitionTimelineLogger.getTimeline()]);
    };

    // Initial update
    updateEvents();

    // Update every second
    const interval = setInterval(updateEvents, 1000);
    return () => clearInterval(interval);
  }, []);

  // Clears all events
  const clearEvents = () => {
    symbolicCognitionTimelineLogger.clear();
    setEvents([]);
  };

  // Exports events using the specified exporter
  const exportEvents = (exporterLabel: string) => {
    const exporter = exporters.find(e => e.label === exporterLabel);
    if (exporter) {
      exporter.export(events);
    }
  };

  return (
    <CognitionLogContext.Provider value={{ events, exporters, clearEvents, exportEvents }}>
      {children}
    </CognitionLogContext.Provider>
  );
};

/**
 * Hook to access the cognition log context
 */
export const useCognitionLog = (): CognitionLogContextType => {
  const context = useContext(CognitionLogContext);
  if (!context) {
    throw new Error('useCognitionLog must be used within a CognitionLogProvider');
  }
  return context;
};

export default CognitionLogContext;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// index.ts
// Export organization of all contexts and services

// Interfaces
export * from './deepgram/interfaces/deepgram/IDeepgramContext';
export * from './deepgram/interfaces/deepgram/IDeepgramService';
export * from './interfaces/IAudioContextService';
export * from './interfaces/IAudioDeviceService';
export * from './interfaces/IMicrophoneContext';
export * from './interfaces/IRecorderService';

// Microphone services
export { AudioContextService } from './microphone/AudioContextService';
export { AudioDeviceService } from './microphone/AudioDeviceService';
export { default as MicrophoneProvider, useMicrophone } from './microphone/MicrophoneContextProvider';
export { RecorderService } from './microphone/RecorderService';

// Deepgram services
export { DeepgramAudioAnalyzer } from './deepgram/AudioAnalyzer';
export { DeepgramConnectionService } from './deepgram/DeepgramConnectionService';
export { default as DeepgramProvider, useDeepgram } from './deepgram/DeepgramContextProvider';
export { DeepgramTranscriptionService } from './deepgram/services/DeepgramTranscriptionService';

// Transcription context
export { TranscriptionProvider, useTranscription } from './transcription';

// Enums
export { ConnectionState } from './deepgram/interfaces/deepgram/IDeepgramService';
export { MicrophoneEvents, MicrophoneState } from './interfaces/IMicrophoneContext';
 // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { createContext, useState, useEffect } from "react";
import { getOption, setOption, subscribeToStorageChanges, STORAGE_KEYS } from "../../services/StorageService";

/**
 * Hook personalizado para gerenciar configurações do storage
 * Permite um "espelhamento neural" de uma configuração global do storage
 */
function useStorageSetting<T>(key: string, defaultValue: T): [T, (value: T) => void] {
  // Estado local que espelha o valor no storage
  const [value, setValue] = useState<T>(() => {
    const storedValue = getOption<T>(key);
    return storedValue !== undefined ? storedValue : defaultValue;
  });

  // Efeito para sincronizar com mudanças no storage
  useEffect(() => {
    const handleStorageChange = (changedKey: string, newValue: any) => {
      if (changedKey === key && newValue !== undefined) {
        console.log(`💾 [STORAGE-MIRROR] Valor atualizado para ${key}:`, newValue);
        setValue(newValue as T);
      }
    };
    
    // Inscreve para mudanças e retorna função de limpeza
    return subscribeToStorageChanges(handleStorageChange);
  }, [key]);

  // Função para atualizar o valor
  const updateValue = (newValue: T) => {
    setValue(newValue);
    setOption(key, newValue);
  };

  return [value, updateValue];
}

// =====================================
// Contexto de linguagem do Orch-OS
// =====================================

export const LanguageContext = createContext<{  
  language: string;
  setLanguage: (language: string) => void;
}>({ language: "pt-BR", setLanguage: () => {} });

export const LanguageProvider = ({ children }: { children: React.ReactNode }) => {
  // Espelha configurações diretamente do storage neural
  const [language, setLanguageInternal] = useStorageSetting(STORAGE_KEYS.DEEPGRAM_LANGUAGE, "pt-BR");
  
  // Simples wrapper para manter a API consistente
  const setLanguage = (newLanguage: string) => {
    setLanguageInternal(newLanguage);
    console.log(`🌎 LanguageContext: Idioma alterado para ${newLanguage}`);
  };
  
  // O contexto agora é sempre atualizado automaticamente graças ao espelhamento neural

  return (
    <LanguageContext.Provider value={{ language, setLanguage }}>
      {children}
    </LanguageContext.Provider>
  );
};// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useEffect, useRef, useState } from "react";
import { MicrophoneState, useMicrophone } from "../../context";

const AudioVisualizer = ({
  width = 500,
  height = 150,
}: {
  width?: number;
  height?: number;
}) => {
  // Analysis frequency (visualizer update rate)
  const ANALYSIS_FREQUENCY = 100; // 100ms = 10fps
  
  // Initialize canvas and context
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const canvasCtxRef = useRef<CanvasRenderingContext2D | null>(null);
  
  // State to control display
  const [isConnected, setIsConnected] = useState(false);
  
  // Access microphone context
  const { microphoneState } = useMicrophone();
  
  // Initialize visualizer and analysis
  useEffect(() => {
    // Verify if the canvas exists
    if (!canvasRef.current) return;
    
    // Get 2D context for canvas drawing
    const canvas = canvasRef.current;
    canvasCtxRef.current = canvas.getContext('2d');
    
    // Configure canvas size
    canvas.width = width;
    canvas.height = height;
    
    // Clear canvas (draw black background)
    if (canvasCtxRef.current) {
      canvasCtxRef.current.fillStyle = 'rgba(0, 0, 0, 0.2)';
      canvasCtxRef.current.fillRect(0, 0, width, height);
    }
  }, [width, height]);
  
  // Observe microphone state to update visualizer
  useEffect(() => {
    // Visualize only when the microphone is active
    const shouldVisualize = microphoneState === MicrophoneState.Open;
    setIsConnected(shouldVisualize);
    
    // Reference for animation timer
    let animationTimer: ReturnType<typeof setInterval> | null = null;
    
    // Start visualization when active
    if (shouldVisualize) {
      animationTimer = setInterval(() => renderFrame(), ANALYSIS_FREQUENCY);
    }
    
    // Function to render a frame of the visualization
    const renderFrame = () => {
      if (!canvasCtxRef.current) return;
      
      const ctx = canvasCtxRef.current;
      const canvas = canvasRef.current;
      if (!canvas) return;
      
      // Fadeout effect
      ctx.fillStyle = 'rgba(0, 0, 0, 0.1)';
      ctx.fillRect(0, 0, canvas.width, canvas.height);
      
      // Generate simulated visualization (when there are no real audio data)
      
      // Draw color lines simulating audio activity
      const centerY = canvas.height / 2;
      const maxHeight = canvas.height * 0.4;
      
      // Generate "random" values but with some continuity for simulation
      const now = Date.now() / 1000;
      
      for (let x = 0; x < canvas.width; x++) {
        // Generate "waves" using sine functions with different frequencies and phases
        // to create a more natural and less random effect
        const phase1 = now * 2;
        const phase2 = now * 3.7;
        
        // Combination of waves to create a more complex shape
        const value = (
          Math.sin(x * 0.02 + phase1) * 0.5 + 
          Math.sin(x * 0.04 + phase2) * 0.3
        ) * maxHeight;
        
        // Height varies based on generated value
        const height = Math.abs(value);
        
        // Y position (centered vertically)
        const y = centerY - (value < 0 ? 0 : height);
        
        // Color based on position and height (gradient effect)
        const hue = (x / canvas.width) * 180 + 180; // blue to purple
        ctx.fillStyle = `hsla(${hue}, 80%, 50%, 0.8)`;
        
        // Draw bar
        ctx.fillRect(x, y, 1, height);
      }
    };
    
    // Clear timer on unmount or state change
    return () => {
      if (animationTimer) {
        clearInterval(animationTimer);
      }
    };
  }, [microphoneState, width, height]);
  
  return (
    <div className="relative w-full rounded-lg overflow-hidden bg-gradient-to-b from-black/20 to-black/50 backdrop-blur-sm">
      <canvas
        ref={canvasRef}
        className="w-full h-auto"
        width={width}
        height={height}
      />
      
      {/* Overlay status when not connected */}
      {!isConnected && (
        <div className="absolute inset-0 flex items-center justify-center text-white/60 text-sm">
          Audio visualizer inactive
        </div>
      )}
    </div>
  );
};

export default AudioVisualizer; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { CognitionEvent } from "../../../../context/deepgram/types/CognitionEvent";

/**
 * Pure UI component for rendering a single cognition event in the timeline.
 * No logic, no mutation, only visual representation.
 * SOLID: Single Responsibility Principle.
 */
export interface CognitionEventUIProps {
  event: CognitionEvent;
  idx: number;
  onClick?: (event: CognitionEvent) => void;
  duration?: { value: string; color: string };
}

// Define the background colors for each event type to match modal colors
const eventColors: Record<string, string> = {
  raw_prompt: "bg-blue-700",
  temporary_context: "bg-purple-700",
  neural_signal: "bg-amber-700",
  neural_collapse: "bg-pink-700",
  symbolic_retrieval: "bg-green-700",
  fusion_initiated: "bg-orange-700",
  symbolic_context_synthesized: "bg-indigo-700",
  gpt_response: "bg-red-700",
  emergent_patterns: "bg-teal-700",
};

// Use the colors to create the icons
const eventTypeIcons: Record<string, React.ReactNode> = {
  raw_prompt: (
    <div
      className={`w-9 h-9 rounded-full ${eventColors.raw_prompt} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🧠
    </div>
  ),
  temporary_context: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.temporary_context} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🧠
    </div>
  ),
  neural_signal: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.neural_signal} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      ⚡
    </div>
  ),
  neural_collapse: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.neural_collapse} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      💥
    </div>
  ),
  symbolic_retrieval: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.symbolic_retrieval} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🔍
    </div>
  ),
  fusion_initiated: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.fusion_initiated} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🔥
    </div>
  ),
  symbolic_context_synthesized: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.symbolic_context_synthesized} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🔗
    </div>
  ),
  gpt_response: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.gpt_response} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      💬
    </div>
  ),
  emergent_patterns: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.emergent_patterns} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🌊
    </div>
  ),
};

// Format event type for display
const formatEventType = (type: string): string => {
  // Para Neural Signal e Raw Prompt, usamos formatos específicos
  if (type === "neural_signal") return "Neural Signal";
  if (type === "raw_prompt") return "Raw Prompt";

  // Para eventos regulares, usamos Title Case
  return type
    .replace(/_/g, " ")
    .split(" ")
    .map((word) => word.charAt(0).toUpperCase() + word.slice(1))
    .join(" ");
};

export const CognitionEventUI: React.FC<CognitionEventUIProps> = React.memo(
  ({ event, idx, onClick, duration }) => {
    // Verificar se é um evento de resposta GPT (geralmente o último)
    const isGptResponse = event.type === "gpt_response";
    const icon = eventTypeIcons[event.type] || (
      <span className="text-gray-400">⬛</span>
    );

    // Extract information based on event type
    const isNeuralSignal = event.type === "neural_signal";
    const isRawPrompt = event.type === "raw_prompt";

    // For neural signals, extract core and intensity
    const neuralCore = isNeuralSignal ? event.core : null;
    // Garantir que intensity seja tratado como número e convertido para percentual
    // Multiplicamos por 100 para converter o valor decimal (0-1) para percentual (0-100)
    const intensityValue = isNeuralSignal
      ? Math.round(event.intensity * 100)
      : 0;
    const neuralValue = isNeuralSignal ? `${intensityValue}%` : null;

    /**
     * Sistema de cores para todos os cores neurais do Orch-OS
     * - Cores escolhidas para refletir os domínios cognitivos específicos
     * - Codificação por cores facilita o reconhecimento de padrões (processamento pré-atentivo)
     * - Cores com boa visibilidade e associações semânticas relevantes
     */
    const getNeuralTypeColor = (type: string | null) => {
      if (!type) return "text-amber-400"; // Default

      switch (type.toLowerCase()) {
        // Cores cognitivos/memória
        case "memory":
          return "text-orange-400"; // Memória - Laranja (lembranças/recuperação)
        case "associative":
          return "text-amber-500"; // Associativo - Âmbar escuro (conexões entre memórias)

        // Cores emocionais/afetivos
        case "valence":
          return "text-amber-400"; // Valência - Âmbar (emoções/afeto)
        case "soul":
          return "text-rose-400"; // Alma - Rosa avermelhado (essência emocional profunda)

        // Cores de linguagem/comunicação
        case "language":
          return "text-indigo-400"; // Linguagem - Índigo (expressão verbal)

        // Cores de metacognição/planejamento
        case "metacognitive":
          return "text-teal-400"; // Metacognitivo - Turquesa (reflexão sobre pensamento)
        case "planning":
          return "text-sky-400"; // Planejamento - Azul céu (estruturação futura)
        case "will":
          return "text-blue-500"; // Vontade - Azul forte (força de intenção)

        // Cores inconscientes/arquetípicos
        case "unconscious":
          return "text-fuchsia-400"; // Inconsciente - Fuchsia (conteúdos não conscientes)
        case "archetype":
          return "text-purple-400"; // Arquétipo - Roxo (padrões universais)
        case "shadow":
          return "text-violet-500"; // Sombra - Violeta escuro (aspectos reprimidos)

        // Cores físicos/encarnação
        case "body":
          return "text-green-500"; // Corpo - Verde escuro (sensações físicas)

        // Cores sociais/identidade
        case "social":
          return "text-blue-400"; // Social - Azul (interações interpessoais)
        case "self":
          return "text-green-400"; // Self - Verde (identidade/auto-percepção)

        // Cores criativos/intuitivos
        case "creativity":
          return "text-pink-400"; // Criatividade - Rosa (inovação/expressão)
        case "intuition":
          return "text-purple-500"; // Intuição - Roxo intenso (conhecimento direto)

        // Default para qualquer outro tipo
        default:
          return "text-amber-400"; // Default - Âmbar como fallback
      }
    };

    /**
     * Cores otimizadas para barras de progresso com maior visibilidade
     * Correspondendo ao sistema de cores do texto, mas com ajustes de intensidade
     * para garantir boa visibilidade contra o fundo escuro
     */
    /**
     * Cores otimizadas para barras de progresso com alta visibilidade
     * Especialmente contra fundos escuros (bg-gray-800/60) usado no tema do Orch-OS
     *
     * Cores problemáticas contra fundos escuros foram ajustadas para:
     * 1. Tons de azul/roxo/violeta - aumentados para 500/600 (roxo e azul escuro absorvem mais luz)
     * 2. Tons de verde escuro - aumentados para 500/600 (verde escuro tem menor percepção luminosa)
     * 3. Cores neutras - aumentadas para 500/600 (cinzas e cores neutras se misturam com o fundo)
     */
    const getProgressBarColor = (type: string | null) => {
      if (!type) return "bg-amber-500"; // Default usando 500 para maior visibilidade

      switch (type.toLowerCase()) {
        // Cores cognitivos/memória
        case "memory":
          return "bg-orange-500"; // Memória - Laranja 500 (boa visibilidade natural)
        case "associative":
          return "bg-amber-600"; // Associativo - Âmbar 600 (mais intenso para maior contraste)

        // Cores emocionais/afetivos
        case "valence":
          return "bg-amber-500"; // Valência - Âmbar 500 (boa visibilidade natural)
        case "soul":
          return "bg-rose-500"; // Alma - Rosa 500 (bom contraste natural)

        // Cores de linguagem/comunicação
        case "language":
          return "bg-indigo-600"; // Linguagem - Índigo 600 (intensificado pois índigo tem baixa visibilidade)

        // Cores de metacognição/planejamento
        case "metacognitive":
          return "bg-teal-500"; // Metacognitivo - Teal 500 (boa visibilidade)
        case "planning":
          return "bg-sky-500"; // Planejamento - Sky 500 (boa visibilidade)
        case "will":
          return "bg-blue-600"; // Vontade - Azul 600 (intensificado para contraste)

        // Cores inconscientes/arquetípicos - Grupo mais problemático para visibilidade
        case "unconscious":
          return "bg-fuchsia-600"; // Inconsciente - Fuchsia 600 (cores roxas/violetas precisam de maior intensidade)
        case "archetype":
          return "bg-purple-600"; // Arquétipo - Roxo 600 (intensificado para garantir visibilidade)
        case "shadow":
          return "bg-violet-600"; // Sombra - Violeta 600 (intensificado para contraste)

        // Cores físicos/encarnação
        case "body":
          return "bg-green-600"; // Corpo - Verde 600 (verdes escuros precisam de maior intensidade)

        // Cores sociais/identidade
        case "social":
          return "bg-blue-600"; // Social - Azul 600 (intensificado pois azul escuro tem menor contraste)
        case "self":
          return "bg-green-500"; // Self - Verde 500 (verde médio tem boa visibilidade)

        // Cores criativos/intuitivos
        case "creativity":
          return "bg-pink-500"; // Criatividade - Rosa 500 (bom contraste natural)
        case "intuition":
          return "bg-purple-600"; // Intuição - Roxo 600 (intensificado para visibilidade)

        // Default para qualquer outro tipo
        default:
          return "bg-amber-500"; // Default usando âmbar 500 para melhor visibilidade
      }
    };

    // Format time as HH:MM:SS.mmm manually to include milliseconds
    const formattedTime = event.timestamp
      ? (() => {
          const date = new Date(event.timestamp);
          const hours = date.getHours().toString().padStart(2, "0");
          const minutes = date.getMinutes().toString().padStart(2, "0");
          const seconds = date.getSeconds().toString().padStart(2, "0");
          const ms = date.getMilliseconds().toString().padStart(3, "0");
          return `${hours}:${minutes}:${seconds}.${ms}`;
        })()
      : "";

    return (
      <div
        className={`relative rounded-xl bg-gray-900/90 p-3 shadow-lg transition-colors cursor-pointer ${
          onClick ? "hover:bg-gray-800/90" : ""
        } ${isGptResponse ? "mb-6 pb-4" : ""}`}
        onClick={() => onClick && onClick(event)}
      >
        {/* Vertical timeline line - visible between consecutive events */}
        {idx > 0 && (
          <div className="absolute left-7 top-0 bottom-0 w-0.5 bg-cyan-800/30 -translate-y-3 h-4" />
        )}
        <div
          className="group flex items-stretch bg-gray-900/90 hover:bg-gray-800/90 rounded-lg border border-gray-800/80 hover:border-gray-700 transition-all duration-200 overflow-hidden cursor-pointer"
          tabIndex={0}
          role="button"
          aria-label={event.type}
        >
          {/* Left column with icon */}
          <div className="w-14 flex-shrink-0 flex items-center justify-center text-2xl select-none border-r border-gray-800/30">
            {icon}
          </div>

          {/* Content section */}
          <div className="flex-1 px-3 py-2">
            <div className="flex items-center justify-between mb-1">
              {/* Event type label */}
              <div className="flex items-center">
                <span
                  className={`inline-block px-2 py-0.5 rounded text-xs font-medium text-white ${
                    eventColors[event.type] || "bg-gray-800"
                  }`}
                >
                  {formatEventType(event.type)}
                </span>
              </div>

              {/* Timestamp and duration */}
              <div className="flex items-center space-x-2">
                <span className="whitespace-nowrap font-mono text-xs text-gray-400">
                  {formattedTime}
                </span>
                {duration && (
                  <span className="flex items-center">
                    <span
                      className={`whitespace-nowrap font-mono text-xs ${duration.color}`}
                    >
                      {duration.value}
                    </span>
                  </span>
                )}
              </div>
            </div>

            {/* Content based on event type */}
            {isNeuralSignal && neuralValue ? (
              <div className="text-base font-medium flex items-center gap-1.5">
                <span className={`${getNeuralTypeColor(neuralCore)} font-mono`}>
                  {neuralCore ? neuralCore.toLowerCase() : ""}
                </span>
                <span className={`${getNeuralTypeColor(neuralCore)} font-mono`}>
                  {neuralValue}
                </span>
                {/* Barra de intensidade junto do percentual */}
                <div className="w-28 h-2 bg-gray-800/60 rounded-full overflow-hidden ml-1">
                  {/* Aplicando largura dinâmica com classes CSS */}
                  <div
                    className={`h-full ${getProgressBarColor(neuralCore)}`}
                    style={{ width: `${intensityValue}%` }}
                  />
                </div>
              </div>
            ) : isRawPrompt ? (
              <div className="text-base text-gray-300 font-medium">
                {event.content
                  ? event.content.substring(0, 50) +
                    (event.content.length > 50 ? "..." : "")
                  : ""}
              </div>
            ) : event.type === "gpt_response" ? (
              <div className="text-base text-gray-300 font-medium">
                {event.response
                  ? event.response.substring(0, 50) +
                    (event.response.length > 50 ? "..." : "")
                  : ""}
              </div>
            ) : (
              <div className="text-base text-gray-300 font-medium">
                {/* Display summary based on event type */}
                {event.type === "symbolic_retrieval"
                  ? `${event.matchCount || 0} matches found`
                  : event.type === "neural_collapse"
                  ? `Collapse: ${event.selectedCore}`
                  : event.type === "emergent_patterns"
                  ? `${event.patterns?.length || 0} patterns`
                  : ""}
              </div>
            )}
          </div>
        </div>
      </div>
    );
  }
);

// Exporta o componente memoizado
export default CognitionEventUI;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { CognitionEvent } from '../../../../context/deepgram/types/CognitionEvent';
import { CognitionEventUI } from '../Event/CognitionEventUI';
import styles from '../../CognitionTimeline.module.css';

/**
 * UI component for a visually grouped cognition cycle (RawPrompt → ... → GPTResponse),
 * accordion-exclusivo: só um ciclo aberto por vez, com scroll interno.
 * SOLID: Single Responsibility Principle.
 */
export interface CognitionLogGroupUIProps {
  events: CognitionEvent[]; // Events in this group, in order
  groupIdx: number;
  onEventClick?: (event: CognitionEvent) => void;
  getDuration?: (idx: number) => { value: string; color: string };
  expanded?: boolean;
  onExpand?: () => void;
}

const MAX_GROUP_HEIGHT = 450; // px, ajustado para o container global

export const CognitionLogGroupUI: React.FC<CognitionLogGroupUIProps> = ({ events, groupIdx, onEventClick, getDuration, expanded = false, onExpand }) => {
  if (!events.length) return null;
  const isExpanded = expanded;
  return (
    <div className="mt-3 mb-0">
      <button
        type="button"
        className="w-full relative flex items-center justify-between px-5 py-2.5 bg-[#121936] rounded-2xl border border-cyan-400/80 shadow-[0_0_6px_rgba(34,211,238,0.2)] group transition-all duration-300 focus:outline-none focus:ring-1 focus:ring-cyan-400 focus:ring-opacity-50 min-h-[48px]"
        onClick={onExpand}
        aria-expanded={isExpanded}
        aria-controls={`cognitive-group-${groupIdx}`}
        aria-label={`Cognitive Cycle ${groupIdx + 1} with ${events.length} events. Click to ${isExpanded ? 'collapse' : 'expand'}`}
      >
        {/* Left side with icon and title */}
        <div className="flex items-center gap-4">
          {/* Neural network brain icon with glow */}
          <div className="relative flex-shrink-0 flex items-center justify-center h-9 w-9 rounded-full shadow-[0_0_8px_rgba(34,211,238,0.2)]">
            <svg 
              className="w-7 h-7 text-cyan-300 drop-shadow-[0_0_3px_rgba(34,211,238,0.4)]" 
              viewBox="0 0 24 24" 
              fill="none" 
              xmlns="http://www.w3.org/2000/svg"
              aria-hidden="true"
            >
              <path d="M22 14.3529C22 19.0968 17.0751 23 10.9999 23C5.33439 23 1 19.2323 1 14.8235C1 10.9343 3.35098 8.34576 6.70584 7.21671C8.73574 6.5246 9.74705 6.17854 10.5291 5.15517C11.3112 4.13179 11.5349 2.74591 11.9823 0" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M16 10.5C16 8.01472 13.9853 6 11.5 6C9.01472 6 7 8.01472 7 10.5C7 12.9853 9.01472 15 11.5 15C13.9853 15 16 12.9853 16 10.5Z" stroke="currentColor" strokeWidth="1.5"/>
              <path d="M11.5 6V4.5" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M16 10.5H17.5" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M11.5 15V16.5" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M7 10.5H5.5" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M14.5281 7.47192L15.5888 6.41116" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M14.5281 13.5281L15.5888 14.5888" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M8.47192 13.5281L7.41116 14.5888" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M8.47192 7.47192L7.41116 6.41116" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
            </svg>
            <div className="absolute inset-0 z-0 bg-gradient-to-br from-[#121936]/90 to-cyan-950/20 rounded-full"></div>
          </div>
          
          {/* Title */}
          <h3 className="text-white font-bold text-xl tracking-wide select-none">
            Cognitive Cycle #{groupIdx + 1}
          </h3>
        </div>
        
        {/* Right side with badge and arrow */}
        <div className="flex items-center gap-3">
          {/* Event count badge matching Export buttons */}
          <div className="px-4 py-1 rounded-full bg-cyan-400 text-[#121936] text-base font-bold flex items-center justify-center min-w-[2.5rem] shadow-[0_0_5px_rgba(34,211,238,0.3)]">
            {events.length}
          </div>
          
          {/* Chevron that changes direction when expanded */}
          <div className="flex items-center justify-center w-6 h-6 text-cyan-400">
            <span className={`text-xl font-bold transition-transform duration-200 inline-block ${isExpanded ? 'rotate-90' : 'rotate-0'}`}>
              &gt;
            </span>
          </div>
        </div>
      </button>
      <div
        id={`cognitive-group-${groupIdx}`}
        className={expanded ? `${styles.fadeIn}` : ''}
        style={{
          // Quando expandido, não limitar altura (controle feito pelo container externo)
          maxHeight: expanded ? 'none' : 0,
          overflow: 'hidden',
          transition: expanded ? 'opacity 0.2s' : 'max-height 0.4s cubic-bezier(0.16,1,0.3,1), opacity 0.2s',
          opacity: expanded ? 1 : 0,
        }}
      >
        {expanded && (
          <div
            className="px-2"
            style={{
              maxHeight: MAX_GROUP_HEIGHT - 30,
              overflowY: 'auto',
              scrollbarWidth: 'thin',
              boxSizing: 'border-box',
              WebkitOverflowScrolling: 'touch'
            }}
          >
            {events.map((event, idx) => (
              <CognitionEventUI
                key={event.timestamp + '-' + event.type + '-' + groupIdx + '-' + idx}
                event={event}
                idx={idx}
                onClick={onEventClick}
                duration={getDuration ? getDuration(idx) : undefined}
              />
            ))}
          </div>
        )}
      </div>
    </div>
  );
};

export default CognitionLogGroupUI;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useState, useRef, useEffect } from 'react';
import { CognitionEvent } from './../../../context/deepgram/types/CognitionEvent';
import CognitionLogGroupUI from './Group/CognitionLogGroupUI';
import { VariableSizeList as List } from 'react-window';

/**
 * UI utility: Groups events into cognitive cycles (RawPrompt to GPTResponse).
 * Exclusive accordion: only one cycle open at a time, but allows all collapsed.
 * SOLID: Single Responsibility Principle.
 */
export class CognitionLogGrouper {
  static groupEvents(events: CognitionEvent[]): CognitionEvent[][] {
    const groups: CognitionEvent[][] = [];
    let current: CognitionEvent[] = [];
    for (const event of events) {
      if (event.type === 'raw_prompt' && current.length > 0) {
        groups.push(current);
        current = [];
      }
      current.push(event);
      if (event.type === 'gpt_response') {
        groups.push(current);
        current = [];
      }
    }
    if (current.length > 0) groups.push(current);
    return groups;
  }
}

export interface CognitionTimelineGroupedUIProps {
  events: CognitionEvent[];
  onEventClick?: (event: CognitionEvent) => void;
  getDuration?: (idx: number) => { value: string; color: string };
}

const MAX_CONTAINER_HEIGHT = 600; // px, ajustável conforme layout do app

export const CognitionTimelineGroupedUI: React.FC<CognitionTimelineGroupedUIProps> = ({ events, onEventClick, getDuration }) => {
  // Memoize event grouping to avoid unnecessary recalculations
  const groups = React.useMemo(() => CognitionLogGrouper.groupEvents(events), [events]);
  const [expandedIdx, setExpandedIdx] = useState<number | null>(0); // null = todos colapsados
  const listRef = useRef<List>(null);

  // Fixed heights for each group
  const COLLAPSED_HEIGHT = 60;
  const EXPANDED_HEIGHT = 380;

  // Calculate item height based on expansion state
  const getItemSize = (index: number) => (expandedIdx === index ? EXPANDED_HEIGHT : COLLAPSED_HEIGHT);

  // When expanding/collapsing, update state and force height recalculation
  // useCallback prevents unnecessary function recreations on each render
  const handleExpand = React.useCallback((idx: number) => {
    setExpandedIdx(current => (current === idx ? null : idx));
    // O efeito abaixo força o recálculo das alturas
    setTimeout(() => {
      if (listRef.current) listRef.current.resetAfterIndex(0);
    }, 0);
  }, []);

  if (!groups.length) return <div className="text-indigo-300 text-sm p-4">No cognitive logs available</div>;

  // If any group is expanded, render ONLY it and navigation controls
  if (expandedIdx !== null) {
    return (
      <div className="h-[450px] w-full overflow-y-auto overflow-scrolling-touch">
        {/* Neural-Symbolic Header - design reflecting cognitive orchestration */}
        <div
          className="flex items-center justify-between px-3 py-2 bg-gradient-to-r from-indigo-950/80 via-indigo-900/70 to-indigo-950/80 backdrop-blur-md border-b border-cyan-400/30 shadow-sm rounded-t-lg transition-all"
          style={{
            backgroundImage: `radial-gradient(circle at 15% 50%, rgba(99, 102, 241, 0.08) 0%, transparent 25%), 
                           radial-gradient(circle at 85% 30%, rgba(6, 182, 212, 0.08) 0%, transparent 25%)`
          }}
        >
          {/* Neural Button (Back) - cognitive return action */}
          <button
            onClick={() => setExpandedIdx(null)}
            className="group flex items-center gap-1 text-cyan-300 hover:text-cyan-100 font-medium text-xs px-2 py-1 rounded-md transition-all duration-200 focus:outline-none focus-visible:ring-1 focus-visible:ring-cyan-400 active:scale-95 border border-transparent hover:border-cyan-500/20 hover:bg-cyan-900/10"
          >
            <span className="text-base transition-transform duration-200 group-hover:-translate-x-0.5">←</span>
            <span>Back</span>
          </button>

          {/* Cognitive Center - focal point of neural information */}
          <div className="relative flex-1 text-center px-1">
            <div className="absolute left-0 right-0 top-1/2 -translate-y-1/2 h-[1px] bg-gradient-to-r from-transparent via-cyan-500/20 to-transparent"></div>
            <span className="relative inline-flex items-center gap-1.5 px-3 py-0.5 bg-indigo-900/40 rounded-full border border-indigo-700/30 shadow-inner">
              <span className="inline-block w-1.5 h-1.5 rounded-full bg-cyan-400/80"></span>
              <span className="text-cyan-100 font-medium text-sm tracking-wide">
                Cycle #{expandedIdx + 1}
              </span>
              <span className="text-cyan-300/70 text-xs font-normal">of {groups.length}</span>
            </span>
          </div>

          {/* Neural Navigation Controls - directional synapses */}
          <div className="flex gap-1">
            <button
              onClick={() => setExpandedIdx(curr => Math.max(0, (curr || 0) - 1))}
              disabled={expandedIdx <= 0}
              className="w-6 h-6 flex items-center justify-center text-cyan-300 hover:text-cyan-100 disabled:text-cyan-800/50 disabled:hover:bg-transparent font-medium rounded-full transition-all duration-200 hover:bg-cyan-900/20 focus:outline-none focus-visible:ring-1 focus-visible:ring-cyan-400 active:scale-95 border border-transparent hover:border-cyan-500/20"
              aria-label="Previous cycle"
            >
              <span className="text-sm">←</span>
            </button>
            <button
              onClick={() => setExpandedIdx(curr => Math.min(groups.length - 1, (curr || 0) + 1))}
              disabled={expandedIdx >= groups.length - 1}
              className="w-6 h-6 flex items-center justify-center text-cyan-300 hover:text-cyan-100 disabled:text-cyan-800/50 disabled:hover:bg-transparent font-medium rounded-full transition-all duration-200 hover:bg-cyan-900/20 focus:outline-none focus-visible:ring-1 focus-visible:ring-cyan-400 active:scale-95 border border-transparent hover:border-cyan-500/20"
              aria-label="Next cycle"
            >
              <span className="text-sm">→</span>
            </button>
          </div>
        </div>
        <div className="pt-5">
          <CognitionLogGroupUI
            key={expandedIdx}
            events={groups[expandedIdx]}
            groupIdx={expandedIdx}
            onEventClick={onEventClick}
            getDuration={getDuration}
            expanded={true}
            onExpand={() => handleExpand(expandedIdx)}
          />
          {/* Removido div fantasma que criava espaço em branco excessivo */}
        </div>
      </div>
    );
  }

  // If all are collapsed, use virtualization
  return (
    <div style={{ height: 450, width: '100%' }}>
      <List
        ref={listRef}
        height={450}
        width={'100%'}
        itemCount={groups.length}
        itemSize={getItemSize}
        overscanCount={4}
      >
        {({ index, style }) => {
          // Apply mb-4 except on the last card for tighter spacing like in the image
          const isLast = index === groups.length - 1;
          return (
            <div style={{ ...style, marginBottom: isLast ? 0 : 16 }}>
              <CognitionLogGroupUI
                key={index}
                events={groups[index]}
                groupIdx={index}
                onEventClick={onEventClick}
                getDuration={getDuration}
                expanded={expandedIdx === index}
                onExpand={() => handleExpand(index)}
              />
            </div>
          );
        }}
      </List>
    </div>
  );
};

export default CognitionTimelineGroupedUI;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { CognitionEvent } from "../../context/deepgram/types/CognitionEvent";
import { SymbolicInsight } from "../../context/deepgram/types/SymbolicInsight";

interface CognitionDetailModalProps {
  isOpen: boolean;
  onClose: () => void;
  event: CognitionEvent | null;
}

const CognitionDetailModal: React.FC<CognitionDetailModalProps> = ({
  isOpen,
  onClose,
  event,
}) => {
  if (!event) return null;

  // Prevent scrolling on body when modal is open
  React.useEffect(() => {
    const toggleBodyScroll = (disable: boolean) => {
      if (disable) {
        document.body.classList.add("overflow-hidden");
      } else {
        document.body.classList.remove("overflow-hidden");
      }
    };

    toggleBodyScroll(isOpen);

    return () => {
      toggleBodyScroll(false);
    };
  }, [isOpen]);

  // Handle click outside modal to close
  const handleBackdropClick = (e: React.MouseEvent<HTMLDivElement>) => {
    if (e.target === e.currentTarget) {
      onClose();
    }
  };

  // Format event type for display
  const formatEventType = (type: string): string => {
    return type
      .replace(/_/g, " ")
      .split(" ")
      .map((word) => word.charAt(0).toUpperCase() + word.slice(1))
      .join(" ");
  };

  // Get event color for display
  const getEventTypeStyles = (
    type: string
  ): { bgColor: string; textColor: string; borderColor: string } => {
    switch (type) {
      case "raw_prompt":
        return {
          bgColor: "bg-blue-700",
          textColor: "text-blue-100",
          borderColor: "border-blue-500",
        };
      case "temporary_context":
        return {
          bgColor: "bg-purple-700",
          textColor: "text-purple-100",
          borderColor: "border-purple-500",
        };
      case "neural_signal":
        return {
          bgColor: "bg-amber-700",
          textColor: "text-amber-100",
          borderColor: "border-amber-500",
        };
      case "neural_collapse":
        return {
          bgColor: "bg-pink-700",
          textColor: "text-pink-100",
          borderColor: "border-pink-500",
        };
      case "symbolic_retrieval":
        return {
          bgColor: "bg-green-700",
          textColor: "text-green-100",
          borderColor: "border-green-500",
        };
      case "fusion_initiated":
        return {
          bgColor: "bg-orange-700",
          textColor: "text-orange-100",
          borderColor: "border-orange-500",
        };
      case "symbolic_context_synthesized":
        return {
          bgColor: "bg-indigo-700",
          textColor: "text-indigo-100",
          borderColor: "border-indigo-500",
        };
      case "gpt_response":
        return {
          bgColor: "bg-red-700",
          textColor: "text-red-100",
          borderColor: "border-red-500",
        };
      case "emergent_patterns":
        return {
          bgColor: "bg-teal-700",
          textColor: "text-teal-100",
          borderColor: "border-teal-500",
        };
      default:
        return {
          bgColor: "bg-gray-700",
          textColor: "text-gray-100",
          borderColor: "border-gray-600",
        };
    }
  };

  // Helper function to render insight items
  const renderInsights = (insights: any[]) => {
    return (
      <ul className="space-y-2 mt-3">
        {insights.map((insight, i) => {
          // Handle string insights
          if (typeof insight === 'string') {
            return (
              <li key={i} className="text-sm bg-gray-800/50 p-2 rounded border border-gray-700">
                {insight}
              </li>
            );
          }
          // Handle SymbolicInsight objects
          else if (insight && typeof insight === 'object' && 'type' in insight) {
            const insightObj = insight as SymbolicInsight;

            return (
              <li key={i} className="text-sm bg-gray-800/50 p-2 rounded border border-gray-700">
                <span className="text-teal-400 font-medium">{insightObj.type}</span>: {' '}
                <span>{String(insightObj.content || '')}</span>
              </li>
            );
          }
          // Fallback for other structures
          return (
            <li key={i} className="text-sm bg-gray-800/50 p-2 rounded border border-gray-700">
              {JSON.stringify(insight)}
            </li>
          );
        })}
      </ul>
    );
  };

  // Helper function to render patterns
  const renderPatterns = (patterns: string[]) => {
    return (
      <ul className="space-y-2 mt-3">
        {patterns.map((pattern, i) => (
          <li
            key={i}
            className="text-sm bg-gray-800/50 p-2 rounded border border-gray-700 text-teal-300"
          >
            {pattern}
          </li>
        ))}
      </ul>
    );
  };

  // Render event details based on type
  const renderEventDetails = (): React.ReactNode => {
    if (!event) return null;

    switch (event.type) {
      case "raw_prompt":
        return (
          <div className="space-y-4">
            <div className="text-lg text-blue-300 font-medium">Raw Prompt</div>
            <div className="p-4 bg-gray-800/50 rounded-lg border border-blue-500/30 whitespace-pre-wrap text-gray-200">
              {event.content}
            </div>
          </div>
        );

      case "temporary_context":
        return (
          <div className="space-y-4">
            <div className="text-lg text-purple-300 font-medium">
              Temporary Context
            </div>
            <div className="p-4 bg-gray-800/50 rounded-lg border border-purple-500/30 whitespace-pre-wrap text-gray-200">
              {event.context}
            </div>
          </div>
        );

      case "neural_signal":
        return (
          <div className="space-y-4">
            <div className="text-lg text-amber-300 font-medium">
              Neural Signal
            </div>

            <div className="flex flex-wrap gap-4">
              <div className="bg-gray-800/50 p-3 rounded-lg border border-amber-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Core</div>
                <div className="text-white font-medium">{event.core}</div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-amber-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Intensity</div>
                <div className="text-amber-400 font-medium text-xl">
                  {Math.round(event.intensity * 100)}%
                </div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-amber-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Top K</div>
                <div className="text-white font-medium">{event.topK}</div>
              </div>
            </div>

            <div>
              <div className="text-gray-400 mb-2">Symbolic Query</div>
              <div className="p-4 bg-gray-800/50 rounded-lg border border-amber-500/30 whitespace-pre-wrap text-gray-200">
                {JSON.stringify(event.symbolic_query, null, 2)}
              </div>
            </div>

            {Object.keys(event.params).length > 0 && (
              <div>
                <div className="text-gray-400 mb-2">Parameters</div>
                <div className="p-4 bg-gray-800/50 rounded-lg border border-amber-500/30 whitespace-pre-wrap text-gray-200 font-mono text-sm">
                  {JSON.stringify(event.params, null, 2)}
                </div>
              </div>
            )}
          </div>
        );

      case "emergent_patterns":
        return (
          <div className="space-y-4">
            <div className="text-lg text-teal-300 font-medium">
              Emergent Patterns
            </div>

            {event.metrics && (
              <div className="flex flex-wrap gap-4">
                {event.metrics.archetypalStability !== undefined && (
                  <div className="bg-gray-800/50 p-3 rounded-lg border border-teal-500/30 flex-1">
                    <div className="text-gray-400 text-sm mb-1">
                      Archetypal Stability
                    </div>
                    <div className="text-teal-400 font-medium text-xl">
                      {event.metrics.archetypalStability.toFixed(3)}
                    </div>
                  </div>
                )}

                {event.metrics.cycleEntropy !== undefined && (
                  <div className="bg-gray-800/50 p-3 rounded-lg border border-amber-500/30 flex-1">
                    <div className="text-gray-400 text-sm mb-1">
                      Cycle Entropy
                    </div>
                    <div className="text-amber-400 font-medium text-xl">
                      {event.metrics.cycleEntropy.toFixed(3)}
                    </div>
                  </div>
                )}

                {event.metrics.insightDepth !== undefined && (
                  <div className="bg-gray-800/50 p-3 rounded-lg border border-cyan-500/30 flex-1">
                    <div className="text-gray-400 text-sm mb-1">
                      Insight Depth
                    </div>
                    <div className="text-cyan-400 font-medium text-xl">
                      {event.metrics.insightDepth.toFixed(3)}
                    </div>
                  </div>
                )}
              </div>
            )}

            {event.patterns.length > 0 ? (
              <div>
                <div className="text-gray-400 mb-2">Detected Patterns</div>
                {renderPatterns(event.patterns)}
              </div>
            ) : (
              <div className="p-4 bg-gray-800/50 rounded-lg border border-gray-700 text-gray-400 italic">
                No patterns detected
              </div>
            )}
          </div>
        );

      case "symbolic_retrieval":
        return (
          <div className="space-y-4">
            <div className="text-lg text-green-300 font-medium">
              Symbolic Retrieval
            </div>

            <div className="flex flex-wrap gap-4">
              <div className="bg-gray-800/50 p-3 rounded-lg border border-green-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Core</div>
                <div className="text-white font-medium">{event.core}</div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-green-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Memory Matches</div>
                <div className="text-white font-medium">{event.matchCount}</div>
              </div>
              
              <div className="bg-gray-800/50 p-3 rounded-lg border border-green-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Duration</div>
                <div className="text-green-400 font-medium">
                  {event.durationMs}ms
                </div>
              </div>
            </div>
            
            {event.insights && event.insights.length > 0 && (
              <div>
                <div className="text-blue-400 mb-2 font-medium">Insights</div>
                {renderInsights(event.insights)}

                {/* 🔧 CORREÇÃO: Mostrar keywords dos insights se disponíveis */}
                {event.insights.some(
                  (insight: any) =>
                    insight.keywords && insight.keywords.length > 0
                ) && (
                  <div className="mt-4">
                    <div className="text-gray-400 mb-2">Keywords</div>
                    <div className="flex flex-wrap gap-2">
                      {event.insights
                        .filter(
                          (insight: any) =>
                            insight.keywords && insight.keywords.length > 0
                        )
                        .flatMap((insight: any) => insight.keywords)
                        .filter(
                          (keyword: string, index: number, arr: string[]) =>
                            arr.indexOf(keyword) === index
                        ) // Remove duplicatas
                        .map((keyword: string, i: number) => (
                          <span
                            key={i}
                            className="px-2 py-1 bg-gray-800 rounded-full text-xs text-green-300 border border-green-500/30"
                          >
                            "{keyword}"
                          </span>
                        ))}
                    </div>
                  </div>
                )}
              </div>
            )}
          </div>
        );

      case "neural_collapse":
        return (
          <div className="space-y-4">
            <div className="text-lg text-pink-300 font-medium">
              Neural Collapse
            </div>

            <div className="flex flex-wrap gap-4">
              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Selected Core</div>
                <div className="text-white font-medium">
                  {event.selectedCore}
                </div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Process</div>
                <div className="text-white font-medium">
                  {event.isDeterministic ? "Deterministic" : "Stochastic"}
                </div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Candidates</div>
                <div className="text-white font-medium">
                  {event.numCandidates}
                </div>
              </div>
            </div>

            <div className="flex flex-wrap gap-4">
              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Emotional Weight</div>
                <div className="text-pink-400 font-medium">{event.emotionalWeight.toFixed(2)}</div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Contradiction Score</div>
                <div className="text-pink-400 font-medium">{event.contradictionScore.toFixed(2)}</div>
              </div>

              {event.temperature !== undefined && (
                <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                  <div className="text-gray-400 text-sm mb-1">Temperature</div>
                  <div className="text-pink-400 font-medium">
                    {event.temperature.toFixed(2)}
                  </div>
                </div>
              )}
            </div>

            {event.justification && (
              <div>
                <div className="text-gray-400 mb-2">Justification</div>
                <div className="p-4 bg-gray-800/50 rounded-lg border border-pink-500/30 whitespace-pre-wrap text-gray-200">
                  {event.justification}
                </div>
              </div>
            )}

            {event.insights && event.insights.length > 0 && (
              <div>
                <div className="text-blue-400 mb-2 font-medium">Insights</div>
                {renderInsights(event.insights)}
              </div>
            )}

            {event.emergentProperties &&
              event.emergentProperties.length > 0 && (
                <div>
                  <div className="text-teal-400 mb-2 font-medium">
                    Emergent Properties
                  </div>
                  {renderPatterns(event.emergentProperties)}
                </div>
              )}
          </div>
        );

      case "symbolic_context_synthesized":
        return (
          <div className="space-y-4">
            <div className="text-lg text-indigo-300 font-medium">
              Symbolic Context Synthesized
            </div>
            <div className="p-4 bg-gray-800/50 rounded-lg border border-indigo-500/30 whitespace-pre-wrap text-gray-200 font-mono text-sm">
              {JSON.stringify(event.context, null, 2)}
            </div>
          </div>
        );

      case "gpt_response":
        return (
          <div className="space-y-4">
            <div className="text-lg text-red-300 font-medium">GPT Response</div>

            <div className="p-4 bg-gray-800/50 rounded-lg border border-red-500/30 whitespace-pre-wrap text-gray-200">
              {event.response}
            </div>

            {event.symbolicTopics && event.symbolicTopics.length > 0 && (
              <div>
                <div className="text-gray-400 mb-2">Symbolic Topics</div>
                <div className="flex flex-wrap gap-2">
                  {event.symbolicTopics.map((topic: string, i: number) => (
                    <span key={i} className="px-2 py-1 bg-gray-800 rounded-full text-xs text-red-300 border border-red-500/30">
                      {topic}
                    </span>
                  ))}
                </div>
              </div>
            )}

            {event.insights && event.insights.length > 0 && (
              <div>
                <div className="text-blue-400 mb-2 font-medium">Insights</div>
                {renderInsights(event.insights)}
              </div>
            )}
          </div>
        );

      case "fusion_initiated":
        return (
          <div className="p-4 bg-gray-800/50 rounded-lg border border-orange-500/30 text-center">
            <div className="text-orange-300 font-medium text-lg mb-2">
              Fusion Process Initiated
            </div>
            <div className="text-gray-400">
              The neural-symbolic fusion process has been triggered
            </div>
          </div>
        );

      default:
        return (
          <div className="p-4 bg-gray-800/50 rounded-lg border border-gray-700 text-gray-400">
            Unknown event type: {(event as any).type}
          </div>
        );
    }
  };

  const typeStyle = getEventTypeStyles(event.type);

  return (
    <>
      {isOpen && (
        <div
          className="fixed inset-0 z-50 flex items-start justify-center overflow-y-auto bg-black/70 backdrop-blur-sm p-4 pt-[2vh] pb-[2vh]"
          onClick={handleBackdropClick}
        >
          <div
            className={`relative bg-gray-900 rounded-lg shadow-2xl max-w-3xl w-full border-t-4 flex flex-col max-h-[96vh] animate-in fade-in slide-in-from-bottom-4 duration-300 ${typeStyle.borderColor}`}
            onClick={(e) => e.stopPropagation()}
          >
            {/* Modal header */}
            <div className="flex items-center justify-between px-6 py-4 border-b border-gray-800">
              <div className="flex items-center">
                <span
                  className={`inline-block px-3 py-1 rounded text-sm font-medium mr-3 ${typeStyle.bgColor} ${typeStyle.textColor}`}
                >
                  {formatEventType(event.type)}
                </span>
                <span className="text-gray-400 text-sm">
                  {new Date(event.timestamp).toLocaleTimeString()}.
                  {new Date(event.timestamp)
                    .getMilliseconds()
                    .toString()
                    .padStart(3, "0")}
                </span>
              </div>
              <button
                onClick={onClose}
                className="text-gray-400 hover:text-white transition-colors focus:outline-none focus:ring-2 focus:ring-indigo-500 rounded-full p-1"
                aria-label="Close modal"
              >
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-6 w-6"
                  fill="none"
                  viewBox="0 0 24 24"
                  stroke="currentColor"
                >
                  <path
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="M6 18L18 6M6 6l12 12"
                  />
                </svg>
              </button>
            </div>

            {/* Modal content - event details */}
            <div className="p-6 overflow-y-auto max-h-[calc(90vh-160px)]">
              {renderEventDetails()}
            </div>

            {/* Modal footer */}
            <div className="px-6 py-4 border-t border-gray-800 flex justify-end">
              <button
                onClick={onClose}
                className="px-4 py-2 bg-gray-800 hover:bg-gray-700 text-white rounded-md transition-colors focus:outline-none focus:ring-2 focus:ring-indigo-500"
              >
                Close
              </button>
            </div>
          </div>
        </div>
      )}
    </>
  );
};

export default CognitionDetailModal;
/* SPDX-License-Identifier: MIT OR Apache-2.0 */
/* Copyright (c) 2025 Guilherme Ferrari Brescia */

/* Animations for cognition timeline component */
.fadeIn {
  animation: fadeIn 0.2s ease-in-out;
}

.slideIn {
  animation: slideIn 0.2s cubic-bezier(0.16, 1, 0.3, 1);
}

.pulse {
  animation: pulse 2s infinite;
}

/* Modal padding */
.modalPadding {
  padding: 2vh 1rem;
}

/* Modal body content */
.modalBody {
  padding: 1.5rem;
  overflow-y: auto;
  max-height: calc(90vh - 160px);
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }
  100% {
    opacity: 1;
  }
}

@keyframes slideIn {
  0% {
    transform: translateY(10px);
    opacity: 0;
  }
  100% {
    transform: translateY(0);
    opacity: 1;
  }
}

@keyframes pulse {
  0% {
    box-shadow: 0 0 0 0 rgba(99, 102, 241, 0.4);
  }
  70% {
    box-shadow: 0 0 0 10px rgba(99, 102, 241, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(99, 102, 241, 0);
  }
}

.eventCard {
  transition: all 0.2s ease;
}

.eventCard:hover {
  transform: translateY(-1px);
}

/* Neural glow effects for different event types */
.neuralGlow {
  position: relative;
  overflow: hidden;
}

.neuralGlow::after {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: radial-gradient(circle at var(--x, 50%) var(--y, 50%), rgba(99, 102, 241, 0.15) 0%, transparent 50%);
  opacity: 0;
  transition: opacity 0.5s;
  pointer-events: none;
}

.neuralGlow:hover::after {
  opacity: 1;
}

/* Modal backdrop animation */
.modalBackdrop {
  -webkit-backdrop-filter: blur(0px);
  backdrop-filter: blur(0px);
  background-color: rgba(0, 0, 0, 0);
  transition: -webkit-backdrop-filter 0.3s ease, backdrop-filter 0.3s ease, background-color 0.3s ease;
}

.modalBackdropActive {
  -webkit-backdrop-filter: blur(5px);
  backdrop-filter: blur(5px);
  background-color: rgba(0, 0, 0, 0.7);
}

/* Modal layout styles */
.modalWrapper {
  position: fixed;
  inset: 0;
  z-index: 50;
  display: flex;
  align-items: flex-start;
  justify-content: center;
  padding: 2vh 1rem;
  background-color: rgba(0, 0, 0, 0.7);
  -webkit-backdrop-filter: blur(5px);
  backdrop-filter: blur(5px);
  overflow-y: auto;
}

.modalContent {
  position: relative;
  background-color: #1f2937;
  border-radius: 0.5rem;
  box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.25);
  max-width: 48rem;
  width: 100%;
  max-height: 96vh;
  display: flex;
  flex-direction: column;
  margin: 2vh 0;
}

/* Event type indicator dot animation */
.eventIndicator {
  position: relative;
}

.eventIndicator::before {
  content: '';
  position: absolute;
  left: -8px;
  top: 50%;
  transform: translateY(-50%);
  width: 4px;
  height: 4px;
  border-radius: 50%;
  background-color: currentColor;
  box-shadow: 0 0 0 rgba(255, 255, 255, 0.7);
  animation: indicatorPulse 2s infinite;
}

@keyframes indicatorPulse {
  0% {
    box-shadow: 0 0 0 0 rgba(255, 255, 255, 0.7);
  }
  70% {
    box-shadow: 0 0 0 4px rgba(255, 255, 255, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(255, 255, 255, 0);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useState } from 'react';
import { CognitionEvent } from '../../context/deepgram/types/CognitionEvent';
import CognitionTimelineGroupedUI from './ui/CognitionTimelineGroupedUI';
import CognitionDetailModal from './CognitionDetailModal';

/**
 * CognitionTimeline (UI grouped version)
 * - UI/UX lendária, disruptiva e simbólica.
 * - Não altera dados, modelo ou lógica.
 * - Segue SOLID e Clean Architecture.
 */
interface CognitionTimelineProps {
  events: CognitionEvent[];
}

export const CognitionTimeline: React.FC<CognitionTimelineProps> = ({ events }) => {
  const [selectedEvent, setSelectedEvent] = useState<CognitionEvent | null>(null);
  const [isModalOpen, setIsModalOpen] = useState(false);

  // Open modal with event details
  const openEventDetails = (event: CognitionEvent) => {
    setSelectedEvent(event);
    setIsModalOpen(true);
  };

  // Calculate duration between consecutive events (for UI only)
  const calcDuration = (events: CognitionEvent[]) => (idx: number): { value: string; color: string } => {
    if (idx === 0) return { value: '-', color: 'text-gray-500' };
    try {
      const current = new Date(events[idx].timestamp).getTime();
      const previous = new Date(events[idx - 1].timestamp).getTime();
      const diff = current - previous;
      let color = 'text-green-400';
      if (diff > 1000) color = 'text-amber-400';
      if (diff > 3000) color = 'text-red-400';
      if (diff < 1000) return { value: `+${diff}ms`, color };
      return { value: `+${(diff / 1000).toFixed(2)}s`, color };
    } catch (e) {
      return { value: '-', color: 'text-gray-500' };
    }
  };

  if (!events || events.length === 0) {
    return (
      <div className="flex items-center justify-center h-32 bg-gray-900/40 rounded-lg border border-gray-800 text-gray-400 italic">
        <div className="flex flex-col items-center">
          <svg xmlns="http://www.w3.org/2000/svg" className="h-8 w-8 mb-2 text-gray-600" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
          </svg>
          <span>No cognitive events recorded</span>
        </div>
      </div>
    );
  }

  return (
    <div className="relative">
      {/* UI grouped cognition cycles */}
      <CognitionTimelineGroupedUI
        events={events}
        onEventClick={openEventDetails}
        getDuration={calcDuration(events)}
      />
      {/* Modal for event details */}
      <CognitionDetailModal
        isOpen={isModalOpen}
        onClose={() => setIsModalOpen(false)}
        event={selectedEvent}
      />
    </div>
  );
};

export default CognitionTimeline;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Exportação de todos os componentes de visualização quântica
export { QuantumSuperposition } from './QuantumSuperposition';
export { WaveCollapse } from './WaveCollapse';
export { QuantumEntanglement } from './QuantumEntanglement';
export { ProbabilityFields } from './ProbabilityFields';
export { InterferencePatterns } from './InterferencePatterns';
export { QuantumField } from './QuantumField';
export { Observer, getCorePosition, getAge } from './QuantumUtils';// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import React, { useRef } from 'react';
import { useFrame } from '@react-three/fiber';
import * as THREE from 'three';

/**
 * Quantum interference patterns
 * 
 * Na teoria Orch OR, os padrões de interferência são fundamentais para entender
 * como a informação quântica é processada nos microtúbulos:
 * 
 * 1. Demonstram o comportamento ondulatório quântico, onde ondas de probabilidade
 *    interferem construtivamente e destrutivamente
 * 
 * 2. Representam como diferentes estados de superposição interagem dentro da 
 *    estrutura dos microtúbulos
 * 
 * 3. São a base da computação quântica nos microtúbulos, permitindo o processamento
 *    paralelo massivo de informação, crucial na teoria Orch OR
 * 
 * 4. A interferência quântica conecta o processamento quântico nos microtúbulos
 *    a fenômenos de campo mais amplos na atividade neural global
 */
interface InterferencePatternsProps {
  coherence?: number;
  collapseActive?: boolean;
}

export function InterferencePatterns({ coherence = 0.3, collapseActive = false }: InterferencePatternsProps) {
  const patterns = useRef<THREE.Group>(null);
  
  // Criando conjuntos de ondas circulares que representam
  // os padrões de interferência quântica nos microtúbulos
  useFrame(({ clock }) => {
    if (patterns.current) {
      const t = clock.getElapsedTime();
      
      // Rotação lenta de todo o grupo
      // Representa a evolução temporal dos padrões de interferência
      patterns.current.rotation.x = Math.sin(t * 0.2) * 0.3;
      patterns.current.rotation.y = t * 0.1;
      
      // Animar materiais independentemente
      // Cada padrão evolui com sua própria dinâmica, mas todos estão inter-relacionados
      patterns.current.children.forEach((child, i) => {
        const material = (child as THREE.Mesh).material as THREE.MeshBasicMaterial;
        
        // Pulsação da opacidade - representa flutuações quânticas
        // Na teoria Orch OR, os estados quânticos oscilam antes do colapso
        // Opacidade dos anéis modulada por coerência global e evento OR
// Opacidade mínima muito baixa em repouso (0.01), crescendo suavemente com coherence
material.opacity = collapseActive ? 1 : (0.01 + 0.9 * coherence) + Math.sin(t * (collapseActive ? 1.2 : (0.5 + i * 0.2))) * 0.2;
        
        // Animar escala para simular propagação de onda quântica
        // Representa a propagação da informação quântica através dos microtúbulos
        child.scale.setScalar(1 + Math.sin(t * (0.3 + i * 0.1)) * 0.3);
      });
    }
  });
  
  return (
    <group ref={patterns}>
      {/* Padrões de interferência - anéis sobrepostos em múltiplos planos */}
      {Array(5).fill(0).map((_, i) => (
        <React.Fragment key={i}>
          {/* Plano horizontal - representa um plano de interferência quântica */}
          <mesh rotation={[Math.PI / 2, 0, 0]}>
            <ringGeometry args={[1.0 + i * 0.4, 1.05 + i * 0.4, 128]} />
            <meshBasicMaterial 
              // Cores alternadas representam diferentes fases de interferência
              // Azul claro/azul escuro - associados a estados coerentes de baixa energia
              color={i % 2 === 0 ? "#80FFFF" : "#8080FF"} 
              transparent 
              opacity={0.3} 
              side={THREE.DoubleSide} 
            />
          </mesh>
          
          {/* Plano vertical - interferência ortogonal ao primeiro plano */}
          <mesh rotation={[0, Math.PI / 2, 0]}>
            <ringGeometry args={[1.1 + i * 0.4, 1.15 + i * 0.4, 128]} />
            <meshBasicMaterial 
              // Rosa/amarelo - associados a estados coerentes de alta energia
              // A alternância de cores representa interferência construtiva/destrutiva
              color={i % 2 === 0 ? "#FF80FF" : "#FFFF80"} 
              transparent 
              opacity={0.3} 
              side={THREE.DoubleSide} 
            />
          </mesh>
        </React.Fragment>
      ))}
    </group>
  );
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import { useFrame } from '@react-three/fiber';
import React, { useCallback, useMemo, useRef } from 'react';
import * as THREE from 'three';

/**
 * Flowing quantum probability fields
 * 
 * Na teoria Orch OR de Penrose-Hameroff, os campos de probabilidade quântica são fundamentais:
 * 
 * 1. Representam a função de onda quântica distribuída através dos microtúbulos
 * 2. Demonstram as propriedades de "não-localidade" quântica do sistema neural
 * 3. Visualizam como os estados de superposição existem como campos de probabilidade
 *    antes do colapso (OR)
 * 
 * Nesta representação, cada partícula representa um componente do campo de probabilidade
 * quântica descrito pela equação de Schrödinger, antes da redução objetiva.
 */
interface ProbabilityFieldsProps {
  particleCount?: number;
  coherence?: number;
  collapseActive?: boolean;
}

const ProbabilityFields = React.memo<ProbabilityFieldsProps>(({ 
  particleCount = 150, 
  coherence = 0.3, 
  collapseActive = false 
}) => {
  const particles = useRef<THREE.Points>(null);
  
  // Criando posições iniciais para partículas
  // Cada partícula representa um "elemento" da função de onda quântica
  const positions = useMemo(() => {
    const pos = [];
    for (let i = 0; i < particleCount; i++) {
      const theta = Math.random() * Math.PI * 2;
      const phi = Math.random() * Math.PI;
      // Distribuir em volume esférico - representa o campo de probabilidade 3D
      const radius = 0.5 + Math.random() * 1.5;
      
      pos.push(
        radius * Math.sin(phi) * Math.cos(theta),
        radius * Math.sin(phi) * Math.sin(theta),
        radius * Math.cos(phi)
      );
    }
    return new Float32Array(pos);
  }, [particleCount]);
  
  // Cores para as partículas - representam diferentes estados quânticos
  // Em Orch OR, os estados quânticos envolvem diferentes configurações de elétrons
  // em proteínas tubulina, cada uma com diferentes níveis energéticos
  const colors = useMemo(() => {
    const cols = [];
    for (let i = 0; i < particleCount; i++) {
      // Gradiente de cor de azul a violeta - representa espectro de energia quântica
      // Azul: Estados de baixa energia
      // Violeta: Estados de alta energia (próximos de colapso OR)
      const h = 180 + (Math.random() * 80); // 180-260 (azul a violeta)
      const s = 60 + (Math.random() * 40);  // Saturação moderada a alta
      const l = 50 + (Math.random() * 30);  // Luminosidade média a alta
      
      const color = new THREE.Color(`hsl(${h}, ${s}%, ${l}%)`);
      cols.push(color.r, color.g, color.b);
    }
    return new Float32Array(cols);
  }, [particleCount]);

  // Optimized animation callback for probability fields
  const animateProbabilityFields = useCallback((state: { clock: { getElapsedTime: () => number } }) => {
    if (!particles.current) return;
    
    const t = state.clock.getElapsedTime();
    const positions = particles.current.geometry.attributes.position.array as Float32Array;
    
    for (let i = 0; i < positions.length; i += 3) {
      const i3 = i / 3;
      
      // Usa funções senoidais para criar movimento fluido
      // Isto simula a evolução da função de onda quântica
      const x = positions[i];
      const y = positions[i + 1];
      const z = positions[i + 2];
      
      // Cálculo de deslocamento baseado em funções senoidais entrelaçadas
      // Simula as interações não-locais dos campos quânticos
      const modulation = Math.sin(t * 0.5 + i3 * 0.1);
      const phase = t * 0.2 + i3 * 0.05;
      
      // Movimento em espiral - representa evolução da função de onda
      // que caracteriza os estados quânticos em proteínas tubulina
      positions[i] = x + Math.sin(phase + y * 0.5) * 0.01 * modulation;
      positions[i + 1] = y + Math.cos(phase + x * 0.5) * 0.01 * modulation;
      positions[i + 2] = z + Math.sin(phase * 1.5) * 0.01 * modulation;
    }
    
    particles.current.geometry.attributes.position.needsUpdate = true;
    
    // Rotação lenta do sistema de partículas
    // Representa a dinâmica global do campo quântico
    particles.current.rotation.y = t * 0.05;
    particles.current.rotation.x = Math.sin(t * 0.1) * 0.2;
  }, []);
  
  // Animação do campo de probabilidade
  // Na teoria Orch OR, os campos quânticos evoluem de acordo com a equação de Schrödinger
  // até atingirem um limiar de massa-energia para colapso gravitacional
  useFrame(animateProbabilityFields);

  // Memoized geometry for better performance
  const geometry = useMemo(() => {
    const geom = new THREE.BufferGeometry();
    geom.setAttribute('position', new THREE.BufferAttribute(positions, 3));
    geom.setAttribute('color', new THREE.BufferAttribute(colors, 3));
    return geom;
  }, [positions, colors]);

  // Memoized material for better performance
  const material = useMemo(() => {
    return new THREE.PointsMaterial({
      size: 0.05,
      vertexColors: true,
      transparent: true,
      opacity: collapseActive ? 1 : (0.03 + 0.8 * coherence),
      sizeAttenuation: true
    });
  }, [collapseActive, coherence]);
  
  return (
    <points ref={particles} geometry={geometry} material={material} />
  );
});

ProbabilityFields.displayName = 'ProbabilityFields';

export { ProbabilityFields };
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useRef, useMemo } from 'react';
import * as THREE from 'three';
import { useFrame } from '@react-three/fiber';

/**
 * Propriedades para o componente de visualização da decoerência quântica
 */
interface QuantumDecoherenceProps {
  /** Intensidade da decoerência (0-1) */
  intensity: number;
  /** Posição do efeito no espaço 3D */
  position?: [number, number, number];
  /** Escala visual do efeito */
  scale?: number;
  /** Velocidade da animação */
  speed?: number;
  /** Direção da propagação da decoerência */
  direction?: [number, number, number];
  /** Cor base do efeito */
  color?: string;
}

/**
 * Componente que visualiza o processo de decoerência quântica
 * 
 * A decoerência quântica é o processo pelo qual estados quânticos perdem
 * suas propriedades de superposição devido à interação com o ambiente.
 * Este é um desafio fundamental para a teoria Orch-OR, que propõe que
 * os microtúbulos possuem mecanismos para proteger contra a decoerência.
 */
export const QuantumDecoherence: React.FC<QuantumDecoherenceProps> = ({
  intensity,
  position = [0, 0, 0],
  scale = 1,
  speed = 1,
  direction = [1, 0, 0],
  color = '#ff3300'
}) => {
  // Referências para os elementos visuais
  const groupRef = useRef<THREE.Group>(null!);
  const particlesRef = useRef<THREE.Points>(null!);
  const waveRef = useRef<THREE.Mesh>(null!);
  
  // Número de partículas baseado na intensidade
  const particleCount = useMemo(() => Math.round(100 * intensity), [intensity]);
  
  // Gerar geometria para as partículas de decoerência
  const particlesGeometry = useMemo(() => {
    const geometry = new THREE.BufferGeometry();
    const positions = new Float32Array(particleCount * 3);
    const sizes = new Float32Array(particleCount);
    const colors = new Float32Array(particleCount * 3);
    
    const colorObj = new THREE.Color(color);
    
    for (let i = 0; i < particleCount; i++) {
      // Posição aleatória em uma esfera
      const radius = 0.1 + Math.random() * 0.2;
      const theta = Math.random() * Math.PI * 2;
      const phi = Math.random() * Math.PI;
      
      positions[i * 3] = radius * Math.sin(phi) * Math.cos(theta);
      positions[i * 3 + 1] = radius * Math.sin(phi) * Math.sin(theta);
      positions[i * 3 + 2] = radius * Math.cos(phi);
      
      // Tamanho baseado na distância do centro (partículas mais distantes são menores)
      sizes[i] = 0.01 + 0.03 * (1 - Math.random() * 0.3);
      
      // Cores gradientes baseadas na distância
      const distance = Math.sqrt(
        positions[i * 3] ** 2 +
        positions[i * 3 + 1] ** 2 +
        positions[i * 3 + 2] ** 2
      );
      
      const colorFactor = Math.max(0, 1 - distance * 2);
      colors[i * 3] = colorObj.r * colorFactor;
      colors[i * 3 + 1] = colorObj.g * colorFactor;
      colors[i * 3 + 2] = colorObj.b * colorFactor;
    }
    
    geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
    geometry.setAttribute('size', new THREE.BufferAttribute(sizes, 1));
    geometry.setAttribute('color', new THREE.BufferAttribute(colors, 3));
    
    return geometry;
  }, [particleCount, color]);
  
  // Material para partículas de decoerência
  const particlesMaterial = useMemo(() => {
    return new THREE.PointsMaterial({
      size: 0.02,
      transparent: true,
      opacity: 0.7,
      vertexColors: true,
      blending: THREE.AdditiveBlending,
      depthWrite: false,
      sizeAttenuation: true
    });
  }, []);
  
  // Criar o material para a onda de decoerência
  const waveMaterial = useMemo(() => {
    return new THREE.MeshBasicMaterial({
      color: new THREE.Color(color).lerp(new THREE.Color('#ffffff'), 0.5),
      transparent: true,
      opacity: 0.3,
      side: THREE.DoubleSide,
      depthWrite: false,
      blending: THREE.AdditiveBlending
    });
  }, [color]);
  
  // Animação da decoerência
  useFrame((state) => {
    if (!groupRef.current || !particlesRef.current || !waveRef.current) return;
    
    // Garantir intensidade mínima para visibilidade em estado de repouso
    const baseIntensity = Math.max(intensity, 0.35);
    const t = state.clock.getElapsedTime() * speed;
    
    // Rotação suave do grupo - mais evidente no estado de repouso
    groupRef.current.rotation.x = Math.sin(t * 0.3) * 0.3;
    groupRef.current.rotation.y = Math.sin(t * 0.2) * 0.4;
    
    // Atualizar posições das partículas para simular dissipação
    const positions = particlesRef.current.geometry.attributes.position.array as Float32Array;
    const sizes = particlesRef.current.geometry.attributes.size.array as Float32Array;
    
    for (let i = 0; i < particleCount; i++) {
      // Movimento para fora (decoerência)
      const idx = i * 3;
      const dirX = positions[idx];
      const dirY = positions[idx + 1];
      const dirZ = positions[idx + 2];
      
      // Normalizar direção
      const len = Math.sqrt(dirX * dirX + dirY * dirY + dirZ * dirZ);
      const normalizedX = dirX / len;
      const normalizedY = dirY / len;
      const normalizedZ = dirZ / len;
      
      // Movimento para fora + direção preferencial - mais visível mesmo com baixa intensidade
      // Adiciona movimento oscilatório mais visível
      const oscillation = Math.sin(t * 2 + i * 0.5) * 0.8 + 0.2;
      positions[idx] += (normalizedX * 0.002 * baseIntensity + direction[0] * 0.001) * oscillation;
      positions[idx + 1] += (normalizedY * 0.002 * baseIntensity + direction[1] * 0.001) * oscillation;
      positions[idx + 2] += (normalizedZ * 0.002 * baseIntensity + direction[2] * 0.001) * oscillation;
      
      // Reduzir tamanho com o tempo (partículas desvanecem) mas manter tamanho mínimo
      sizes[i] = Math.max(sizes[i] * 0.998, 0.01);
    }
    
    particlesRef.current.geometry.attributes.position.needsUpdate = true;
    particlesRef.current.geometry.attributes.size.needsUpdate = true;
    
    // Animar a onda de decoerência
    if (waveRef.current) {
      // Expandir a onda para fora
      waveRef.current.scale.setScalar(1 + 0.2 * Math.sin(t * 2) + 0.3 * intensity);
      
      // Reduzir opacidade da onda com o tempo
      const material = waveRef.current.material as THREE.MeshBasicMaterial;
      material.opacity = 0.3 * (0.5 + 0.5 * Math.sin(t * 3)) * intensity;
    }
  });
  
  return (
    <group ref={groupRef} position={position} scale={scale}>
      {/* Partículas de decoerência */}
      <points ref={particlesRef} geometry={particlesGeometry} material={particlesMaterial} />
      
      {/* 
      Removida a onda esférica central que causava o efeito de bolinha intermitente
      Mantendo apenas as partículas de decoerência, que são mais consistentes visualmente
      e representam melhor o fenômeno físico de perda de coerência quântica
      */}
    </group>
  );
};

/**
 * Versão memorizada do componente para otimização de renderização
 */
export default React.memo(QuantumDecoherence);// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import { useFrame } from '@react-three/fiber';
import { Fragment, memo, useCallback, useMemo, useRef } from 'react';
import * as THREE from 'three';

/**
 * Quantum entanglement component - representa a coerência quântica entre microtúbulos na teoria Orch OR
 * 
 * Na teoria Orch OR de Penrose-Hameroff, o emaranhamento quântico entre microtúbulos
 * em diferentes neurônios permite:
 * 
 * 1. Coerência em escala macroscópica através do cérebro
 * 2. Sincronização de atividade neural que transcende conexões sinápticas
 * 3. Correlações não-locais que contribuem para a "unidade" da experiência consciente
 * 
 * Este componente visualiza essas conexões quânticas não-locais que são
 * fundamentais para a integração de informação na teoria Orch OR.
 */
interface QuantumEntanglementProps {
  pairs?: number;
  coherence?: number;
  collapseActive?: boolean;
}

const QuantumEntanglement = memo<QuantumEntanglementProps>(({
  pairs = 8,
  coherence = 0.3,
  collapseActive = false
}) => {
  const lines = useRef<THREE.Group>(null);
  const points = useRef<THREE.Group>(null);

  // Pares de partículas emaranhadas - representam pares de dímeros de tubulina 
  // em estado de emaranhamento quântico através de diferentes regiões cerebrais
  const entangledPairs = useMemo(() => {
    return Array(pairs).fill(0).map((_, i) => {
      // Para cada par, definimos dois pontos no espaço 3D
      // A distância entre pontos simula distintas regiões cerebrais
      const theta1 = (i / pairs) * Math.PI * 2;
      const phi1 = Math.random() * Math.PI;
      const radius1 = 0.7 + Math.random() * 0.5;

      // Pontos em regiões distantes, simulando emaranhamento não-local
      const theta2 = theta1 + Math.PI * (0.5 + Math.random() * 0.5); // Pontos aproximadamente opostos
      const phi2 = Math.PI - phi1 + (Math.random() - 0.5) * 0.5;
      const radius2 = 0.7 + Math.random() * 0.5;

      const point1 = new THREE.Vector3(
        radius1 * Math.sin(phi1) * Math.cos(theta1),
        radius1 * Math.sin(phi1) * Math.sin(theta1),
        radius1 * Math.cos(phi1)
      );

      const point2 = new THREE.Vector3(
        radius2 * Math.sin(phi2) * Math.cos(theta2),
        radius2 * Math.sin(phi2) * Math.sin(theta2),
        radius2 * Math.cos(phi2)
      );

      return {
        point1,
        point2,
        originalPoint1: point1.clone(),
        originalPoint2: point2.clone(),
        // Fase quântica - simula fase coerente entre pares
        // Na física quântica, sistemas emaranhados compartilham fases correlacionadas
        phase: Math.random() * Math.PI * 2,
        // Frequência de oscilação quântica
        // Representa oscilações de Fröhlich nos microtúbulos
        frequency: 2 + Math.random() * 3,
        // Força do emaranhamento - afeta a intensidade da correlação quântica
        // Na teoria Orch-OR, emaranhamento quântico é significativo mesmo em repouso
        // Aumentamos o valor mínimo para refletir esse aspecto fundamental
        entanglementStrength: 0.5 + Math.random() * 0.5,
        // Cor única para cada par emaranhado
        color: new THREE.Color().setHSL(Math.random(), 0.7, 0.5)
      };
    });
  }, [pairs]);

  // Optimized animation callback for quantum entanglement
  const animateQuantumEntanglement = useCallback((state: { clock: { getElapsedTime: () => number } }) => {
    const t = state.clock.getElapsedTime();

    // Atualização dos pontos de partículas emaranhadas
    if (points.current) {
      points.current.children.forEach((point, idx) => {
        // Os dois pontos de cada par têm índices i e i+1
        const pairIdx = Math.floor(idx / 2);
        const isFirstPoint = idx % 2 === 0;

        if (pairIdx >= entangledPairs.length) return;

        const pair = entangledPairs[pairIdx];
        const mesh = point as THREE.Mesh;

        // Fase compartilhada - simula correlação quântica
        // Em sistemas emaranhados, a medida de uma propriedade em uma partícula
        // instantaneamente determina a propriedade correspondente na outra
        const sharedPhase = pair.phase + t * pair.frequency;

        // Oscilações correlacionadas - quando um vai para cima, o outro vai para baixo
        // Este comportamento anti-correlacionado é característico de sistemas emaranhados
        const oscillation = Math.sin(sharedPhase) * 0.1;

        // Atualizando posição - movimento em anti-fase (correlação quântica)
        if (isFirstPoint) {
          const originalPos = pair.originalPoint1;
          pair.point1.set(
            originalPos.x + oscillation * Math.sin(sharedPhase),
            originalPos.y + oscillation * Math.cos(sharedPhase),
            originalPos.z + oscillation
          );
          mesh.position.copy(pair.point1);

          // Cor varia com fase - simula estados quânticos
          const hue = (0.6 + 0.2 * Math.sin(sharedPhase)) % 1;
          (mesh.material as THREE.MeshBasicMaterial).color.setHSL(hue, 0.7, 0.6);
        } else {
          const originalPos = pair.originalPoint2;
          // Movimento em anti-fase - correlação quântica em estados opostos
          pair.point2.set(
            originalPos.x - oscillation * Math.sin(sharedPhase),
            originalPos.y - oscillation * Math.cos(sharedPhase),
            originalPos.z - oscillation
          );
          mesh.position.copy(pair.point2);

          // Cor correlacionada com o outro ponto - emaranhamento
          const hue = (0.6 + 0.2 * Math.sin(sharedPhase + Math.PI)) % 1;
          (mesh.material as THREE.MeshBasicMaterial).color.setHSL(hue, 0.7, 0.6);
        }

        // Pulso das partículas - representa flutuações quânticas
        const pulse = 0.8 + 0.2 * Math.sin(sharedPhase * 2);
        mesh.scale.setScalar(pulse * (0.05 + 0.05 * pair.entanglementStrength));
      });
    }

    // Atualização das linhas de emaranhamento
    if (lines.current) {
      lines.current.children.forEach((lineObj, i) => {
        if (i >= entangledPairs.length) return;

        const pair = entangledPairs[i];
        // Para primitives, o objeto Three.js real está no userData ou como child
        const line = (lineObj as any).object || lineObj as THREE.Line;

        try {
          // Atualiza a geometria da linha para conectar os pontos
          const lineGeometry = line.geometry as THREE.BufferGeometry;
          const positions = new Float32Array([
            pair.point1.x, pair.point1.y, pair.point1.z,
            pair.point2.x, pair.point2.y, pair.point2.z
          ]);

          lineGeometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
          lineGeometry.attributes.position.needsUpdate = true;

          // Intensidade da linha varia com a força do emaranhamento e fase
          // Sistemas fortemente emaranhados mostram correlações mais intensas
          const lineIntensity = 0.3 + 0.4 * Math.sin(pair.phase + t * pair.frequency);
          (line.material as THREE.LineBasicMaterial).opacity = lineIntensity * pair.entanglementStrength;

          // A cor da linha pulsa em tons da cor base
          const baseColor = pair.color;
          const hue = baseColor.getHSL({ h: 0, s: 0, l: 0 }).h;
          const hueShift = 0.05 * Math.sin(pair.phase + t * pair.frequency);
          (line.material as THREE.LineBasicMaterial).color.setHSL(
            (hue + hueShift) % 1,
            0.7,
            0.6 + 0.2 * Math.sin(pair.phase + t * pair.frequency * 1.5)
          );
        } catch (error) {
          console.warn('Error updating line geometry:', error);
        }
      });
    }
  }, [entangledPairs]);

  // Animação do emaranhamento quântico
  // Simula a natureza correlacionada das propriedades quânticas
  useFrame(animateQuantumEntanglement);

  // Memoized particle components for better performance
  const particleComponents = useMemo(() => {
    return entangledPairs.map((pair, i) => (
      <Fragment key={`points-${i}`}>
        {/* Primeira partícula do par emaranhado */}
        <mesh position={pair.point1}>
          <sphereGeometry args={[0.05, 8, 8]} />
          {/* Opacidade aumentada para refletir a presença significativa do emaranhamento mesmo em estado basal */}
          <meshBasicMaterial color={pair.color} transparent opacity={collapseActive ? 1 : (0.25 + 0.75 * coherence)} />
        </mesh>

        {/* Segunda partícula do par emaranhado */}
        <mesh position={pair.point2}>
          <sphereGeometry args={[0.05, 8, 8]} />
          {/* Opacidade aumentada para refletir a presença significativa do emaranhamento mesmo em estado basal */}
          <meshBasicMaterial color={pair.color} transparent opacity={collapseActive ? 1 : (0.25 + 0.75 * coherence)} />
        </mesh>
      </Fragment>
    ));
  }, [entangledPairs, collapseActive, coherence]);

  // Memoized line components for better performance
  const lineComponents = useMemo(() => {
    return entangledPairs.map((pair, i) => {
      const lineGeometry = new THREE.BufferGeometry();
      const positions = new Float32Array([
        pair.point1.x, pair.point1.y, pair.point1.z,
        pair.point2.x, pair.point2.y, pair.point2.z
      ]);
      lineGeometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));

      const lineMaterial = new THREE.LineBasicMaterial({
        color: pair.color,
        transparent: true,
        opacity: 0.5 * pair.entanglementStrength
      });

      const line = new THREE.Line(lineGeometry, lineMaterial);

      return (
        <primitive key={`line-${i}`} object={line} />
      );
    });
  }, [entangledPairs]);

  return (
    <group>
      {/* Partículas emaranhadas - representam dímeros de tubulina */}
      <group ref={points}>
        {particleComponents}
      </group>

      {/* Linhas de emaranhamento - conexões quânticas não-locais */}
      <group ref={lines}>
        {lineComponents}
      </group>
    </group>
  );
});

export default QuantumEntanglement;// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import React from 'react';
import { QUANTUM_PHENOMENA } from '../QuantumLegend';
import { QuantumFrequencyBand, useQuantumVisualization } from '../QuantumVisualizationContext';
import { InterferencePatterns } from './InterferencePatterns';
import { ProbabilityFields } from './ProbabilityFields';
import { QuantumDecoherence } from './QuantumDecoherence';
import QuantumEntanglement from './QuantumEntanglement';
import { QuantumIsolationField } from './QuantumIsolation';
import { QuantumSuperposition } from './QuantumSuperposition';
import { Observer, getCorePosition } from './QuantumUtils';
import WaveCollapse from './WaveCollapse';

/**
 * Main component that orchestrates the complete visualization
 * Integra todos os componentes de visualização quântica e os orquestra 
 * de acordo com os estados quânticos e eventos cognitivos baseados em Orch OR
 * 
 * Baseado na teoria de Penrose-Hameroff, que propõe que a consciência emerge
 * de processos quânticos em microtúbulos neurais que culminam em eventos de
 * "redução objetiva orquestrada" (Orch OR).
 */
export function QuantumField() {
  // Obter os estados quânticos da teoria Orch OR
  const {
    quantumSuperpositions,
    quantumEntanglements,
    objectiveReductions,
    consciousStates,
    tubulinCoherenceLevel,
    clearAllEffects,
    
    activeVisualFilters
  } = useQuantumVisualization();

  // Timers de referência  // Ref para gerenciar timers de ciclo quântico
  const visualDisplayTimerRef = React.useRef<NodeJS.Timeout | null>(null);
  const fadeoutTimerRef = React.useRef<NodeJS.Timeout | null>(null);
  const newCycleTimerRef = React.useRef<NodeJS.Timeout | null>(null);
  
  // Referências para o estado temporal da animação

  // Efeito: Implementa o ciclo temporal da teoria Orch-OR com precisão científica
  React.useEffect(() => {
    // Se detectamos um evento OR (colapso)...
    if (objectiveReductions.length > 0) {
      // 1. Limpa timers anteriores para evitar sobreposição
      [visualDisplayTimerRef, fadeoutTimerRef, newCycleTimerRef].forEach(timer => {
        if (timer.current) {
          clearTimeout(timer.current);
          timer.current = null;
        }
      });
      
      // 2. EXIBIÇÃO VISUAL DO COLAPSO: ~600ms (dilatado dos 25-40ms reais)
      visualDisplayTimerRef.current = setTimeout(() => {
        console.log('[OrchOR] Completing OR collapse visual display'); 
        
        // 3. RETORNO À SUPERPOSIÇÃO COM FADEOUT: ~250ms
        fadeoutTimerRef.current = setTimeout(() => {
          console.log('[OrchOR] Quantum fadeout - returning to coherent superposition');
          
          // 4. NOVO CICLO QUÂNTICO: ~1.5s (ciclo quântico completo)
          newCycleTimerRef.current = setTimeout(() => {
            console.log('[OrchOR] Initiating new quantum cycle');
            clearAllEffects();
          }, 1500); 
          
        }, 250); 
        
      }, 600);
    }
    
    // Cleanup na desmontagem
    return () => {
      [visualDisplayTimerRef, fadeoutTimerRef, newCycleTimerRef].forEach(timer => {
        if (timer.current) {
          clearTimeout(timer.current);
          timer.current = null;
        }
      });
    };
  }, [objectiveReductions, clearAllEffects]);
  
  // Determinar quais fenômenos quânticos mostrar baseado nos eventos cognitivos
  // Traduzindo o modelo Orch OR para efeitos visuais com maior precisão científica
  const showConsciousStates = consciousStates.length > 0;
  // Ser mais conservador no limiar de coerência: na teoria Orch-OR, 
  // coerência quântica significativa (>60%) é necessária para emaranhamento macroscópico
  const showQuantumCoherence = quantumEntanglements.length > 0 || tubulinCoherenceLevel > 0.6;
  
  // Na teoria Orch-OR, sempre há algum nível de atividade quântica nos microtúbulos
  // Vamos implementar dois níveis de atividade: estimulado e base/repouso

  // Verificar se há algum efeito quântico estimulado - AJUSTE CIENTÍFICO IMPORTANTE
  const hasStimulatedQuantumEffects = 
    quantumSuperpositions.length > 2 || // permitir até 2 superposições em estado basal
    quantumEntanglements.length > 1 || // permitir 1 entanglement em estado basal
    objectiveReductions.length > 0 || // CORREÇÃO: NÃO deve haver reduções objetivas em estado basal (teoria Orch-OR)
    consciousStates.length > 0;      // consciousStates só existem sob estímulo
    
  // Filter visibility based on activeVisualFilters (multiple selection)
  const shouldShowComponent = (componentType: string): boolean => {
    // Se não há filtros ativos, mostrar tudo
    if (!activeVisualFilters || activeVisualFilters.length === 0) return true;

    // Mapeamento de tipo para id de fenômeno
    const typeToId: Record<string, string> = {
      superposition: QUANTUM_PHENOMENA.SUPERPOSITION.id,
      reduction: QUANTUM_PHENOMENA.REDUCTION.id,
      entanglement: QUANTUM_PHENOMENA.ENTANGLEMENT.id,
      conscious: QUANTUM_PHENOMENA.CONSCIOUS.id,
      coherence: QUANTUM_PHENOMENA.COHERENCE.id,
      orchestration: QUANTUM_PHENOMENA.ORCHESTRATION.id,
      observer: QUANTUM_PHENOMENA.OBSERVER.id,
      decoherence: QUANTUM_PHENOMENA.DECOHERENCE.id,
      isolation: QUANTUM_PHENOMENA.ISOLATION.id
    };
    const id = typeToId[componentType];
    return !!id && activeVisualFilters.includes(id);
  };

  // Se não houver estímulo, mostrar a atividade quântica basal
  // Baseado na teoria Orch-OR de Penrose-Hameroff que prevê oscilações quânticas constantes nos microtúbulos
  if (!hasStimulatedQuantumEffects) {
    return (
      <group>
        {/* Campos de probabilidade quântica - representação de atividade base constante */}
        {shouldShowComponent('coherence') && (
          <ProbabilityFields particleCount={60} />
        )}
        
        {/* Padrões de interferência quântica - sempre presentes em nível basal */}
        {shouldShowComponent('orchestration') && (
          <InterferencePatterns />
        )}
        
        {/* Superposições quânticas em nível basal - oscilações Fröhlich (8MHz) */}
        {shouldShowComponent('superposition') && (
          <>
            <group position={[0, 0, 0]}>
              <QuantumSuperposition amount={2} />
            </group>
            <group position={[1.5, 0.5, -0.5]}>
              <QuantumSuperposition amount={1} />
            </group>
          </>
        )}
        
        {/* Entanglement quântico em nível basal - coerência quântica fundamental */}
        {/* Na teoria Orch-OR, mesmo em estado basal, há emaranhamento significativo entre microtúbulos */}
        {/* O emaranhamento ocorre em múltiplas frequências (gigahertz, megahertz, kilohertz) */}
        {shouldShowComponent('entanglement') && (
          <>
            {/* Emaranhamento quântico central - entre microtúbulos regionais */}
            {/* Representa o emaranhamento de maior intensidade nas regiões de alta densidade neural */}
            <group position={[0, 0.2, 0.3]}>
              <QuantumEntanglement pairs={8} coherence={0.7} />
            </group>
            
            {/* Emaranhamentos secundários em posições distribuídas */}
            {/* Representa a coerência quântica distribuída entre regiões cerebrais distantes */}
            <group position={[-1.2, 0.3, -0.4]}>
              <QuantumEntanglement pairs={6} coherence={0.6} />
            </group>
            
            <group position={[1.0, 0.2, -0.5]}>
              <QuantumEntanglement pairs={6} coherence={0.6} />
            </group>
          </>
        )}
        
        {/* Decoerência quântica em nível basal - sempre presente como desafio */}
        {shouldShowComponent('decoherence') && (
          <group position={[1.2, 0.3, -0.8]}>
            <QuantumDecoherence 
              intensity={0.4}
              scale={0.8} 
              speed={0.5}
              color="#FF5500"
            />
          </group>
        )}
        
        {/* Mecanismos de isolamento quântico - protegem contra decoerência */}
        {shouldShowComponent('isolation') && (
          <group position={[0, 0, 0]}>
            <QuantumIsolationField 
              isolationFactor={0.5} 
              size={1.5}
              pulseSpeed={0.3}
            />
          </group>
        )}
        
        {/* Observador quântico em estado basal - proto-consciência */}
        {shouldShowComponent('observer') && (
          <Observer active={false} />
        )}
        
        {/* Redução objetiva em nível basal - eventos OR espontâneos de baixo nível */}
        {shouldShowComponent('reduction') && (
          <group position={[0, 0.2, -1]}>
            <WaveCollapse 
              active={true} 
              isNonComputable={false}
              color="#00B4D8"
            />
          </group>
        )}
      </group>
    );
  }
  
  return (
    <group>
      {/* Probability fields - representam campos quânticos em microtúbulos */}
      {shouldShowComponent('coherence') && quantumEntanglements.map((effect) => {
        // O número de partículas representa a intensidade da coerência quântica
        // Em Orch OR, a coerência entre tubulinas é essencial para a consciência
        // Em repouso, mantenha apenas 10 partículas. Em alta amplitude, aumente suavemente.
        const minParticles = 10;
        const maxParticles = 200;
        const particleCount = Math.round(minParticles + (maxParticles - minParticles) * Math.max(0, Math.min(1, effect.amplitude)));
        const collapseActive = objectiveReductions.length > 0;
        
        return (
          <group key={effect.id} position={getCorePosition(effect.core)}>
            <ProbabilityFields
              particleCount={particleCount}
              coherence={tubulinCoherenceLevel}
              collapseActive={collapseActive}
            />
          </group>
        );
      })}
      
      {/* Superposições quânticas na estrutura microtubular */}
      {shouldShowComponent('superposition') && quantumSuperpositions.slice(0, 13).map((effect) => {
        // Nível de coerência quântica afeta a quantidade de elementos
        // Isso representa os dímeros de tubulina em estado de superposição
        // Limite científico: máximo de 13 elementos de superposição, conforme número de protofilamentos/microtúbulo na teoria Orch-OR
        // O valor é derivado de 'effect.amplitude' (intensidade do efeito)
        // Em repouso, mantenha 1 elemento de superposição. Em alta amplitude, aumente suavemente até 13.
        const minSuperpositions = 1;
        const maxSuperpositions = 13;
        const amount = Math.max(minSuperpositions, Math.round(minSuperpositions + ((effect.amplitude ?? 0) * (maxSuperpositions - minSuperpositions))));
        return (
          <group key={effect.id} position={getCorePosition(effect.core)}>
            <QuantumSuperposition 
              amount={amount} 
              coherence={tubulinCoherenceLevel}
              collapseActive={objectiveReductions.length > 0}
            />
          </group>
        );
      })}
      
      {/* Entanglement quântico - representação dos pares emaranhados */}
      {shouldShowComponent('entanglement') && quantumEntanglements.map((effect) => {
        // O número de pares emaranhados representa a intensidade da interconexão quântica
        // Em Orch OR, o emaranhamento conecta regiões do cérebro através de microtuábulos
        const minPairs = 1;
        const maxPairs = 5;
        const pairs = Math.max(minPairs, Math.round(minPairs + ((effect.amplitude ?? 0) * (maxPairs - minPairs))));
        return (
          <group key={effect.id} position={getCorePosition(effect.core)}>
            <QuantumEntanglement 
              pairs={pairs} 
              coherence={effect.phaseCoherence ?? 0.5}
              collapseActive={objectiveReductions.length > 0}
            />
          </group>
        );
      })}
      
      {/* Padrões de interferência - sempre presentes */}
      {shouldShowComponent('orchestration') && (
        <InterferencePatterns 
          coherence={tubulinCoherenceLevel} 
          collapseActive={objectiveReductions.length > 0}
        />
      )}
      
      {/* Esta instância do Observer foi removida para evitar duplicação */}
      
      {/* Reduções objetivas (OR) - momentos de consciência segundo Orch OR */}
      {shouldShowComponent('reduction') && objectiveReductions.map((effect) => {
        // Na teoria Orch OR, a redução objetiva (OR) causa momentos de consciência
        // quando a auto-energia gravitacional atinge um limiar crítico
        // O aspecto "não-computável" é central na teoria de Penrose
        const isNonComputable = effect.nonComputable ?? false;
        
        return (
          <group key={effect.id} position={getCorePosition(effect.core)}>
            <WaveCollapse 
              active={true} 
              isNonComputable={isNonComputable}
              // Cores baseadas na banda de frequência - cada frequência representa 
              // diferentes níveis hierárquicos na atividade neuronal
              color={effect.frequencyBand === QuantumFrequencyBand.TERAHERTZ ? "#FF00FF" : // Nível quântico fundamental
                    effect.frequencyBand === QuantumFrequencyBand.GIGAHERTZ ? "#00FFFF" :  // Oscilações de Fröhlich
                    effect.frequencyBand === QuantumFrequencyBand.MEGAHERTZ ? "#FFFF00" :  // Coerência de microtúbulos
                    "#00FF00"}                                                          // Atividade macro-neuronal
            />
          </group>
        );
      })}
      
      {/* Quantum entanglement between regions - coerência quântica entre regiões cerebrais */}
      {showQuantumCoherence && (
        <group>
          {/* Limite científico: máximo de 32 pares de emaranhamento, conforme plausibilidade física da teoria Orch-OR */}
          {/* Em repouso (coerência baixa), renderize apenas 1 par de emaranhamento. Em alta coerência, aumente suavemente até 32 pares. */}
          <QuantumEntanglement
            pairs={Math.max(1, Math.round(1 + tubulinCoherenceLevel * 31))}
            coherence={tubulinCoherenceLevel}
            collapseActive={objectiveReductions.length > 0}
          />
        </group>
      )}
      
      {/* Quantum Decoherence - processos de perda de coerência quântica */}
      {shouldShowComponent('decoherence') && (
        <group position={[0, 0.5, -1]}>
          <QuantumDecoherence 
            intensity={0.7} 
            scale={1.5}
            speed={0.8}
            color="#FF5500"
          />
        </group>
      )}
      
      {/* Quantum Isolation - mecanismos de proteção contra decoerência quântica */}
      {shouldShowComponent('isolation') && (
        <group position={[0, 0, 0]}>
          <QuantumIsolationField 
            isolationFactor={0.8} 
            size={2.0}
            pulseSpeed={0.5}
          />
        </group>
      )}
      
      {/* Padrões de interferência - emergem quando há alta coerência quântica */}
      {tubulinCoherenceLevel > 0.7 && (
        <InterferencePatterns
          coherence={tubulinCoherenceLevel}
          collapseActive={objectiveReductions.length > 0}
        />
      )}
      
      {/* O observador quântico - aspecto central na teoria Orch OR */}
      {shouldShowComponent('observer') && (
        <Observer active={consciousStates.length > 0} />
      )}
    </group>
  );
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { QuantumEffect, QuantumFrequencyBand } from '../QuantumVisualizationContext';
import * as THREE from 'three';
import React, { useMemo, useRef } from 'react';
import { useFrame } from '@react-three/fiber';

/**
 * Fatores biofísicos que contribuem para o isolamento quântico nos microtúbulos
 * segundo a teoria Orch-OR e pesquisas mais recentes
 */
export const QUANTUM_ISOLATION_FACTORS = {
  // Proteínas MAP2 e tau estabilizam os microtúbulos e isolam do ambiente
  MAP_PROTEINS: 0.35,
  
  // Água estruturada em estado gel dentro e ao redor dos microtúbulos
  // (Jibu, Hagan, Pribram, Yasue, 1994; Del Giudice et al. 2005)
  ORDERED_WATER: 0.25,
  
  // Citoesqueleto atua como suporte mecânico e isolante
  CYTOSKELETON: 0.15,
  
  // Estados de coerência atuam como proteção quântica
  // Efeito Zenão Quântico - medições frequentes evitam decoerência
  QUANTUM_ZENO_EFFECT: 0.15,
  
  // Cavidades internas dos microtúbulos (11nm) fornecem isolamento
  TUBULAR_CAVITIES: 0.10
};

/**
 * Calcula o fator de isolamento quântico para um efeito quântico específico
 * O isolamento explica como a coerência quântica é mantida no cérebro
 * 
 * @param effect Efeito quântico para calcular isolamento
 * @param environmentalNoise Nível de ruído ambiental (0-1)
 * @returns Fator de isolamento quântico (maior = melhor isolamento)
 */
export function calculateQuantumIsolation(
  effect: QuantumEffect, 
  environmentalNoise: number = 0.3
): number {
  // Calcular eficácia do isolamento para a frequência específica
  // Frequências mais altas são mais difíceis de isolar (mais vulneráveis à decoerência)
  let frequencyIsolationFactor = 1.0;
  switch (effect.frequencyBand) {
    case QuantumFrequencyBand.TERAHERTZ:
      frequencyIsolationFactor = 0.5; // Mais difícil de isolar
      break;
    case QuantumFrequencyBand.GIGAHERTZ:
      frequencyIsolationFactor = 0.7;
      break;
    case QuantumFrequencyBand.MEGAHERTZ:
      frequencyIsolationFactor = 0.85;
      break;
    case QuantumFrequencyBand.KILOHERTZ:
      frequencyIsolationFactor = 0.95;
      break;
    case QuantumFrequencyBand.HERTZ:
      frequencyIsolationFactor = 1.0; // Mais fácil de isolar
      break;
    default:
      frequencyIsolationFactor = 0.7;
  }
  
  // Calcular isolamento total baseado nos fatores biofísicos
  const totalIsolation = Object.values(QUANTUM_ISOLATION_FACTORS).reduce(
    (sum, factor) => sum + factor, 
    0
  );
  
  // Ajustar com a fase de coerência do efeito (maior coerência = melhor isolamento)
  const coherenceFactor = effect.phaseCoherence || 0.5;
  
  // Aplicar ruído ambiental (temperatura, campos eletromagnéticos, etc.)
  // Este fator é crucial para entender como o cérebro mantém estados quânticos
  // em temperatura corporal (310K), um desafio fundamental para a teoria Orch-OR
  const effectiveIsolation = totalIsolation * 
    frequencyIsolationFactor * 
    coherenceFactor * 
    (1 - environmentalNoise);
  
  return Math.min(1, effectiveIsolation);
}

interface QuantumIsolationFieldProps {
  isolationFactor: number;
  size?: number;
  pulseSpeed?: number;
}

/**
 * Componente visual que representa o isolamento quântico
 * Visualiza como os microtúbulos protegem estados quânticos da decoerência
 */
export const QuantumIsolationField: React.FC<QuantumIsolationFieldProps> = ({ 
  isolationFactor, 
  size = 0.15, 
  pulseSpeed = 1.0 
}) => {
  const ref = useRef<THREE.Mesh>(null!);
  
  // Cor baseada no fator de isolamento (azul = bom isolamento, vermelho = isolamento fraco)
  // Ajustado para cores mais intensas e visíveis mesmo em estado de repouso
  const color = useMemo(() => {
    // Garantir um valor base para isolação para melhor visibilidade
    const baseIsolation = Math.max(isolationFactor, 0.4);
    
    // Aumentar saturação das cores para maior impacto visual
    const r = Math.max(0, 1 - baseIsolation) * 0.8;
    const g = Math.max(0, baseIsolation * 0.7); // Aumentar componente verde para brilho
    const b = Math.max(0, baseIsolation * 1.2); // Intensificar azul para maior destaque
    
    return new THREE.Color(r, g, b);
  }, [isolationFactor]);
  
  // Animação suave do campo de isolamento
  useFrame((state) => {
    if (!ref.current) return;
    
    // Garantir intensidade mínima para visibilidade em estado de repouso
    const baseIsolation = Math.max(isolationFactor, 0.4); // Aumentar valor mínimo para visibilidade
    const t = state.clock.getElapsedTime();
    
    // Pulsar suavemente para representar o campo de isolamento dinâmico
    // Aumentar o fator de pulsação para maior visibilidade
    const pulseFactor = 0.2 * Math.sin(t * pulseSpeed * 1.5) * baseIsolation;
    ref.current.scale.setScalar(1 + pulseFactor);
    
    // Adicionar rotação suave para melhor percepção 3D
    ref.current.rotation.x = Math.sin(t * 0.2) * 0.1;
    ref.current.rotation.y = Math.sin(t * 0.3) * 0.1;
    
    // Transparência oscilante para representar a natureza quântica do isolamento
    // Aumentar opacidade base para maior visibilidade
    if (ref.current.material instanceof THREE.Material) {
      const material = ref.current.material as THREE.MeshBasicMaterial;
      material.opacity = 0.3 * baseIsolation * (0.7 + 0.3 * Math.sin(t * 2));
    }
  });
  
  // Sempre renderizar com um valor mínimo para visibilidade em estado de repouso
  // (remover condição que impedia a renderização)
  
  // Verificar se a opacidade é suficiente para ser visível
  // Evitar artefatos visuais quando quase transparente
  const opacity = 0.3 * Math.max(isolationFactor, 0.4);
  if (opacity < 0.05) return null;
  
  return (
    <mesh ref={ref}>
      <meshBasicMaterial 
          color={color} 
          transparent={true} 
        opacity={opacity}
        depthWrite={false} // Previne problemas de renderização de ordem de profundidade
        />
    </mesh>
  );
};

export default QuantumIsolationField;// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import { Trail } from '@react-three/drei';
import React, { useCallback, useMemo, useRef } from 'react';
import * as THREE from 'three';
import { useOptimizedQuantumFrame } from '../utils/performance';

/**
 * Component for quantum state superposition in microtubules (Orch OR theory)
 * 
 * Representa a superposição quântica nas proteínas tubulina dentro dos microtúbulos
 * conforme descrito na teoria de Penrose-Hameroff. Na teoria Orch OR, as tubulinas
 * (proteínas dos microtúbulos) podem existir em superposição quântica de estados,
 * criando computação quântica em escala neuronal. Esta superposição precede a
 * Redução Objetiva (OR) que resulta em momentos conscientes.
 * 
 * Estrutura dos microtúbulos:
 * - Compostos de dímeros de tubulina (α e β) em um arranjo cilíndrico
 * - Típicamente 13 protofilamentos em arranjo hexagonal/circular
 * - Cada tubulina pode existir em superposição quântica, conforme Orch OR
 */
interface QuantumSuperpositionProps {
  amount?: number;
  coherence?: number;
  collapseActive?: boolean; // Sinaliza evento de Redução Objetiva (OR)
}

const QuantumSuperposition = React.memo<QuantumSuperpositionProps>(({ 
  amount = 7, 
  coherence = 0.3, 
  collapseActive = false 
}) => {
  const group = useRef<THREE.Group>(null);
  const tubuleRefs = useRef<THREE.Mesh[]>([]);
  
  // Criando padrão de arranjo hexagonal para simular a estrutura dos microtúbulos
  // De acordo com Hameroff, os microtúbulos têm formato hexagonal com 13 protofilamentos
  const paths = useMemo(() => {
    // Criamos um arranjo hexagonal para simular a estrutura tubular
    const hexRadius = 0.8;
    const basePoints = [];
    
    // Pontos centrais - representam o eixo dos microtúbulos
    basePoints.push(new THREE.Vector3(0, 0, 0));
    
    // Pontos na estrutura hexagonal - representam dímeros de tubulina
    for (let i = 0; i < 6; i++) {
      const angle = (i / 6) * Math.PI * 2;
      basePoints.push(new THREE.Vector3(
        hexRadius * Math.cos(angle),
        hexRadius * Math.sin(angle),
        0
      ));
    }
    
    // Criamos multiplicações do padrão para representar os diferentes níveis da estrutura
    const all = [];
    for (let j = 0; j < amount; j++) {
      // Espaçamento vertical representa o comprimento de uma seção microtubular
      const z = (j - amount/2) * 0.3;
      
      // Para cada nível, criamos uma cópia do arranjo com pequenas variações
      for (const point of basePoints) {
        all.push(new THREE.Vector3(
          point.x + (Math.random() - 0.5) * 0.1, // Pequena variação para simular as moléculas de tubulina
          point.y + (Math.random() - 0.5) * 0.1, 
          z
        ));
      }
    }
    
    return all;
  }, [amount]);
  
  // Preparando a configuração de estados de tubulina quântica
  // Na teoria Orch OR, cada dímero de tubulina pode existir em estados quânticos
  // de superposição, contribuindo para a computação quântica no cérebro
  const tubulinStates = useMemo(() => {
    // Cada estado de tubulina contém probabilidades quânticas
    return paths.map(() => ({
      // Probabilidade quântica de estados 0/1 em superposição
      probability: Math.random(),
      // Fase quântica (representando coerência quântica)
      phase: Math.random() * Math.PI * 2,
      // Frequência de oscilação (representando oscilações de Fröhlich)
      // Fröhlich propôs que proteínas como tubulina podem manter
      // estados de excitação quântica coerente em temperaturas biológicas
      frequency: 3 + Math.random() * 10
    }));
  }, [paths]);

  // Optimized animation callback with performance monitoring
  const animateQuantumStates = useCallback((state: { clock: { getElapsedTime: () => number } }) => {
    if (!group.current) return;
    
    const t = state.clock.getElapsedTime();
    
    // Rotação lenta da estrutura - representando os microtúbulos em movimento browniano
    group.current.rotation.y = t * 0.05;
    group.current.rotation.x = Math.sin(t * 0.1) * 0.1;
    
    // Atualizando cada "quantum bit" de tubulina individualmente
    group.current.children.forEach((child, i) => {
      if (i >= paths.length) return;
      
      const mesh = child as THREE.Mesh;
      const originalPos = paths[i];
      const state = tubulinStates[i];
      
      // Movimento quântico que segue a equação de Schrödinger (simplificada)
      // Aqui simulamos a evolução temporal dos estados quânticos de forma simplificada
      const quantum_phase = state.phase + t * state.frequency;
      const probability_amplitude = Math.cos(quantum_phase) * Math.sin(t * 0.1 + i);
      
      // Oscilação quântica - representando superposição quântica nas moléculas de tubulina
      // Em Orch OR, estas oscilações podem ser mantidas em estado coerente,
      // protegidas da decoerência por processos de isolamento quântico
      const quantum_offset = 0.1 * probability_amplitude;
      
      // Atualizando posição com oscilação quântica
      mesh.position.set(
        originalPos.x + quantum_offset * Math.sin(quantum_phase),
        originalPos.y + quantum_offset * Math.cos(quantum_phase),
        originalPos.z + quantum_offset * Math.sin(quantum_phase * 0.7)
      );
      
      // A cor representa o estado quântico da tubulina
      // Vermelho/laranja: mais "excitado"
      // Azul/verde: mais próximo do estado fundamental
      const intensity = 0.5 + 0.5 * probability_amplitude;
      const hue = 240 - 180 * intensity; // 240=azul, 60=amarelo/vermelho
      
      // Atualizando cor diretamente através do material
      if (mesh.material) {
        (mesh.material as THREE.MeshBasicMaterial).color.setHSL(hue/360, 0.8, 0.6 + 0.4 * intensity);
        
        // Opacidade mínima muito baixa em repouso (0.05), crescendo suavemente com coherence
        // Corrige: aplica opacidade e transparência apenas se material for MeshBasicMaterial ou array de MeshBasicMaterial
        // Durante evento OR (colapso), aumenta opacidade e pulsação, representando o "momento de consciência" da teoria Orch-OR
        if (Array.isArray(mesh.material)) {
          mesh.material.forEach((mat) => {
            if (mat instanceof THREE.MeshBasicMaterial) {
              mat.transparent = true;
              // Durante colapso (OR), opacidade máxima; em repouso, proporcional à coerência
              mat.opacity = collapseActive ? 1 : (0.05 + 0.9 * coherence);
            }
          });
        } else if (mesh.material instanceof THREE.MeshBasicMaterial) {
          mesh.material.transparent = true;
          mesh.material.opacity = collapseActive ? 1 : (0.05 + 0.9 * coherence);
        }
      }
      
      // Atualiza escala para representar "expansão quântica"
      // Escala modulada por intensidade local e coerência global
      // Durante evento OR (colapso), expansão máxima; em repouso, proporcional à intensidade e coerência
      // Represent a expansão temporal durante o colapso da função de onda (segundo Penrose)
      mesh.scale.setScalar(collapseActive ? 
        (0.8 + 0.5 * intensity) : // Expansão máxima durante OR
        (0.6 + 0.4 * intensity * (0.8 + 0.4 * coherence))); // Escala normal em coerência
    });
  }, [paths, tubulinStates, coherence, collapseActive]);

  // Animação da superposição quântica em tubulinas
  // Simulando a dinâmica quântica descrita na teoria Orch OR
  useOptimizedQuantumFrame(animateQuantumStates, 'medium');

  // Memoized trail components for better performance
  const trailComponents = useMemo(() => {
    return paths.map((path, i) => {
      // Define o padrão de cores inicial para cada estado de tubulina
      const state = tubulinStates[i];
      const initialProbability = state.probability;
      const hue = 240 - 180 * initialProbability; // 240=azul, 60=amarelo/vermelho
      
      return (
        <Trail
          key={i}
          width={1.5}
          length={3 + Math.floor(initialProbability * 6)}
          color={new THREE.Color().setHSL(hue/360, 0.8, 0.6)}
          attenuation={(t) => Math.pow(1-t, 1.5)} // Queda mais rápida das "caudas quânticas"
        >
          <mesh 
            position={path}
            ref={(el) => { 
              if (el) tubuleRefs.current[i] = el;
            }}
          >
            <sphereGeometry args={[0.05 + (initialProbability * 0.03), 8, 8]} />
            <meshBasicMaterial color={new THREE.Color().setHSL(hue/360, 0.8, 0.6)} />
          </mesh>
        </Trail>
      );
    });
  }, [paths, tubulinStates]);

  return (
    <group ref={group}>
      {trailComponents}
    </group>
  );
});

QuantumSuperposition.displayName = 'QuantumSuperposition';

export { QuantumSuperposition };
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import { useRef } from 'react';
import { useFrame } from '@react-three/fiber';
import * as THREE from 'three';
import { QuantumCore } from '../QuantumVisualizationContext';

/**
 * Funções auxiliares para a visualização Orch OR
 */

// Calcula a idade de um efeito quântico
export function getAge(createdAt: number): number {
  return Date.now() - createdAt;
}

// Mapeia uma região do cérebro para coordenadas 3D
export function getCorePosition(core: QuantumCore): [number, number, number] {
  // Mapeamento de regiões cerebrais para posições espaciais
  // seguindo a teoria Orch OR de Penrose-Hameroff sobre microtúbulos em regiões
  // cerebrais que sustentam a consciência
  switch(core) {
    case 'PREFRONTAL':
      return [0, 1.5, -1.2];
    case 'VISUAL':
      return [0, 0.2, -2];
    case 'TEMPORAL':
      return [-1.5, 0.5, -0.5];
    case 'PARIETAL':
      return [1.5, 0.5, -0.5];
    case 'THALAMUS':
      return [0, 0, 0];
    case 'HIPPOCAMPUS':
      return [0.8, -0.3, -0.6];
    default:
      // Posição aleatória nas proximidades do centro
      return [
        (Math.random() - 0.5) * 2, 
        (Math.random() - 0.5) * 2,
        (Math.random() - 0.5) * 2
      ];
  }
}

/**
 * Component that represents the abstract "Observer"
 * Na teoria de Penrose-Hameroff, o observador é um aspecto importante
 * da redução objetiva (OR) que leva à consciência
 */
interface ObserverProps {
  active?: boolean;
}

/**
 * Representa o substrato microtubular da consciência quântica na teoria Orch-OR (Penrose-Hameroff)
 * 
 * Na teoria Orch-OR, a consciência emerge das vibrações quânticas (redução objetiva orquestrada)
 * em microtúbulos dentro dos neurônios cerebrais. Microtúbulos são estruturas cilíndricas
 * compostas por 13 filamentos de proteínas tubulina, organizados em uma estrutura
 * hexagonal/pentagonal.
 * 
 * Este componente representa visualmente a estrutura dos microtúbulos como observador
 * quântico, com estado de vibração variável baseado no nível de consciência.
 */
export function Observer({ active = true }: ObserverProps) {
  const outerRef = useRef<THREE.Group>(null);
  const innerRef = useRef<THREE.Mesh>(null);
  
  // Animação baseada nas vibrações quânticas dos microtúbulos conforme descrito na
  // teoria Orch-OR - vibrações em múltiplas frequências: terahertz, gigahertz,
  // megahertz, kilohertz e hertz formando uma hierarquia multiescalar
  useFrame(({ clock }) => {
    if (outerRef.current && innerRef.current) {
      const t = clock.getElapsedTime();
      
      // Vibrações quânticas coerentes - mais intensas durante consciência ativa
      // Em estado ativo: vibrações de alta frequência (40Hz - gamma synchrony em Orch-OR)
      // Em estado basal: vibrações de baixa frequência (8-12Hz - alpha em Orch-OR)
      const teraHzFactor = active ? 0.15 : 0.03; // Vibrações terahertz (mais rápidas)
      const gigaHzFactor = active ? 0.12 : 0.02; // Vibrações gigahertz
      const hertzFactor = active ? 0.10 : 0.06;  // Vibrações hertz (mais lentas)
      
      // Frequências de vibração coerente - replicando as frequências neurais
      const gammaFreq = 40; // 40Hz = gamma (consciência ativa)
      const alphaFreq = 10; // 10Hz = alpha (estado relaxado/basal)
      
      // Frequência principal depende do estado de consciência
      const primaryFreq = active ? gammaFreq : alphaFreq;
      
      // Microvibrações em escalas diferentes simulando a hierarquia vibracional quântica
      // Esta abordagem simula as vibrações da tubulina nos microtúbulos
      innerRef.current.scale.x = 1 + teraHzFactor * Math.sin(t * 0.6 * primaryFreq);
      innerRef.current.scale.y = 1 + gigaHzFactor * Math.sin(t * 0.4 * primaryFreq + 0.2);
      innerRef.current.scale.z = 1 + hertzFactor * Math.sin(t * 0.2 * primaryFreq + 0.5);
      
      // Rotação - representa a propagação das ondas quânticas ao longo dos microtúbulos
      // Proteínas tubulina podem existir em dois estados conformacionais diferentes
      // que se alternam em um padrão de "onda" ao longo do microtúbulo
      const rotationSpeed = active ? 0.1 : 0.03;
      outerRef.current.rotation.y = t * rotationSpeed;
      innerRef.current.rotation.z = t * rotationSpeed * 0.7;
      
      // Não-localidade quântica (emaranhamento) - essencial na teoria Orch-OR
      // Movimentação que simula estados quânticos não-locais
      if (active) {
        // No estado ativo, mais movimento não-local (emaranhamento quântico estendido)
        outerRef.current.position.x = Math.sin(t * 0.3) * 0.04;
        outerRef.current.position.y = Math.cos(t * 0.2) * 0.04;
      } else {
        // Mesmo em repouso, existe um mínimo de movimento quântico não-local
        outerRef.current.position.x = Math.sin(t * 0.1) * 0.01;
        outerRef.current.position.y = Math.cos(t * 0.1) * 0.01;
      }
    }
  });

  // Representação visual baseada na estrutura real dos microtúbulos
  // conforme descrito na teoria Orch-OR
  return (
    <group ref={outerRef} position={[0, 0.2, 0]} rotation={[0, Math.PI / 4, 0]}>
      {/* Estrutura interna - representa os dímeros de tubulina */}
      <mesh ref={innerRef}>
        {/* Forma cilíndrica similar à estrutura microtubular real */}
        <cylinderGeometry args={[0.25, 0.25, 0.6, 13, 4]} /> {/* 13 segmentos representando os 13 protofilamentos */}
        <meshPhysicalMaterial 
          color={active ? "#FFFFFF" : "#88CCEE"} // Azul claro em repouso (estado quântico basal)
          emissive={active ? "#FFFFFF" : "#4488AA"}
          emissiveIntensity={active ? 0.8 : 0.3} // Mais luminoso quando ativo (colapso OR)
          metalness={0.2}
          roughness={0.3}
          transmission={active ? 0.75 : 0.9}
          transparent
          opacity={active ? 0.8 : 0.4}
        />
      </mesh>

      {/* Malha externa - representa o campo eletromagnético do microtúbulo */}
      <mesh scale={1.15}>
        <cylinderGeometry args={[0.25, 0.25, 0.6, 13, 1]} />
        <meshBasicMaterial
          color={active ? "#E0F4FF" : "#77AADD"}
          wireframe={true}
          transparent
          opacity={active ? 0.4 : 0.15}
        />
      </mesh>
      
      {/* Campo quântico externo - representa a não-localidade e estados de superposição */}
      <mesh scale={active ? 1.3 : 1.2}>
        <sphereGeometry args={[0.3, 12, 12]} />
        <meshBasicMaterial
          color={active ? "#FFFFFF" : "#88AADD"}
          transparent
          opacity={active ? 0.12 : 0.06}
          side={THREE.BackSide}
        />
      </mesh>
      
      {/* Partículas de energia quântica - visíveis apenas em estado ativo */}
      {active && (
        <group>
          {[...Array(8)].map((_, i) => {
            // Distribuir uniformemente em torno do cilindro
            const angle = (i / 8) * Math.PI * 2;
            const radius = 0.32;
            const x = Math.cos(angle) * radius;
            const y = (Math.random() - 0.5) * 0.6; // Altura variável ao longo do cilindro
            const z = Math.sin(angle) * radius;
            
            return (
              <mesh key={i} position={[x, y, z]} scale={0.05}>
                <sphereGeometry args={[1, 8, 8]} />
                <meshBasicMaterial color="#FFFFFF" transparent opacity={0.8} />
              </mesh>
            );
          })}
        </group>
      )}
    </group>
  );
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import { useFrame } from '@react-three/fiber';
import React, { useCallback, useMemo, useRef } from 'react';
import * as THREE from 'three';

/**
 * Wave collapse component - representa a Redução Objetiva (OR) de Penrose-Hameroff
 * 
 * Na teoria Orch OR, a gravitação causa o colapso da função de onda quântica
 * criando um momento de consciência quando o limiar E=ħ/t é atingido.
 * 
 * Segundo Penrose e Hameroff, estes colapsos gravitacionais envolvem:
 * 1. Superposição quântica em proteínas tubulina nos microtúbulos
 * 2. Crescimento da superposição até um limiar crítico de massa-energia
 * 3. Colapso gravitacional (OR) que resulta num momento de consciência
 * 4. A natureza "não-computável" deste processo é central para explicar aspectos
 *    da consciência que, segundo Penrose, não podem ser simulados por algoritmos
 */
interface WaveCollapseProps {
  position?: [number, number, number];
  active?: boolean;
  color?: string;
  isNonComputable?: boolean;
  collapseActive?: boolean;
}

const WaveCollapse = React.memo<WaveCollapseProps>(({ 
  position = [0, 0, 0], 
  active = false, 
  color = "#00FFFF",
  isNonComputable = false,
  collapseActive = false
}) => {
  // Referências para animação
  const outerRing = useRef<THREE.Mesh>(null);
  const middleRing = useRef<THREE.Mesh>(null);
  const innerRing = useRef<THREE.Mesh>(null);
  const quantum_particle = useRef<THREE.Group>(null);
  const spacetime_curvature = useRef<THREE.Group>(null);
  
  // Pontos para representar a curvatura espaço-temporal durante OR
  // A curvatura do espaço-tempo é central na teoria de Penrose para explicar a OR
  const spacetimePoints = useMemo(() => {
    const points = [];
    const count = 80;
    for (let i = 0; i < count; i++) {
      const theta = (i / count) * Math.PI * 2;
      const radius = 0.6 + Math.sin(theta * 3) * 0.1;
      points.push(new THREE.Vector3(
        radius * Math.cos(theta),
        radius * Math.sin(theta), 
        0
      ));
    }
    return points;
  }, []);
  
  // Partículas quânticas que se condensam durante o colapso
  // Representam os estados quânticos de tubulina convergindo durante OR
  const quantumParticles = useMemo(() => {
    const particles = [];
    const count = 12;
    for (let i = 0; i < count; i++) {
      const theta = (i / count) * Math.PI * 2;
      const radius = 0.4;
      particles.push({
        position: new THREE.Vector3(
          radius * Math.cos(theta),
          radius * Math.sin(theta),
          0
        ),
        originalRadius: radius,
        phase: Math.random() * Math.PI * 2,
        frequency: 3 + Math.random() * 5
      });
    }
    return particles;
  }, []);

  // Optimized animation callback for OR (Objective Reduction)
  const animateObjectiveReduction = useCallback((state: { clock: { getElapsedTime: () => number } }) => {
    if (!active) return;
    
    const t = state.clock.getElapsedTime();
    
    // Se o colapso ativo for forçado externamente, controlamos a fase diretamente
    // Isso permite sincronização entre colapsos e eventos de OR (Objective Reduction)
    let normalizedTime;
    let period = 3;
    
    if (collapseActive) {
      // Se colapso está ativo externamente, focamos na fase principal do colapso (0.3-0.7)
      normalizedTime = 0.5; // Meio do colapso - fase mais intensa
    } else {
      // Simulação normal do colapso quântico de Penrose
      // A equação E = ħ/t de Penrose determina quando ocorre a redução objetiva
      normalizedTime = (t % period) / period;
    }
    
    // Animação dos anéis - representam a frente de onda quântica
    if (outerRing.current && middleRing.current && innerRing.current) {
      // Fase 1: Expansão dos anéis - superposição inicial
      if (normalizedTime < 0.3) {
        const expansionFactor = Math.pow(normalizedTime / 0.3, 0.5);
        outerRing.current.scale.setScalar(0.2 + expansionFactor * 1.3);
        middleRing.current.scale.setScalar(0.2 + expansionFactor * 1.0);
        innerRing.current.scale.setScalar(0.2 + expansionFactor * 0.7);
        
        // Opacidade aumenta - representando o aumento da coerência quântica
        (outerRing.current.material as THREE.MeshBasicMaterial).opacity = 0.3 + expansionFactor * 0.5;
        (middleRing.current.material as THREE.MeshBasicMaterial).opacity = 0.4 + expansionFactor * 0.4;
        (innerRing.current.material as THREE.MeshBasicMaterial).opacity = 0.5 + expansionFactor * 0.3;
      } 
      // Fase 2: Colapso quântico - momento da redução objetiva
      // Representa o limiar de Penrose (quando E=ħ/t é atingido)
      else if (normalizedTime < 0.7) {
        const collapseFactor = Math.pow((normalizedTime - 0.3) / 0.4, 1.5);
        // Colapso dos anéis - representando OR
        outerRing.current.scale.setScalar(1.5 - collapseFactor * 1.3);
        middleRing.current.scale.setScalar(1.2 - collapseFactor * 1.0);
        innerRing.current.scale.setScalar(0.9 - collapseFactor * 0.5);
        
        // Pulsos de intensidade durante o colapso - representam a transição de energia
        // Na teoria Orch OR, este é o momento em que a energia gravitacional
        // causa o colapso da função de onda
        const pulseIntensity = 0.8 + Math.sin(collapseFactor * Math.PI * 6) * 0.2;
        (outerRing.current.material as THREE.MeshBasicMaterial).opacity = 0.8 * pulseIntensity;
        (middleRing.current.material as THREE.MeshBasicMaterial).opacity = 0.8 * pulseIntensity;
        (innerRing.current.material as THREE.MeshBasicMaterial).opacity = 0.8 * pulseIntensity;
      }
      // Fase 3: Dissipação - após OR
      // Representa o retorno ao estado clássico pós-colapso
      else {
        const dissipationFactor = (normalizedTime - 0.7) / 0.3;
        
        // Redução da escala e opacidade - representa o estado pós-colapso
        outerRing.current.scale.setScalar(0.2 * (1 - dissipationFactor));
        middleRing.current.scale.setScalar(0.2 * (1 - dissipationFactor));
        innerRing.current.scale.setScalar(0.4 * (1 - dissipationFactor));
        
        // Opacidade diminui
        (outerRing.current.material as THREE.MeshBasicMaterial).opacity = 0.8 * (1 - dissipationFactor);
        (middleRing.current.material as THREE.MeshBasicMaterial).opacity = 0.8 * (1 - dissipationFactor);
        (innerRing.current.material as THREE.MeshBasicMaterial).opacity = 0.8 * (1 - dissipationFactor);
      }
      
      // Rotação continua - representa a fase quântica
      outerRing.current.rotation.z = t * 1.5;
      middleRing.current.rotation.z = -t * 1.0;
      innerRing.current.rotation.z = t * 0.5;
    }
    
    // Animação das partículas quânticas
    if (quantum_particle.current) {
      quantum_particle.current.children.forEach((child, i) => {
        const mesh = child as THREE.Mesh;
        const particle = quantumParticles[i % quantumParticles.length];
        
        // Fase 1: Movimento quântico - partículas em superposição
        if (normalizedTime < 0.3) {
          const expansionFactor = normalizedTime / 0.3;
          const quantum_phase = particle.phase + t * particle.frequency;
          const quantum_radius = particle.originalRadius * (1 + expansionFactor * 0.5);
          
          // Movimento em superposição quântica - representa tubulinas em superposição
          mesh.position.set(
            quantum_radius * Math.cos(quantum_phase) * Math.sin(t + i),
            quantum_radius * Math.sin(quantum_phase) * Math.cos(t + i),
            0.1 * Math.sin(quantum_phase * 2)
          );
          
          // Partículas ficam mais brilhantes - aumento da energia quântica
          mesh.scale.setScalar(0.5 + 0.5 * expansionFactor);
        }
        // Fase 2: Colapso quântico - partículas convergem
        else if (normalizedTime < 0.7) {
          const collapseFactor = (normalizedTime - 0.3) / 0.4;
          const collapseRadius = particle.originalRadius * (1 - collapseFactor);
          
          // Movimento de convergência (colapso) - redução objetiva
          mesh.position.set(
            collapseRadius * Math.cos(particle.phase),
            collapseRadius * Math.sin(particle.phase),
            0.05 * Math.cos(t * 5 + i) * (1 - collapseFactor)
          );
          
          // Partículas brilham intensamente durante o colapso - energia liberada
          const pulseIntensity = 1.0 + Math.sin(collapseFactor * Math.PI * 8) * 0.5;
          mesh.scale.setScalar(1.0 * pulseIntensity);
        }
        // Fase 3: Pós-colapso - partículas se condensam
        else {
          const dissipationFactor = (normalizedTime - 0.7) / 0.3;
          
          // Partículas convergem para o centro - estado clássico pós-OR
          mesh.position.set(
            0.1 * Math.cos(particle.phase) * (1 - dissipationFactor),
            0.1 * Math.sin(particle.phase) * (1 - dissipationFactor),
            0
          );
          
          // Partículas diminuem - energia dissipada
          mesh.scale.setScalar(1.0 * (1 - dissipationFactor));
        }
      });
    }
    
    // Animação da curvatura espaço-temporal
    if (spacetime_curvature.current) {
      spacetime_curvature.current.children.forEach((child, i) => {
        const mesh = child as THREE.Mesh;
        const point = spacetimePoints[i % spacetimePoints.length];
        
        // Curvatura do espaço-tempo durante OR
        // Segundo Penrose, a gravitação é responsável pelo colapso
        const curvature_intensity = normalizedTime < 0.7 ? 
          Math.sin(normalizedTime * Math.PI * 2) * 0.3 : 
          0.1 * (1 - (normalizedTime - 0.7) / 0.3);
        
        mesh.position.set(
          point.x * (1 + curvature_intensity),
          point.y * (1 + curvature_intensity),
          curvature_intensity * Math.sin(t * 2 + i * 0.1)
        );
        
        // Escala representa a intensidade da curvatura
        mesh.scale.setScalar(0.3 + curvature_intensity * 0.7);
      });
    }
  }, [active, collapseActive, quantumParticles, spacetimePoints]);
  
  // Animação da Redução Objetiva (OR)
  // Simula o processo de colapso descrito por Penrose-Hameroff
  useFrame(animateObjectiveReduction);
  
  if (!active) return null;
  
  // Cor base depende se é um processo não-computável (teoria de Penrose)
  // Processos não-computáveis são fundamentais na teoria de Penrose para explicar
  // aspectos da consciência que transcendem computação algoritmica
  const baseColor = isNonComputable ? "#FF00FF" : color;
  const hue = new THREE.Color(baseColor).getHSL({h:0, s:0, l:0}).h;
  
  // Cores para diferentes elementos do colapso
  const outerColor = new THREE.Color().setHSL(hue, 0.8, 0.7);
  const middleColor = new THREE.Color().setHSL((hue + 0.05) % 1, 0.9, 0.6);
  const innerColor = new THREE.Color().setHSL((hue + 0.1) % 1, 1.0, 0.5);
  
  return (
    <group position={new THREE.Vector3(...position)}>
      {/* Anéis de colapso quântico - representam a frente de onda durante OR */}
      <mesh ref={outerRing}>
        <torusGeometry args={[0.4, 0.02, 16, 100]} />
        {/* Opacidade dos anéis modulada por evento OR */}
{/* Opacidade mínima muito baixa em repouso (0.05), crescendo suavemente com collapseActive */}
<meshBasicMaterial color={outerColor} transparent opacity={collapseActive ? 1 : 0.05} />
      </mesh>
      <mesh ref={middleRing}>
        <torusGeometry args={[0.3, 0.02, 16, 100]} />
        {/* Opacidade dos anéis modulada por evento OR */}
{/* Opacidade mínima muito baixa em repouso (0.05), crescendo suavemente com collapseActive */}
<meshBasicMaterial color={middleColor} transparent opacity={collapseActive ? 1 : 0.05} />
      </mesh>
      <mesh ref={innerRing}>
        <torusGeometry args={[0.2, 0.02, 16, 100]} />
        {/* Opacidade dos anéis modulada por evento OR */}
{/* Opacidade mínima muito baixa em repouso (0.05), crescendo suavemente com collapseActive */}
<meshBasicMaterial color={innerColor} transparent opacity={collapseActive ? 1 : 0.05} />
      </mesh>
      
      {/* Partículas quânticas que se condensam durante o colapso */}
      <group ref={quantum_particle}>
        {quantumParticles.map((particle, i) => (
          <mesh key={i} position={particle.position}>
            <sphereGeometry args={[0.03, 8, 8]} />
            <meshBasicMaterial color={new THREE.Color().setHSL(hue, 0.8, 0.8)} />
          </mesh>
        ))}
      </group>
      
      {/* Curvatura espaço-temporal - representa o mecanismo gravitacional de Penrose */}
      <group ref={spacetime_curvature}>
        <line>
          <bufferGeometry>
            <bufferAttribute 
              attach="attributes-position" 
              args={[new Float32Array(spacetimePoints.length * 3), 3]}
            />
          </bufferGeometry>
          <lineBasicMaterial color={outerColor} transparent opacity={0.5} />
        </line>
      </group>
    </group>
  );
});

export default WaveCollapse;// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { NON_PASSIVE_EVENT_OPTIONS, PASSIVE_EVENT_OPTIONS, PASSIVE_EVENT_TYPES } from './PerformanceConstants';

/**
 * Event Optimizer for Orch-OS Neural Processing
 * Single Responsibility: Event listener optimization and passive event handling
 */
export class EventOptimizer {
  private static originalAddEventListener = EventTarget.prototype.addEventListener;
  private static originalRemoveEventListener = EventTarget.prototype.removeEventListener;
  private static isOptimized = false;

  /**
   * Configures global passive event listeners optimization
   */
  static configurePassiveEvents(): () => void {
    if (EventOptimizer.isOptimized) {
      return () => {}; // Already optimized
    }

    EventOptimizer.overrideEventListeners();
    EventOptimizer.addGlobalStyles();
    EventOptimizer.isOptimized = true;

    return EventOptimizer.cleanup;
  }

  /**
   * Overrides addEventListener to force passive events where appropriate
   */
  private static overrideEventListeners(): void {
    EventTarget.prototype.addEventListener = function(
      type: string,
      listener: EventListenerOrEventListenerObject,
      options?: boolean | AddEventListenerOptions
    ) {
      // Convert boolean options to object format
      let optionsObj: AddEventListenerOptions = {};
      
      if (typeof options === 'boolean') {
        optionsObj = { capture: options };
      } else if (options) {
        optionsObj = { ...options };
      }

      // Force passive: true for problematic events if not explicitly set to false
      if (PASSIVE_EVENT_TYPES.has(type) && optionsObj.passive !== false) {
        optionsObj.passive = true;
      }

      return EventOptimizer.originalAddEventListener.call(this, type, listener, optionsObj);
    };
  }

  /**
   * Adds global CSS styles for touch optimization
   */
  private static addGlobalStyles(): void {
    if (typeof document === 'undefined') return;

    const style = document.createElement('style');
    style.id = 'quantum-performance-styles';
    style.textContent = `
      * {
        touch-action: manipulation;
      }
      
      .quantum-three-canvas {
        touch-action: none;
      }
    `;
    document.head.appendChild(style);
  }

  /**
   * Cleans up optimizations and restores original behavior
   */
  private static cleanup(): void {
    if (!EventOptimizer.isOptimized) return;

    EventTarget.prototype.addEventListener = EventOptimizer.originalAddEventListener;
    EventTarget.prototype.removeEventListener = EventOptimizer.originalRemoveEventListener;

    const style = document.getElementById('quantum-performance-styles');
    if (style) {
      document.head.removeChild(style);
    }

    EventOptimizer.isOptimized = false;
  }

  /**
   * Optimizes specific canvas elements for OrbitControls
   */
  static optimizeCanvasElements(): void {
    if (typeof document === 'undefined') return;

    const canvasElements = document.querySelectorAll('.quantum-three-canvas');
    
    canvasElements.forEach(canvas => {
      if (canvas instanceof HTMLElement) {
        // Set specific touch-action for better OrbitControls performance
        canvas.style.touchAction = 'none';
        
        // Add passive event listeners for common gestures
        const passiveOptions = { passive: true };
        
        ['wheel', 'touchstart', 'touchmove'].forEach(eventType => {
          canvas.addEventListener(eventType, () => {}, passiveOptions);
        });
      }
    });
  }

  /**
   * Gets the appropriate event options for a given event type
   */
  static getEventOptions(eventType: string, forcePassive = false): AddEventListenerOptions {
    if (forcePassive || PASSIVE_EVENT_TYPES.has(eventType)) {
      return PASSIVE_EVENT_OPTIONS;
    }
    
    return NON_PASSIVE_EVENT_OPTIONS;
  }

  /**
   * Fixes passive event listener issues with OrbitControls
   */
  static fixOrbitControlsEvents(): void {
    if (typeof document === 'undefined') return;

    // Wait for React Three Fiber to mount
    setTimeout(() => {
      const canvasElements = document.querySelectorAll('canvas');
      
      canvasElements.forEach(canvas => {
        if (canvas.parentElement?.classList.contains('quantum-three-canvas') || 
            canvas.closest('.quantum-three-canvas')) {
          
          // Override addEventListener for wheel events
          const originalAddEventListener = canvas.addEventListener.bind(canvas);
          
          canvas.addEventListener = function(type: string, listener: any, options?: any) {
            // Force non-passive for wheel events to allow preventDefault
            if (type === 'wheel' || type === 'mousewheel') {
              const nonPassiveOptions = typeof options === 'object' 
                ? { ...options, passive: false }
                : { passive: false };
              return originalAddEventListener(type, listener, nonPassiveOptions);
            }
            return originalAddEventListener(type, listener, options);
          };
          
          // Also set touch-action for better mobile support
          canvas.style.touchAction = 'none';
        }
      });
    }, 100); // Small delay to ensure canvas is mounted
  }
}

// Export convenience functions
export const configurePassiveEvents = EventOptimizer.configurePassiveEvents;
export const optimizeCanvasElements = EventOptimizer.optimizeCanvasElements;
export const getEventOptions = EventOptimizer.getEventOptions; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import type { IMemoryOptimizer } from '../interfaces/PerformanceInterfaces';
import { NEURAL_FRAME_CONSTRAINTS } from './PerformanceConstants';

/**
 * Memory Optimizer for Orch-OS Neural Processing
 * Single Responsibility: Memory management and optimization
 */
export class MemoryOptimizer implements IMemoryOptimizer {
  private lastOptimizationTime = 0;
  private readonly optimizationCooldown = 5000; // 5 seconds

  /**
   * Optimizes memory usage by triggering garbage collection and cache clearing
   */
  optimize(): void {
    const now = performance.now();
    
    // Prevent too frequent optimizations
    if (now - this.lastOptimizationTime < this.optimizationCooldown) {
      return;
    }

    this.lastOptimizationTime = now;

    // Force garbage collection hint (if available)
    this.triggerGarbageCollection();

    // Clear Three.js cache if available
    this.clearThreeJSCache();
  }

  /**
   * Checks if the system is under memory pressure
   */
  checkPressure(): boolean {
    const usage = this.getMemoryUsage();
    return usage !== null && usage > NEURAL_FRAME_CONSTRAINTS.MEMORY_PRESSURE_THRESHOLD;
  }

  /**
   * Gets current memory usage in bytes
   */
  getMemoryUsage(): number | null {
    if (typeof window !== 'undefined' && 'performance' in window && 'memory' in window.performance) {
      const memory = (window.performance as any).memory;
      return memory?.usedJSHeapSize || null;
    }
    return null;
  }

  /**
   * Triggers garbage collection if available
   */
  private triggerGarbageCollection(): void {
    if (typeof window !== 'undefined' && 'gc' in window) {
      try {
        (window as any).gc();
      } catch {
        // Silent fail - gc not available
      }
    }
  }

  /**
   * Clears Three.js cache if available
   */
  private clearThreeJSCache(): void {
    if (typeof window !== 'undefined') {
      try {
        // Try to access THREE from global scope
        const THREE = (window as any).THREE;
        if (THREE?.Cache) {
          THREE.Cache.clear();
        }
      } catch {
        // Silent fail - Three.js not available
      }
    }
  }
}

// Singleton instance for global use
export const memoryOptimizer = new MemoryOptimizer(); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import type { FrameConstraints } from '../interfaces/PerformanceInterfaces';

/**
 * Performance Constants for Orch-OS Neural Processing
 * Single Responsibility: Central configuration for performance parameters
 */

export const NEURAL_FRAME_CONSTRAINTS: FrameConstraints = {
  TARGET_FPS: 60,
  MIN_FRAME_TIME: 16.67, // 1000ms / 60fps
  MAX_FRAME_TIME: 33.33, // 1000ms / 30fps (fallback)
  PERFORMANCE_THRESHOLD: 0.8,
  FRAME_BUDGET_MS: 8, // Aggressive optimization
  HEAVY_TASK_BUDGET_MS: 3, // Very strict budget during heavy tasks
  THROTTLE_HEAVY_TASKS: true,
  PAUSE_DURING_HEAVY_TASKS: false, // Disable pausing to prevent infinite loops
  HEAVY_TASK_DETECTION_THRESHOLD: 50, // More reasonable threshold (50ms instead of 100ms)
  MEMORY_PRESSURE_THRESHOLD: 50 * 1024 * 1024 // 50MB memory threshold
} as const;

export const PASSIVE_EVENT_OPTIONS = {
  passive: true,
  capture: false
} as const;

export const NON_PASSIVE_EVENT_OPTIONS = {
  passive: false,
  capture: false
} as const;

export const PASSIVE_EVENT_TYPES = new Set([
  'wheel',
  'mousewheel',
  'touchstart',
  'touchmove',
  'touchend',
  'scroll'
]);

export const THROTTLE_DELAYS = {
  WHEEL_EVENT: 16, // ~60fps throttling
  MESSAGE_HANDLER: 100,
  DEBOUNCE_DEFAULT: 100,
  DEBOUNCE_MAX_WAIT: 500
} as const; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useCallback, useEffect, useRef } from 'react';
import { MemoryOptimizer } from '../core/MemoryOptimizer';
import { NEURAL_FRAME_CONSTRAINTS } from '../core/PerformanceConstants';
import type { IAnimationFrameCallback, IAnimationFrameOptimizer } from '../interfaces/PerformanceInterfaces';
import { PerformanceMonitor } from '../monitors/PerformanceMonitor';

/**
 * Optimized Animation Frame Hook for Orch-OS Neural Processing
 * Single Responsibility: Optimized requestAnimationFrame handling with performance monitoring
 * 
 * Performance Optimizations:
 * - Defer expensive operations using requestIdleCallback
 * - Implement frame budgeting to prevent heavy frames
 * - Use time-slicing for long-running tasks
 * - Reduce logging frequency to prevent performance overhead
 */
export function useOptimizedAnimationFrame(
  callback: IAnimationFrameCallback,
  enabled: boolean = true
): IAnimationFrameOptimizer {
  const requestRef = useRef<number | null>(null);
  const previousTimeRef = useRef<number | null>(null);
  const frameTimeRef = useRef<number>(0);
  const lastHeavyTaskRef = useRef<number>(0);
  const heavyTaskActiveRef = useRef<boolean>(false);
  const memoryPressureRef = useRef<boolean>(false);
  const performanceMonitorRef = useRef(new PerformanceMonitor());
  const memoryOptimizerRef = useRef(new MemoryOptimizer());
  const lastLogRef = useRef<number>(0);
  const frameSkipCountRef = useRef<number>(0);
  const frameBudgetRef = useRef<number>(NEURAL_FRAME_CONSTRAINTS.FRAME_BUDGET_MS);
  const isIdleRef = useRef<boolean>(true);

  // Memory pressure detection using requestIdleCallback to defer work
  const checkMemoryPressure = useCallback(() => {
    if (typeof window !== 'undefined' && 'requestIdleCallback' in window) {
      window.requestIdleCallback(() => {
        const hasMemoryPressure = memoryOptimizerRef.current.checkPressure();
        if (hasMemoryPressure) {
          memoryPressureRef.current = true;
        }
      }, { timeout: 1000 });
    } else {
      // Fallback for environments without requestIdleCallback
      setTimeout(() => {
        const hasMemoryPressure = memoryOptimizerRef.current.checkPressure();
        if (hasMemoryPressure) {
          memoryPressureRef.current = true;
        }
      }, 0);
    }
  }, []);

  const optimizedCallback = useCallback((timestamp: number) => {
    const frameStartTime = performance.now();
    
    if (previousTimeRef.current !== null) {
      const deltaTime = timestamp - previousTimeRef.current;
      frameTimeRef.current = deltaTime;
      
      // Time-sliced performance monitoring - only update every few frames
      if (timestamp % 3 === 0) {
        const metrics = performanceMonitorRef.current.update(timestamp);
        
        // Check for memory pressure much less frequently (every 180 frames ≈ 3 seconds at 60fps)
        if (metrics.totalFrames % 180 === 0) {
          checkMemoryPressure();
        }
        
        // Adaptive frame budget based on recent performance
        const targetFrameTime = NEURAL_FRAME_CONSTRAINTS.MIN_FRAME_TIME;
        const avgFrameTime = metrics.averageFrameTime;
        
        // More conservative budget adjustments
        if (avgFrameTime > targetFrameTime * 2) {
          frameBudgetRef.current = Math.max(4, frameBudgetRef.current * 0.9); // Slower reduction
        } else if (avgFrameTime < targetFrameTime * 0.6) {
          frameBudgetRef.current = Math.min(NEURAL_FRAME_CONSTRAINTS.FRAME_BUDGET_MS, frameBudgetRef.current * 1.05); // Slower increase
        }
      }
      
      // More relaxed heavy task detection - focus on truly problematic frames
      const timeSinceLastHeavyTask = timestamp - lastHeavyTaskRef.current;
      
      // Only consider frames > 50ms as truly "heavy" (was 25ms)
      if (deltaTime > 50) {
        lastHeavyTaskRef.current = timestamp;
        heavyTaskActiveRef.current = true;
        
        // Much more throttled logging - only log once every 10 seconds
        const timeSinceLastLog = timestamp - lastLogRef.current;
        if (process.env.NODE_ENV !== 'production' && timeSinceLastLog > 10000) {
          // Use requestIdleCallback for logging to not block the main thread
          if (typeof window !== 'undefined' && 'requestIdleCallback' in window) {
            window.requestIdleCallback(() => {
              console.warn(
                `[QuantumPerformance] Heavy frame detected (${deltaTime.toFixed(2)}ms)`
              );
            });
          }
          lastLogRef.current = timestamp;
        }
      }
      
      // Reset heavy task flag after longer period for stability
      if (timeSinceLastHeavyTask > 2000) {
        heavyTaskActiveRef.current = false;
        memoryPressureRef.current = false;
        frameSkipCountRef.current = 0;
        isIdleRef.current = true;
      }
      
      // More conservative frame skipping strategy
      const shouldSkipFrame = (
        heavyTaskActiveRef.current && 
        NEURAL_FRAME_CONSTRAINTS.THROTTLE_HEAVY_TASKS &&
        frameSkipCountRef.current < 2 && // Reduced max consecutive skips
        deltaTime > 80 // Only skip on really bad frames
      );
      
      if (shouldSkipFrame) {
        frameSkipCountRef.current++;
        // Skip callback execution but continue animation loop
      } else {
        frameSkipCountRef.current = 0;
        
        try {
          // Execute callback with time budgeting
          const callbackStartTime = performance.now();
          
          // Use time-slicing: if we're running behind, reduce callback complexity
          const isRunningBehind = deltaTime > 32; // 2 frames behind at 60fps
          if (isRunningBehind) {
            // Potentially pass a flag to callback to reduce complexity
            callback(deltaTime, timestamp, { simplified: true });
          } else {
            callback(deltaTime, timestamp);
          }
          
          const callbackDuration = performance.now() - callbackStartTime;
          
          // More relaxed budget checking - only warn on significant violations
          if (callbackDuration > frameBudgetRef.current * 1.5) {
            heavyTaskActiveRef.current = true;
            lastHeavyTaskRef.current = timestamp;
            isIdleRef.current = false;
            
            // Throttled budget violation logging
            const timeSinceLastLog = timestamp - lastLogRef.current;
            if (process.env.NODE_ENV !== 'production' && timeSinceLastLog > 5000) {
              if (typeof window !== 'undefined' && 'requestIdleCallback' in window) {
                window.requestIdleCallback(() => {
                  console.warn(
                    `[QuantumPerformance] Frame budget exceeded: ${callbackDuration.toFixed(2)}ms (budget: ${frameBudgetRef.current.toFixed(2)}ms)`
                  );
                });
              }
              lastLogRef.current = timestamp;
            }
          } else {
            isIdleRef.current = true;
          }
        } catch (error) {
          // Defer error logging to not impact frame performance
          if (typeof window !== 'undefined' && 'requestIdleCallback' in window) {
            window.requestIdleCallback(() => {
              console.warn('[QuantumPerformance] Frame callback error:', error);
            });
          }
        }
      }
    }
    
    previousTimeRef.current = timestamp;
    
    // Always schedule next frame if enabled
    if (enabled) {
      requestRef.current = requestAnimationFrame(optimizedCallback);
    }
  }, [callback, enabled, checkMemoryPressure]);

  useEffect(() => {
    if (enabled) {
      requestRef.current = requestAnimationFrame(optimizedCallback);
    }
    
    return () => {
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current);
        requestRef.current = null;
      }
    };
  }, [enabled, optimizedCallback]);

  // Return performance metrics for monitoring
  return {
    currentFrameTime: frameTimeRef.current,
    performanceMetrics: performanceMonitorRef.current.getMetrics(),
    isHeavyTaskActive: heavyTaskActiveRef.current,
    hasMemoryPressure: memoryPressureRef.current
  };
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useEffect } from 'react';

/**
 * 🧠 Symbolic OrbitControls Fix Hook for Orch-OS
 * 
 * Neural Fix Cortex - resolves passive event listener conflicts with @react-three/drei
 * OrbitControls by ensuring wheel events can call preventDefault() properly.
 */
export function useOrbitControlsFix(): void {
  useEffect(() => {
    if (typeof document === 'undefined') return;

    // Global fix for OrbitControls passive event listener issues
    const fixPassiveEvents = () => {
      // Override addEventListener globally to fix wheel events
      const originalAddEventListener = EventTarget.prototype.addEventListener;
      
      EventTarget.prototype.addEventListener = function(
        type: string,
        listener: EventListenerOrEventListenerObject,
        options?: boolean | AddEventListenerOptions
      ) {
        // For wheel events, force non-passive to allow preventDefault
        if (type === 'wheel' || type === 'mousewheel') {
          let opts: AddEventListenerOptions;
          
          if (typeof options === 'boolean') {
            opts = { capture: options, passive: false };
          } else if (options) {
            opts = { ...options, passive: false };
          } else {
            opts = { passive: false };
          }
          
          return originalAddEventListener.call(this, type, listener, opts);
        }
        
        return originalAddEventListener.call(this, type, listener, options);
      };
    };

    // Apply the fix
    fixPassiveEvents();

    // Additional canvas-specific fixes
    const applyCanvasFixes = () => {
      const canvases = document.querySelectorAll('canvas');
      canvases.forEach(canvas => {
        // Ensure touch-action is set for better mobile support
        canvas.style.touchAction = 'none';
        
        // Add a custom wheel event listener that won't be passive
        const wheelHandler = (e: WheelEvent) => {
          // Allow default OrbitControls behavior
          // This is just to register a non-passive listener
        };
        
        canvas.addEventListener('wheel', wheelHandler, { passive: false });
      });
    };

    // Apply canvas fixes immediately and after a delay for dynamic content
    applyCanvasFixes();
    const timeoutId = setTimeout(applyCanvasFixes, 1000);

    return () => {
      clearTimeout(timeoutId);
    };
  }, []);
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useFrame } from '@react-three/fiber';
import { useCallback, useRef } from 'react';

/**
 * Simple throttled frame hook for React Three Fiber
 * Single Responsibility: Throttle animation frames to prevent performance issues
 */

type FrameCallback = (state: any, delta: number) => void;

interface ThrottleOptions {
  /** Maximum frames per second for this component (default: 30) */
  maxFPS?: number;
  /** Frame budget in milliseconds (default: 8ms) */
  frameBudget?: number;
  /** Whether to skip frames during heavy load (default: true) */
  adaptiveSkipping?: boolean;
  /** Priority level: 'high' | 'medium' | 'low' (default: 'medium') */
  priority?: 'high' | 'medium' | 'low';
  /** Enable debug logging every N frames */
  debugInterval?: number;
}

/**
 * Simple throttled useFrame replacement
 */
export function useThrottledFrame(
  callback: FrameCallback,
  options: ThrottleOptions = {}
): void {
  const lastRunRef = useRef<number>(0);
  const frameCountRef = useRef<number>(0);
  const performanceRef = useRef<{ sum: number; count: number }>({ sum: 0, count: 0 });

  const maxFPS = options.maxFPS ?? 30;
  const frameBudget = options.frameBudget ?? 8;
  const adaptiveSkipping = options.adaptiveSkipping ?? true;
  const priority = options.priority ?? 'medium';
  const debugInterval = options.debugInterval;

  const targetInterval = 1000 / maxFPS;

  const throttledCallback = useCallback((state: any, delta: number) => {
    const now = performance.now();
    const timeSinceLastRun = now - lastRunRef.current;

    // Always increment total frame count for debugging
    frameCountRef.current++;

    // Check if enough time has passed
    if (timeSinceLastRun < targetInterval) {
      return;
    }

    // Simple adaptive skipping based on recent performance
    if (adaptiveSkipping) {
      const avgFrameTime = performanceRef.current.count > 0 
        ? performanceRef.current.sum / performanceRef.current.count 
        : 0;

      // Skip low priority tasks if average frame time is high
      if (priority === 'low' && avgFrameTime > 20) {
        return;
      }

      // Skip medium priority tasks if performance is really bad
      if (priority === 'medium' && avgFrameTime > 33) {
        return;
      }
    }

    try {
      const startTime = performance.now();
      callback(state, delta);
      const duration = performance.now() - startTime;
      
      // Track performance
      performanceRef.current.sum += duration;
      performanceRef.current.count++;
      
      // Reset performance tracking periodically
      if (performanceRef.current.count >= 60) {
        performanceRef.current.sum *= 0.1;
        performanceRef.current.count = 1;
      }

      // Debug logging if enabled
      if (debugInterval && frameCountRef.current % debugInterval === 0 && process.env.NODE_ENV !== 'production') {
        const avgFrameTime = performanceRef.current.count > 0 
          ? (performanceRef.current.sum / performanceRef.current.count).toFixed(2)
          : '0.00';
        console.log(
          `[ThrottledFrame] Frame ${frameCountRef.current}: avg ${avgFrameTime}ms, current ${duration.toFixed(2)}ms, priority: ${priority}`
        );
      }

      // Warn if budget exceeded
      if (duration > frameBudget && process.env.NODE_ENV !== 'production') {
        console.warn(
          `[ThrottledFrame] Frame ${frameCountRef.current} exceeded budget: ${duration.toFixed(2)}ms (budget: ${frameBudget}ms)`
        );
      }

    } catch (error) {
      console.warn(`[ThrottledFrame] Frame ${frameCountRef.current} callback error:`, error);
    }

    lastRunRef.current = now;
  }, [callback, targetInterval, frameBudget, adaptiveSkipping, priority, debugInterval]);

  useFrame(throttledCallback);
}

/**
 * Performance-optimized useFrame replacement for quantum visualization
 * Use this instead of direct useFrame for quantum visualization components
 */
export function useOptimizedQuantumFrame(
  callback: FrameCallback,
  priority: 'high' | 'medium' | 'low' = 'medium'
): void {
  useThrottledFrame(callback, {
    maxFPS: priority === 'high' ? 60 : priority === 'medium' ? 30 : 15,
    frameBudget: priority === 'high' ? 12 : priority === 'medium' ? 8 : 4,
    adaptiveSkipping: true,
    priority,
    // Enable debug logging every 300 frames in development for quantum components
    debugInterval: process.env.NODE_ENV !== 'production' ? 300 : undefined
  });
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Performance Interfaces for Orch-OS Neural Processing
 * Implements Interface Segregation Principle - specific interfaces for specific needs
 */

// === Core Performance Types ===
export interface PerformanceMetrics {
  readonly fps: number;
  readonly averageFrameTime: number;
  readonly slowFrames: number;
  readonly totalFrames: number;
  readonly consecutiveSlowFrames: number;
  readonly heavyTaskPeriods: number;
  readonly isPerformanceGood: boolean;
  readonly slowFrameRatio: number;
}

export interface FrameConstraints {
  readonly TARGET_FPS: number;
  readonly MIN_FRAME_TIME: number;
  readonly MAX_FRAME_TIME: number;
  readonly PERFORMANCE_THRESHOLD: number;
  readonly FRAME_BUDGET_MS: number;
  readonly HEAVY_TASK_BUDGET_MS: number;
  readonly THROTTLE_HEAVY_TASKS: boolean;
  readonly PAUSE_DURING_HEAVY_TASKS: boolean;
  readonly HEAVY_TASK_DETECTION_THRESHOLD: number;
  readonly MEMORY_PRESSURE_THRESHOLD: number;
}

// === Animation Frame Interfaces ===
export interface IAnimationFrameCallback {
  (deltaTime: number, timestamp: number): void;
}

export interface IAnimationFrameOptimizer {
  readonly currentFrameTime: number;
  readonly performanceMetrics: PerformanceMetrics;
  readonly isHeavyTaskActive: boolean;
  readonly hasMemoryPressure: boolean;
}

// === Event Optimization Interfaces ===
export interface IEventOptions {
  readonly passive?: boolean;
  readonly capture?: boolean;
  readonly once?: boolean;
}

export interface IPassiveEventHandler<T extends Event = Event> {
  (event: T): void;
}

// === Memory Management Interfaces ===
export interface IMemoryOptimizer {
  optimize(): void;
  checkPressure(): boolean;
  getMemoryUsage(): number | null;
}

// === Task Management Interfaces ===
export interface IHeavyTask {
  readonly id: string;
  readonly fn: () => Promise<void>;
  readonly priority: number;
}

export interface IHeavyTaskManager {
  addTask(fn: () => Promise<void>, id: string, priority?: number): Promise<void>;
  isProcessing(): boolean;
  getQueueSize(): number;
}

// === Performance Monitoring Interfaces ===
export interface IPerformanceMonitor {
  update(timestamp: number): PerformanceMetrics;
  reset(): void;
  getMetrics(): PerformanceMetrics;
}

// === Message Handler Interfaces ===
export interface IMessageHandler<T = any> {
  (data: T): void;
}

export interface IOptimizedMessageHandler<T = any> {
  (data: T): void;
}

// === Configuration Interfaces ===
export interface IPerformanceConfig {
  readonly frameConstraints: FrameConstraints;
  readonly enableMemoryOptimization: boolean;
  readonly enableEventOptimization: boolean;
  readonly enableTaskManagement: boolean;
  readonly debugMode: boolean;
}

export interface IOptimizationLevel {
  readonly quality: number; // 0.25 to 1.0
  readonly adaptiveQuality: boolean;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import type { IHeavyTask, IHeavyTaskManager } from '../interfaces/PerformanceInterfaces';

/**
 * Heavy Task Manager for Orch-OS Neural Processing
 * Single Responsibility: Management of heavy computational tasks
 */
export class HeavyTaskManager implements IHeavyTaskManager {
  private tasks: IHeavyTask[] = [];
  private processing = false;

  /**
   * Adds a heavy task to the processing queue
   */
  async addTask(fn: () => Promise<void>, id: string, priority: number = 0): Promise<void> {
    const task: IHeavyTask = { fn, id, priority };
    
    this.tasks.push(task);
    this.tasks.sort((a, b) => b.priority - a.priority);
    
    if (!this.processing) {
      await this.processTasks();
    }
  }

  /**
   * Checks if the manager is currently processing tasks
   */
  isProcessing(): boolean {
    return this.processing;
  }

  /**
   * Gets the current queue size
   */
  getQueueSize(): number {
    return this.tasks.length;
  }

  /**
   * Processes all tasks in the queue
   */
  private async processTasks(): Promise<void> {
    this.processing = true;
    
    while (this.tasks.length > 0) {
      const task = this.tasks.shift();
      if (!task) break;

      try {
        await task.fn();
      } catch (error) {
        console.warn(`[HeavyTaskManager] Task ${task.id} failed:`, error);
      }
      
      // Yield control between tasks
      await this.yieldControl();
    }
    
    this.processing = false;
  }

  /**
   * Yields control to allow other operations to proceed
   */
  private async yieldControl(): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, 1));
  }

  /**
   * Clears all pending tasks
   */
  clearTasks(): void {
    this.tasks = [];
  }
}

// Singleton instance for global use
export const heavyTaskManager = new HeavyTaskManager(); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { NEURAL_FRAME_CONSTRAINTS } from '../core/PerformanceConstants';
import type { IPerformanceMonitor, PerformanceMetrics } from '../interfaces/PerformanceInterfaces';

/**
 * Performance Monitor for Orch-OS Neural Processing
 * Single Responsibility: Performance metrics collection and analysis
 */
export class PerformanceMonitor implements IPerformanceMonitor {
  private frameCount = 0;
  private lastTime = 0;
  private fps = 0;
  private averageFrameTime = NEURAL_FRAME_CONSTRAINTS.MIN_FRAME_TIME;
  private slowFrameCount = 0;
  private consecutiveSlowFrames = 0;
  private heavyTaskPeriods = 0;
  private readonly maxSlowFrames: number;

  constructor(maxSlowFrames = 30) {
    this.maxSlowFrames = maxSlowFrames;
  }

  /**
   * Updates performance metrics with new frame data
   */
  update(timestamp: number): PerformanceMetrics {
    this.frameCount++;
    
    if (this.lastTime === 0) {
      this.lastTime = timestamp;
      return this.getMetrics();
    }

    const deltaTime = timestamp - this.lastTime;
    this.lastTime = timestamp;

    // Update average frame time with exponential smoothing
    this.averageFrameTime = this.averageFrameTime * 0.9 + deltaTime * 0.1;

    // Count slow frames
    if (deltaTime > NEURAL_FRAME_CONSTRAINTS.MAX_FRAME_TIME) {
      this.slowFrameCount++;
      this.consecutiveSlowFrames++;
    } else {
      this.consecutiveSlowFrames = 0;
    }

    // Detect heavy task periods
    if (deltaTime > NEURAL_FRAME_CONSTRAINTS.HEAVY_TASK_DETECTION_THRESHOLD) {
      this.heavyTaskPeriods++;
    }

    // Calculate FPS every 60 frames
    if (this.frameCount % 60 === 0) {
      this.fps = Math.round(1000 / this.averageFrameTime);
      
      // Reset slow frame count periodically
      if (this.frameCount % 300 === 0) {
        this.slowFrameCount = Math.max(0, this.slowFrameCount - 10);
      }
    }

    return this.getMetrics();
  }

  /**
   * Resets all performance metrics
   */
  reset(): void {
    this.frameCount = 0;
    this.lastTime = 0;
    this.fps = 0;
    this.averageFrameTime = NEURAL_FRAME_CONSTRAINTS.MIN_FRAME_TIME;
    this.slowFrameCount = 0;
    this.consecutiveSlowFrames = 0;
    this.heavyTaskPeriods = 0;
  }

  /**
   * Gets current performance metrics
   */
  getMetrics(): PerformanceMetrics {
    const slowFrameRatio = this.frameCount > 0 ? this.slowFrameCount / this.frameCount : 0;
    const isPerformanceGood = 
      this.fps >= NEURAL_FRAME_CONSTRAINTS.TARGET_FPS * NEURAL_FRAME_CONSTRAINTS.PERFORMANCE_THRESHOLD &&
      slowFrameRatio < 0.1;

    return {
      fps: this.fps,
      averageFrameTime: this.averageFrameTime,
      slowFrames: this.slowFrameCount,
      totalFrames: this.frameCount,
      consecutiveSlowFrames: this.consecutiveSlowFrames,
      heavyTaskPeriods: this.heavyTaskPeriods,
      isPerformanceGood,
      slowFrameRatio
    };
  }
}

// Factory function for creating monitor instances
export const createPerformanceMonitor = (maxSlowFrames?: number): PerformanceMonitor => {
  return new PerformanceMonitor(maxSlowFrames);
}; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Orch-OS Performance Optimization Suite
 * SOLID Architecture Implementation for Neural Processing Performance
 * 
 * This module follows SOLID principles:
 * - Single Responsibility: Each class/module has one clear purpose
 * - Open/Closed: Extensible through interfaces without modification
 * - Liskov Substitution: Interface implementations are interchangeable
 * - Interface Segregation: Specific interfaces for specific needs
 * - Dependency Inversion: Depend on abstractions, not concretions
 */

// === Core Interfaces (Interface Segregation Principle) ===
export type {
  FrameConstraints,
  IAnimationFrameCallback,
  IAnimationFrameOptimizer,
  IEventOptions, IHeavyTask,
  IHeavyTaskManager, IMemoryOptimizer, IMessageHandler, IOptimizationLevel, IOptimizedMessageHandler, IPassiveEventHandler, IPerformanceConfig, IPerformanceMonitor, PerformanceMetrics
} from './interfaces/PerformanceInterfaces';

// === Core Constants ===
export {
  NEURAL_FRAME_CONSTRAINTS, NON_PASSIVE_EVENT_OPTIONS, PASSIVE_EVENT_OPTIONS, PASSIVE_EVENT_TYPES,
  THROTTLE_DELAYS
} from './core/PerformanceConstants';

// === Core Optimizers (Single Responsibility Principle) ===
export { configurePassiveEvents, EventOptimizer, getEventOptions, optimizeCanvasElements } from './core/EventOptimizer';
export { MemoryOptimizer, memoryOptimizer } from './core/MemoryOptimizer';

// === Managers (Single Responsibility Principle) ===
export { HeavyTaskManager, heavyTaskManager } from './managers/HeavyTaskManager';

// === Monitors (Single Responsibility Principle) ===
export { createPerformanceMonitor, PerformanceMonitor } from './monitors/PerformanceMonitor';

// === Hooks (Open/Closed Principle - extensible) ===
export { useOptimizedAnimationFrame } from './hooks/useOptimizedAnimationFrame';
export { useOptimizedQuantumFrame, useThrottledFrame } from './hooks/useThrottledFrame';

// === Utilities (Single Responsibility Principle) ===
export {
  createDebouncedFunction, createOptimizedMessageHandler,
  createThrottledFunction
} from './utils/MessageHandlerOptimizer';

// === Local Imports for Legacy Functions ===
import { configurePassiveEvents as _configurePassiveEvents, optimizeCanvasElements as _optimizeCanvasElements } from './core/EventOptimizer';
import { memoryOptimizer as _memoryOptimizer } from './core/MemoryOptimizer';
import type { IHeavyTaskManager, IMemoryOptimizer, IPerformanceMonitor } from './interfaces/PerformanceInterfaces';
import { heavyTaskManager as _heavyTaskManager } from './managers/HeavyTaskManager';
import { createPerformanceMonitor as _createPerformanceMonitor } from './monitors/PerformanceMonitor';

// === Backwards Compatibility Exports ===
// These maintain compatibility with the old API while using the new SOLID architecture

/**
 * Legacy hook wrapper that maintains the old API
 */
export function useGlobalPassiveEventOptimization() {
  // Use the new EventOptimizer
  const cleanup = _configurePassiveEvents();
  _optimizeCanvasElements();
  
  return cleanup;
}

/**
 * Legacy hook wrapper for OrbitControls optimization
 */
export function useOptimizedOrbitControls() {
  _optimizeCanvasElements();
}

/**
 * Legacy function wrapper for Three.js memory optimization
 */
export function optimizeThreeJSMemory() {
  _memoryOptimizer.optimize();
}

/**
 * Legacy hook wrapper for heavy task management
 */
export function useHeavyTaskManager() {
  return {
    queueHeavyTask: (task: () => Promise<void>) => 
      _heavyTaskManager.addTask(task, `task-${Date.now()}`)
  };
}

/**
 * Convenience function to initialize all performance optimizations
 * Following the Dependency Inversion Principle - depends on abstractions
 */
export function initializeQuantumPerformanceOptimizations(): () => void {
  const cleanupFunctions: Array<() => void> = [];

  // Initialize event optimizations
  cleanupFunctions.push(_configurePassiveEvents());
  
  // Optimize canvas elements
  _optimizeCanvasElements();
  
  // Initialize memory optimization
  _memoryOptimizer.optimize();

  // Return cleanup function
  return () => {
    cleanupFunctions.forEach(cleanup => cleanup());
  };
}

/**
 * Factory function for creating a complete performance optimization suite
 * Follows Dependency Inversion - you can inject your own implementations
 */
export function createPerformanceOptimizationSuite(config?: {
  memoryOptimizer?: IMemoryOptimizer;
  heavyTaskManager?: IHeavyTaskManager;
  performanceMonitor?: IPerformanceMonitor;
}) {
  return {
    memoryOptimizer: config?.memoryOptimizer || _memoryOptimizer,
    heavyTaskManager: config?.heavyTaskManager || _heavyTaskManager,
    performanceMonitor: config?.performanceMonitor || _createPerformanceMonitor(),
    initialize: initializeQuantumPerformanceOptimizations
  };
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { CognitionEvent } from '../../../context/deepgram/types/CognitionEvent';
import { QuantumFrequencyBand } from '../QuantumVisualizationContext';

// Type guards para os diferentes tipos de eventos cognitivos
function isNeuralCollapseEvent(event: CognitionEvent): event is Extract<CognitionEvent, { type: 'neural_collapse' }> {
  return event.type === 'neural_collapse';
}

function isNeuralSignalEvent(event: CognitionEvent): event is Extract<CognitionEvent, { type: 'neural_signal' }> {
  return event.type === 'neural_signal';
}

function isSymbolicRetrievalEvent(event: CognitionEvent): event is Extract<CognitionEvent, { type: 'symbolic_retrieval' }> {
  return event.type === 'symbolic_retrieval';
}

function isFusionInitiatedEvent(event: CognitionEvent): event is Extract<CognitionEvent, { type: 'fusion_initiated' }> {
  return event.type === 'fusion_initiated';
}

function isSymbolicContextSynthesizedEvent(event: CognitionEvent): event is Extract<CognitionEvent, { type: 'symbolic_context_synthesized' }> {
  return event.type === 'symbolic_context_synthesized';
}

function isEmergentPatternsEvent(event: CognitionEvent): event is Extract<CognitionEvent, { type: 'emergent_patterns' }> {
  return event.type === 'emergent_patterns';
}

/**
 * Constantes científicas da teoria Orch-OR (Penrose-Hameroff)
 * Valores refinados baseados em publicações científicas recentes
 */
export const ORCH_OR_CONSTANTS = {
  // Duração dos eventos quânticos (milissegundos)
  // Dilatados para visualização didática
  SUPERPOSITION_DURATION_MS: 600,   // Dilatação didática dos ~25ms reais
  COHERENCE_BUILDUP_MS: 250,        // Dilatação didática dos ~75ms reais
  CONSCIOUS_MOMENT_MS: 1500,        // Dilatação didática dos ~100ms reais
  
  // Níveis de coerência quântica (valores normalizados)
  MIN_COHERENCE: 0.05,              // Coerência mínima em repouso
  BASELINE_COHERENCE: 0.3,          // Coerência basal normal
  ENTANGLEMENT_THRESHOLD: 0.6,      // Limiar para emaranhamento quântico significativo
  COLLAPSE_THRESHOLD: 0.85,         // Limiar para colapso iminente
};

/**
 * Determina a banda de frequência quântica correspondente a um tipo de evento cognitivo
 * TERAHERTZ: Atividade quântica isolada (alta energia)
 * GIGAHERTZ: Atividade quântica em grupos de tubulinas
 * MEGAHERTZ: Coerência em grupos de microtúbulos
 * KILOHERTZ: Integração microtubular em escala neuronal
 * HERTZ: Oscilações macroscópicas em redes neurais (EEG)
 */
export const getFrequencyBandForEvent = (event: CognitionEvent): QuantumFrequencyBand => {
  const type = event.type;
  
  // Eventos de alto nível cognitivo - baixa frequência
  if (type === 'neural_collapse' || type === 'symbolic_context_synthesized') {
    return QuantumFrequencyBand.HERTZ; // EEG theta-gamma
  }
  
  // Eventos de processamento intermediário - frequências médias
  if (type === 'fusion_initiated' || type === 'symbolic_retrieval') {
    return QuantumFrequencyBand.KILOHERTZ; // Atividade neural local
  }
  
  // Eventos de nível básico/sensorial - maiores frequências
  if (type === 'neural_signal' || type === 'raw_prompt') {
    return QuantumFrequencyBand.MEGAHERTZ; // Atividade de microtúbulos
  }
  
  // Eventos extremamente rápidos/primitivos - frequências ultrarápidas
  if (type === 'emergent_patterns') {
    return QuantumFrequencyBand.GIGAHERTZ; // Vibrações quânticas
  }
  
  // Processos macroscópicos - EEG
  if (type === 'gpt_response') {
    return QuantumFrequencyBand.HERTZ;
  }
  
  // Nível fundamental para outras interações
  return QuantumFrequencyBand.TERAHERTZ; // Nível base
};

/**
 * Calcula a amplitude do efeito quântico baseado no tipo e propriedades do evento
 * Baseado na teoria Orch-OR sobre intensidade de processos quânticos
 */
export const getAmplitudeForEvent = (event: CognitionEvent): number => {
  // Para eventos com informação explícita de intensidade
  if (isNeuralSignalEvent(event)) {
    return Math.max(0.2, Math.min(1, event.intensity));
  }
  
  // Para eventos de colapso que usam carga emocional
  if (isNeuralCollapseEvent(event)) {
    return Math.max(0.5, Math.min(1, event.emotionalWeight));
  }
  
  // Symbolic_retrieval - baseado na quantidade de insights
  if (isSymbolicRetrievalEvent(event)) {
    return Math.max(0.3, Math.min(1, (event.insights.length / 10)));
  }
  
  // Variação padrão dependendo do tipo de evento
  if (isNeuralCollapseEvent(event)) return 0.9;  // Colapsos são eventos de alta amplitude
  if (isFusionInitiatedEvent(event)) return 0.7; // Fusão tem amplitude moderada-alta
  if (isSymbolicContextSynthesizedEvent(event)) return 0.8; // Síntese tem alta amplitude
  
  return 0.5 + (Math.random() * 0.2); // Outros eventos - amplitude média com variação
};

/**
 * Determina se um evento cognitivo é não-computável 
 * Baseado na teoria de Penrose sobre a não-computabilidade da consciência
 */
export const isNonComputableEvent = (event: CognitionEvent): boolean => {
  // Neural_collapse possui informação sobre determinismo
  if (isNeuralCollapseEvent(event)) {
    return 'isDeterministic' in event ? !event.isDeterministic : true; // Não-determinístico = não-computável
  }
  
  // Probabilidades de não-computabilidade baseadas na teoria
  if (isEmergentPatternsEvent(event)) {
    return Math.random() < 0.7; // Padrões emergentes frequentemente não-computáveis
  }
  
  if (isSymbolicContextSynthesizedEvent(event)) {
    return Math.random() < 0.4; // Síntese parcialmente não-computável
  }
  
  return false; // Maioria dos eventos são computáveis
};

/**
 * Faz mapeamento refinado de eventos cognitivos para regiões cerebrais
 * Baseado na teoria Orch-OR e nos modelos de consciência quântica
 */
export const getCoreForEvent = (event: CognitionEvent): string => {
  const type = event.type;
  
  if (type === 'raw_prompt' || type === 'temporary_context') {
    return 'SENSORY_CORTEX';      // Entrada sensorial inicial
  }
  if (type === 'neural_signal') {
    return 'THALAMUS';            // Processamento subcortical
  }
  if (type === 'symbolic_retrieval') {
    return 'HIPPOCAMPUS';         // Memória e recuperação
  }
  if (type === 'fusion_initiated') {
    return 'PREFRONTAL_CORTEX';   // Integração e fusão de informação
  }
  if (type === 'neural_collapse') {
    return 'GLOBAL_WORKSPACE';    // Espaço de trabalho global - consciência
  }
  if (type === 'symbolic_context_synthesized') {
    return 'CORTICAL_COLUMNS';    // Minicoluna cortical - unidade de processamento
  }
  if (type === 'gpt_response') {
    return 'LANGUAGE_CENTERS';    // Áreas de linguagem
  }
  if (type === 'emergent_patterns') {
    return 'ASSOCIATION_AREAS';   // Áreas associativas multimodais
  }
  return 'THALAMUS';            // Default: tálamo como integrador central
};

/**
 * Mapeia eventos cognitivos para propriedades quânticas segundo a teoria Orch-OR
 * Esta é a função principal para tradução de estados cognitivos para fenômenos quânticos
 */
export const mapCognitionEventToQuantumProperties = (event: CognitionEvent) => {
  return {
    core: getCoreForEvent(event),
    frequencyBand: getFrequencyBandForEvent(event),
    amplitude: getAmplitudeForEvent(event),
    nonComputable: isNonComputableEvent(event),
    // Uma função auxiliar para determinar o nível do tripleto de acordo com a teoria
    tripletLevel: getTripletLevel(event)
  };
};

/**
 * Determina o nível do tripleto de acordo com a teoria Orch-OR
 * Na teoria, informações são organizadas em "triplets of triplets"
 */
export const getTripletLevel = (event: CognitionEvent): 'primary' | 'secondary' | 'tertiary' => {
  if (isNeuralCollapseEvent(event)) return 'tertiary';
  if (isSymbolicContextSynthesizedEvent(event)) return 'secondary';
  if (isFusionInitiatedEvent(event)) return 'secondary';
  if (isSymbolicRetrievalEvent(event) && event.insights.length > 5) return 'secondary';
  return 'primary';
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Performance Optimizations for Orch-OS Neural Processing
 * 
 * This file now serves as a compatibility layer that re-exports 
 * the new SOLID architecture implementation.
 * 
 * The new architecture is located in the ./performance/ directory
 * and follows SOLID principles for better maintainability and extensibility.
 */

// Re-export everything from the new SOLID architecture
export * from './performance';

// Import required functions for legacy support
import {
    configurePassiveEvents,
    createPerformanceMonitor,
    createThrottledFunction
} from './performance';

// Maintain backwards compatibility by re-exporting specific functions
// that were previously defined in this file
export {
    createOptimizedMessageHandler,
    NEURAL_FRAME_CONSTRAINTS as NEURAL_FRAME_CONSTRAINTS_LEGACY, NON_PASSIVE_EVENT_OPTIONS as nonPassiveEventOptions, optimizeThreeJSMemory, PASSIVE_EVENT_OPTIONS as passiveEventOptions, useGlobalPassiveEventOptimization, useHeavyTaskManager, useOptimizedAnimationFrame, useOptimizedOrbitControls, useOptimizedQuantumFrame, useThrottledFrame
} from './performance';

// Additional legacy exports with different names for backwards compatibility
import { PerformanceMonitor } from './performance';

export class QuantumPerformanceMonitor extends PerformanceMonitor {
  // Legacy alias - extends the new PerformanceMonitor
}

// Legacy function aliases
export const configurePassiveOrbitControls = () => {
  return configurePassiveEvents();
};

export const usePassiveEventListener = <K extends keyof HTMLElementEventMap>(
  element: HTMLElement | null,
  eventType: K,
  handler: (event: HTMLElementEventMap[K]) => void,
  options?: {
    passive?: boolean;
    capture?: boolean;
    once?: boolean;
  }
) => {
  // Simple implementation for backwards compatibility
  if (!element || typeof window === 'undefined') return;

  const eventOptions = {
    passive: options?.passive ?? true,
    capture: options?.capture ?? false,
    once: options?.once ?? false
  };

  element.addEventListener(eventType, handler as EventListener, eventOptions);

  return () => {
    element.removeEventListener(eventType, handler as EventListener, eventOptions);
  };
};

export const useOptimizedWheelHandler = (
  element: HTMLElement | null,
  onWheel: (deltaY: number, event: WheelEvent) => void,
  enabled: boolean = true
) => {
  if (!element || !enabled || typeof window === 'undefined') return;

  const throttleRef = { current: 0 };
  const THROTTLE_MS = 16; // ~60fps throttling

  const handler = (event: WheelEvent) => {
    const now = performance.now();
    if (now - throttleRef.current >= THROTTLE_MS) {
      throttleRef.current = now;
      
      const deltaY = Math.sign(event.deltaY) * Math.min(Math.abs(event.deltaY), 100);
      
      try {
        onWheel(deltaY, event);
      } catch (error) {
        console.warn('[QuantumPerformance] Wheel handler error:', error);
      }
    }
  };

  element.addEventListener('wheel', handler, { passive: true });

  return () => {
    element.removeEventListener('wheel', handler);
  };
};

// Legacy debounce function
export const usePerformanceDebounce = <T extends (...args: any[]) => any>(
  callback: T,
  delay: number = 100
): T => {
  return createThrottledFunction(callback, delay);
};

export const useQuantumPerformanceAutoOptimization = () => {
  const monitor = createPerformanceMonitor();
  
  return {
    optimizationLevel: 1.0,
    performanceMetrics: monitor
  };
}; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

export { default as QuantumModel } from './QuantumModel';
export { default as QuantumVisualizationContainer, QuantumVisualizationContainer as default } from './QuantumVisualizationContainer';
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useState } from 'react';
import { useQuantumVisualization, OrchORState } from './QuantumVisualizationContext';
import './QuantumVisualization.css';

// Define the quantum phenomena with their respective colors and descriptions
export const QUANTUM_PHENOMENA = {
  SUPERPOSITION: {
    id: 'quantum-superposition',
    label: 'Quantum Superposition',
    color: '#9D6AFF',
    description: 'Multiple quantum states existing simultaneously in microtubules before collapse.'
  },
  REDUCTION: {
    id: 'objective-reduction',
    label: 'Objective Reduction',
    color: '#00B4D8',
    description: 'Orchestrated collapse of quantum states through gravity-induced reduction.'
  },
  ENTANGLEMENT: {
    id: 'quantum-entanglement',
    label: 'Quantum Entanglement',
    color: '#E63B7A',
    description: 'Non-local quantum connections between microtubules across neural networks.'
  },
  CONSCIOUS: {
    id: 'conscious-state',
    label: 'Conscious State',
    color: '#3BE669',
    description: 'Emergent consciousness arising from orchestrated quantum collapses.'
  },
  COHERENCE: {
    id: 'tubulin-coherence',
    label: 'Tubulin Coherence',
    color: '#FFD166',
    description: 'Quantum resonance maintained in microtubule structures.'
  },
  ORCHESTRATION: {
    id: 'orchestration',
    label: 'Orchestration',
    color: '#00CBD1',
    description: 'Synchronization of quantum processes across neural architectures.'
  },
  DECOHERENCE: {
    id: 'quantum-decoherence',
    label: 'Quantum Decoherence',
    color: '#FF5500', // Laranja mais vivo para melhor visibilidade
    description: 'Process by which quantum states lose coherence due to environmental interaction, a key challenge in Orch-OR.'
  },
  ISOLATION: {
    id: 'quantum-isolation',
    label: 'Quantum Isolation',
    color: '#00CCFF', // Azul mais brilhante para melhor visibilidade
    description: 'Protective mechanisms that shield microtubules from decoherence, enabling quantum effects at body temperature.'
  },
  OBSERVER: {
    id: 'observer',
    label: 'Observer',
    color: '#FFFFFF',
    description: 'Self-referential quantum measurement process in consciousness.'
  }
};

interface LegendItemProps {
  id: string;
  label: string;
  color: string;
  active?: boolean;
  selected?: boolean;
  description: string;
  onClick: (id: string, multiSelect: boolean) => void;
}

const LegendItem: React.FC<LegendItemProps> = ({ 
  id, 
  label, 
  color, 
  active = false, 
  selected = false,
  description,
  onClick 
}) => {
  const [showTooltip, setShowTooltip] = useState(false);
  
  // Handler para detectar clique com Command (Mac) ou Ctrl (Windows/Linux) pressionado
  const handleClick = (e: React.MouseEvent) => {
    // Se Command ou Ctrl está pressionado, permitir seleção múltipla
    onClick(id, e.metaKey || e.ctrlKey); // metaKey = Command no Mac, ctrlKey = Ctrl em outras plataformas
  };
  
  return (
    <div 
      className={`quantum-legend-item ${active ? 'active' : ''} ${selected ? 'selected' : ''}`} 
      onClick={handleClick}
      onMouseEnter={() => setShowTooltip(true)}
      onMouseLeave={() => setShowTooltip(false)}
      title={`${label}${selected ? ' (selected)' : ''} - ⌘/Ctrl+Click for multiple selection`}
    >
      <div 
        className={`quantum-legend-indicator quantum-color-${id}`} 
        data-color={color}
      />
      <span className="quantum-legend-label">{label}</span>
      {showTooltip && (
        <div className={`quantum-legend-tooltip tooltip-color-${id}`}>
          {description}
          {selected ? " (selected)" : ""}
          <div className="tooltip-hint">
            ⌘/Ctrl+Click for multiple selection
          </div>
        </div>
      )}
    </div>
  );
};

/**
 * Component that displays legends for quantum phenomena in the visualization
 * Shows which phenomena are active at a specific moment and allows filtering
 */
export const QuantumLegend: React.FC = () => {
  const {
    quantumSuperpositions,
    objectiveReductions,
    quantumEntanglements,
    consciousStates,
    observerState,
    activeVisualFilters,
    setActiveVisualFilter
  } = useQuantumVisualization();

  // Determine if each Orch OR quantum phenomenon is active based on events
  const isSuperpositionActive = quantumSuperpositions.length > 0;
  const isObjectiveReductionActive = objectiveReductions.length > 0;
  const isQuantumEntanglementActive = quantumEntanglements.length > 0;
  const isConsciousStateActive = consciousStates.length > 0;
  
  // Assegurar que tubulina e orquestração estejam ativas no estado de repouso
  // Mesmo sem eventos cognitivos, há um nível básico de coerência quântica
  // de acordo com a teoria Orch-OR
  const isCoherenceActive = true; // Sempre ativo para refletir o estado quântico de base
  const isOrchestrationActive = true; // Sempre ativo para refletir orquestração em nível base
  const isObserverActive = observerState === 'active' || consciousStates.length > 0;
  
  // Decoerência e isolamento são fenômenos físicos sempre presentes
  // em diferentes graus, de acordo com a teoria Orch-OR
  const isDecoherenceActive = true; // A decoerência é sempre uma ameaça presente
  const isIsolationActive = true;   // Os mecanismos de isolamento estão sempre ativos
  
  // Handle legend item click to toggle filtering, com suporte para seleção múltipla
  const handleLegendClick = (id: string, multiSelect: boolean = false) => {
    // Passa o id e o estado do Command/Ctrl para o contexto
    setActiveVisualFilter(id, multiSelect);
  };

  // Add the title at the top of the visualization
  React.useEffect(() => {
    // Check if title already exists
    const existingTitle = document.querySelector('.quantum-matrix-title');
    if (!existingTitle) {
      const container = document.querySelector('.quantum-visualization-container');
      if (container) {
        const titleElement = document.createElement('h1');
        titleElement.className = 'quantum-matrix-title';
        titleElement.textContent = 'QUANTUM CONSCIOUSNESS MATRIX';
        container.insertBefore(titleElement, container.firstChild);
      }
    }
  }, []);

  return (
    <div className="quantum-legend">
      <LegendItem 
        id={QUANTUM_PHENOMENA.SUPERPOSITION.id}
        label={QUANTUM_PHENOMENA.SUPERPOSITION.label}
        color={QUANTUM_PHENOMENA.SUPERPOSITION.color}
        description={QUANTUM_PHENOMENA.SUPERPOSITION.description}
        active={isSuperpositionActive}
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.SUPERPOSITION.id)}
        onClick={handleLegendClick}
      />
      <LegendItem 
        id={QUANTUM_PHENOMENA.REDUCTION.id}
        label={QUANTUM_PHENOMENA.REDUCTION.label}
        color={QUANTUM_PHENOMENA.REDUCTION.color}
        description={QUANTUM_PHENOMENA.REDUCTION.description}
        active={isObjectiveReductionActive}
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.REDUCTION.id)}
        onClick={handleLegendClick}
      />
      <LegendItem 
        id={QUANTUM_PHENOMENA.ENTANGLEMENT.id}
        label={QUANTUM_PHENOMENA.ENTANGLEMENT.label}
        color={QUANTUM_PHENOMENA.ENTANGLEMENT.color}
        description={QUANTUM_PHENOMENA.ENTANGLEMENT.description}
        active={isQuantumEntanglementActive}
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.ENTANGLEMENT.id)}
        onClick={handleLegendClick}
      />
      <LegendItem 
        id={QUANTUM_PHENOMENA.CONSCIOUS.id}
        label={QUANTUM_PHENOMENA.CONSCIOUS.label}
        color={QUANTUM_PHENOMENA.CONSCIOUS.color}
        description={QUANTUM_PHENOMENA.CONSCIOUS.description}
        active={isConsciousStateActive} 
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.CONSCIOUS.id)}
        onClick={handleLegendClick}
      />
      <LegendItem 
        id={QUANTUM_PHENOMENA.COHERENCE.id}
        label={QUANTUM_PHENOMENA.COHERENCE.label}
        color={QUANTUM_PHENOMENA.COHERENCE.color}
        description={QUANTUM_PHENOMENA.COHERENCE.description}
        active={isCoherenceActive}
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.COHERENCE.id)}
        onClick={handleLegendClick}
      />
      <LegendItem 
        id={QUANTUM_PHENOMENA.ORCHESTRATION.id}
        label={QUANTUM_PHENOMENA.ORCHESTRATION.label}
        color={QUANTUM_PHENOMENA.ORCHESTRATION.color}
        description={QUANTUM_PHENOMENA.ORCHESTRATION.description}
        active={isOrchestrationActive}
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.ORCHESTRATION.id)}
        onClick={handleLegendClick}
      />
      <LegendItem 
        id={QUANTUM_PHENOMENA.DECOHERENCE.id}
        label={QUANTUM_PHENOMENA.DECOHERENCE.label}
        color={QUANTUM_PHENOMENA.DECOHERENCE.color}
        description={QUANTUM_PHENOMENA.DECOHERENCE.description}
        active={isDecoherenceActive}
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.DECOHERENCE.id)}
        onClick={handleLegendClick}
      />
      <LegendItem 
        id={QUANTUM_PHENOMENA.ISOLATION.id}
        label={QUANTUM_PHENOMENA.ISOLATION.label}
        color={QUANTUM_PHENOMENA.ISOLATION.color}
        description={QUANTUM_PHENOMENA.ISOLATION.description}
        active={isIsolationActive}
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.ISOLATION.id)}
        onClick={handleLegendClick}
      />
      <LegendItem 
        id={QUANTUM_PHENOMENA.OBSERVER.id}
        label={QUANTUM_PHENOMENA.OBSERVER.label}
        color={QUANTUM_PHENOMENA.OBSERVER.color}
        description={QUANTUM_PHENOMENA.OBSERVER.description}
        active={isObserverActive} 
        selected={activeVisualFilters.includes(QUANTUM_PHENOMENA.OBSERVER.id)}
        onClick={handleLegendClick}
      />
    </div>
  );
};

export default QuantumLegend;// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { OrbitControls } from '@react-three/drei';
import { Canvas } from '@react-three/fiber';
import React, { Suspense, useEffect, useMemo, useState } from 'react';
import { QuantumField } from './components/QuantumField';
import './QuantumVisualizationCSS.css';
import { useOrbitControlsFix } from './utils/performance/hooks/useOrbitControlsFix';

// === Performance Configuration ===
const PERFORMANCE_CONFIG = {
  frameRate: 60,
  pixelRatio: typeof window !== 'undefined' ? Math.min(window.devicePixelRatio, 2) : 1,
  shadowMapSize: 1024,
  antialias: false, // Disable for performance
  powerPreference: 'high-performance' as const,
  stencil: false,
  depth: true,
  alpha: false // Disable transparency for better performance
} as const;

// === Constants ===
// Classes CSS dos elementos estáticos
const QUANTUM_ELEMENT_CLASSES = [
  'quantum-element-1',
  'quantum-element-2',
  'quantum-element-3',
  'quantum-element-4',
  'quantum-element-5',
  'quantum-element-6'
] as const;

// === CSS Fallback Component ===
const CssQuantumFallback: React.FC = React.memo(() => (
  <div className="quantum-visualization-css">
    {/* Fundo */}
    <div className="quantum-background" />

    {/* Elementos quânticos estáticos */}
    {QUANTUM_ELEMENT_CLASSES.map((cls, idx) => (
      <div key={`${cls}-${idx}`} className={cls} />
    ))}

    {/* Partículas adicionais */}
    <div className="quantum-particles-container">
      {Array.from({ length: 10 }, (_, idx) => {
        // Posição e atraso determinísticos (5 posições x 5 delays)
        const posCls = `quantum-particle-pos-${(idx % 5) + 1}`;
        const delayCls = `quantum-particle-delay-${(idx % 5) + 1}`;
        return <div key={`particle-${idx}`} className={`quantum-particle ${posCls} ${delayCls}`} />;
      })}
    </div>
  </div>
));

CssQuantumFallback.displayName = 'CssQuantumFallback';

// === Optimized OrbitControls Configuration ===
const ORBIT_CONTROLS_CONFIG = {
  enablePan: true,
  enableZoom: true,
  enableRotate: true,
  minDistance: 8,
  maxDistance: 30,
  enableDamping: true,
  dampingFactor: 0.05,
  rotateSpeed: 0.5,
  zoomSpeed: 0.8,
  panSpeed: 0.8,
  // Performance optimizations
  maxPolarAngle: Math.PI,
  minPolarAngle: 0,
  autoRotate: false,
  autoRotateSpeed: 2.0,
  // Fix passive event listener issues
  mouseButtons: {
    LEFT: 0,  // Rotate
    MIDDLE: 1, // Dolly
    RIGHT: 2   // Pan
  },
  touches: {
    ONE: 0,   // Rotate
    TWO: 2    // Dolly & Pan
  }
} as const;

// === Three.js Scene Components ===
const QuantumThreeScene: React.FC = React.memo(() => (
  <>
    <ambientLight intensity={1} />
    <OrbitControls {...ORBIT_CONTROLS_CONFIG} />
    <QuantumField />
  </>
));

QuantumThreeScene.displayName = 'QuantumThreeScene';

/**
 * Performance-optimized error boundary for WebGL context loss
 * Implements symbolic graceful degradation for neural processing stability
 */
class WebGLErrorBoundary extends React.Component<
  { children: React.ReactNode; fallback: React.ComponentType },
  { hasError: boolean }
> {
  constructor(props: { children: React.ReactNode; fallback: React.ComponentType }) {
    super(props);
    this.state = { hasError: false };
  }

  static getDerivedStateFromError(): { hasError: boolean } {
    return { hasError: true };
  }

  componentDidCatch(error: Error, errorInfo: React.ErrorInfo) {
    console.warn('[QuantumModel] WebGL context error, falling back to CSS:', error, errorInfo);
  }

  render() {
    if (this.state.hasError) {
      const FallbackComponent = this.props.fallback;
      return <FallbackComponent />;
    }

    return this.props.children;
  }
}

// === Main Component ===
export const QuantumModel: React.FC = React.memo(() => {
  const [webglAvailable, setWebglAvailable] = useState<boolean>(true);

  // Memoized WebGL detection for performance
  const webglSupport = useMemo(() => {
    try {
      const canvas = document.createElement('canvas');
      const gl =
        canvas.getContext('webgl2', { powerPreference: PERFORMANCE_CONFIG.powerPreference }) ||
        canvas.getContext('webgl', { powerPreference: PERFORMANCE_CONFIG.powerPreference }) ||
        canvas.getContext('experimental-webgl', { powerPreference: PERFORMANCE_CONFIG.powerPreference });
      
      if (gl && 'clearColor' in gl && 'clear' in gl && 'COLOR_BUFFER_BIT' in gl) {
        // Test basic rendering capability
        (gl as WebGLRenderingContext).clearColor(0, 0, 0, 1);
        (gl as WebGLRenderingContext).clear((gl as WebGLRenderingContext).COLOR_BUFFER_BIT);
        return true;
      }
      return false;
    } catch {
      return false;
    }
  }, []);

  useEffect(() => {
    setWebglAvailable(webglSupport);
  }, [webglSupport]);

  // Fix passive event listener issues with OrbitControls
  useOrbitControlsFix();

  // Performance-optimized Canvas configuration
  const canvasConfig = useMemo(() => ({
    className: "quantum-three-canvas",
    shadows: true,
    camera: { position: [0, 0, 15] as [number, number, number], fov: 40 },
    dpr: PERFORMANCE_CONFIG.pixelRatio,
    performance: { min: 0.8 }, // Maintain 80% frame rate minimum
    gl: {
      powerPreference: PERFORMANCE_CONFIG.powerPreference,
      antialias: PERFORMANCE_CONFIG.antialias,
      stencil: PERFORMANCE_CONFIG.stencil,
      depth: PERFORMANCE_CONFIG.depth,
      alpha: PERFORMANCE_CONFIG.alpha,
      preserveDrawingBuffer: false,
      premultipliedAlpha: false,
      failIfMajorPerformanceCaveat: false
    },
    onCreated: ({ gl, size }: { gl: any; size: { width: number; height: number } }) => {
      try {
        // Optimize WebGL context for performance
        if (gl.shadowMap) {
          gl.shadowMap.enabled = true;
          gl.shadowMap.type = 2; // PCFShadowMap for performance
        }
        
        // Use renderer setSize if available
        if (gl.setSize && typeof gl.setSize === 'function') {
          gl.setSize(size.width, size.height);
        }
        
        if (gl.setClearColor && typeof gl.setClearColor === 'function') {
          gl.setClearColor(0x000000);
        }
        
        // Enable performance optimizations
        if (gl.setPixelRatio && typeof gl.setPixelRatio === 'function') {
          gl.setPixelRatio(PERFORMANCE_CONFIG.pixelRatio);
        }
        
        if (gl.outputEncoding !== undefined) {
          gl.outputEncoding = 3001; // sRGBEncoding
        }
        
        // Fix passive event listener issues with OrbitControls
        if (gl.domElement) {
          // Set touch-action to prevent default browser behavior
          gl.domElement.style.touchAction = 'none';
          
          // Override wheel event handler to be non-passive
          const originalAddEventListener = gl.domElement.addEventListener;
          gl.domElement.addEventListener = function(type: string, listener: any, options?: any) {
            // For wheel events used by OrbitControls, ensure they're non-passive
            if (type === 'wheel' || type === 'mousewheel') {
              const opts = typeof options === 'object' 
                ? { ...options, passive: false }
                : { passive: false };
              return originalAddEventListener.call(this, type, listener, opts);
            }
            return originalAddEventListener.call(this, type, listener, options);
          };
        }
        
      } catch (error) {
        console.warn('[QuantumModel] WebGL configuration warning:', error);
      }
    }
  }), []);

  if (webglAvailable) {
    return (
      <WebGLErrorBoundary fallback={CssQuantumFallback}>
        <Canvas {...canvasConfig}>
          <Suspense fallback={null}>
            <QuantumThreeScene />
          </Suspense>
        </Canvas>
      </WebGLErrorBoundary>
    );
  }

  console.log('[QuantumModel] WebGL não disponível, usando fallback CSS com otimizações neurais');
  return <CssQuantumFallback />;
});

QuantumModel.displayName = 'QuantumModel';

export default QuantumModel;/* SPDX-License-Identifier: MIT OR Apache-2.0 */
/* Copyright (c) 2025 Guilherme Ferrari Brescia */

.quantum-visualization-container {
  position: relative;
  border-radius: 0;
  overflow: visible;
  width: 100%;
  height: 100%;
  background: transparent !important;
  z-index: 2;
}

.quantum-visualization-homogeneous {
  background: transparent !important;
  border-radius: 0 !important;
}

.quantum-visualization-container.fixed-height {
  height: 280px;
}

.quantum-visualization-container.fixed-width {
  width: 100%;
}

/* Classes for custom sizes */
.quantum-visualization-container[data-width],
.quantum-visualization-container[data-height] {
  width: var(--custom-width, 100%);
  height: var(--custom-height, 280px);
}

.quantum-model-container {
  width: 100%;
  height: 100%;
  min-height: 280px;
}

/* Quantum Matrix Title */
/* Título principal da Matrix - Estilo sci-fi otimizado para performance */
.quantum-matrix-title {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  text-align: center;
  font-family: 'Share Tech Mono', 'Orbitron', monospace;
  font-weight: 400;
  font-size: 14px;
  letter-spacing: 2px; /* Reduzido para melhor performance de renderização */
  color: #00f5ff; /* Cor sólida em vez de rgba para melhor renderização */
  z-index: 30;
  pointer-events: none;
  text-transform: uppercase;
  margin: 0;
  
  /* Container otimizado */
  padding: 12px 25px 12px 35px;
  background-color: rgba(8, 20, 30, 0.35); /* background-color em vez de background */
  -webkit-backdrop-filter: blur(5px);
  backdrop-filter: blur(5px);
  width: 100%;
  
  /* Efeitos simplificados - apenas uma sombra */
  border: none;
  box-shadow: 0 0 10px rgba(0, 245, 255, 0.2);
  
  /* Apenas um gradiente para o scanner - otimização */
  background-image: linear-gradient(90deg, 
    transparent 0%, 
    rgba(0, 245, 255, 0.05) 20%, 
    rgba(0, 245, 255, 0.1) 50%, 
    rgba(0, 245, 255, 0.05) 80%, 
    transparent 100%);
  background-size: 200% 100%;
  
  /* Sombra de texto única - reduz custo de renderização */
  text-shadow: 0 0 8px rgba(0, 245, 255, 0.8);
  
  /* Animação única - usa transform para aceleração por hardware */
  animation: quantumHoloScan 8s infinite linear;
  
  /* Will-change - informa o browser para otimizar a animação */
  will-change: background-position, color;
    
  /* Decorador holográfico */
  display: flex;
  align-items: center;
  justify-content: center;
}

/* Círculo decorativo esquerdo otimizado */
.quantum-matrix-title::before {
  content: '';
  margin-right: 12px;
  width: 12px;
  height: 12px;
  display: inline-block;
  border-radius: 50%;
  background-color: #00f5ff; /* Cor sólida */
  box-shadow: 0 0 6px #00f5ff; /* Sombra única e simplificada */
  animation: quantumBeatPulse 3s infinite alternate ease-in-out;
  will-change: transform, opacity; /* Prepara o browser para estas mudanças */
}

/* Removido o pseudo-elemento ::after para economizar recursos de renderização */

/* Animação otimizada de scanner para o título - usando apenas background-position */
@keyframes quantumHoloScan {
  0% {
    background-position: -100% 0;
  }
  100% {
    background-position: 200% 0;
  }
}

/* Animação otimizada para o indicador cíclico - usando transform para GPU acceleration */
@keyframes quantumBeatPulse {
  0% {
    transform: scale(0.85);
    opacity: 0.7;
  }
  100% {
    transform: scale(1.15);
    opacity: 1;
  }
}

/* Removidas as animações não utilizadas: quantumHoloPulse e quantumCodeScan */

/* Styles for quantum legend - Reposicionada seguindo boas práticas UI/UX */
.quantum-legend {
  position: absolute;
  bottom: 20px; /* Reposicionada para o canto inferior direito para melhor acessibilidade */
  right: 20px;
  display: flex;
  flex-direction: column;
  gap: 4px;
  background-color: rgba(0, 0, 0, 0.85); /* Mais opaco para melhor legibilidade */
  border-radius: 12px;
  padding: 12px;
  -webkit-backdrop-filter: blur(10px);
  backdrop-filter: blur(10px);
  z-index: 5;
  max-width: 220px; /* Ligeiramente mais largo para melhor leitura */
  font-size: 12px; /* Tamanho ligeiramente maior para melhor legibilidade */
  box-shadow: 0 4px 16px rgba(0, 0, 0, 0.2); /* Sombra mais definida */
  border: 1px solid rgba(255, 255, 255, 0.15);
  transition: opacity 0.3s ease, transform 0.3s ease;
  opacity: 0.9; /* Levemente mais visível por padrão */
}

.quantum-legend:hover {
  opacity: 1; /* Fully opaque on hover */
  transform: translateY(-2px); /* Subtle lift on hover */
}

.quantum-legend-item {
  display: flex;
  align-items: center;
  color: rgba(255, 255, 255, 0.8);
  transition: all 0.3s ease;
  opacity: 0.7;
  padding: 6px 8px; /* Espaçamento para melhor clickabilidade */
  border-radius: 6px; /* Leve arredondamento para cada item */
  position: relative;
  cursor: pointer;
}

.quantum-legend-item:hover {
  background-color: rgba(255, 255, 255, 0.1);
  transform: translateX(2px);
}

.quantum-legend-item.active {
  color: rgba(255, 255, 255, 1);
  opacity: 1;
}

/* Item da legenda com estado selecionado - estilo melhorado para seleção múltipla */
.quantum-legend-item.selected {
  background-color: rgba(255, 255, 255, 0.15);
  box-shadow: 
    inset 0 0 0 1px rgba(255, 255, 255, 0.3),
    0 0 10px rgba(255, 255, 255, 0.2);
  transform: translateX(4px);
  transition: all 0.2s ease-out;
}

/* Hover effect with Command/Ctrl+Click hint */
.quantum-legend-item:hover:not(.selected):after {
  position: absolute;
  bottom: -20px;
  right: 10px;
  font-size: 9px;
  color: rgba(255, 255, 255, 0.6);
  opacity: 0;
  transform: translateY(5px);
  pointer-events: none;
  transition: all 0.2s ease-out;
  animation: fadeInHelp 1s forwards 0.5s;
}

@keyframes fadeInHelp {
  to { opacity: 0.8; transform: translateY(0); }
}

.quantum-legend-indicator {
  width: 10px;
  height: 10px;
  border-radius: 50%;
  margin-right: 8px;
  transition: all 0.3s ease;
  box-shadow: 0 0 0 rgba(255, 255, 255, 0);
}

.quantum-legend-item.active .quantum-legend-indicator {
  box-shadow: 0 0 8px currentColor;
}

.quantum-legend-item.selected .quantum-legend-indicator {
  transform: scale(1.2);
  box-shadow: 0 0 12px currentColor;
}

/* Tooltip for legend items - positioned above items */
.quantum-legend-tooltip {
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  width: 220px;
  padding: 10px 14px;
  border-radius: 8px;
  margin-bottom: 12px;
  font-size: 12px;
  line-height: 1.5;
  pointer-events: none;
  z-index: 20;
  opacity: 0;
  animation: fadeInTop 0.2s forwards;
  border: 1.5px solid;
  background-color: rgba(0, 0, 0, 0.85);
  -webkit-backdrop-filter: blur(12px);
  backdrop-filter: blur(12px);
  color: #FFFFFF;
  font-weight: 500;
  letter-spacing: 0.3px;
  box-shadow: 0 4px 20px rgba(0, 0, 0, 0.5), 0 0 0 1px rgba(255, 255, 255, 0.1);
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.5);
}

@keyframes fadeInTop {
  from { opacity: 0; transform: translateX(-50%) translateY(10px); }
  to { opacity: 1; transform: translateX(-50%) translateY(0); }
}

/* Style for Command/Ctrl+Click hint inside tooltip */
.tooltip-hint {
  margin-top: 8px;
  padding-top: 6px;
  border-top: 1px solid rgba(255, 255, 255, 0.2);
  font-size: 10px;
  color: rgba(255, 255, 255, 0.7);
  font-style: italic;
  display: flex;
  align-items: center;
  gap: 5px;
}

/* Ícone de atalho */
.tooltip-hint:before {
  content: '\2318'; /* Símbolo do comando/tecla */
  font-size: 12px;
  color: rgba(0, 245, 255, 0.9);
}

/* Tooltip colors for each phenomenon */
.tooltip-color-quantum-superposition {
  background-color: rgba(157, 106, 255, 0.25); /* Opacidade aumentada */
  border-color: #9D6AFF;
  box-shadow: 0 0 15px rgba(157, 106, 255, 0.3); /* Glow sutil para destaque */
}

.tooltip-color-objective-reduction {
  background-color: rgba(0, 180, 216, 0.25);
  border-color: #00B4D8;
  box-shadow: 0 0 15px rgba(0, 180, 216, 0.3);
}

.tooltip-color-quantum-entanglement {
  background-color: rgba(230, 59, 122, 0.25);
  border-color: #E63B7A;
  box-shadow: 0 0 15px rgba(230, 59, 122, 0.3);
}

.tooltip-color-conscious-state {
  background-color: rgba(59, 230, 105, 0.25);
  border-color: #3BE669;
  box-shadow: 0 0 15px rgba(59, 230, 105, 0.3);
}

.tooltip-color-tubulin-coherence {
  background-color: rgba(255, 209, 102, 0.25);
  border-color: #FFD166;
  box-shadow: 0 0 15px rgba(255, 209, 102, 0.3);
}

.tooltip-color-orchestration {
  background-color: rgba(0, 203, 209, 0.25);
  border-color: #00CBD1;
  box-shadow: 0 0 15px rgba(0, 203, 209, 0.3);
}

.tooltip-color-observer {
  background-color: rgba(255, 255, 255, 0.25);
  border-color: #FFFFFF;
  box-shadow: 0 0 15px rgba(255, 255, 255, 0.3);
}

.tooltip-color-quantum-decoherence {
  background-color: rgba(255, 85, 0, 0.25);
  border-color: #FF5500;
  box-shadow: 0 0 15px rgba(255, 85, 0, 0.3);
}

.tooltip-color-quantum-isolation {
  background-color: rgba(0, 204, 255, 0.25);
  border-color: #00CCFF;
  box-shadow: 0 0 15px rgba(0, 204, 255, 0.3);
}

@keyframes fadeIn {
  from { opacity: 0; transform: translate(10px, -50%); }
  to { opacity: 1; transform: translate(0, -50%); }
}

@keyframes fadeInLeft {
  from { opacity: 0; transform: translate(-10px, -50%); }
  to { opacity: 1; transform: translate(0, -50%); }
}

/* Specific colors for each quantum phenomenon - scientifically based on the frequencies and phenomena they represent */
/* Superposition: Violeta (frequência alta - terahertz) - baseado no espectro eletromagnético */
.quantum-color-quantum-superposition {
  background-color: #9D6AFF;
}

/* Objective Reduction: Azul esverdeado (colapso de função de onda) - baseado nas distribuições de probabilidade */
.quantum-color-objective-reduction {
  background-color: #00B4D8;
}

/* Quantum Entanglement: Magenta (fenômeno não-local) - baseado no espectro de ressonância magnética */
.quantum-color-quantum-entanglement {
  background-color: #E63B7A;
}

/* Conscious State: Verde (emergente) - baseado nos sinais EEG de consciência */
.quantum-color-conscious-state {
  background-color: #3BE669;
}

/* Tubulin Coherence: Ouro/Amarelo (oscilações de Fröhlich) - baseado na frequência ressonante 4-8 MHz */
.quantum-color-tubulin-coherence {
  background-color: #FFD166;
}

/* Orchestration: Turquesa (coordenação de múltiplos processos) - baseado na modulação de polarização */
.quantum-color-orchestration {
  background-color: #00CBD1;
}

/* Observer: Branco (luz completa - todos comprimentos de onda) - baseado na física do observador quântico */
.quantum-color-observer {
  background-color: #FFFFFF;
}

/* Decoherence: Laranja (perda de coerência quântica) - baseado na termodinâmica de sistemas abertos */
.quantum-color-quantum-decoherence {
  background-color: #FF5500;
}

/* Isolation: Azul ciano (mecanismos protetores) - baseado nos campos de exclusão de água estruturada */
.quantum-color-quantum-isolation {
  background-color: #00CCFF;
}

/* Legend toggle button */
.quantum-legend-toggle {
  position: absolute;
  bottom: 20px; /* Alinhado com a parte inferior da legenda */
  right: 246px; /* Posicionado à esquerda da legenda */
  width: 36px;
  height: 36px;
  border-radius: 50%;
  background: rgba(0, 0, 0, 0.85);
  border: 1px solid rgba(255, 255, 255, 0.2);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 5;
  cursor: pointer;
  opacity: 0.9;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  transition: all 0.2s ease;
  -webkit-backdrop-filter: blur(10px);
  backdrop-filter: blur(10px);
}

.quantum-legend-toggle:hover {
  transform: scale(1.1);
  background-color: rgba(20, 20, 40, 0.9);
  border-color: rgba(255, 255, 255, 0.3);
}

.quantum-legend-toggle svg {
  width: 18px;
  height: 18px;
  color: rgba(255, 255, 255, 0.9);
  vertical-align: middle;
}

.material-symbols-outlined {
  font-size: 18px !important;
  line-height: 1;
  font-variation-settings: 'FILL' 1, 'wght' 400, 'GRAD' 0, 'opsz' 24;
  font-family: 'Material Symbols Outlined';
  -webkit-font-feature-settings: 'liga';
  font-feature-settings: 'liga';
  -webkit-font-smoothing: antialiased;
  text-rendering: optimizeLegibility;
}

/* Material icons style - legacy support */
.material-symbols-outlined {
  font-size: 16px;
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useState, useEffect, useRef } from 'react';
import './QuantumVisualization.css';
import { QuantumVisualizationProvider, useQuantumVisualization, QuantumFrequencyBand, QuantumCore } from './QuantumVisualizationContext';
import { CognitionEvent } from '../../context/deepgram/types/CognitionEvent';
import { QuantumModel } from './index';
import { QuantumLegend } from './QuantumLegend';
// Import científico refinado da tradução cognitiva para fenômenos quânticos
import { mapCognitionEventToQuantumProperties } from './utils/CognitionMapper';

interface QuantumVisualizationContainerProps {
  cognitionEvents: CognitionEvent[] | null;
  width?: string;
  height?: string;
  showLegend?: boolean;
  lowPerformanceMode?: boolean; // Modo de baixa performance para dispositivos menos potentes
}

/**
 * Container principal para visualização quântica de eventos cognitivos segundo a teoria Orch-OR
 * 
 * Este componente implementa a interface visual para a teoria Penrose-Hameroff de
 * Redução Objetiva Orquestrada (Orch-OR), modelando a transformação de sinais neurais 
 * em fenômenos quânticos como superposição, emaranhamento e colapso (OR).
 * 
 * Simbolicamente, representa o conector entre processos neurais simbólicos e fenômenos quânticos,
 * servindo como interface cortical entre cognição e estruturas quânticas de Planck.
 */
export const QuantumVisualizationContainer: React.FC<QuantumVisualizationContainerProps> = ({
  cognitionEvents,
  width = '100%',
  height = '280px',
  showLegend = true,
  lowPerformanceMode = false
}) => {
  return (
    <QuantumVisualizationProvider>
      <QuantumVisualizationContent 
        cognitionEvents={cognitionEvents} 
        width={width} 
        height={height}
        showLegend={showLegend}
        lowPerformanceMode={lowPerformanceMode}
      />
    </QuantumVisualizationProvider>
  );
};

// Inner component to use hooks with context
const QuantumVisualizationContent: React.FC<QuantumVisualizationContainerProps> = ({
  cognitionEvents,
  width,
  height,
  showLegend,
  lowPerformanceMode = false
}) => {
  // Get quantum context functions from the Orch OR model - apenas os realmente utilizados
  const {
    // Estados quânticos realmente utilizados na renderização ou processamento
    quantumSuperpositions,
    quantumEntanglements,
    objectiveReductions,
    consciousStates,
    
    // Métodos para adicionar efeitos quânticos
    addQuantumSuperposition,
    addQuantumEntanglement,
    addObjectiveReduction,
    addConsciousState,
    setActiveRegion,
    
    // Métricas de orquestração utilizadas no processamento
    tubulinCoherenceLevel,
    orchestrationIntensity,
    setPlanckScaleFeedback,
    
    // Gerenciamento de efeitos
    clearAllEffects,
    setTubulinCoherenceLevel,
    setOrchestrationIntensity
  } = useQuantumVisualization();
  
  // Referência para rastrear se já inicializamos os efeitos quânticos basais
  const initializedRef = useRef(false);
  
  // Initialize quantum state effects - baseado na teoria Orch-OR, apenas UMA VEZ
  useEffect(() => {
    // Evitar múltiplas inicializações
    if (initializedRef.current) {
      return;
    }
    
    // Apenas reseta os estados uma vez para garantir início limpo
    clearAllEffects(); // Isso agora mantém um estado basal (Modificamos o clearAllEffects)
    
    // Marcar como inicializado para evitar chamadas repetidas
    initializedRef.current = true;
    
    // Coerencia e orquestração nos níveis iniciais adequados
    setTubulinCoherenceLevel(0.3); // Coerência inicial moderada 
    setOrchestrationIntensity(0.5); // Orquestração inicial moderada
    
    // Active region inicial - tálamo como integrador central
    setActiveRegion('THALAMUS');
  }, [clearAllEffects, setTubulinCoherenceLevel, setOrchestrationIntensity, setActiveRegion]);
  
  // State to control whether the legend is visible
  const [legendVisible, setLegendVisible] = useState(showLegend);
  
  // Ref para o container DOM
  const containerRef = React.useRef<HTMLDivElement>(null);
  
  // Event processing - rastreia eventos processados por seus timestamps
  const processedEventTimestampsRef = React.useRef<Set<string>>(new Set());
  
  // Ref para controle de processamento
  const processingEventRef = React.useRef<boolean>(false);
  
  // Utilidade para identificar eventos de forma única por timestamp e tipo
  const getEventKey = (event: CognitionEvent): string => {
    return `${event.type}-${event.timestamp}`;
  };
  
  // REMOVIDO: O timer automático de reset estava causando problemas, fazendo tudo desaparecer
  
  /**
   * Processa um evento cognitivo e o traduz para fenômenos quânticos segundo a teoria Orch-OR
   * Esta função implementa a tradução científica entre cognição e fenômenos quânticos
   */
  const handleCognitionEvent = React.useCallback((event: CognitionEvent) => {
    // Usa o CognitionMapper para traduzir o evento cognitivo para propriedades quânticas
    const quantumProperties = mapCognitionEventToQuantumProperties(event);
    
    // Ajusta a coerência e intensidade de orquestração baseado no evento
    if (event.type === 'neural_collapse') {
      // Eventos de colapso reduzem temporariamente a coerência e aumentam orquestração
      const newCoherence = Math.max(0.1, tubulinCoherenceLevel * 0.7);
      setTubulinCoherenceLevel(newCoherence);
      
      const newIntensity = Math.min(1, orchestrationIntensity * 1.3);
      setOrchestrationIntensity(newIntensity);
      
      // No momento do colapso, adiciona um efeito de reduction (OR)
      addObjectiveReduction(quantumProperties.core as QuantumCore);
      
      // Com cada colapso, atualizamos a região ativa
      setActiveRegion(quantumProperties.core as QuantumCore);
    } 
    else if (event.type === 'neural_signal') {
      // Sinais neurais aumentam a coerência quântica e superposição
      const newCoherence = Math.min(0.95, tubulinCoherenceLevel + 0.1);
      setTubulinCoherenceLevel(newCoherence);
      
      // Adiciona superposição com propriedades mapeadas do evento
      addQuantumSuperposition(
        quantumProperties.core as QuantumCore, 
        quantumProperties.frequencyBand as QuantumFrequencyBand
      );
    }
    else if (event.type === 'symbolic_retrieval') {
      // Recuperação simbólica cria entanglement entre regiões
      addQuantumEntanglement(
        'HIPPOCAMPUS' as QuantumCore, // Origem (hipocampo - memória)
        quantumProperties.frequencyBand as QuantumFrequencyBand // Banda de frequência
      );
    }
    else if (event.type === 'symbolic_context_synthesized' || event.type === 'fusion_initiated') {
      // Síntese e fusão criam estados conscientes
      addConsciousState(
        quantumProperties.core as QuantumCore,
        quantumProperties.frequencyBand as QuantumFrequencyBand
      );
    }
    // NOVO: Padrões emergentes geram entrelamento complexo entre múltiplas regiões
    else if (event.type === 'emergent_patterns') {
      // Na teoria Orch-OR, padrões emergentes representam a formação de estados cognitivos complexos
      // através de múltiplos entrelamentos quânticos
      addQuantumEntanglement(
        'PREFRONTAL' as QuantumCore,
        QuantumFrequencyBand.KILOHERTZ
      );
      
      // Segundo entrelamento para região relacionada à memória
      addQuantumEntanglement(
        'HIPPOCAMPUS' as QuantumCore,
        QuantumFrequencyBand.MEGAHERTZ
      );
      
      // Aumenta a orquestração global para refletir integração de informações
      setOrchestrationIntensity(Math.min(1, orchestrationIntensity + 0.2));
    }
    // Nota: 'raw_prompt' não gera efeitos quânticos diretos conforme teoria Orch-OR
    // Na teoria, processos quânticos só ocorrem após processamento neural inicial
    
    // MELHORADO: Eventos não-computáveis têm efeito quântico mais significativo
    if (quantumProperties.nonComputable) {
      setPlanckScaleFeedback(true); // Ativa o feedback de escala de Planck
      
      // Na teoria Orch-OR, eventos não-computáveis representam aspectos da consciência 
      // que transcendem algoritmização e emergem da física quântica
      addQuantumSuperposition(
        quantumProperties.core as QuantumCore,
        QuantumFrequencyBand.TERAHERTZ // Maior frequência - nível quântico fundamental
      );
      
      // Aumento significativo de coerência - característico de eventos não-computáveis
      setTubulinCoherenceLevel(Math.min(0.98, tubulinCoherenceLevel + 0.15));
    }
  }, [tubulinCoherenceLevel, orchestrationIntensity, setTubulinCoherenceLevel, 
      setOrchestrationIntensity, addObjectiveReduction, addQuantumSuperposition, 
      addQuantumEntanglement, addConsciousState, setActiveRegion, setPlanckScaleFeedback]);

  // Processar novo evento cognitivo quando disponível - mantém o ciclo Orch-OR
  useEffect(() => {
    // Não processa se não houver eventos ou se ainda não inicializamos
    if (!cognitionEvents || cognitionEvents.length === 0 || !initializedRef.current) {
      return;
    }
    
    // Identifica apenas os eventos novos que ainda não foram processados
    const newEvents = cognitionEvents.filter(event => {
      const eventKey = getEventKey(event);
      return !processedEventTimestampsRef.current.has(eventKey);
    });
    
    // Se não há eventos novos, não processa nada
    if (newEvents.length === 0) {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[OrchORContainer] No new events to process');
      }
      return;
    }
    
    // Se já estiver processando um evento, ignora para evitar sobreposições
    if (processingEventRef.current) {
      if (process.env.NODE_ENV !== 'production') {
        console.log(`[OrchORContainer] Ignorando processamento: já existe um evento em processamento`);
      }
      return;
    }
    
    // Marca como em processamento
    processingEventRef.current = true;

    // Processa os novos eventos
    if (process.env.NODE_ENV !== 'production') {
      console.log(`[OrchORContainer] Processing ${newEvents.length} new cognition events`);
    }
    
    // Marca os eventos como processados e os processa
    newEvents.forEach(event => {
      // Adiciona o evento à lista de eventos já processados
      const eventKey = getEventKey(event);
      processedEventTimestampsRef.current.add(eventKey);
      
      // Processa o evento
      handleCognitionEvent(event);
    });
    
    // Limita o tamanho do set de eventos processados (evita memory leak)
    if (processedEventTimestampsRef.current.size > 100) {
      // Mantém apenas os 50 mais recentes se exceder 100 eventos
      const keysArray = Array.from(processedEventTimestampsRef.current);
      const toRemove = keysArray.slice(0, keysArray.length - 50);
      toRemove.forEach(key => processedEventTimestampsRef.current.delete(key));
    }
    
    // Libera para novo processamento depois que este for concluído
    setTimeout(() => {
      processingEventRef.current = false;
    }, 100);
  }, [cognitionEvents, handleCognitionEvent]);

  // Toggle legend visibility
  const toggleLegend = () => setLegendVisible(prev => !prev);

  // Create custom CSS properties if custom sizes are needed
  useEffect(() => {
    if (containerRef.current && (width !== '100%' || height !== '280px')) {
      containerRef.current.style.setProperty('--custom-width', width || '100%');
      containerRef.current.style.setProperty('--custom-height', height || '280px');
    }
  }, [width, height]);

  // Adiciona classes específicas para os estados de Orch OR
  const getOrchORClassNames = () => {
    const classNames = [];
    
    // Classes refletindo intensidades de coerência
    if (tubulinCoherenceLevel > 0.7) classNames.push('high-coherence');
    else if (tubulinCoherenceLevel > 0.3) classNames.push('medium-coherence');
    else classNames.push('low-coherence');
    
    // Classes refletindo intensidades de orquestração
    if (orchestrationIntensity > 0.7) classNames.push('intense-orchestration');
    else if (orchestrationIntensity > 0.3) classNames.push('medium-orchestration');
    else classNames.push('minimal-orchestration');
    
    // Classes baseadas no estado predominante
    if (quantumSuperpositions.length > 0) classNames.push('quantum-superposition-active');
    if (quantumEntanglements.length > 0) classNames.push('quantum-entanglement-active');
    if (objectiveReductions.length > 0) classNames.push('objective-reduction-active');
    if (consciousStates.length > 0) classNames.push('conscious-moment-active');
    
    return classNames.join(' ');
  };

  return (
    <div 
      ref={containerRef}
      className={`quantum-visualization-container 
        ${height === '280px' ? 'fixed-height' : ''} 
        ${width === '100%' ? 'fixed-width' : ''} 
        ${lowPerformanceMode ? 'low-performance-mode' : ''}
        ${getOrchORClassNames()}
      `}
      data-width={width !== '100%' ? width : undefined}
      data-height={height !== '280px' ? height : undefined}
      data-coherence={tubulinCoherenceLevel.toFixed(2)}
      data-orchestration={orchestrationIntensity.toFixed(2)}
    >
      {/* Quantum Visualization always visible */}
      <QuantumModel />

      {/* Botão de toggle para a legenda quântica */}
      <button 
        className="quantum-legend-toggle" 
        onClick={toggleLegend}
        aria-label="Toggle quantum legend"
        title="Toggle quantum legend"
      >
        {legendVisible ? (
          <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="white">
            <path d="M12 7c2.76 0 5 2.24 5 5 0 .65-.13 1.26-.36 1.83l2.92 2.92c1.51-1.26 2.7-2.89 3.43-4.75-1.73-4.39-6-7.5-11-7.5-1.4 0-2.74.25-3.98.7l2.16 2.16C10.74 7.13 11.35 7 12 7zM2 4.27l2.28 2.28.46.46C3.08 8.3 1.78 10.02 1 12c1.73 4.39 6 7.5 11 7.5 1.55 0 3.03-.3 4.38-.84l.42.42L19.73 22 21 20.73 3.27 3 2 4.27zM7.53 9.8l1.55 1.55c-.05.21-.08.43-.08.65 0 1.66 1.34 3 3 3 .22 0 .44-.03.65-.08l1.55 1.55c-.67.33-1.41.53-2.2.53-2.76 0-5-2.24-5-5 0-.79.2-1.53.53-2.2zm4.31-.78l3.15 3.15.02-.16c0-1.66-1.34-3-3-3l-.17.01z"/>
          </svg>
        ) : (
          <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="white">
            <path d="M12 4C7 4 2.73 7.11 1 12c1.73 4.89 6 8 11 8s9.27-3.11 11-8c-1.73-4.89-6-8-11-8zm0 14c-3.31 0-6-2.69-6-6s2.69-6 6-6 6 2.69 6 6-2.69 6-6 6zm0-10c-2.21 0-4 1.79-4 4s1.79 4 4 4 4-1.79 4-4-1.79-4-4-4z"/>
          </svg>
        )}
      </button>
      
      {/* Legend for quantum phenomena */}
      {legendVisible && <QuantumLegend />}
    </div>
  );
};

export default QuantumVisualizationContainer;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { createContext, useContext, useState, useCallback } from 'react';
import { nanoid } from 'nanoid';

// Define the type for cognition cores - represents different regions/modules of the brain's quantum processing
export type QuantumCore = string;

// Quantum frequency bands based on research by Anirban Bandyopadhyay's team
// These correspond to the different oscillation frequency ranges observed in microtubules
export enum QuantumFrequencyBand {
  TERAHERTZ = 'terahertz',   // Fastest oscillations at quantum level
  GIGAHERTZ = 'gigahertz',   // Quantum vibrations in microtubules
  MEGAHERTZ = 'megahertz',   // Neural quantum resonance 
  KILOHERTZ = 'kilohertz',   // Medium-scale neural activity
  HERTZ = 'hertz'            // Macroscopic brain waves/EEG
}

// Quantum consciousness states from the Orch OR theory
export enum OrchORState {
  QUANTUM_SUPERPOSITION = 'quantum_superposition',  // Quantum states in superposition (pre-conscious)
  QUANTUM_COHERENCE = 'quantum_coherence',         // Coherent quantum states across microtubules
  OBJECTIVE_REDUCTION = 'objective_reduction',     // Orchestrated collapse (Orch OR)
  CONSCIOUS_MOMENT = 'conscious_moment'            // Post-collapse state (moment of consciousness)
}

// Triplet level - implements the "triplets of triplets" pattern described in Orch OR research
export type TripletLevel = 'primary' | 'secondary' | 'tertiary';

// Quantum effect - represents a single quantum phenomenon in the Orch OR model
export interface QuantumEffect {
  id: string;                        // Unique identifier
  core: QuantumCore;                 // Brain region/module affected
  orchORState: OrchORState;          // Current state in the Orch OR process
  frequencyBand: QuantumFrequencyBand; // Oscillation frequency
  tripletLevel: TripletLevel;        // Level in the triplet hierarchy
  tripletGroup?: number;             // Group identifier within the triplet pattern
  amplitude: number;                 // Quantum amplitude (0-1)
  phaseCoherence?: number;          // Quantum coherence measure (0-1)
  createdAt: number;                 // Creation timestamp
  collapseThreshold?: number;        // Threshold for objective reduction (τ ≈ ħ/EG)
  planckScale?: boolean;             // Whether effect operates at Planck scale
  nonComputable?: boolean;           // Penrose's non-computable decision (true for conscious moments)
}

// Context interface for quantum visualization based on Orch OR
export interface QuantumVisualizationContextType {
  // Quantum effects organized by Orch OR states
  quantumSuperpositions: QuantumEffect[];   // Pre-conscious quantum states
  quantumEntanglements: QuantumEffect[];    // Quantum entanglement between tubulins
  objectiveReductions: QuantumEffect[];     // Collapse events (conscious moments)
  consciousStates: QuantumEffect[];         // Post-reduction states
  
  // Methods to add quantum effects
  addQuantumSuperposition: (core: QuantumCore, frequencyBand?: QuantumFrequencyBand) => void;
  addQuantumEntanglement: (core: QuantumCore, frequencyBand?: QuantumFrequencyBand) => void;
  addObjectiveReduction: (core: QuantumCore, frequencyBand?: QuantumFrequencyBand) => void;
  addConsciousState: (core: QuantumCore, frequencyBand?: QuantumFrequencyBand) => void;
  
  // Observer and orchestration state
  observerState: 'active' | 'inactive';
  setObserverState: (state: 'active' | 'inactive') => void;
  activeRegion: QuantumCore | null;
  setActiveRegion: (region: QuantumCore | null) => void;
  tubulinCoherenceLevel: number;           // Overall coherence level (0-1)
  setTubulinCoherenceLevel: (level: number) => void; // Setter for coherence level
  orchestrationIntensity: number;         // Orchestration intensity (0-1)
  setOrchestrationIntensity: (intensity: number) => void; // Setter for orchestration intensity
  setPlanckScaleFeedback: (active: boolean) => void;
  
  // Visual filtering for legend interaction
  activeVisualFilters: string[];      // IDs of the active filters (empty array means show all)
  setActiveVisualFilter: (filterId: string, multiSelect?: boolean) => void; // Set active visual filter
  removeVisualFilter: (filterId: string) => void; // Remove a specific filter
  clearVisualFilters: () => void; // Clear all filters
  // Effect management
  clearAllEffects: (preserveBasalState?: boolean, resetLevel?: number) => void;
}

export const QuantumVisualizationContext = createContext<QuantumVisualizationContextType | null>(null);

interface QuantumVisualizationProviderProps {
  children: React.ReactNode;
}

export const QuantumVisualizationProvider: React.FC<QuantumVisualizationProviderProps> = ({ children }) => {
  // Orch OR quantum states from the theory
  const [quantumSuperpositions, setQuantumSuperpositions] = useState<QuantumEffect[]>([]);
  const [quantumEntanglements, setQuantumEntanglements] = useState<QuantumEffect[]>([]);
  const [objectiveReductions, setObjectiveReductions] = useState<QuantumEffect[]>([]);
  const [consciousStates, setConsciousStates] = useState<QuantumEffect[]>([]);
  
  // Observer and region states
  const [observerState, _setObserverState] = useState<'active' | 'inactive'>('inactive');
  const [activeRegion, _setActiveRegion] = useState<QuantumCore | null>(null);
  
  // Orchestration metrics based on Orch OR theory
  const [tubulinCoherenceLevel, setTubulinCoherenceLevel] = useState<number>(0);
  const [orchestrationIntensity, setOrchestrationIntensity] = useState<number>(0);
  const [planckScaleFeedback, setPlanckScaleFeedback] = useState<boolean>(false);
  
  // State for visual filtering (activated by legend)
  const [activeVisualFilters, setActiveVisualFilters] = useState<string[]>([]);
  
  // Handler to add/remove a filter with multiSelect support
  const setActiveVisualFilter = useCallback((filterId: string, multiSelect: boolean = false) => {
    setActiveVisualFilters(prev => {
      // Se já está selecionado
      if (prev.includes(filterId)) {
        // Remove o filtro
        return prev.filter(id => id !== filterId);
      } else {
        // Se multiSelect (Command/Ctrl pressionado), adiciona à seleção atual
        if (multiSelect) {
          return [...prev, filterId];
        }
        // Sem multiSelect, substitui toda a seleção atual
        return [filterId];
      }
    });
  }, []);
  
  // Remove um filtro específico
  const removeVisualFilter = useCallback((filterId: string) => {
    setActiveVisualFilters(prev => prev.filter(id => id !== filterId));
  }, []);
  
  // Limpa todos os filtros
  const clearVisualFilters = useCallback(() => {
    setActiveVisualFilters([]);
  }, []);
  
  // Memoized state updaters
  const setObserverState = useCallback((state: 'active' | 'inactive') => _setObserverState(state), []);
  const setActiveRegion = useCallback((region: QuantumCore | null) => _setActiveRegion(region), []);

  // No automatic clearing interval to avoid update depth exceeded errors
  // We'll manage lifetime of effects more carefully through add/clear functions

  // Implementation of "triplets of triplets" pattern from Orch OR research
  // Each quantum effect type follows the hierarchy of 3 primary triplets, each with 3 secondary triplets
  
  /**
   * Add a quantum superposition effect - represents pre-conscious quantum states in microtubules
   * Maps to: neural_signal, emergent_patterns events
   */
  const addQuantumSuperposition = React.useCallback((core: QuantumCore, frequencyBand: QuantumFrequencyBand = QuantumFrequencyBand.TERAHERTZ) => {
    try {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[OrchOR] addQuantumSuperposition:', core, frequencyBand, new Date().toISOString());
      }
      
      setQuantumSuperpositions(prev => {
        // Implement "triplets of triplets" pattern
        const primaryTripletCount = prev.filter(p => 
          p.tripletLevel === 'primary' && 
          p.frequencyBand === frequencyBand
        ).length;
        
        // Limit to max 9 effects (3 primary triplets × 3 per triplet)
        if (primaryTripletCount >= 9) return prev;
        
        // Calculate which triplet group this belongs to
        const tripletGroup = Math.floor(primaryTripletCount / 3) + 1;
        
        // Create new quantum effect with Orch OR properties
        const newEffect: QuantumEffect = {
          id: nanoid(),
          core,
          orchORState: OrchORState.QUANTUM_SUPERPOSITION,
          frequencyBand,
          tripletLevel: 'primary',
          tripletGroup,
          amplitude: Math.random() * 0.5 + 0.5, // Strong amplitude for superpositions
          createdAt: Date.now(),
          planckScale: frequencyBand === QuantumFrequencyBand.TERAHERTZ, // Only terahertz operates at Planck scale
          // Calculate theoretical collapse threshold based on Penrose's equation τ ≈ ħ/EG
          collapseThreshold: Math.random() * 300 + 100 // Simulated milliseconds until collapse
        };
        
        return [...prev, newEffect];
      });
      
      // Update orchestration metrics
      updateOrchestrationMetrics();
      
    } catch (error) {
      console.error('[OrchOR] Error in addQuantumSuperposition:', error);
    }
  }, []);

  /**
   * Add a quantum entanglement effect - represents quantum coherence across microtubules
   * Maps to: fusion_initiated events
   */
  const addQuantumEntanglement = React.useCallback((core: QuantumCore, frequencyBand: QuantumFrequencyBand = QuantumFrequencyBand.GIGAHERTZ) => {
    try {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[OrchOR] addQuantumEntanglement:', core, frequencyBand, new Date().toISOString());
      }
      
      setQuantumEntanglements(prev => {
        // Maintain triplet pattern
        const existingCount = prev.filter(p => 
          p.tripletLevel === 'primary' && 
          p.frequencyBand === frequencyBand
        ).length;
        
        if (existingCount >= 9) return prev;
        
        const tripletGroup = Math.floor(existingCount / 3) + 1;
        
        const newEffect: QuantumEffect = {
          id: nanoid(),
          core,
          orchORState: OrchORState.QUANTUM_COHERENCE,
          frequencyBand,
          tripletLevel: 'primary',
          tripletGroup,
          amplitude: Math.random() * 0.3 + 0.7, // High amplitude for entanglements
          phaseCoherence: Math.random() * 0.4 + 0.6, // High coherence
          createdAt: Date.now(),
          // Entanglements can span multiple frequency bands
          planckScale: frequencyBand === QuantumFrequencyBand.TERAHERTZ || frequencyBand === QuantumFrequencyBand.GIGAHERTZ
        };
        
        return [...prev, newEffect];
      });
      
      // Increase coherence level when entanglements occur
      setTubulinCoherenceLevel(prev => Math.min(1, prev + 0.1));
      updateOrchestrationMetrics();
      
    } catch (error) {
      console.error('[OrchOR] Error in addQuantumEntanglement:', error);
    }
  }, []);

  /**
   * Add an objective reduction effect - represents quantum collapse (moment of proto-consciousness)
   * Maps to: neural_collapse, symbolic_retrieval events
   */
  const addObjectiveReduction = React.useCallback((core: QuantumCore, frequencyBand: QuantumFrequencyBand = QuantumFrequencyBand.MEGAHERTZ) => {
    try {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[OrchOR] addObjectiveReduction:', core, frequencyBand, new Date().toISOString());
      }
      
      setObjectiveReductions(prev => {
        // Maintain triplet pattern
        const existingCount = prev.filter(p => 
          p.tripletLevel === 'primary' && 
          p.frequencyBand === frequencyBand
        ).length;
        
        if (existingCount >= 9) return prev;
        
        const tripletGroup = Math.floor(existingCount / 3) + 1;
        
        const newEffect: QuantumEffect = {
          id: nanoid(),
          core,
          orchORState: OrchORState.OBJECTIVE_REDUCTION,
          frequencyBand,
          tripletLevel: 'primary',
          tripletGroup,
          amplitude: Math.random() * 0.6 + 0.4, // Medium-high amplitude
          createdAt: Date.now(),
          // According to Penrose-Hameroff, OR is a gravitational process at the quantum-classical boundary
          planckScale: false,
          // OR connects to non-computable processes in fundamental spacetime
          nonComputable: Math.random() > 0.3 // 70% chance of non-computable reduction
        };
        
        return [...prev, newEffect];
      });
      
      // Objective reductions temporarily decrease coherence
      setTubulinCoherenceLevel(prev => Math.max(0, prev - 0.15));
      updateOrchestrationMetrics();
      
    } catch (error) {
      console.error('[OrchOR] Error in addObjectiveReduction:', error);
    }
  }, []);

  /**
   * Add a conscious state effect - represents post-reduction conscious moment
   * Maps to: symbolic_context_synthesized, GPT_response events
   */
  const addConsciousState = React.useCallback((core: QuantumCore, frequencyBand: QuantumFrequencyBand = QuantumFrequencyBand.KILOHERTZ) => {
    try {
      if (process.env.NODE_ENV !== 'production') {
        console.log('[OrchOR] addConsciousState:', core, frequencyBand, new Date().toISOString());
      }
      
      setConsciousStates(prev => {
        // Maintain triplet pattern
        const existingCount = prev.filter(p => 
          p.tripletLevel === 'primary' && 
          p.frequencyBand === frequencyBand
        ).length;
        
        if (existingCount >= 9) return prev;
        
        const tripletGroup = Math.floor(existingCount / 3) + 1;
        
        const newEffect: QuantumEffect = {
          id: nanoid(),
          core,
          orchORState: OrchORState.CONSCIOUS_MOMENT,
          frequencyBand,
          tripletLevel: 'primary',
          tripletGroup,
          amplitude: 1.0, // Maximum amplitude for conscious states
          phaseCoherence: 1.0, // Perfect coherence achieved
          createdAt: Date.now(),
          planckScale: false,
          nonComputable: true // Per Penrose, consciousness involves non-computable processes
        };
        
        return [...prev, newEffect];
      });
      
      // Conscious states temporarily max out orchestration
      setOrchestrationIntensity(1.0);
      
      // Decay orchestration gradually
      setTimeout(() => {
        setOrchestrationIntensity(prev => Math.max(0, prev - 0.2));
      }, 500);
      
    } catch (error) {
      console.error('[OrchOR] Error in addConsciousState:', error);
    }
  }, []);
  
  // Helper to update overall orchestration metrics based on current quantum state
  const updateOrchestrationMetrics = useCallback(() => {
    // Calculate orchestration intensity based on quantum effects present
    const totalEffects = 
      quantumSuperpositions.length + 
      quantumEntanglements.length + 
      objectiveReductions.length + 
      consciousStates.length;
      
    // Weight consciousness states higher in orchestration
    const weightedSum = 
      quantumSuperpositions.length * 0.2 + 
      quantumEntanglements.length * 0.3 + 
      objectiveReductions.length * 0.5 + 
      consciousStates.length * 1.0;
      
    // Normalize to 0-1 range
    const normalizedIntensity = totalEffects > 0 ? 
      Math.min(1.0, weightedSum / (totalEffects * 0.5)) : 0;
      
    setOrchestrationIntensity(normalizedIntensity);
  }, [quantumSuperpositions, quantumEntanglements, objectiveReductions, consciousStates, setOrchestrationIntensity]);
  
  /**
   * Limpa todos os efeitos quânticos, com opção de manter ou não um estado basal
   * @param preserveBasalState Se true, mantém um estado quântico basal conforme teoria Orch-OR 
   * @param resetLevel Nível de coerência para resetar (0-1)
   */
  const clearAllEffects = useCallback((preserveBasalState = true, resetLevel = 0.3) => {
    try {
      // Primeiro limpar todos os estados existentes
      setQuantumSuperpositions([]);
      setQuantumEntanglements([]);
      setObjectiveReductions([]);
      setConsciousStates([]);
      
      // Se não está preservando estado basal, apenas zera tudo e retorna
      if (!preserveBasalState) {
        if (process.env.NODE_ENV !== 'production') {
          console.log('[OrchOR] clearAllEffects - full reset (no basal state)');
        }
        setTubulinCoherenceLevel(0.1); // Ainda mantém um mínimo de coerência (10%)
        setOrchestrationIntensity(0.1);
        setActiveRegion(null);
        return;
      }
      
      // Imediatamente adicionar o estado quântico basal (Orch-OR)
      // Duas superposições quânticas (oscilações de Fröhlich nos microtuúbulos)
      const baseSuper1: QuantumEffect = {
        id: nanoid(),
        core: 'THALAMUS', // Tálamo como base essencial da consciência quântica
        orchORState: OrchORState.QUANTUM_SUPERPOSITION,
        frequencyBand: QuantumFrequencyBand.MEGAHERTZ, // 8MHz (banda Fröhlich)
        tripletLevel: 'primary',
        tripletGroup: 1,
        amplitude: 0.2,  // Reduzido um pouco para não ser tão intenso
        phaseCoherence: 0.2,
        createdAt: Date.now()
      };
      
      const baseSuper2: QuantumEffect = {
        id: nanoid(),
        core: 'PREFRONTAL', // Segundo centro essencial para consciência
        orchORState: OrchORState.QUANTUM_SUPERPOSITION,
        frequencyBand: QuantumFrequencyBand.MEGAHERTZ,
        tripletLevel: 'primary',
        tripletGroup: 1,
        amplitude: 0.15,  // Reduzido um pouco para não ser tão intenso
        phaseCoherence: 0.2,
        createdAt: Date.now()
      };
      
      // Um entrelamento quântico básico (coerência quântica de base)
      const baseEntanglement: QuantumEffect = {
        id: nanoid(),
        core: 'HIPPOCAMPUS',
        orchORState: OrchORState.QUANTUM_COHERENCE,
        frequencyBand: QuantumFrequencyBand.MEGAHERTZ,
        tripletLevel: 'primary',
        tripletGroup: 1,
        amplitude: 0.2,  // Reduzido um pouco para não ser tão intenso
        phaseCoherence: 0.3,
        createdAt: Date.now()
      };
      
      // Adicionar estado quântico basal
      setQuantumSuperpositions([baseSuper1, baseSuper2]);
      setQuantumEntanglements([baseEntanglement]);
      
      // Atualizar coerência basal (Faixa típica de repouso em Orch-OR)
      setTubulinCoherenceLevel(resetLevel); // Nível de coerência configurado
      setOrchestrationIntensity(resetLevel - 0.1); // Ligeiramente menor que a coerência
      
      // Manter o tálamo como região ativa mesmo em repouso
      setActiveRegion('THALAMUS');
      setObserverState('active'); // Observer sempre ativo em nível basal
      
    } catch (error) {
      console.error('[OrchOR] Error in clearAllEffects:', error);
    }
  }, [
    setQuantumSuperpositions,
    setQuantumEntanglements,
    setObjectiveReductions,
    setConsciousStates,
    setTubulinCoherenceLevel,
    setOrchestrationIntensity,
    setObserverState,
    setActiveRegion
  ]);

  return (
    <QuantumVisualizationContext.Provider
      value={{
        // Quantum states based on Orch OR theory
        quantumSuperpositions,
        quantumEntanglements,
        objectiveReductions,
        consciousStates,
        
        // Methods to add quantum effects
        addQuantumSuperposition,
        addQuantumEntanglement,
        addObjectiveReduction,
        addConsciousState,
        
        // Observer and orchestration state
        observerState,
        setObserverState,
        activeRegion,
        setActiveRegion,
        tubulinCoherenceLevel,
        setTubulinCoherenceLevel: (level: number) => setTubulinCoherenceLevel(level),
        orchestrationIntensity,
        setOrchestrationIntensity: (intensity: number) => setOrchestrationIntensity(intensity),
        setPlanckScaleFeedback: (active: boolean) => setPlanckScaleFeedback(active),
        
        // Visual filtering for legend interaction
        activeVisualFilters,
        setActiveVisualFilter,
        removeVisualFilter,
        clearVisualFilters,
        // Effect management
        clearAllEffects
      }}
    >
      {children}
    </QuantumVisualizationContext.Provider>
  );
};

export const useQuantumVisualization = () => {
  const context = useContext(QuantumVisualizationContext);
  if (!context) {
    throw new Error('useQuantumVisualization must be used within a QuantumVisualizationProvider');
  }
  return context;
};
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

/* 
  Visualização Quântica CSS
*/

.quantum-three-canvas {
  width: 100%;
  height: 280px;
  min-height: 200px;
  display: block;
}

/* Implementação de efeitos quânticos usando CSS puro para 
  garantir visibilidade mesmo sem suporte a WebGL
*/

.quantum-visualization-css {
  position: relative;
  width: 100%;
  height: 100%;
  min-height: 280px;
  overflow: hidden;
  border-radius: 8px;
  background: linear-gradient(135deg, rgb(2,0,36) 0%, rgb(9,9,121) 35%, rgb(0,25,60) 100%);
}

.quantum-background {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  opacity: 0.7;
  background: 
    radial-gradient(circle at 30% 30%, rgba(45, 85, 255, 0.1) 0%, rgba(5, 5, 30, 0) 50%),
    radial-gradient(circle at 70% 60%, rgba(120, 70, 255, 0.1) 0%, rgba(5, 5, 30, 0) 50%),
    radial-gradient(circle at 50% 50%, rgba(10, 10, 40, 0.3) 0%, rgba(5, 5, 30, 0) 100%);
  animation: pulse 15s ease-in-out infinite alternate;
}

/* Elementos quânticos de visualização para o QuantumModel */
.quantum-element-1 {
  position: absolute;
  top: 25%;
  left: 10%;
  width: 40px;
  height: 40px;
  border-radius: 50%;
  background: radial-gradient(circle, rgba(0,90,255,1) 0%, rgba(0,50,200,0.7) 50%, rgba(0,0,150,0) 100%);
  filter: blur(2px);
  opacity: 0.8;
  animation: float 4s ease-in-out infinite;
}

.quantum-element-2 {
  position: absolute;
  top: 45%;
  left: 40%;
  width: 120px;
  height: 15px;
  background: linear-gradient(90deg, rgba(200,0,200,0.9) 0%, rgba(255,100,255,0.7) 50%, rgba(200,0,255,0.9) 100%);
  box-shadow: 0 0 15px rgba(200, 0, 255, 0.8);
  animation: glow 3s infinite alternate;
  transform: rotate(-15deg);
  transform-origin: left center;
}

.quantum-element-3 {
  position: absolute;
  top: 30%;
  right: 20%;
  width: 60px;
  height: 60px;
  border-radius: 50%;
  border: 2px solid rgba(100, 255, 255, 0.8);
  box-shadow: 0 0 20px rgba(100, 255, 255, 0.5), inset 0 0 20px rgba(100, 255, 255, 0.5);
  animation: collapse 3s infinite;
  opacity: 0.8;
}

.quantum-element-4 {
  position: absolute;
  top: 65%;
  left: 60%;
  width: 90px;
  height: 30px;
  border-radius: 40%;
  background: radial-gradient(ellipse, rgba(180,255,140,0.7) 0%, rgba(120,200,100,0.3) 70%, rgba(0,0,0,0) 100%);
  filter: blur(5px);
  animation: float 5s infinite alternate;
  transform: rotate(20deg);
}

.quantum-element-5 {
  position: absolute;
  top: 40%;
  left: 70%;
  width: 20px;
  height: 20px;
  border-radius: 50%;
  background: radial-gradient(circle, rgba(255,255,100,0.9) 0%, rgba(255,200,50,0.3) 70%, rgba(0,0,0,0) 100%);
  box-shadow: 0 0 10px rgba(255, 255, 0, 0.8);
  animation: pulse2 2s infinite alternate;
  opacity: 0.8;
}

.quantum-element-6 {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: radial-gradient(circle at 30% 30%, rgba(0,40,80,0.6) 0%, rgba(0,10,20,0.1) 70%);
  opacity: 0.4;
  animation: shift 15s infinite alternate;
}

.quantum-particles-container {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  pointer-events: none;
  z-index: 5;
}

/* Classes de posicionamento para as partículas */
.quantum-particle-pos-1 {
  left: 15%;
  top: 20%;
}

.quantum-particle-pos-2 {
  left: 35%;
  top: 35%;
}

.quantum-particle-pos-3 {
  left: 65%;
  top: 25%;
}

.quantum-particle-pos-4 {
  left: 80%;
  top: 60%;
}

.quantum-particle-pos-5 {
  left: 45%;
  top: 75%;
}

/* Classes de atraso para animações das partículas */
.quantum-particle-delay-1 {
  animation-delay: 0.5s;
}

.quantum-particle-delay-2 {
  animation-delay: 1.2s;
}

.quantum-particle-delay-3 {
  animation-delay: 2.3s;
}

.quantum-particle-delay-4 {
  animation-delay: 3.1s;
}

.quantum-particle-delay-5 {
  animation-delay: 4.5s;
}

/* Partículas quânticas - representam estados quânticos em superposição */
.quantum-particle {
  position: absolute;
  border-radius: 50%;
  filter: blur(1px);
  transform-origin: center center;
  transition: all 0.5s ease;
  pointer-events: none;
  animation: quantum-flicker 3s ease-in-out infinite alternate;
}

/* Linhas de conexão - representam entrelamento quântico */
.quantum-connection {
  position: absolute;
  filter: blur(1px);
  transform-origin: left center;
  pointer-events: none;
  animation: quantum-pulse 4s ease-in-out infinite alternate;
}

/* Colapso de onda - representa redução objetiva */
.quantum-wave-collapse {
  position: absolute;
  border-radius: 50%;
  border: 2px solid rgba(100, 200, 255, 0.8);
  box-shadow: 0 0 10px rgba(120, 180, 255, 0.5), inset 0 0 15px rgba(100, 200, 255, 0.3);
  background: transparent;
  pointer-events: none;
  animation: collapse-pulse 3s ease-in-out infinite;
}

/* Animações quânticas */
@keyframes quantum-flicker {
  0%, 100% {
    transform: scale(1);
    filter: blur(1px);
  }
  50% {
    transform: scale(1.2);
    filter: blur(2px);
  }
}

@keyframes quantum-pulse {
  0%, 100% {
    opacity: 0.3;
    filter: blur(1px);
  }
  50% {
    opacity: 0.7;
    filter: blur(2px);
  }
}

@keyframes collapse-pulse {
  0% {
    transform: scale(0.1);
    opacity: 0.8;
  }
  100% {
    transform: scale(1);
    opacity: 0;
  }
}

@keyframes pulse {
  0%, 100% {
    transform: scale(1);
  }
  50% {
    transform: scale(1.05);
  }
}

/* Adaptações para diferentes tamanhos */
@media screen and (max-width: 768px) {
  .quantum-visualization-css {
    min-height: 200px;
  }
}
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

/* Neural card container - base styling */
.orchos-card {
  font-family: 'Inter', Arial, sans-serif;
  min-height: 0;
  min-width: 0;
  display: flex;
  flex-direction: column;
  height: 100%; /* Preenche altura disponível */
  width: 100%; /* Preenche largura disponível */
  padding: clamp(0.45rem, 2vw, 0.7rem); /* Padding menor */
  border-radius: 0.85rem;               /* Raio menor */
  border: 3px solid var(--neural-accent-color, #00faff);
  background: rgba(24,24,40,0.55);
  box-shadow: 0 8px 40px 0 #000b, 0 0 32px 4px rgba(var(--neural-accent-rgb, 0, 250, 255), 0.2) inset;
  -webkit-backdrop-filter: blur(12px) saturate(1.2);
  backdrop-filter: blur(12px) saturate(1.2);
  position: relative; /* Garante contexto para popup absoluto */
  overflow: hidden;
  min-width: 0;
  max-width: 100%;
  box-sizing: border-box;
  margin: 0;
  transition: box-shadow 0.3s ease, border-color 0.3s ease;
}


/* Hover/focus padronizado para todos os cards (inclusive ai) */
.orchos-card:hover,
.orchos-card:active,
.orchos-card:focus-visible {
  box-shadow: 0 8px 48px 0 #000d, 0 0 40px 6px rgba(var(--neural-accent-rgb, 0, 250, 255), 0.32) inset, 0 0 0 2px var(--neural-accent-color, #00faff);
  border-color: var(--neural-accent-color, #00faff);
  z-index: 3;
  transition: box-shadow 0.25s, border-color 0.25s;
}


/* Classes para o conteúdo do card */
.transcription-card-content {
  min-height: 0;
  min-width: 0;
  display: flex;
  flex-direction: column;
  justify-content: flex-start;
  gap: clamp(0.18rem, 1vw, 0.35rem); /* Gap menor */
  padding: 0;
  margin: clamp(0.1rem, 1vw, 0.25rem) 0 clamp(0.25rem, 2vw, 0.5rem) 0;
  position: relative;
  z-index: 1;
  flex: 1 1 auto;
  min-height: 0;
}

/* Estilo para áreas de texto dentro dos cards */
.transcription-card-content textarea,
.transcription-card-content .text-area-container {
  border: 0.5px solid rgba(var(--neural-accent-rgb), 0.2);
  border-radius: 0.75rem;
  background-color: rgba(24,24,40,0.35);
  transition: border-color 0.2s ease, height 0.2s ease;
  width: 100%; /* Preenche toda a largura disponível */
  flex: 1 1 auto; /* Cresce para preencher espaço disponível */
  min-height: 0; /* Permite encolher quando necessário */
  resize: none; /* Remove resize handle para layout mais limpo */
  display: flex; /* Para conteúdo interno */
  flex-direction: column; /* Organiza conteúdo */
  height: auto !important; /* Força altura automática, substituindo inline styles */
  min-height: min(120px, 25vh) !important; /* Altura mínima responsiva */
  max-height: 100% !important; /* Nunca ultrapassa o container */
}

/* Textarea específico - componente TextEditor */
textarea.orchos-textarea-neural,
.orchos-textarea-neural {
  height: auto !important; /* Força altura automática */
  min-height: min(30vh, 150px) !important; /* Altura mínima responsível */
  max-height: 100% !important; /* Nunca ultrapassa o card */
  flex: 1 1 auto !important; /* Prioriza crescimento */
  overflow-y: auto !important; /* Scroll vertical quando necessário */
  box-sizing: border-box !important;
  padding: clamp(0.75rem, 2.5vw, 1rem) !important;
  font-size: clamp(0.9rem, 1.5vw, 1rem) !important;
  line-height: 1.5 !important;
  color: rgba(255, 255, 255, 0.9) !important;
}

/* Seletores específicos para sobrescrever estilos inline */
textarea[style*="height"],
div[style*="height"] textarea {
  height: auto !important;
  min-height: min(30vh, 150px) !important;
}

/* Para conteúdo maior, adiciona scroll interno ao invés de crescer indefinidamente */
.transcription-card-content textarea[rows],
.transcription-card-content textarea.orchos-textarea-neural {
  overflow-y: auto !important;
  max-height: 100% !important;
}

/* Container para TextEditor */
.flex-1.flex.flex-col.min-h-0 {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  min-height: 0;
  height: 100%;
  width: 100%;
}

.transcription-card-content textarea:focus,
.transcription-card-content .text-area-container:focus-within {
  border-color: var(--neural-accent-color);
  box-shadow: 0 0 8px rgba(var(--neural-accent-rgb), 0.2);
}

/* Estilos para mensagens dentro dos cards */
.message-container {
  padding: clamp(0.5rem, 2.5%, 0.75rem);
  border-radius: 0.5rem;
  background: rgba(0, 0, 0, 0.25);
  margin: clamp(0.25rem, 2%, 0.5rem) 0;
  border-left: 2px solid var(--neural-accent-color, #00faff);
  width: 100%; /* Preenche toda a largura disponível */
  box-sizing: border-box;
  overflow-wrap: break-word; /* Quebra palavras longas */
  word-break: break-word; /* Ajuda em nomes de arquivos */
}

/* Cabeçalho do card */
.transcription-card-header {
  --neural-accent-color: inherit; /* Garante herança da cor do card pai */
  display: flex;
  align-items: center;
  gap: clamp(0.5rem, 2%, 0.75rem); /* Gap responsivo */
  margin-bottom: clamp(0.5rem, 3%, 1rem); /* Margem responsiva */
  min-height: 32px; /* Altura mínima para garantir visibilidade */
  flex-wrap: wrap;
  position: relative;
  z-index: 2; /* Garante que fique acima do conteúdo */
  width: 100%; /* Preenche toda a largura disponível */
}

/* Título do card */
.transcription-card-title {
  color: var(--neural-accent-color, #00faff) !important; /* Força prioridade da cor simbólica */
  font-size: 1.15rem; /* Ligeiramente aumentado */
  font-weight: 600;
  letter-spacing: 0.025rem; /* Aumentado para melhor legibilidade */
  margin: 0;
  padding: 0;
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

/* Ícone do card */
.transcription-card-icon {
  color: var(--neural-accent-color, #00faff) !important;
  fill: var(--neural-accent-color, #00faff) !important;
  stroke: var(--neural-accent-color, #00faff) !important;
  display: flex;
  align-items: center;
  justify-content: center;
  height: 24px; /* Tamanho fixo para consistência */
  width: 24px; /* Tamanho fixo para consistência */
}

/* Ações do cabeçalho do card */
.transcription-card-header-actions {
  display: flex;
  flex-wrap: wrap;
  gap: clamp(0.35rem, 2%, 0.75rem); /* Gap responsivo */
  margin-left: auto;
  align-items: center;
  min-height: 32px; /* Altura mínima consistente */
  justify-content: flex-end; /* Alinha à direita */
}

/* Estilos para tipos específicos de cards */
[data-type="context"] {
  --neural-accent-color: #00faff;
  --neural-accent-rgb: 0, 250, 255;
  border-color: #00faff;
}

[data-type="transcription"] {
  --neural-accent-color: #ff416c;
  --neural-accent-rgb: 255, 65, 108;
  border-color: #ff416c;
}

[data-type="cognition"] {
  --neural-accent-color: #7c4dff;
  --neural-accent-rgb: 124, 77, 255;
  border-color: #7c4dff;
}

[data-type="ai"] {
  --neural-accent-color: #ff80ab;
  --neural-accent-rgb: 255, 128, 171;
  border-color: #ff80ab;
}

/* Rodapé do card */
.orchos-card-footer {
  display: flex;
  align-items: center;
  justify-content: flex-end;
  gap: clamp(0.35rem, 2%, 0.75rem); /* Gap responsivo */
  margin-top: auto; /* Empurra para o final do card */
  padding-top: clamp(0.5rem, 2%, 0.75rem); /* Padding responsivo */
  border-top: 1px solid rgba(var(--neural-accent-rgb), 0.1); /* Separador sutil */
  position: relative;
  z-index: 2;
  width: 100%; /* Preenche toda a largura disponível */
  flex-wrap: wrap; /* Permite quebra em telas menores */
}

/* Estilo para botões dentro dos cards */
.orchos-card button {
  font-family: 'Inter', Arial, sans-serif;
  font-weight: 500;
  min-height: 28px; /* Altura mínima consistente */
  padding: clamp(0.15rem, 1%, 0.25rem) clamp(0.5rem, 2%, 0.75rem); /* Padding responsivo */
  border-radius: 0.5rem; /* Arredondamento consistente */
  transition: all 0.2s ease; /* Transição suave */
  display: inline-flex;
  align-items: center;
  justify-content: center;
  gap: clamp(0.25rem, 1.5%, 0.5rem);
  flex-shrink: 0; /* Evita encolhimento excessivo */
  white-space: nowrap; /* Evita quebra de texto */
  overflow: hidden; /* Esconde overflow */
  text-overflow: ellipsis; /* Mostra elipses (...) quando necessário */
}

/* Corrige o foco em botões para não afetar o card pai */
.orchos-card button:focus {
  outline: 2px solid var(--neural-accent-color, #00faff);
  box-shadow: 0 0 8px rgba(var(--neural-accent-rgb), 0.5);
}

/* Hover state para botões */
.orchos-card button:hover {
  transform: translateY(-1px);
  box-shadow: 0 2px 8px rgba(var(--neural-accent-rgb), 0.3);
}

.transcription-panel-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 2.5rem;
  margin: 0 auto;
  max-width: 1200px;
  align-items: flex-start;
  padding: 0 1.5rem 2.5rem 1.5rem;
}

@media (max-width: 1100px) {
  .transcription-panel-grid {
    grid-template-columns: 1fr;
    gap: 2rem;
    max-width: 700px;
    padding: 0 0.5rem 2rem 0.5rem;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import "./SimpleCard.css";

/**
 * SimpleCard — Implementação concreta de um córtex neural de interface.
 * 
 * Intent simbólico: Neurônio de interface que organiza informação cognitiva em
 * um container espacial com propriedades quânticas de contexto.
 * 
 * Linhagem neural: Interface -> Córtex Visual -> Representação Simbólica
 */
export interface SimpleCardProps {
  /** Title of the card, displayed in the header */
  title: string;
  /** Card content */
  children: React.ReactNode;
  /** Optional debug border */
  debugBorder?: boolean;
  /** Symbolic type for color/glow (context, transcription, cognition, ai) */
  type?: 'context' | 'transcription' | 'cognition' | 'ai';
  /** Optional icon (JSX.Element) to show in header */
  icon?: React.ReactNode;
  /** Optional actions (e.g., buttons) to render in the header, right-aligned */
  headerActions?: React.ReactNode;
  /** Optional footer actions (e.g., buttons) to render at the bottom of the card */
  footerActions?: React.ReactNode;
  /** For backward compatibility with previous APIs */
  defaultOpen?: boolean;
};

/**
 * Implementação concreta do córtex neural para o painel de transcrição.
 * Alinha-se às interfaces de domínio para modularização e reutilização em outros módulos.
 */


const SimpleCard: React.FC<SimpleCardProps> = ({ 
  title, 
  children, 
  debugBorder, 
  type, 
  icon, 
  headerActions = undefined,
  footerActions = undefined
}) => {
  const contentId = `neural-content-${title.replace(/\s+/g, '-').toLowerCase()}`;
  
  // Componente final com estrutura semântica e acessível
  return (
    <div
      className="orchos-card neural-card"
      data-state="expanded"
      data-debugborder={debugBorder ? "true" : undefined}
      data-type={type}
    >
      <div className="transcription-card-header">
        {icon && (
          <span className="transcription-card-icon" aria-hidden="true">
            {icon}
          </span>
        )}
        <span
          className="transcription-card-title"
          title={title}
        >
          {title}
        </span>
        {headerActions && (
          <div className="transcription-card-header-actions ml-auto">
            {headerActions}
          </div>
        )}
      </div>
      
      {/* Conteúdo do card neural */}
      <div 
        className="transcription-card-content" 
        id={contentId}
      >
        {children}
      </div>
      
      {footerActions && (
        <div className="orchos-card-footer">
          {footerActions}
        </div>
      )}
    </div>
  );
};

export default SimpleCard;// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { createContext, ReactNode, useContext, useState } from 'react';
import { SimpleModuleState } from '../../../domain/core/interfaces/components/SimpleModule';

/**
 * Contexto para o estado de colapso/expansão de módulos neurais da interface.
 * 
 * Intent Simbólico: Permite orquestração coordenada de múltiplos estados de colapso
 * nos módulos corticais, possibilitando comunicação neural entre componentes irmãos.
 */
interface SimpleContextState {
  expandedModules: Set<string>;
  toggleModule: (id: string) => void;
  isExpanded: (id: string) => boolean;
  collapseAll: () => void;
  expandAll: (moduleIds: string[]) => void;
}

const SimpleContext = createContext<SimpleContextState | null>(null);

/**
 * Provider para gerenciamento de estado de colapso/expansão entre múltiplos módulos.
 * 
 * Linhagem Neural: Orquestrador Cortical → Adaptação Dinâmica → Interface Visual
 */
export const SimpleProvider: React.FC<{ children: ReactNode }> = ({ children }) => {
  const [expandedModules, setExpandedModules] = useState<Set<string>>(new Set());

  const toggleModule = (id: string) => {
    setExpandedModules(prev => {
      const newSet = new Set(prev);
      if (newSet.has(id)) {
        newSet.delete(id);
      } else {
        newSet.add(id);
      }
      return newSet;
    });
  };

  const isExpanded = (id: string) => expandedModules.has(id);
  
  const collapseAll = () => setExpandedModules(new Set());
  const expandAll = (moduleIds: string[]) => setExpandedModules(new Set(moduleIds));

  return (
    <SimpleContext.Provider value={{ expandedModules, toggleModule, isExpanded, collapseAll, expandAll }}>
      {children}
    </SimpleContext.Provider>
  );
};

/**
 * Hook para utilização do contexto de collapse/expand nos módulos corticais.
 * Retorna funções e estado para sincronizar estados de colapso.
 */
export const useSimple = (): SimpleContextState => {
  const context = useContext(SimpleContext);
  if (!context) {
    throw new Error('useSimple deve ser usado dentro de um SimpleProvider');
  }
  return context;
};

/**
 * Hook para um módulo colapsável individual.
 * Fornece estado e funções para gerenciar colapso/expansão sincronizada.
 */
export const useSimpleModule = (id: string, defaultOpen = false): SimpleModuleState => {
  const context = useContext(SimpleContext);
  const [localOpen, setLocalOpen] = useState(defaultOpen);
  
  // Se não houver contexto, usa state local
  if (!context) {
    return {
      isExpanded: localOpen,
      toggle: () => setLocalOpen(!localOpen)
    };
  }
  
  // Inicializa o módulo como aberto no contexto se for padrão
  if (defaultOpen && !context.isExpanded(id)) {
    context.toggleModule(id);
  }
  
  return {
    isExpanded: context.isExpanded(id),
    toggle: () => context.toggleModule(id)
  };
};import React, { useEffect, useRef } from "react";
import { SelectedDevices } from "../../../../../context";
import AudioSettingsSimple from "../../settings/AudioSettingsSimple";

interface AudioSettingsPopoverProps {
  show: boolean;
  onClose: () => void;
  anchorRef?: React.RefObject<HTMLElement>;
  settings: {
    // Language
    language: string;
    setLanguage: (value: string) => void;

    // Device selection
    isMicrophoneOn: boolean;
    setIsMicrophoneOn: (value: boolean) => void;
    isSystemAudioOn: boolean;
    setIsSystemAudioOn: (value: boolean) => void;
    audioDevices: MediaDeviceInfo[];
    selectedDevices: SelectedDevices;
    handleDeviceChange: (deviceId: string, isSystemAudio: boolean) => void;
  };
}

/**
 * Audio Settings Popover component
 * Shows audio configuration options in a floating panel
 */
export const AudioSettingsPopover: React.FC<AudioSettingsPopoverProps> = ({
  show,
  onClose,
  anchorRef,
  settings,
}) => {
  const popoverRef = useRef<HTMLDivElement>(null);

  // Close popover when clicking outside
  useEffect(() => {
    const handleClickOutside = (event: MouseEvent) => {
      if (
        popoverRef.current &&
        !popoverRef.current.contains(event.target as Node) &&
        anchorRef?.current &&
        !anchorRef.current.contains(event.target as Node)
      ) {
        onClose();
      }
    };

    if (show) {
      document.addEventListener("mousedown", handleClickOutside);
    }

    return () => {
      document.removeEventListener("mousedown", handleClickOutside);
    };
  }, [show, onClose, anchorRef]);

  // Handle escape key
  useEffect(() => {
    const handleEscape = (event: KeyboardEvent) => {
      if (event.key === "Escape") {
        onClose();
      }
    };

    if (show) {
      document.addEventListener("keydown", handleEscape);
    }

    return () => {
      document.removeEventListener("keydown", handleEscape);
    };
  }, [show, onClose]);

  if (!show) return null;

  return (
    <div ref={popoverRef} className="audio-settings-popover-container">
      <div
        className="audio-settings-popover"
        style={{
          backgroundColor: "rgba(17, 24, 39, 0.95)",
          backdropFilter: "blur(20px)",
          border: "1px solid rgba(0, 240, 255, 0.2)",
          borderRadius: "12px",
          boxShadow:
            "0 10px 30px rgba(0, 0, 0, 0.5), 0 0 20px rgba(0, 240, 255, 0.1)",
          padding: "24px",
          width: "360px",
          zIndex: 50,
        }}
      >
        <AudioSettingsSimple
          // Language
          language={settings.language}
          setLanguage={settings.setLanguage}
          // Device selection
          isMicrophoneOn={settings.isMicrophoneOn}
          setIsMicrophoneOn={settings.setIsMicrophoneOn}
          isSystemAudioOn={settings.isSystemAudioOn}
          setIsSystemAudioOn={settings.setIsSystemAudioOn}
          audioDevices={settings.audioDevices}
          selectedDevices={settings.selectedDevices}
          handleDeviceChange={settings.handleDeviceChange}
        />
      </div>
    </div>
  );
};
import React from "react";
import { MicrophoneState } from "../../../../../context";
import { ChatControlsProps } from "../types/ChatTypes";

/**
 * Chat controls component with modern futuristic icons
 * Enhanced with better visual feedback and animations
 */
export const ChatControls: React.FC<ChatControlsProps> = ({
  microphoneState,
  onToggleRecording,
  onSend,
  onToggleContext,
  canSend,
  showContext,
  onToggleAudioSettings,
  showAudioSettings,
  audioSettingsButtonRef,
}) => {
  return (
    <div className="input-controls">
      {/* Context Toggle Button - Modern design */}
      <button
        className={`control-btn context-btn ${showContext ? "active" : ""}`}
        onClick={onToggleContext}
        title={showContext ? "Hide context field" : "Add context"}
        type="button"
      >
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
          <rect
            x="3"
            y="3"
            width="18"
            height="18"
            rx="4"
            stroke="currentColor"
            strokeWidth="1.5"
            strokeLinecap="round"
            strokeLinejoin="round"
          />
          <path
            d="M8 12h8M12 8v8"
            stroke="currentColor"
            strokeWidth="1.5"
            strokeLinecap="round"
          />
          <circle cx="12" cy="12" r="3" fill="currentColor" opacity="0.3" />
        </svg>
      </button>

      {/* Audio Settings Button - Futuristic wave design */}
      <button
        ref={audioSettingsButtonRef as React.RefObject<HTMLButtonElement>}
        className={`control-btn audio-settings-btn ${
          showAudioSettings ? "active" : ""
        }`}
        onClick={onToggleAudioSettings}
        title="Audio settings"
        type="button"
      >
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
          <path d="M3 12h4l4-4v12l-4-4H3z" fill="currentColor" opacity="0.8" />
          <path
            d="M14 8c1.5 1 2.5 2.5 2.5 4s-1 3-2.5 4"
            stroke="currentColor"
            strokeWidth="1.5"
            strokeLinecap="round"
          />
          <path
            d="M17 5c2.5 1.5 4 4 4 7s-1.5 5.5-4 7"
            stroke="currentColor"
            strokeWidth="1.5"
            strokeLinecap="round"
            opacity="0.6"
          />
        </svg>
      </button>

      {/* Microphone Button - Modern minimal design */}
      <button
        className={`control-btn mic-btn ${
          microphoneState === MicrophoneState.Open ? "recording" : ""
        }`}
        onClick={onToggleRecording}
        title={
          microphoneState === MicrophoneState.Open
            ? "Stop recording"
            : "Start recording"
        }
        type="button"
      >
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
          {microphoneState === MicrophoneState.Open ? (
            // Recording state - animated square
            <g>
              <rect x="8" y="8" width="8" height="8" fill="currentColor" rx="2">
                <animate
                  attributeName="rx"
                  values="2;4;2"
                  dur="1.5s"
                  repeatCount="indefinite"
                />
              </rect>
              <circle
                cx="12"
                cy="12"
                r="9"
                stroke="currentColor"
                strokeWidth="1.5"
                opacity="0.3"
              />
            </g>
          ) : (
            // Microphone icon - sleek design
            <>
              <rect
                x="9"
                y="3"
                width="6"
                height="11"
                rx="3"
                fill="currentColor"
              />
              <path
                d="M5 10v2a7 7 0 0014 0v-2"
                stroke="currentColor"
                strokeWidth="1.5"
                strokeLinecap="round"
              />
              <line
                x1="12"
                y1="19"
                x2="12"
                y2="22"
                stroke="currentColor"
                strokeWidth="1.5"
              />
              <line
                x1="8"
                y1="22"
                x2="16"
                y2="22"
                stroke="currentColor"
                strokeWidth="1.5"
                strokeLinecap="round"
              />
            </>
          )}
        </svg>
      </button>

      {/* Send Button - Modern arrow design */}
      <button
        className={`control-btn send-btn ${canSend ? "ready" : "disabled"}`}
        onClick={onSend}
        disabled={!canSend}
        title="Send message (Enter)"
        type="button"
      >
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
          <path
            d="M3 12L5 4l16 8-16 8 2-8zm2 0h7"
            stroke="currentColor"
            strokeWidth="1.5"
            strokeLinecap="round"
            strokeLinejoin="round"
            fill="none"
          />
          <circle cx="12" cy="12" r="2" fill="currentColor" opacity="0.3" />
        </svg>
      </button>
    </div>
  );
};
import React, { useState } from "react";
import { ChatConversation } from "../types/ChatHistoryTypes";

interface ChatHistorySidebarProps {
  conversations: ChatConversation[];
  currentConversationId: string | null;
  onSelectConversation: (id: string) => void;
  onCreateNewConversation: () => void;
  onDeleteConversation: (id: string) => void;
  onSearchConversations: (query: string) => ChatConversation[];
  isProcessing?: boolean; // Disable new conversation while AI is processing
}

export const ChatHistorySidebar: React.FC<ChatHistorySidebarProps> = ({
  conversations,
  currentConversationId,
  onSelectConversation,
  onCreateNewConversation,
  onDeleteConversation,
  onSearchConversations,
  isProcessing = false,
}) => {
  const [searchQuery, setSearchQuery] = useState("");
  const [hoveredId, setHoveredId] = useState<string | null>(null);

  // Filter conversations based on search
  const displayedConversations = searchQuery
    ? onSearchConversations(searchQuery)
    : conversations;

  // Format date for display
  const formatDate = (date: Date): string => {
    const now = new Date();
    const diffMs = now.getTime() - date.getTime();
    const diffMins = Math.floor(diffMs / 60000);
    const diffHours = Math.floor(diffMs / 3600000);
    const diffDays = Math.floor(diffMs / 86400000);

    if (diffMins < 1) return "Agora";
    if (diffMins < 60) return `${diffMins}m atrás`;
    if (diffHours < 24) return `${diffHours}h atrás`;
    if (diffDays < 7) return `${diffDays}d atrás`;

    return date.toLocaleDateString("pt-BR", {
      day: "2-digit",
      month: "short",
    });
  };

  return (
    <div className="chat-history-sidebar">
      {/* Header */}
      <div className="sidebar-header">
        <h2 className="sidebar-title">Conversas</h2>
        <button
          className={`new-chat-button ${isProcessing ? "disabled" : ""}`}
          onClick={onCreateNewConversation}
          disabled={isProcessing}
          title={
            isProcessing ? "Aguarde o processamento terminar" : "Nova conversa"
          }
        >
          <svg width="20" height="20" viewBox="0 0 20 20" fill="none">
            <path
              d="M10 4v12m6-6H4"
              stroke="currentColor"
              strokeWidth="2"
              strokeLinecap="round"
            />
          </svg>
        </button>
      </div>

      {/* Search */}
      <div className="sidebar-search">
        <input
          type="text"
          placeholder="Buscar conversas..."
          value={searchQuery}
          onChange={(e) => setSearchQuery(e.target.value)}
          className="search-input"
        />
        <svg
          className="search-icon"
          width="16"
          height="16"
          viewBox="0 0 16 16"
          fill="none"
        >
          <circle cx="7" cy="7" r="5" stroke="currentColor" strokeWidth="1.5" />
          <path
            d="M11 11l3 3"
            stroke="currentColor"
            strokeWidth="1.5"
            strokeLinecap="round"
          />
        </svg>
      </div>

      {/* Conversations List */}
      <div className="conversations-list">
        {displayedConversations.length === 0 ? (
          <div className="no-conversations">
            {searchQuery
              ? "Nenhuma conversa encontrada"
              : "Nenhuma conversa ainda"}
          </div>
        ) : (
          displayedConversations.map((conv) => (
            <div
              key={conv.id}
              className={`conversation-item ${
                conv.id === currentConversationId ? "active" : ""
              }`}
              onClick={() => onSelectConversation(conv.id)}
              onMouseEnter={() => setHoveredId(conv.id)}
              onMouseLeave={() => setHoveredId(null)}
            >
              <div className="conversation-content">
                <h3 className="conversation-title">{conv.title}</h3>
                <p className="conversation-preview">
                  {conv.lastMessage || "Conversa vazia"}
                </p>
                <span className="conversation-time">
                  {formatDate(conv.lastMessageTime)}
                </span>
              </div>

              {/* Delete button */}
              {hoveredId === conv.id && (
                <button
                  className="delete-button"
                  onClick={(e) => {
                    e.stopPropagation();
                    if (window.confirm("Excluir esta conversa?")) {
                      onDeleteConversation(conv.id);
                    }
                  }}
                  title="Excluir conversa"
                >
                  <svg width="16" height="16" viewBox="0 0 16 16" fill="none">
                    <path
                      d="M4 4l8 8m0-8l-8 8"
                      stroke="currentColor"
                      strokeWidth="1.5"
                      strokeLinecap="round"
                    />
                  </svg>
                </button>
              )}
            </div>
          ))
        )}
      </div>
    </div>
  );
};
import React from "react";
import { useDeepgram } from "../../../../../context";
import { ChatState, ConversationalChatProps } from "../types/ChatTypes";
import { ChatControls } from "./ChatControls";
import { ContextInput } from "./ContextInput";
// import { DebugControls } from "./DebugControls"; // Removed - debug controls disabled
import { MessageInput } from "./MessageInput";
import { TranscriptionDisplay } from "./TranscriptionDisplay";

interface ChatInputAreaProps extends ConversationalChatProps {
  chatState: ChatState;
  onSendMessage: () => void;
  onKeyPress: (e: React.KeyboardEvent) => void;
  onToggleContext: () => void;
  onAddTestMessage: () => void;
  onAddTestAI: () => void;
  onRestore: () => void;
  onClearAll: () => void;
  hasBackup: boolean;
  onToggleAudioSettings?: () => void;
  showAudioSettings?: boolean;
  audioSettingsButtonRef?: React.RefObject<HTMLElement>;
}

/**
 * Chat input area component
 * Follows composition pattern - combines input-related components
 * Debug controls removed for cleaner UI
 */
export const ChatInputArea: React.FC<ChatInputAreaProps> = ({
  transcriptionText,
  onClearTranscription,
  microphoneState,
  onToggleRecording,
  chatState,
  onSendMessage,
  onKeyPress,
  onToggleContext,
  onAddTestMessage,
  onAddTestAI,
  onRestore,
  onClearAll,
  hasBackup,
  onToggleAudioSettings,
  showAudioSettings,
  audioSettingsButtonRef,
}) => {
  const canSend =
    !!(chatState.inputMessage.trim() || transcriptionText.trim()) &&
    !chatState.isProcessing;

  // Get transcriptions with status from Deepgram context
  const { getAllTranscriptionsWithStatus } = useDeepgram();
  const transcriptionsWithStatus = getAllTranscriptionsWithStatus
    ? getAllTranscriptionsWithStatus()
    : [];

  return (
    <div className="chat-input-area">
      {/* Context Input */}
      <ContextInput
        value={chatState.currentContext}
        onChange={chatState.setCurrentContext}
        onClose={() => {
          chatState.setCurrentContext("");
          chatState.setShowContextField(false);
        }}
        show={chatState.showContextField}
      />

      {/* Main Input Area */}
      <div className="main-input-wrapper">
        <div className="input-row">
          {/* Transcription Display */}
          <TranscriptionDisplay
            text={transcriptionText}
            onClear={onClearTranscription}
            transcriptions={transcriptionsWithStatus}
          />

          {/* Input Bottom Row - Input + Controls */}
          <div className="input-bottom-row">
            {/* Message Input */}
            <MessageInput
              value={chatState.inputMessage}
              onChange={chatState.setInputMessage}
              onSend={onSendMessage}
              onKeyPress={onKeyPress}
              placeholder="Type your message or use voice transcription..."
            />

            {/* Main Chat Controls - Debug controls removed */}
            <ChatControls
              microphoneState={microphoneState}
              onToggleRecording={onToggleRecording}
              onSend={onSendMessage}
              onToggleContext={onToggleContext}
              canSend={canSend}
              showContext={
                chatState.showContextField || !!chatState.currentContext
              }
              onToggleAudioSettings={onToggleAudioSettings}
              showAudioSettings={showAudioSettings}
              audioSettingsButtonRef={audioSettingsButtonRef}
            />
          </div>
        </div>
      </div>

      {/* TODO: Add AudioSettingsPopover component here
      <AudioSettingsPopover
        show={showAudioSettings}
        onClose={() => setShowAudioSettings(false)}
        anchorRef={audioSettingsButtonRef}
        settings={audioSettings}
      />
      */}
    </div>
  );
};
import React from "react";
import { ChatMessage, ChatMessagesContainerProps } from "../types/ChatTypes";
import { ScrollToBottomButton } from "./ScrollToBottomButton";

// Memoized message component for performance
const MessageItem: React.FC<{ message: ChatMessage }> = React.memo(
  ({ message }) => {
    // Generate unique IDs for gradients to avoid conflicts
    const messageId = message.id || Math.random().toString(36).substr(2, 9);
    const gradientId = `gradient-${message.type}-${messageId}`;

    return (
      <div className={`message ${message.type}-message`}>
        <div className="message-avatar">
          {message.type === "user" ? (
            <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
              <defs>
                <linearGradient
                  id={gradientId}
                  x1="0%"
                  y1="0%"
                  x2="100%"
                  y2="100%"
                >
                  <stop offset="0%" stopColor="#00faff" />
                  <stop offset="100%" stopColor="#0066cc" />
                </linearGradient>
              </defs>
              <circle cx="12" cy="12" r="10" fill={`url(#${gradientId})`} />
              {/* Bonequinho centralizado - cabeça e corpo */}
              <path
                d="M12 10c1.66 0 3-1.34 3-3s-1.34-3-3-3-3 1.34-3 3 1.34 3 3 3zm0 1c-2 0-6 1-6 3v1.5c0 0.28 0.22 0.5 0.5 0.5h11c0.28 0 0.5-0.22 0.5-0.5V14c0-2-4-3-6-3z"
                fill="white"
              />
            </svg>
          ) : (
            <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
              <defs>
                <linearGradient
                  id={gradientId}
                  x1="0%"
                  y1="0%"
                  x2="100%"
                  y2="100%"
                >
                  <stop offset="0%" stopColor="#ff4dd2" />
                  <stop offset="100%" stopColor="#7c4dff" />
                </linearGradient>
              </defs>
              <circle cx="12" cy="12" r="10" fill={`url(#${gradientId})`} />
              {/* AI brain icon - melhor centralizado */}
              <rect
                x="8.5"
                y="8.5"
                width="7"
                height="7"
                rx="1.5"
                fill="white"
              />
              <circle cx="12" cy="12" r="1.5" fill={`url(#${gradientId})`} />
            </svg>
          )}
        </div>

        <div className="message-bubble-wrapper">
          <div className="message-content">
            {message.hasContext && message.contextContent && (
              <div className="message-context">
                <div className="context-label">Context:</div>
                <div className="context-content">{message.contextContent}</div>
              </div>
            )}
            <div className="message-text">{message.content}</div>
          </div>

          {/* Timestamp fora do bubble, similar ao WhatsApp */}
          <div className="message-timestamp">
            {message.timestamp.toLocaleTimeString([], {
              hour: "2-digit",
              minute: "2-digit",
            })}
          </div>
        </div>
      </div>
    );
  }
);

// Typing indicator component
const TypingIndicator: React.FC = React.memo(() => {
  const gradientId = `gradient-typing-${Date.now()}`;

  return (
    <div className="message system-message typing-indicator">
      <div className="message-avatar">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
          <defs>
            <linearGradient id={gradientId} x1="0%" y1="0%" x2="100%" y2="100%">
              <stop offset="0%" stopColor="#ff4dd2" />
              <stop offset="100%" stopColor="#7c4dff" />
            </linearGradient>
          </defs>
          <circle cx="12" cy="12" r="10" fill={`url(#${gradientId})`} />
          {/* AI brain icon - melhor centralizado */}
          <rect x="8.5" y="8.5" width="7" height="7" rx="1.5" fill="white" />
          <circle cx="12" cy="12" r="1.5" fill={`url(#${gradientId})`} />
        </svg>
      </div>
      <div className="message-bubble-wrapper">
        <div className="message-content">
          <div className="message-text typing-animation">
            <span></span>
            <span></span>
            <span></span>
          </div>
        </div>
      </div>
    </div>
  );
});

// Welcome message component
const WelcomeMessage: React.FC<{
  onAddTestMessage: () => void;
  onResetState: () => void;
  onClearMessages: () => void;
}> = React.memo(({ onAddTestMessage, onResetState, onClearMessages }) => {
  const gradientId = `gradient-welcome-${Date.now()}`;

  return (
    <div className="welcome-message">
      <div className="welcome-icon">
        <svg width="48" height="48" viewBox="0 0 24 24" fill="none">
          <defs>
            <linearGradient id={gradientId} x1="0%" y1="0%" x2="100%" y2="100%">
              <stop offset="0%" stopColor="#00faff" />
              <stop offset="100%" stopColor="#7c4dff" />
            </linearGradient>
          </defs>
          <circle
            cx="12"
            cy="12"
            r="10"
            stroke={`url(#${gradientId})`}
            strokeWidth="2"
          />
          <path
            d="M8 12l2 2 4-4"
            stroke={`url(#${gradientId})`}
            strokeWidth="2"
            strokeLinecap="round"
            strokeLinejoin="round"
          />
        </svg>
      </div>
      <h3>Welcome to Orch-OS Neural Chat</h3>
      <p>
        Start a conversation by typing a message or using voice transcription.
      </p>

      {/* Debug buttons removed for cleaner UI */}
    </div>
  );
});

/**
 * Chat messages container component
 * Follows Single Responsibility Principle - only handles message display and scroll
 */
export const ChatMessagesContainer: React.FC<ChatMessagesContainerProps> = ({
  messages,
  isProcessing,
  onScrollChange,
  scrollRef,
  showScrollButton,
  onScrollToBottom,
  onAddTestMessage = () => {},
  onResetState = () => {},
  onClearMessages = () => {},
}) => {
  const handleScroll = () => {
    if (scrollRef.current) {
      const { scrollTop, scrollHeight, clientHeight } = scrollRef.current;
      const distanceFromBottom = scrollHeight - scrollTop - clientHeight;
      // Tolerância mínima de 2 pixels, consistente com useChatScroll
      const isNearBottom = distanceFromBottom <= 2;
      onScrollChange(isNearBottom);
    }
  };

  return (
    <div className="chat-messages-container">
      <div className="chat-messages" ref={scrollRef} onScroll={handleScroll}>
        <div className="messages-wrapper">
          {messages.map((message) => (
            <MessageItem key={message.id} message={message} />
          ))}

          {isProcessing && <TypingIndicator />}

          {messages.length === 0 && !isProcessing && (
            <WelcomeMessage
              onAddTestMessage={onAddTestMessage}
              onResetState={onResetState}
              onClearMessages={onClearMessages}
            />
          )}

          {/* Hidden div for scroll to bottom */}
          <div style={{ float: "left", clear: "both" }} />
        </div>
      </div>

      {/* Scroll to bottom button */}
      <ScrollToBottomButton
        show={showScrollButton}
        onClick={onScrollToBottom}
      />
    </div>
  );
};
import React from "react";
import { ContextInputProps } from "../types/ChatTypes";

/**
 * Context input component
 * Follows Single Responsibility Principle - only handles context input
 */
export const ContextInput: React.FC<ContextInputProps> = ({
  value,
  onChange,
  onClose,
  show,
}) => {
  if (!show && !value) return null;

  const handleChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    onChange(e.target.value);
  };

  return (
    <div className="context-input-wrapper">
      <div className="context-label">
        <svg width="16" height="16" viewBox="0 0 20 20" fill="none">
          <circle cx="10" cy="10" r="8" stroke="currentColor" strokeWidth="2" />
          <path
            d="M10 5v5l3 3"
            stroke="currentColor"
            strokeWidth="2"
            strokeLinecap="round"
          />
        </svg>
        Context (will be included with next message)
        <button
          className="context-close-btn"
          onClick={onClose}
          title="Remove context"
          type="button"
        >
          ×
        </button>
      </div>
      <textarea
        className="context-input"
        value={value}
        onChange={handleChange}
        placeholder="Add situational context..."
        rows={2}
        autoFocus={show}
      />
    </div>
  );
};
import React, { useEffect, useRef, useState } from "react";
import { ChatInputProps } from "../types/ChatTypes";

/**
 * Message input component
 * Follows Single Responsibility Principle - only handles message input
 */
export const MessageInput: React.FC<ChatInputProps> = ({
  value,
  onChange,
  onSend,
  onKeyPress,
  disabled = false,
  placeholder = "Type your message or use voice transcription...",
}) => {
  const inputRef = useRef<HTMLTextAreaElement>(null);
  const [isTyping, setIsTyping] = useState(false);
  const typingTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // Auto-resize textarea (KISS principle - simple and focused)
  useEffect(() => {
    if (inputRef.current) {
      inputRef.current.style.height = "44px";
      inputRef.current.style.height = `${inputRef.current.scrollHeight}px`;
    }
  }, [value]);

  // Handle typing indicator
  useEffect(() => {
    if (value.trim().length > 0) {
      setIsTyping(true);

      // Clear existing timeout
      if (typingTimeoutRef.current) {
        clearTimeout(typingTimeoutRef.current);
      }

      // Set new timeout to remove typing indicator
      typingTimeoutRef.current = setTimeout(() => {
        setIsTyping(false);
      }, 1000);
    } else {
      setIsTyping(false);
    }

    return () => {
      if (typingTimeoutRef.current) {
        clearTimeout(typingTimeoutRef.current);
      }
    };
  }, [value]);

  const handleChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    onChange(e.target.value);
  };

  return (
    <div className={`message-input-wrapper ${isTyping ? "typing" : ""}`}>
      <textarea
        ref={inputRef}
        className="message-input"
        value={value}
        onChange={handleChange}
        onKeyPress={onKeyPress}
        placeholder={placeholder}
        disabled={disabled}
        rows={1}
        style={{
          minHeight: "44px",
          maxHeight: "120px",
          resize: "none",
          overflow: "auto",
        }}
      />
    </div>
  );
};
import React, { useEffect, useRef } from "react";

interface ScrollToBottomButtonProps {
  show: boolean;
  onClick: () => void;
}

/**
 * Scroll to bottom button component
 * Follows KISS principle - simple button with single purpose
 */
export const ScrollToBottomButton: React.FC<ScrollToBottomButtonProps> = ({
  show,
  onClick,
}) => {
  const buttonRef = useRef<HTMLButtonElement>(null);

  useEffect(() => {
    const button = buttonRef.current;
    if (!button) return;

    // Handle wheel events on the button
    const handleWheel = (e: WheelEvent) => {
      // Prevent default to avoid any conflicts
      e.preventDefault();

      // Find the chat messages container
      const chatMessages = document.querySelector(".chat-messages");
      if (chatMessages) {
        // Manually scroll the chat container
        chatMessages.scrollTop += e.deltaY;
      }
    };

    // Add wheel event listener
    button.addEventListener("wheel", handleWheel, { passive: false });

    return () => {
      button.removeEventListener("wheel", handleWheel);
    };
  }, []);

  return (
    <button
      ref={buttonRef}
      className={`scroll-to-bottom-btn ${show ? "visible" : ""}`}
      onClick={onClick}
      aria-label="Scroll to bottom"
      type="button"
    >
      <svg
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
        xmlns="http://www.w3.org/2000/svg"
      >
        <defs>
          <linearGradient
            id="scrollGradient"
            x1="0%"
            y1="0%"
            x2="100%"
            y2="100%"
          >
            <stop offset="0%" stopColor="#ffffff" stopOpacity="0.9" />
            <stop offset="100%" stopColor="#ffffff" stopOpacity="0.7" />
          </linearGradient>
        </defs>
        <path
          d="M12 4v12m0 0l-6-6m6 6l6-6"
          stroke="url(#scrollGradient)"
          strokeWidth="2.5"
          strokeLinecap="round"
          strokeLinejoin="round"
        />
        <path
          d="M5 20h14"
          stroke="url(#scrollGradient)"
          strokeWidth="2.5"
          strokeLinecap="round"
        />
      </svg>
    </button>
  );
};
import React from "react";
import { useAutoScroll } from "../hooks/useAutoScroll";
import { TranscriptionDisplayProps } from "../types/ChatTypes";

/**
 * Transcription display component
 * Follows Single Responsibility Principle - only displays transcription
 */
export const TranscriptionDisplay: React.FC<TranscriptionDisplayProps> = ({
  text,
  onClear,
  transcriptions,
}) => {
  // Auto-scroll hook - observa mudanças em transcriptions ou text
  const scrollRef = useAutoScroll<HTMLDivElement>([transcriptions, text], {
    behavior: "smooth",
  });

  // Check if we should show the component
  const hasContent = transcriptions
    ? transcriptions.some((t) => !t.sent)
    : text && text.trim().length > 0;

  // Don't render anything if there's no content
  if (!hasContent) return null;

  // If we have structured transcriptions with sent status, use that
  if (transcriptions && transcriptions.length > 0) {
    // Concatenate all pending (not sent) transcriptions into a single string
    const pendingTranscriptions = transcriptions
      .filter((t) => !t.sent)
      .map((t) => t.text)
      .join(" "); // Join with space instead of newline for continuous text

    // If no pending transcriptions, don't show the component
    if (!pendingTranscriptions) return null;

    return (
      <div className="transcription-display">
        <div className="transcription-label">
          <svg width="16" height="16" viewBox="0 0 24 24" fill="none">
            <circle
              cx="12"
              cy="12"
              r="3"
              fill="currentColor"
              className="pulse-dot"
            />
            <circle
              cx="12"
              cy="12"
              r="8"
              stroke="currentColor"
              strokeWidth="1.5"
              opacity="0.5"
            />
            <circle
              cx="12"
              cy="12"
              r="11"
              stroke="currentColor"
              strokeWidth="1"
              opacity="0.3"
            />
            <path
              d="M12 3v2M12 19v2M3 12h2M19 12h2"
              stroke="currentColor"
              strokeWidth="1.5"
              opacity="0.4"
            />
          </svg>
          Live Transcription:
        </div>
        <div
          className="transcription-text transcription-scrollable"
          ref={scrollRef}
        >
          {pendingTranscriptions}
        </div>
        <button
          className="transcription-clear-btn"
          onClick={onClear}
          title="Clear transcription"
          type="button"
        >
          ×
        </button>
      </div>
    );
  }

  // Fallback to simple text display
  return (
    <div className="transcription-display">
      <div className="transcription-label">
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none">
          <circle
            cx="12"
            cy="12"
            r="3"
            fill="currentColor"
            className="pulse-dot"
          />
          <circle
            cx="12"
            cy="12"
            r="8"
            stroke="currentColor"
            strokeWidth="1.5"
            opacity="0.5"
          />
          <circle
            cx="12"
            cy="12"
            r="11"
            stroke="currentColor"
            strokeWidth="1"
            opacity="0.3"
          />
          <path
            d="M12 3v2M12 19v2M3 12h2M19 12h2"
            stroke="currentColor"
            strokeWidth="1.5"
            opacity="0.4"
          />
        </svg>
        Live Transcription:
      </div>
      <div
        className="transcription-text transcription-scrollable"
        ref={scrollRef}
      >
        {text}
      </div>
      <button
        className="transcription-clear-btn"
        onClick={onClear}
        title="Clear transcription"
        type="button"
      >
        ×
      </button>
    </div>
  );
};
import { MutableRefObject, useEffect, useRef } from "react";

/**
 * Custom hook for auto-scrolling to bottom when content changes
 * Based on chat scroll pattern from https://davelage.com/posts/chat-scroll-react/
 *
 * @param deps - Dependencies that trigger scroll when changed
 * @param options - Scroll behavior options
 * @returns ref to attach to the scrollable element
 */
export function useAutoScroll<T extends HTMLElement>(
  deps: any[],
  options: ScrollToOptions = { behavior: "smooth" }
): MutableRefObject<T | null> {
  const ref = useRef<T>(null);

  useEffect(() => {
    if (ref.current) {
      // Small delay to ensure DOM is updated
      const scrollTimer = setTimeout(() => {
        if (ref.current) {
          ref.current.scrollTo({
            top: ref.current.scrollHeight,
            left: 0,
            ...options,
          });
        }
      }, 50);

      return () => clearTimeout(scrollTimer);
    }
  }, deps);

  return ref;
}

/**
 * Alternative version that uses scrollIntoView
 * Useful when you want to scroll a specific element into view
 */
export function useScrollIntoView<T extends HTMLElement>(
  deps: any[],
  alignToTop: boolean = false
): MutableRefObject<T | null> {
  const ref = useRef<T>(null);

  useEffect(() => {
    if (ref.current) {
      const scrollTimer = setTimeout(() => {
        if (ref.current) {
          ref.current.scrollIntoView(alignToTop);
        }
      }, 50);

      return () => clearTimeout(scrollTimer);
    }
  }, deps);

  return ref;
}
import { nanoid } from "nanoid";
import { useCallback, useEffect, useState } from "react";
import {
  ChatConversation,
  UseChatHistoryReturn,
} from "../types/ChatHistoryTypes";
import { migrateOldChatMessages } from "../utils/chatHistoryMigration";
import { ChatMessage } from "./usePersistentMessages";

const STORAGE_KEY = "orch-chat-history";
const MAX_TITLE_LENGTH = 50;

// Helper to generate a title from the first message
const generateTitleFromMessage = (message: string): string => {
  const cleaned = message.trim();
  if (cleaned.length <= MAX_TITLE_LENGTH) return cleaned;
  return cleaned.substring(0, MAX_TITLE_LENGTH) + "...";
};

// Helper to safely load from localStorage
const loadFromStorage = (): {
  conversations: ChatConversation[];
  currentId: string | null;
} => {
  try {
    const data = localStorage.getItem(STORAGE_KEY);
    if (data) {
      const parsed = JSON.parse(data);
      // Convert date strings back to Date objects
      const conversations = parsed.conversations.map((conv: any) => ({
        ...conv,
        lastMessageTime: new Date(conv.lastMessageTime),
        createdAt: new Date(conv.createdAt),
        messages: conv.messages.map((msg: any) => ({
          ...msg,
          timestamp: new Date(msg.timestamp),
        })),
      }));
      return { conversations, currentId: parsed.currentId };
    }
  } catch (error) {
    console.error("Error loading chat history:", error);
  }
  return { conversations: [], currentId: null };
};

// Helper to save to localStorage
const saveToStorage = (
  conversations: ChatConversation[],
  currentId: string | null
) => {
  try {
    console.log("[CHAT_HISTORY] Saving to storage:", {
      conversationCount: conversations.length,
      currentId,
      firstConvId: conversations[0]?.id,
    });
    const data = JSON.stringify({ conversations, currentId });
    localStorage.setItem(STORAGE_KEY, data);
  } catch (error) {
    console.error("Error saving chat history:", error);
  }
};

export const useChatHistory = (): UseChatHistoryReturn => {
  const [conversations, setConversations] = useState<ChatConversation[]>([]);
  const [currentConversationId, setCurrentConversationId] = useState<
    string | null
  >(null);

  // Helper to create a new conversation object - moved up before useEffect
  const createNewConversationObject = (): ChatConversation => {
    const now = new Date();
    return {
      id: nanoid(),
      title: "Nova Conversa",
      lastMessage: "",
      lastMessageTime: now,
      createdAt: now,
      messages: [],
      isActive: true,
    };
  };

  // Load from storage on mount
  useEffect(() => {
    // Try to migrate old messages first
    const migrationPerformed = migrateOldChatMessages();
    if (migrationPerformed) {
      console.log("[CHAT_HISTORY] Migration completed, reloading data...");
    }

    const { conversations: loadedConvs, currentId } = loadFromStorage();
    if (loadedConvs.length > 0) {
      setConversations(loadedConvs);
      setCurrentConversationId(currentId || loadedConvs[0].id);
    } else {
      // Create initial conversation if none exist
      const initialConv = createNewConversationObject();
      setConversations([initialConv]);
      setCurrentConversationId(initialConv.id);
    }
  }, []);

  // Save to storage whenever conversations or currentId changes
  useEffect(() => {
    if (conversations.length > 0) {
      saveToStorage(conversations, currentConversationId);
    }
  }, [conversations, currentConversationId]);

  // Create a new conversation
  const createNewConversation = useCallback((): string => {
    const newConv = createNewConversationObject();
    console.log("[CHAT_HISTORY] Creating new conversation:", newConv.id);
    setConversations((prev) => {
      // Mark all previous conversations as inactive
      const updatedPrev = prev.map((conv) => ({
        ...conv,
        isActive: false,
      }));
      return [newConv, ...updatedPrev];
    });
    setCurrentConversationId(newConv.id);
    return newConv.id;
  }, []);

  // Select a conversation
  const selectConversation = useCallback((id: string) => {
    console.log("[CHAT_HISTORY] Selecting conversation:", id);
    // Update isActive status for all conversations
    setConversations((prev) =>
      prev.map((conv) => ({
        ...conv,
        isActive: conv.id === id,
      }))
    );
    setCurrentConversationId(id);
  }, []);

  // Delete a conversation
  const deleteConversation = useCallback(
    (id: string) => {
      console.log("[CHAT_HISTORY] Deleting conversation:", id);
      setConversations((prev) => {
        const filtered = prev.filter((conv) => conv.id !== id);

        // If we're deleting the current conversation, switch to another
        if (currentConversationId === id && filtered.length > 0) {
          console.log(
            "[CHAT_HISTORY] Switching to conversation:",
            filtered[0].id
          );
          setCurrentConversationId(filtered[0].id);
        } else if (filtered.length === 0) {
          // If no conversations left, create a new one
          const newConv = createNewConversationObject();
          console.log(
            "[CHAT_HISTORY] No conversations left, creating new:",
            newConv.id
          );
          setCurrentConversationId(newConv.id);
          return [newConv];
        }

        return filtered;
      });
    },
    [currentConversationId]
  );

  // Update conversation title
  const updateConversationTitle = useCallback((id: string, title: string) => {
    setConversations((prev) =>
      prev.map((conv) => (conv.id === id ? { ...conv, title } : conv))
    );
  }, []);

  // Add message to a conversation
  const addMessageToConversation = useCallback(
    (conversationId: string, message: ChatMessage) => {
      setConversations((prev) =>
        prev.map((conv) => {
          if (conv.id === conversationId) {
            const updatedMessages = [...conv.messages, message];
            const isFirstUserMessage =
              message.type === "user" &&
              conv.title === "Nova Conversa" &&
              updatedMessages.filter((m) => m.type === "user").length === 1;

            return {
              ...conv,
              messages: updatedMessages,
              lastMessage: message.content,
              lastMessageTime: message.timestamp,
              // Auto-generate title from first user message
              title: isFirstUserMessage
                ? generateTitleFromMessage(message.content)
                : conv.title,
            };
          }
          return conv;
        })
      );
    },
    []
  );

  // Search conversations
  const searchConversations = useCallback(
    (query: string): ChatConversation[] => {
      const lowerQuery = query.toLowerCase();
      return conversations.filter(
        (conv) =>
          conv.title.toLowerCase().includes(lowerQuery) ||
          conv.messages.some((msg) =>
            msg.content.toLowerCase().includes(lowerQuery)
          )
      );
    },
    [conversations]
  );

  // Clear messages from a conversation
  const clearConversationMessages = useCallback((conversationId: string) => {
    setConversations((prev) =>
      prev.map((conv) => {
        if (conv.id === conversationId) {
          return {
            ...conv,
            messages: [],
            lastMessage: "",
            lastMessageTime: new Date(),
          };
        }
        return conv;
      })
    );
  }, []);

  // Get current conversation
  const currentConversation =
    conversations.find((conv) => conv.id === currentConversationId) || null;

  return {
    conversations,
    currentConversation,
    currentConversationId,
    createNewConversation,
    selectConversation,
    deleteConversation,
    updateConversationTitle,
    addMessageToConversation,
    searchConversations,
    clearConversationMessages,
  };
};
import { useCallback, useEffect, useRef, useState } from "react";
import { ChatMessage } from "../types/ChatTypes";

interface UseChatScrollProps {
  messages: ChatMessage[];
  messagesContainerRef: React.RefObject<HTMLDivElement>;
}

/**
 * Custom hook to manage chat scroll behavior
 * Follows SOLID principle - Single responsibility for scroll management
 */
export const useChatScroll = ({
  messages,
  messagesContainerRef,
}: UseChatScrollProps) => {
  const isUserScrollingRef = useRef(false);
  const programmaticScrollRef = useRef(false);
  const userScrollTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const hasInitialScrolledRef = useRef(false);

  // State to track if scroll button should be shown
  const [showScrollButton, setShowScrollButton] = useState(false);

  /**
   * Scroll to bottom of messages
   * DRY principle - reusable scroll logic
   */
  const scrollToBottom = useCallback(
    (smooth = true) => {
      if (!messagesContainerRef.current) return;

      // Set programmatic scroll flag
      programmaticScrollRef.current = true;

      // Scroll the messages container to absolute bottom
      const container = messagesContainerRef.current;
      container.scrollTop = container.scrollHeight;

      // Reset programmatic scroll flag after animation
      setTimeout(
        () => {
          programmaticScrollRef.current = false;
        },
        smooth ? 500 : 100
      );
    },
    [messagesContainerRef]
  );

  /**
   * Check if user is at bottom of scroll
   * KISS principle - simple bottom detection
   */
  const isAtBottom = useCallback(() => {
    if (!messagesContainerRef.current) return true;

    const { scrollTop, scrollHeight, clientHeight } =
      messagesContainerRef.current;
    // Reduced threshold for more accurate detection
    return scrollHeight - scrollTop - clientHeight < 5;
  }, [messagesContainerRef]);

  /**
   * Handle scroll events to detect manual scrolling
   */
  const handleScroll = useCallback(() => {
    if (!messagesContainerRef.current) return;

    // Check if at bottom and update button visibility
    const atBottom = isAtBottom();
    setShowScrollButton(!atBottom);

    // Skip if this is a programmatic scroll
    if (programmaticScrollRef.current) return;

    if (!atBottom && !isUserScrollingRef.current) {
      // User started scrolling up
      isUserScrollingRef.current = true;

      // Clear any existing timeout
      if (userScrollTimeoutRef.current) {
        clearTimeout(userScrollTimeoutRef.current);
      }

      // Set timeout to re-enable auto-scroll after 3 seconds
      userScrollTimeoutRef.current = setTimeout(() => {
        isUserScrollingRef.current = false;
      }, 3000);
    } else if (atBottom && isUserScrollingRef.current) {
      // User scrolled back to bottom
      isUserScrollingRef.current = false;

      // Clear the timeout since user is back at bottom
      if (userScrollTimeoutRef.current) {
        clearTimeout(userScrollTimeoutRef.current);
        userScrollTimeoutRef.current = null;
      }
    }
  }, [isAtBottom, messagesContainerRef]);

  /**
   * Initial scroll to bottom on mount
   */
  useEffect(() => {
    if (!hasInitialScrolledRef.current && messages.length > 0) {
      // Use instant scroll for initial load
      scrollToBottom(false);
      hasInitialScrolledRef.current = true;
    }
  }, [messages.length, scrollToBottom]);

  /**
   * Auto-scroll when new messages arrive (if user hasn't scrolled up)
   */
  useEffect(() => {
    if (messages.length > 0 && !isUserScrollingRef.current) {
      scrollToBottom();
    }
  }, [messages, scrollToBottom]);

  /**
   * Setup scroll event listener
   */
  useEffect(() => {
    const container = messagesContainerRef.current;
    if (!container) return;

    container.addEventListener("scroll", handleScroll, { passive: true });

    // Check initial scroll position
    const atBottom = isAtBottom();
    setShowScrollButton(!atBottom);

    return () => {
      container.removeEventListener("scroll", handleScroll);

      // Cleanup timeouts
      if (userScrollTimeoutRef.current) {
        clearTimeout(userScrollTimeoutRef.current);
      }
    };
  }, [handleScroll, messagesContainerRef, isAtBottom]);

  return {
    scrollToBottom,
    isAtBottom,
    showScrollButton,
  };
};
import { useRef, useState } from "react";
import { ChatState } from "../types/ChatTypes";

/**
 * Custom hook for managing chat state
 * Follows Single Responsibility Principle - only manages chat state
 */
export const useChatState = (): ChatState & {
  resetState: () => void;
} => {
  const [inputMessage, setInputMessage] = useState("");
  const [currentContext, setCurrentContext] = useState("");
  const [showContextField, setShowContextField] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);

  const processingTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  const resetState = () => {
    setInputMessage("");
    setCurrentContext("");
    setShowContextField(false);
    setIsProcessing(false);
    if (processingTimeoutRef.current) {
      clearTimeout(processingTimeoutRef.current);
      processingTimeoutRef.current = null;
    }
  };

  return {
    inputMessage,
    setInputMessage,
    currentContext,
    setCurrentContext,
    showContextField,
    setShowContextField,
    isProcessing,
    setIsProcessing,
    processingTimeoutRef,
    resetState,
  };
};
import { useCallback, useEffect } from "react";
import { ChatConversation } from "../types/ChatHistoryTypes";
import { ChatMessage } from "./usePersistentMessages";

interface AddMessageParams {
  type: "user" | "system" | "error";
  content: string;
  hasContext?: boolean;
  contextContent?: string;
}

interface UseConversationMessagesReturn {
  messages: ChatMessage[];
  addMessage: (params: AddMessageParams) => void;
  clearMessages: () => void;
}

interface UseConversationMessagesProps {
  currentConversation: ChatConversation | null;
  onAddMessage: (conversationId: string, message: ChatMessage) => void;
  onClearConversation: (conversationId: string) => void;
}

export const useConversationMessages = ({
  currentConversation,
  onAddMessage,
  onClearConversation,
}: UseConversationMessagesProps): UseConversationMessagesReturn => {
  // Get messages from current conversation
  const messages = currentConversation?.messages || [];

  // Add message to current conversation
  const addMessage = useCallback(
    (params: AddMessageParams) => {
      if (!currentConversation) return;

      const newMessage: ChatMessage = {
        id: `${currentConversation.id}-${Date.now()}-${Math.random()}`,
        timestamp: new Date(),
        ...params,
      };

      onAddMessage(currentConversation.id, newMessage);
    },
    [currentConversation, onAddMessage]
  );

  // Clear messages from current conversation
  const clearMessages = useCallback(() => {
    if (!currentConversation) return;

    if (onClearConversation) {
      onClearConversation(currentConversation.id);
    }
  }, [currentConversation, onClearConversation]);

  // Log conversation changes for debugging
  useEffect(() => {
    if (currentConversation) {
      console.log(`[CONVERSATION] Switched to conversation:`, {
        id: currentConversation.id,
        title: currentConversation.title,
        messageCount: messages.length,
      });
    } else {
      console.log(`[CONVERSATION] No current conversation`);
    }
  }, [currentConversation?.id, messages.length]);

  return {
    messages,
    addMessage,
    clearMessages,
  };
};
import { nanoid } from "nanoid";
import { useCallback, useEffect, useRef, useState } from "react";

export interface ChatMessage {
  id: string;
  type: "user" | "system" | "error";
  content: string;
  timestamp: Date;
  hasContext?: boolean;
  contextContent?: string;
}

interface AddMessageParams {
  type: "user" | "system" | "error";
  content: string;
  hasContext?: boolean;
  contextContent?: string;
}

interface UsePersistentMessagesReturn {
  messages: ChatMessage[];
  addMessage: (params: AddMessageParams) => void;
  clearMessages: () => void;
  recovery: {
    hasBackup: boolean;
    restoreFromBackup: () => void;
    clearBackup: () => void;
    integrityCheck: () => boolean;
    lastSaveTime: number;
  };
}

const STORAGE_KEY = "orch-chat-messages";
const BACKUP_KEY = "orch-chat-backup";
const REDUNDANT_KEY = "orch-chat-redundant";
const COMPONENT_ID_KEY = "orch-chat-component-id";

// Enhanced logging with detailed timestamps
const logWithDetails = (level: string, message: string, data?: any) => {
  const timestamp = new Date().toISOString();
  const logData = {
    timestamp,
    level,
    message,
    data,
  };

  console.log(`${level} [PERSISTENT_MESSAGES] ${message}`, logData);

  // Store critical logs in localStorage for debugging
  try {
    const logs = JSON.parse(localStorage.getItem("orch-chat-logs") || "[]");
    logs.push(logData);
    if (logs.length > 50) logs.splice(0, logs.length - 50);
    localStorage.setItem("orch-chat-logs", JSON.stringify(logs));
  } catch (e) {
    // Ignore logging errors
  }
};

// Helper function to safely parse and restore messages from a storage key
const loadMessagesFromKey = (key: string, logLabel: string): ChatMessage[] => {
  try {
    const data = localStorage.getItem(key);
    if (data) {
      const parsed = JSON.parse(data);
      const messagesArray = Array.isArray(parsed) ? parsed : parsed.messages;
      if (Array.isArray(messagesArray) && messagesArray.length > 0) {
        logWithDetails("💾", `Loaded ${messagesArray.length} messages from ${logLabel}`);
        return messagesArray.map((m: any) => ({ ...m, timestamp: new Date(m.timestamp) }));
      }
    }
  } catch (error) {
    logWithDetails("❌", `Error loading from ${logLabel}`, { key, error });
  }
  return [];
};

export const usePersistentMessages = (): UsePersistentMessagesReturn => {
  const componentId = useRef<string>(nanoid());

  const [messages, setMessages] = useState<ChatMessage[]>(() => {
    logWithDetails("🚀", "Initializing persistent messages state (lazy)");
    let initialMessages = loadMessagesFromKey(STORAGE_KEY, "primary storage");
    if (initialMessages.length > 0) return initialMessages;

    initialMessages = loadMessagesFromKey(BACKUP_KEY, "backup storage");
    if (initialMessages.length > 0) {
      localStorage.setItem(STORAGE_KEY, JSON.stringify(initialMessages));
      return initialMessages;
    }
    
    initialMessages = loadMessagesFromKey(REDUNDANT_KEY, "redundant storage");
    if (initialMessages.length > 0) {
        localStorage.setItem(STORAGE_KEY, JSON.stringify(initialMessages));
        return initialMessages;
    }

    logWithDetails("ℹ️", "No messages found in any storage for lazy init");
    return [];
  });

  const [lastSaveTime, setLastSaveTime] = useState<number>(0);

  useEffect(() => {
    logWithDetails("✅", "Persistent messages hook mounted", {
      componentId: componentId.current,
      initialMessageCount: messages.length,
    });
    try {
      localStorage.setItem(COMPONENT_ID_KEY, componentId.current);
    } catch (error) {
      logWithDetails("❌", "Error setting component ID", error);
    }
  }, []);

  const saveToStorage = useCallback((currentMessages: ChatMessage[]) => {
    try {
      const dataToSave = JSON.stringify(currentMessages);
      localStorage.setItem(STORAGE_KEY, dataToSave);
      
      const backupData = JSON.stringify({
        timestamp: Date.now(),
        count: currentMessages.length,
        messages: currentMessages,
      });
      localStorage.setItem(BACKUP_KEY, backupData);
      localStorage.setItem(REDUNDANT_KEY, dataToSave);

      setLastSaveTime(Date.now());
    } catch (error) {
      logWithDetails("❌", "Error saving messages to storage", error);
    }
  }, []);

  const debouncedSave = useRef(
    ((callback: (msgs: ChatMessage[]) => void, delay: number) => {
      let timeout: NodeJS.Timeout;
      return (msgs: ChatMessage[]) => {
        clearTimeout(timeout);
        timeout = setTimeout(() => callback(msgs), delay);
      };
    })(saveToStorage, 500)
  ).current;

  useEffect(() => {
    debouncedSave(messages);
  }, [messages, debouncedSave]);

  const addMessage = useCallback(
    (params: AddMessageParams) => {
      const newMessage: ChatMessage = {
        id: nanoid(),
        timestamp: new Date(),
        ...params,
      };
      setMessages((prevMessages) => [...prevMessages, newMessage]);
    },
    []
  );

  const clearMessages = useCallback(() => {
    logWithDetails("🗑️", "Clearing all messages");
    setMessages([]);
    try {
      localStorage.removeItem(STORAGE_KEY);
      localStorage.removeItem(BACKUP_KEY);
      localStorage.removeItem(REDUNDANT_KEY);
    } catch (error) {
      logWithDetails("❌", "Error clearing storage", error);
    }
  }, []);

  const hasBackup = useCallback(() => {
    try {
      return !!(localStorage.getItem(BACKUP_KEY) || localStorage.getItem(REDUNDANT_KEY));
    } catch {
      return false;
    }
  }, []);

  const restoreFromBackup = useCallback(() => {
    logWithDetails("🔄", "Manual restore requested");
    const backupMessages = loadMessagesFromKey(BACKUP_KEY, "manual backup restore");
    if (backupMessages.length > 0) {
      setMessages(backupMessages);
      logWithDetails("✅", "Manual restore completed", { recoveredCount: backupMessages.length });
      return true;
    }

    const redundantMessages = loadMessagesFromKey(REDUNDANT_KEY, "manual redundant restore");
    if (redundantMessages.length > 0) {
        setMessages(redundantMessages);
        logWithDetails("✅", "Manual restore completed", { recoveredCount: redundantMessages.length });
        return true;
    }

    logWithDetails("⚠️", "No backup data found for manual restore");
    return false;
  }, []);

  const clearBackup = useCallback(() => {
    try {
      localStorage.removeItem(BACKUP_KEY);
      localStorage.removeItem(REDUNDANT_KEY);
      logWithDetails("🗑️", "Backup cleared");
    } catch (error) {
      logWithDetails("❌", "Error clearing backup", error);
    }
  }, []);
  
  const performIntegrityCheck = useCallback(() => {
      try {
        const primaryCount = (loadMessagesFromKey(STORAGE_KEY, "integrity check")).length;
        const backupCount = (loadMessagesFromKey(BACKUP_KEY, "integrity check")).length;
        const isHealthy = primaryCount >= backupCount;
        logWithDetails("🩺", "Performed integrity check", { isHealthy, primaryCount, backupCount });
        return isHealthy;
      } catch (e) {
        return false;
      }
  }, []);

  return {
    messages,
    addMessage,
    clearMessages,
    recovery: {
      hasBackup: hasBackup(),
      restoreFromBackup,
      clearBackup,
      integrityCheck: performIntegrityCheck,
      lastSaveTime,
    },
  };
};
/* ================================================================
 * ANIMATIONS - Keyframes & Transitions
 * ================================================================
 * Contains all animation definitions, keyframes, and transition
 * effects used throughout the chat interface
 */

/* ===== KEYFRAME ANIMATIONS ===== */

/* Message Animation */
@keyframes messageSlideIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes messageAppear {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Context Input Slide Down */
@keyframes slideDown {
  from {
    opacity: 0;
    transform: translateY(-10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Microphone Recording Pulse */
@keyframes pulse {
  0% {
    box-shadow: 0 0 0 0 rgba(255, 68, 85, 0.4);
  }
  70% {
    box-shadow: 0 0 0 10px rgba(255, 68, 85, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(255, 68, 85, 0);
  }
}

/* Settings Popup Slide In */
@keyframes popupSlideIn {
  from {
    opacity: 0;
    transform: translateY(-10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Typing Indicator Animation */
@keyframes typingDot {
  0%,
  60%,
  100% {
    transform: scale(1);
    opacity: 0.5;
  }
  30% {
    transform: scale(1.2);
    opacity: 1;
  }
}

/* Neural Quantum Scroll Button Animations */
@keyframes fadeInScale {
  from {
    opacity: 0;
    transform: scale(0.8);
  }
  to {
    opacity: 1;
    transform: scale(1);
  }
}

@keyframes neuralPulse {
  0%,
  100% {
    transform: scale(1) translateY(0);
    box-shadow: 
      0 0 16px rgba(0, 250, 255, 0.4),
      0 6px 20px rgba(0, 0, 0, 0.25),
      inset 0 0 12px rgba(255, 255, 255, 0.1);
  }
  50% {
    transform: scale(1.02) translateY(-0.5px); /* Pulsação mais sutil */
    box-shadow: 
      0 0 24px rgba(0, 250, 255, 0.5),
      0 8px 24px rgba(0, 0, 0, 0.3),
      inset 0 0 16px rgba(255, 255, 255, 0.12);
  }
}

@keyframes pulseScale {
  0% {
    transform: scale(0);
    opacity: 0;
  }
  50% {
    transform: scale(1.2);
    opacity: 1;
  }
  100% {
    transform: scale(1);
    opacity: 1;
  }
}

/* ===== APPLIED ANIMATIONS ===== */

/* Message Animation Application */
.message {
  animation: messageSlideIn 0.3s ease-out;
}

/* Context Input Animation */
.context-input-wrapper {
  animation: slideDown 0.3s ease-out;
}

/* Transcription Display Animation */
.transcription-display {
  animation: slideDown 0.3s ease-out;
}

/* Settings Popup Animation */
.settings-popup {
  animation: popupSlideIn 0.2s ease-out;
}

/* Microphone Recording Animation */
.mic-btn.recording {
  animation: pulse 1.5s infinite;
}

/* Typing Animation Application */
.typing-animation span {
  animation: typingDot 1.4s infinite ease-in-out;
}

.typing-animation span:nth-child(1) {
  animation-delay: 0s;
}

.typing-animation span:nth-child(2) {
  animation-delay: 0.2s;
}

.typing-animation span:nth-child(3) {
  animation-delay: 0.4s;
}

/* ===== REDUCED MOTION SUPPORT ===== */
@media (prefers-reduced-motion: reduce) {
  /* Disable animations for users who prefer reduced motion */
  .message,
  .context-input-wrapper,
  .transcription-display,
  .settings-popup {
    animation: none !important;
  }
  
  .mic-btn.recording {
    animation: none !important;
  }
  
  .typing-animation span {
    animation: none !important;
  }
  
  /* Keep only opacity transitions for reduced motion */
  .message,
  .context-input-wrapper,
  .transcription-display {
    transition: opacity 0.2s ease !important;
  }
} /* ================================================================
 * BASE STYLES - Core Layout & Containers
 * ================================================================
 * Contém a estrutura de layout definitiva para o chat.
 * Esta abordagem usa uma cadeia de flexbox para garantir que
 * a área de mensagens ocupe o espaço correto.
 */

/* Container principal do chat */
.conversational-chat {
  display: flex;
  flex-direction: column;
  height: 100%;
  width: 100%;
  overflow: hidden;
  position: relative;
  /* Isolamento do background do painel */
  isolation: isolate;
  /* Background sólido para evitar vazamentos */
  background: #0a0f1a;
}

/* Nível 1: Container de Mensagens (Filho do .conversational-chat) */
.chat-messages-container {
  flex: 1; /* ESSENCIAL: Faz este container crescer e ocupar o espaço disponível */
  min-height: 0; /* PREVENÇÃO DE BUG FLEXBOX: Impede que o container exceda o pai */
  position: relative; /* Contexto para o botão absoluto */
  display: flex; /* Cria um novo contexto flex para seu filho */
  flex-direction: column;
  overflow: hidden; /* IMPORTANTE: Contém o overflow no container */
}

/* Nível 2: A Área de Scroll (Filho do .chat-messages-container) */
.chat-messages {
  flex: 1; /* ESSENCIAL: Faz a área de scroll preencher o .chat-messages-container */
  min-height: 0; /* PREVENÇÃO DE BUG FLEXBOX */
  height: 0; /* TRUQUE: Força o container a respeitar o flex: 1 */
  overflow-y: auto;
  overflow-x: hidden;
  padding: 16px 12px 32px 12px; /* Aumentado padding-bottom para garantir espaço de scroll */
  display: flex;
  flex-direction: column;
  scroll-behavior: smooth;
}

/* Pseudo-elemento para garantir espaço extra de scroll */
.chat-messages::after {
  content: "";
  display: block;
  height: 2px;
  min-height: 2px;
  visibility: hidden;
  flex-shrink: 0;
}

/* Nível 3: O Wrapper das Mensagens */
.messages-wrapper {
  display: flex;
  flex-direction: column;
  gap: 16px;
  width: 100%;
  margin-top: auto;
}

/* Scrollbar Styling */
.chat-messages::-webkit-scrollbar {
  width: 6px;
}

.chat-messages::-webkit-scrollbar-track {
  background: rgba(0, 0, 0, 0.2);
  border-radius: 3px;
}

.chat-messages::-webkit-scrollbar-thumb {
  background: linear-gradient(135deg, #00faff 0%, #7c4dff 100%);
  border-radius: 3px;
}

/* Welcome Message */
.welcome-message {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  text-align: center;
  padding: 40px 20px;
  color: rgba(255, 255, 255, 0.7);
  width: 100%;
  /* flex-grow permite que ele ocupe o espaço vertical quando for o único elemento */
  flex-grow: 1;
}

.welcome-icon {
  margin-bottom: 20px;
  opacity: 0.8;
}

.welcome-message h3 {
  font-size: 24px;
  font-weight: 600;
  margin-bottom: 12px;
  background: linear-gradient(135deg, #00faff 0%, #7c4dff 100%);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}

.welcome-message p {
  font-size: 16px;
  line-height: 1.5;
  opacity: 0.8;
}

/* Debug button styles removed for cleaner UI */ /* ================================================================
 * COMPONENT STYLES - Unique UI Elements
 * ================================================================
 * Contains styles for specialized components like scroll buttons
 * and other unique interface elements
 */

/* ===== SCROLL TO BOTTOM BUTTON - Quantum Neural Design ===== */

/* Botão de scroll direto sem wrapper */
.scroll-to-bottom-btn {
  /* Posicionamento */
  position: absolute;
  bottom: 24px;
  right: 24px;
  z-index: 15;
  
  /* Dimensões */
  width: 48px;
  height: 48px;
  border-radius: 50%;
  
  /* Layout */
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  
  /* Remove default button styles and focus */
  outline: none;
  -webkit-tap-highlight-color: transparent;
  -webkit-focus-ring-color: transparent;
  
  /* Quantum glass effect with enhanced glow */
  background: linear-gradient(
    135deg, 
    rgba(0, 250, 255, 0.95) 0%, 
    rgba(124, 77, 255, 0.85) 100%
  );
  
  /* Neural border with glow */
  border: 2px solid rgba(255, 255, 255, 0.2);
  
  /* Multi-layer shadow for depth */
  box-shadow: 
    /* Outer glow */
    0 0 20px rgba(0, 250, 255, 0.4),
    /* Drop shadow */
    0 4px 15px rgba(0, 0, 0, 0.3),
    /* Inner light */
    inset 0 0 15px rgba(255, 255, 255, 0.1);
  
  /* Backdrop filter for glass effect */
  -webkit-backdrop-filter: blur(10px) saturate(150%); /* Safari support */
  backdrop-filter: blur(10px) saturate(150%);
  
  /* Smooth transitions */
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  
  /* Estado inicial oculto */
  opacity: 0;
  transform: scale(0.8) translateY(10px);
  pointer-events: none;
}

/* Estado visível */
.scroll-to-bottom-btn.visible {
  opacity: 1;
  transform: scale(1) translateY(0);
  pointer-events: auto;
}

/* Hover state - enhanced glow and slight scale */
.scroll-to-bottom-btn:hover {
  transform: scale(1.05);
  box-shadow: 
    0 0 30px rgba(0, 250, 255, 0.6),
    0 6px 20px rgba(0, 0, 0, 0.4),
    inset 0 0 20px rgba(255, 255, 255, 0.2);
  
  /* Brighten background slightly */
  background: linear-gradient(
    135deg, 
    rgba(0, 250, 255, 1) 0%, 
    rgba(124, 77, 255, 0.9) 100%
  );
}

/* Active/Pressed state */
.scroll-to-bottom-btn:active,
.scroll-to-bottom-btn.pressed {
  transform: scale(0.95);
  box-shadow: 
    0 0 15px rgba(0, 250, 255, 0.3),
    0 2px 10px rgba(0, 0, 0, 0.3),
    inset 0 0 10px rgba(0, 0, 0, 0.2);
}

/* Focus state - remove outline */
.scroll-to-bottom-btn:focus {
  outline: none;
}

/* Focus visible - para acessibilidade via teclado */
.scroll-to-bottom-btn:focus-visible {
  outline: none;
  box-shadow: 
    0 0 0 3px rgba(0, 250, 255, 0.5),
    0 0 30px rgba(0, 250, 255, 0.6),
    0 6px 20px rgba(0, 0, 0, 0.4);
}

/* Icon styling within button */
.scroll-to-bottom-btn svg {
  width: 24px;
  height: 24px;
  filter: drop-shadow(0 1px 2px rgba(0, 0, 0, 0.2));
}

/* Pulse animation for attention */
@keyframes scrollButtonPulse {
  0% {
    box-shadow: 
      0 0 20px rgba(0, 250, 255, 0.4),
      0 4px 15px rgba(0, 0, 0, 0.3);
  }
  50% {
    box-shadow: 
      0 0 35px rgba(0, 250, 255, 0.6),
      0 4px 15px rgba(0, 0, 0, 0.3);
  }
  100% {
    box-shadow: 
      0 0 20px rgba(0, 250, 255, 0.4),
      0 4px 15px rgba(0, 0, 0, 0.3);
  }
}

/* Apply pulse animation when needed */
.scroll-to-bottom-btn.pulse {
  animation: scrollButtonPulse 2s ease-in-out infinite;
}

/* ===== NEW MESSAGE BADGE - Otimizado para FAB 48px ===== */
.scroll-to-bottom-btn .new-messages-badge {
  position: absolute;
  top: -6px;
  right: -6px;
  background-color: #ff4757;
  color: white;
  border-radius: 50%;
  width: 18px;    /* Menor para proporção com botão 48px */
  height: 18px;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 9px;  /* Fonte menor */
  font-weight: bold;
  font-family: Inter, sans-serif;
  border: 2px solid rgba(255, 255, 255, 0.3);
  box-shadow: 0 2px 6px rgba(255, 71, 87, 0.4);
  animation: pulseScale 0.5s ease-out;
  z-index: 20;
}

/* ===== REDUCED MOTION SUPPORT ===== */
@media (prefers-reduced-motion: reduce) {
  .scroll-to-bottom-btn {
    animation: none !important;
    transition: opacity 0.2s ease !important;
  }

  @keyframes neuralPulse {
    0%,
    100% {
      opacity: 1;
    }
  }
} /* ================================================================
 * INPUT STYLES - Chat Input Area & Controls
 * ================================================================
 * Contains all styles related to the chat input area,
 * context input, transcription display, and input controls
 */

/* Input Area - ALTURA FIXA */
.chat-input-area {
  flex-shrink: 0; /* Não encolhe */
  border-top: 1px solid rgba(0, 250, 255, 0.2);
  background: rgba(0, 0, 0, 0.3);
  -webkit-backdrop-filter: blur(10px);
  backdrop-filter: blur(10px);
}

/* Context Input */
.context-input-wrapper {
  background: rgba(0, 250, 255, 0.08);
  -webkit-backdrop-filter: blur(12px);
  backdrop-filter: blur(12px);
  border: 1px solid rgba(0, 250, 255, 0.2);
  border-radius: 16px;
  padding: 16px 20px;
  margin-bottom: 16px;
  position: relative;
  box-shadow: 
    0 8px 32px rgba(0, 250, 255, 0.1),
    inset 0 1px 0 rgba(255, 255, 255, 0.1);
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.context-label {
  display: flex;
  align-items: center;
  justify-content: space-between;
  gap: 8px;
  color: #00faff;
  font-size: 14px;
  font-weight: 500;
  margin-bottom: 8px;
}

.context-close-btn {
  background: none;
  border: none;
  color: #ff4455;
  font-size: 18px;
  font-weight: bold;
  cursor: pointer;
  padding: 0;
  width: 20px;
  height: 20px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 50%;
  transition: all 0.2s ease;
}

.context-close-btn:hover {
  background: rgba(255, 68, 85, 0.2);
  transform: scale(1.1);
}

.context-input {
  background: transparent;
  border: none;
  color: rgba(255, 255, 255, 0.9);
  font-size: 14px;
  line-height: 1.6;
  outline: none;
  padding: 0;
  width: 100%;
  min-height: 60px;
  resize: none;
  font-family: inherit;
}

.context-input::placeholder {
  color: rgba(255, 255, 255, 0.4);
}

/* Main Input Area */
.main-input-wrapper {
  padding: 16px 24px 20px 24px; /* Increased padding for better breathing room */
  background: linear-gradient(
    180deg,
    rgba(0, 0, 0, 0.2) 0%,
    rgba(0, 0, 0, 0.4) 100%
  );
  position: relative;
}

/* Subtle glow effect for depth */
.main-input-wrapper::before {
  content: '';
  position: absolute;
  bottom: 0;
  left: 50%;
  transform: translateX(-50%);
  width: 80%;
  height: 1px;
  background: linear-gradient(
    90deg,
    transparent 0%,
    rgba(0, 250, 255, 0.5) 50%,
    transparent 100%
  );
  opacity: 0.8;
}

.input-row {
  display: flex;
  flex-direction: column;
  gap: 16px; /* Increased gap for better separation */
}

/* Input bottom row - Enhanced layout */
.input-bottom-row {
  display: flex;
  align-items: center;
  gap: 12px; /* Add gap between input and controls */
  position: relative;
}

/* Transcription Display - Modern glassmorphism */
.transcription-display {
  background: rgba(0, 250, 255, 0.08);
  -webkit-backdrop-filter: blur(12px);
  backdrop-filter: blur(12px);
  border: 1px solid rgba(0, 250, 255, 0.2);
  border-radius: 16px;
  padding: 16px 20px;
  position: relative;
  max-height: 120px;
  overflow: hidden;
  box-shadow: 
    0 8px 32px rgba(0, 250, 255, 0.1),
    inset 0 1px 0 rgba(255, 255, 255, 0.1);
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.transcription-display:hover {
  border-color: rgba(0, 250, 255, 0.3);
  box-shadow: 
    0 12px 40px rgba(0, 250, 255, 0.15),
    inset 0 1px 0 rgba(255, 255, 255, 0.15);
}

.transcription-label {
  display: flex;
  align-items: center;
  gap: 10px;
  color: #00faff;
  font-size: 11px;
  font-weight: 700;
  margin-bottom: 10px;
  text-transform: uppercase;
  letter-spacing: 1.2px;
  opacity: 0.9;
}

.transcription-label svg {
  width: 18px;
  height: 18px;
  animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite;
}

.transcription-text {
  color: #ffffff;
  font-size: 14px;
  line-height: 1.6;
  padding-right: 32px;
  max-height: 80px;
  overflow-y: auto;
  overflow-x: hidden;
}

.transcription-clear-btn {
  position: absolute;
  top: 12px;
  right: 12px;
  background: rgba(255, 68, 85, 0.1);
  border: 1px solid rgba(255, 68, 85, 0.3);
  color: #ff4455;
  font-size: 14px;
  font-weight: 600;
  cursor: pointer;
  padding: 0;
  width: 28px;
  height: 28px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 8px;
  transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
}

.transcription-clear-btn:hover {
  background: rgba(255, 68, 85, 0.2);
  border-color: rgba(255, 68, 85, 0.5);
  transform: scale(1.05);
}

/* Message Input */
.message-input-wrapper {
  flex: 1;
  background: rgba(0, 0, 0, 0.3);
  -webkit-backdrop-filter: blur(20px);
  backdrop-filter: blur(20px);
  border: 1px solid rgba(0, 250, 255, 0.2);
  border-radius: 20px;
  padding: 0 20px;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  position: relative;
  overflow: hidden;
  min-height: 52px;
  box-shadow: 
    0 4px 24px rgba(0, 0, 0, 0.2),
    inset 0 1px 0 rgba(255, 255, 255, 0.05);
}

/* Animated border glow effect */
.message-input-wrapper::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: linear-gradient(
    45deg,
    transparent 30%,
    rgba(0, 250, 255, 0.1) 50%,
    transparent 70%
  );
  transform: translateX(-100%);
  transition: transform 0.6s;
}

.message-input-wrapper:focus-within::before {
  transform: translateX(100%);
}

.message-input-wrapper:focus-within {
  border-color: rgba(0, 250, 255, 0.4);
  box-shadow: 
    0 0 0 3px rgba(0, 250, 255, 0.1),
    0 4px 24px rgba(0, 0, 0, 0.3),
    0 0 40px rgba(0, 250, 255, 0.1),
    inset 0 1px 0 rgba(255, 255, 255, 0.1);
  background: rgba(0, 0, 0, 0.4);
}

/* Message Input - Clean and modern */
.message-input {
  background: transparent;
  border: none;
  color: rgba(255, 255, 255, 0.95);
  font-size: 15px;
  outline: none;
  padding: 14px 0;
  line-height: 1.6;
  font-family: inherit;
  width: 100%;
  transition: color 0.2s ease;
}

.message-input::placeholder {
  color: rgba(255, 255, 255, 0.35);
  transition: color 0.2s ease;
}

.message-input:focus::placeholder {
  color: rgba(255, 255, 255, 0.25);
}

/* Typing indicator animation */
.message-input-wrapper.typing {
  animation: typingPulse 1.5s ease-in-out infinite;
}

@keyframes typingPulse {
  0%, 100% {
    box-shadow: 
      0 4px 24px rgba(0, 0, 0, 0.2),
      inset 0 1px 0 rgba(255, 255, 255, 0.05);
  }
  50% {
    box-shadow: 
      0 4px 24px rgba(0, 250, 255, 0.1),
      inset 0 1px 0 rgba(255, 255, 255, 0.1);
  }
}

/* Input Controls - Modern button design */
.input-controls {
  display: flex;
  align-items: center;
  gap: 8px;
  flex-shrink: 0;
}

/* Control Buttons - Futuristic design */
.control-btn {
  background: rgba(0, 0, 0, 0.3);
  -webkit-backdrop-filter: blur(10px);
  backdrop-filter: blur(10px);
  border: 1px solid rgba(0, 250, 255, 0.2);
  border-radius: 14px;
  width: 44px;
  height: 44px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
  position: relative;
  overflow: hidden;
}

.control-btn svg {
  width: 22px;
  height: 22px;
  color: rgba(255, 255, 255, 0.7);
  transition: all 0.2s ease;
  z-index: 1;
}

.control-btn:hover {
  background: rgba(0, 250, 255, 0.1);
  border-color: rgba(0, 250, 255, 0.4);
  transform: translateY(-1px);
  box-shadow: 
    0 4px 20px rgba(0, 250, 255, 0.2),
    0 0 0 1px rgba(0, 250, 255, 0.2);
}

.control-btn:hover svg {
  color: #00faff;
  transform: scale(1.1);
}

.control-btn:active {
  transform: translateY(0);
  box-shadow: 
    0 2px 10px rgba(0, 250, 255, 0.2),
    inset 0 1px 0 rgba(0, 0, 0, 0.2);
}

.control-btn:disabled {
  opacity: 0.4;
  cursor: not-allowed;
  background: rgba(0, 0, 0, 0.2);
}

.control-btn:disabled:hover {
  transform: none;
  border-color: rgba(0, 250, 255, 0.2);
  box-shadow: none;
}

/* Specific button states */
.control-btn.active {
  background: rgba(0, 250, 255, 0.15);
  border-color: rgba(0, 250, 255, 0.5);
  box-shadow: 
    0 0 0 2px rgba(0, 250, 255, 0.2),
    0 4px 20px rgba(0, 250, 255, 0.3);
}

.control-btn.active svg {
  color: #00faff;
}

/* Microphone button recording state */
.control-btn.mic-btn.recording {
  background: rgba(255, 68, 85, 0.15);
  border-color: rgba(255, 68, 85, 0.5);
  animation: recordingPulse 1.5s ease-in-out infinite;
}

@keyframes recordingPulse {
  0%, 100% {
    box-shadow: 
      0 0 0 0 rgba(255, 68, 85, 0.4),
      0 4px 20px rgba(255, 68, 85, 0.3);
  }
  50% {
    box-shadow: 
      0 0 0 8px rgba(255, 68, 85, 0.1),
      0 4px 20px rgba(255, 68, 85, 0.4);
  }
}

.control-btn.mic-btn.recording svg {
  color: #ff4455;
}

/* Send button ready state */
.control-btn.send-btn.ready {
  background: rgba(0, 250, 255, 0.15);
  border-color: rgba(0, 250, 255, 0.5);
}

.control-btn.send-btn.ready svg {
  color: #00faff;
}

.control-btn.send-btn.ready:hover {
  background: rgba(0, 250, 255, 0.25);
  border-color: rgba(0, 250, 255, 0.7);
  box-shadow: 
    0 0 0 3px rgba(0, 250, 255, 0.2),
    0 4px 24px rgba(0, 250, 255, 0.4);
}

/* Focus states for accessibility */
.control-btn:focus {
  outline: none;
  box-shadow: 
    0 0 0 3px rgba(0, 250, 255, 0.4),
    0 4px 20px rgba(0, 250, 255, 0.2);
}

/* Remove outline on mouse click but keep for keyboard navigation */
.control-btn:focus:not(:focus-visible) {
  outline: none;
  box-shadow: none;
}

/* Show focus ring only for keyboard navigation */
.control-btn:focus-visible {
  outline: none;
  box-shadow: 
    0 0 0 3px rgba(0, 250, 255, 0.4),
    0 4px 20px rgba(0, 250, 255, 0.2);
}

/* Remove default button focus for all browsers */
.control-btn::-moz-focus-inner {
  border: 0;
  padding: 0;
}

/* Ensure no outline on active state (when clicking) */
.control-btn:active {
  outline: none;
  transform: translateY(0);
  box-shadow: 
    0 2px 10px rgba(0, 250, 255, 0.2),
    inset 0 1px 0 rgba(0, 0, 0, 0.2);
}

.message-input:focus {
  outline: none;
}

/* Also remove outline from message input */
.message-input:focus:not(:focus-visible) {
  outline: none;
}

/* Remove outline from transcription clear button */
.transcription-clear-btn:focus {
  outline: none;
}

.transcription-clear-btn:focus:not(:focus-visible) {
  outline: none;
}

.transcription-clear-btn:focus-visible {
  outline: none;
  box-shadow: 0 0 0 2px rgba(255, 68, 85, 0.4);
}

/* Ripple effect for buttons */
.control-btn::after {
  content: '';
  position: absolute;
  top: 50%;
  left: 50%;
  width: 0;
  height: 0;
  border-radius: 50%;
  background: rgba(255, 255, 255, 0.3);
  transform: translate(-50%, -50%);
  transition: width 0.4s, height 0.4s;
  pointer-events: none;
}

.control-btn:active::after {
  width: 100%;
  height: 100%;
}

/* Input separator - DEPRECATED */
.input-separator {
  display: none;
}

/* Specific button styles */
.context-btn {
  color: #00faff;
}

.mic-btn {
  color: #ff4dd2;
}

.mic-btn.recording {
  background: rgba(255, 68, 85, 0.2);
  border-color: #ff4455;
  color: #ff4455;
}

.settings-btn.active {
  background: rgba(124, 77, 255, 0.2);
  color: #7c4dff;
}

.send-btn {
  background: linear-gradient(135deg, #00faff 0%, #7c4dff 100%);
  color: white;
}

.send-btn:hover:not(:disabled) {
  background: linear-gradient(135deg, #00faff 10%, #7c4dff 90%);
  transform: translateY(-2px);
  box-shadow: 0 4px 12px rgba(0, 250, 255, 0.3);
}

/* Settings Popup */
.settings-popup {
  background: rgba(0, 10, 20, 0.95);
  border: 1px solid rgba(0, 250, 255, 0.3);
  border-radius: 12px;
  -webkit-backdrop-filter: blur(20px);
  backdrop-filter: blur(20px);
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);
  max-width: 350px;
}

/* Context Button Active State */
.context-btn.active {
  background: rgba(0, 250, 255, 0.2);
  border-color: #00faff;
  color: #00faff;
  box-shadow: 0 0 12px rgba(0, 250, 255, 0.3);
}

/* Send Button States */
.send-btn.ready {
  background: rgba(0, 250, 255, 0.2);
  border-color: #00faff;
  color: #00faff;
  box-shadow: 0 0 12px rgba(0, 250, 255, 0.3);
}

.send-btn.ready:hover {
  background: rgba(0, 250, 255, 0.3);
  transform: scale(1.1);
}

.send-btn.disabled {
  background: rgba(0, 0, 0, 0.2);
  border-color: rgba(255, 255, 255, 0.1);
  color: rgba(255, 255, 255, 0.3);
}

/* Accessibility Improvements */
.control-btn:focus {
  outline: 2px solid #00faff;
  outline-offset: 2px;
}

.context-input:focus,
.message-input:focus {
  outline: none;
  border-color: #00faff;
  box-shadow: 0 0 0 2px rgba(0, 250, 255, 0.2);
}

/* Audio Settings Button */
.audio-settings-btn {
  color: #00faff;
  transition: all 0.2s ease;
}

.audio-settings-btn:hover {
  color: #ffffff;
  background: rgba(0, 250, 255, 0.2);
}

.audio-settings-btn.active {
  background: rgba(0, 250, 255, 0.2);
  border-color: #00faff;
  color: #00faff;
  box-shadow: 0 0 12px rgba(0, 250, 255, 0.3);
}

/* Audio Settings Popover */
.audio-settings-popover-container {
  position: absolute;
  bottom: 80px;
  right: 80px;
  z-index: 1000;
}

.audio-settings-popover {
  animation: popover-appear 0.2s ease-out;
}

@keyframes popover-appear {
  0% {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
  100% {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
}

/* Popover arrow pointing to button */
.audio-settings-popover::after {
  content: '';
  position: absolute;
  bottom: -8px;
  right: 40px;
  width: 16px;
  height: 16px;
  background: rgba(17, 24, 39, 0.95);
  border-right: 1px solid rgba(0, 250, 255, 0.3);
  border-bottom: 1px solid rgba(0, 250, 255, 0.3);
  transform: rotate(45deg);
}

/* Mobile responsiveness for popover */
@media (max-width: 640px) {
  .audio-settings-popover-container {
    position: fixed;
    bottom: 0;
    left: 0;
    right: 0;
    padding: 16px;
  }
  
  .audio-settings-popover {
    width: 100%;
    max-width: none;
    border-radius: 16px 16px 0 0;
  }
  
  .audio-settings-popover::after {
    display: none;
  }
}

/* Fade in animation for device selectors */
@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(-10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.animate-fadeIn {
  animation: fadeIn 0.3s ease-out forwards;
}

/* Add pulse animation for transcription icon */
.pulse-dot {
  animation: pulseDot 2s cubic-bezier(0.4, 0, 0.6, 1) infinite;
}

@keyframes pulseDot {
  0%, 100% {
    opacity: 1;
    transform: scale(1);
  }
  50% {
    opacity: 0.6;
    transform: scale(0.8);
  }
}

/* Mobile Responsive Improvements */
@media (max-width: 768px) {
  .main-input-wrapper {
    padding: 12px 16px 16px 16px;
  }

  .input-row {
    gap: 12px;
  }

  .control-btn {
    width: 40px;
    height: 40px;
  }

  .control-btn svg {
    width: 20px;
    height: 20px;
  }

  .message-input-wrapper {
    border-radius: 16px;
    padding: 0 16px;
    min-height: 48px;
  }

  .message-input {
    font-size: 14px;
    padding: 12px 0;
  }

  .transcription-display {
    padding: 12px 16px;
    border-radius: 12px;
  }

  .transcription-label {
    font-size: 10px;
  }

  .transcription-clear-btn {
    width: 24px;
    height: 24px;
    font-size: 12px;
  }
}

@media (max-width: 480px) {
  .main-input-wrapper {
    padding: 8px 12px 12px 12px;
  }

  .input-bottom-row {
    gap: 8px;
  }

  .input-controls {
    gap: 4px;
  }

  .control-btn {
    width: 36px;
    height: 36px;
  }

  .control-btn svg {
    width: 18px;
    height: 18px;
  }

  .message-input-wrapper {
    min-height: 44px;
  }
}

/* Dark mode enhancements */
@media (prefers-color-scheme: dark) {
  .message-input-wrapper {
    background: rgba(0, 0, 0, 0.4);
  }

  .control-btn {
    background: rgba(0, 0, 0, 0.4);
  }

  .transcription-display {
    background: rgba(0, 250, 255, 0.06);
  }
}

/* Smooth transitions for all interactive elements */
* {
  -webkit-tap-highlight-color: transparent;
}

/* Improve touch targets on mobile */
@media (hover: none) and (pointer: coarse) {
  .control-btn {
    min-width: 44px;
    min-height: 44px;
  }
}

/* Accessibility improvements */
@media (prefers-reduced-motion: reduce) {
  * {
    animation-duration: 0.01ms !important;
    animation-iteration-count: 1 !important;
    transition-duration: 0.01ms !important;
  }
}

/* Additional focus management for all interactive elements */
button:focus:not(:focus-visible),
input:focus:not(:focus-visible),
textarea:focus:not(:focus-visible) {
  outline: none;
}

/* Ensure context input also follows the same pattern */
.context-input:focus {
  outline: none;
}

.context-input:focus:not(:focus-visible) {
  outline: none;
}

/* Message input wrapper focus states */
.message-input-wrapper:focus-within {
  outline: none;
}

/* Ensure all buttons in the app follow the same pattern */
button {
  -webkit-tap-highlight-color: transparent;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/* Remove blue highlight on touch devices */
* {
  -webkit-tap-highlight-color: transparent;
  -webkit-touch-callout: none;
} /* ================================================================
 * MESSAGE STYLES - WhatsApp-style Layout
 * ================================================================
 * Contains all styles related to message display, avatars,
 * bubbles, timestamps, and context information
 */

/* Message Styles - Layout estilo WhatsApp */
.message {
  display: flex;
  gap: 8px;
  margin: 0; /* Remove margin padrão - será controlado individualmente */
  min-height: 40px;
  width: 100%;
  position: relative;
  padding: 0; /* Remove qualquer padding que possa estar causando diferença */
}

.message-avatar {
  flex-shrink: 0;
  width: 32px;
  height: 32px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  margin-top: 4px; /* Alinha com o topo do bubble */
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
  overflow: hidden; /* Garante que o SVG não saia do círculo */
}

.message-avatar svg {
  width: 24px;
  height: 24px;
  display: block; /* Remove espaço em branco abaixo do SVG */
  margin: auto; /* Centraliza o SVG dentro do container */
}

/* Garantir que os ícones do usuário e AI sejam bem visíveis */
.user-message .message-avatar svg path {
  filter: drop-shadow(0 1px 2px rgba(0, 0, 0, 0.3));
}

.system-message .message-avatar svg rect,
.system-message .message-avatar svg circle:last-child {
  filter: drop-shadow(0 1px 2px rgba(0, 0, 0, 0.2));
}

/* Wrapper para bubble + timestamp */
.message-bubble-wrapper {
  display: flex;
  flex-direction: column;
  gap: 4px;
  max-width: 70%;
  flex: 1;
  margin: 0; /* Remove qualquer margem padrão */
}

/* Mensagens do usuário - lado direito */
.user-message {
  flex-direction: row-reverse;
  justify-content: flex-start;
  padding: 0; /* Remove qualquer padding */
  margin: 0 0 16px 0; /* Apenas margin-bottom */
}

.user-message .message-avatar {
  margin-left: 8px;
  margin-right: 0;
}

.system-message .message-avatar {
  margin-left: 0;
  margin-right: 8px; /* Adiciona margem igual ao avatar do usuário */
}

.user-message .message-bubble-wrapper {
  align-items: flex-end; /* Alinha bubble e timestamp à direita */
  margin: 0; /* Remove todas as margens */
}

/* Mensagens do sistema - lado esquerdo */
.system-message {
  justify-content: flex-start;
  padding: 0; /* Remove qualquer padding */
  margin: 0 0 16px 0; /* Apenas margin-bottom, igual ao usuário */
}

.system-message .message-bubble-wrapper {
  align-items: flex-start; /* Alinha bubble e timestamp à esquerda */
  margin: 0; /* Remove todas as margens */
}

/* Bubble das mensagens do usuário */
.user-message .message-content {
  background: linear-gradient(
    135deg,
    rgba(0, 250, 255, 0.3) 0%,
    rgba(0, 102, 204, 0.3) 100%
  );
  border: 1px solid rgba(0, 250, 255, 0.3);
  border-radius: 18px 18px 4px 18px;
}

/* Bubble das mensagens do sistema */
.system-message .message-content {
  background: linear-gradient(
    135deg,
    rgba(255, 77, 210, 0.15) 0%,
    rgba(124, 77, 255, 0.15) 100%
  );
  border: 1px solid rgba(255, 77, 210, 0.3);
  border-radius: 18px 18px 18px 4px;
}

/* Conteúdo da mensagem - padronizado */
.message-content {
  padding: 12px 16px;
  position: relative;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
  word-wrap: break-word;
  overflow-wrap: break-word;
}

.message-context {
  margin-bottom: 12px;
  padding: 8px 12px;
  background: rgba(0, 0, 0, 0.3);
  border-radius: 8px;
  border-left: 3px solid #00faff;
}

.context-label {
  font-size: 12px;
  font-weight: 600;
  color: #00faff;
  margin-bottom: 4px;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.context-content {
  font-size: 13px;
  color: rgba(255, 255, 255, 0.8);
  line-height: 1.4;
}

.message-text {
  font-size: 15px;
  line-height: 1.6;
  color: rgba(255, 255, 255, 0.9);
  word-wrap: break-word;
  margin: 0; /* Remove margem para eliminar espaço extra */
}

/* Timestamp - estilo WhatsApp (fora do bubble) */
.message-timestamp {
  font-size: 11px;
  color: rgba(255, 255, 255, 0.4);
  font-weight: 400;
  padding: 0 4px;
  white-space: nowrap;
  margin-top: 2px;
}

/* Alinhamento do timestamp para mensagens do usuário */
.user-message .message-timestamp {
  text-align: right;
  align-self: flex-end;
}

/* Alinhamento do timestamp para mensagens do sistema */
.system-message .message-timestamp {
  text-align: left;
  align-self: flex-start;
}

/* Message Context Improvements */
.message-context {
  background: rgba(0, 250, 255, 0.1);
  border-left: 3px solid #00faff;
  padding: 8px 12px;
  margin-bottom: 8px;
  border-radius: 0 8px 8px 0;
}

.context-label {
  color: #00faff;
  font-size: 12px;
  font-weight: 600;
  margin-bottom: 4px;
}

.context-content {
  color: rgba(255, 255, 255, 0.8);
  font-size: 13px;
  line-height: 1.4;
}

/* Typing Indicator */
.typing-indicator {
  /* Herda todos os estilos da classe .message e .system-message */
  /* Removido estilos que estavam sobrescrevendo o layout padrão */
}

.typing-animation {
  display: flex;
  align-items: center;
  justify-content: flex-start;
  gap: 4px;
  height: 24px;
  padding: 0; /* Removido padding extra que estava desalinhando */
}

.typing-animation span {
  width: 8px;
  height: 8px;
  border-radius: 50%;
  background: linear-gradient(135deg, #ff4dd2 0%, #7c4dff 100%);
}

/* Messages container styles */
.chat-messages {
  flex: 1;
  overflow-y: auto;
  padding: 16px;
  padding-bottom: 24px; /* Extra padding to ensure scroll space */
  display: flex;
  flex-direction: column;
  min-height: 0;
  position: relative;
  /* Background sólido para cobrir qualquer transparência */
  background: #0a0f1a;
}

/* Add a pseudo-element for extra scroll space */
.chat-messages::after {
  content: "";
  display: block;
  height: 1px;
  min-height: 1px;
  visibility: hidden;
}

.messages-wrapper {
  display: flex;
  flex-direction: column;
  gap: 12px;
  min-height: min-content;
  /* Background sólido para evitar vazamentos */
  background: #0a0f1a;
  position: relative;
  z-index: 1;
}

/* ================================================================
 * MESSAGE STYLES - Message Display & Layout
 * ================================================================
 */

/* Message item base */
.message-item {
  display: flex;
  align-items: flex-start;
  gap: 12px;
  animation: messageSlideIn 0.3s ease-out;
  width: 100%;
}

/* Message avatar */
.message-avatar {
  width: 32px;
  height: 32px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  font-size: 16px;
}

.message-avatar.user {
  background: linear-gradient(135deg, #00faff 0%, #7c4dff 100%);
}

.message-avatar.system {
  background: linear-gradient(135deg, #ff4dd2 0%, #7c4dff 100%);
}

.message-avatar.error {
  background: linear-gradient(135deg, #ff4757 0%, #ff6348 100%);
}

/* Message content */
.message-content {
  flex: 1;
  display: flex;
  flex-direction: column;
  gap: 4px;
  min-width: 0;
}

.message-header {
  display: flex;
  align-items: center;
  gap: 8px;
  font-size: 12px;
  opacity: 0.7;
}

.message-type {
  font-weight: 600;
  text-transform: capitalize;
}

.message-time {
  font-size: 11px;
}

.message-text {
  color: rgba(255, 255, 255, 0.9);
  line-height: 1.5;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* Context indicator */
.context-indicator {
  display: inline-flex;
  align-items: center;
  gap: 4px;
  padding: 2px 8px;
  background: rgba(124, 77, 255, 0.2);
  border-radius: 12px;
  font-size: 11px;
  color: #7c4dff;
  margin-top: 4px;
}/* ================================================================
 * RESPONSIVE STYLES - Media Queries & Screen Adaptations
 * ================================================================
 * Contains all responsive design adjustments for different
 * screen sizes and device types
 */

/* ===== HIGH RESOLUTION / LARGE SCREENS ===== */
@media (min-width: 1200px) {
  .chat-messages {
    padding: 20px 16px;
  }
  
  .main-input-wrapper {
    padding: 20px 32px;
  }
  
  .message-bubble-wrapper {
    max-width: 65%; /* Slightly smaller on large screens */
  }
}

/* ===== MEDIUM HEIGHT SCREENS ===== */
@media (max-height: 800px) {
  .message-content {
    padding: 10px 14px;
  }

  .message {
    gap: 6px;
    margin: 0;
  }

  /* Espaçamento específico para telas baixas */
  .user-message,
  .system-message {
    margin: 0 0 12px 0;
  }

  .message-timestamp {
    font-size: 10px;
    margin-top: 2px;
  }

  .chat-messages {
    padding: 16px;
  }
}

/* ===== SMALL HEIGHT SCREENS ===== */
@media (max-height: 600px) {
  .chat-messages {
    padding: 12px;
  }
  
  .welcome-message {
    padding: 20px 16px;
  }
  
  .welcome-message h3 {
    font-size: 20px;
    margin-bottom: 8px;
  }
  
  .welcome-message p {
    font-size: 14px;
  }
}

/* ===== TABLET LANDSCAPE ===== */
@media (max-width: 1024px) and (orientation: landscape) {
  .message-bubble-wrapper {
    max-width: 75%;
  }
}

/* ===== TABLET PORTRAIT ===== */
@media (max-width: 768px) {
  .chat-messages {
    padding: 16px;
  }

  .main-input-wrapper {
    padding: 12px 16px;
  }

  /* Aumenta a largura máxima das mensagens em telas menores */
  .message-bubble-wrapper {
    max-width: 85%;
  }

  /* Reduz gap entre avatar e mensagem no tablet */
  .message {
    gap: 6px;
    margin: 0;
  }

  /* Espaçamento específico para mensagens no tablet */
  .user-message,
  .system-message {
    margin: 0 0 14px 0;
  }

  /* Ajusta padding das mensagens para tablet */
  .message-content {
    padding: 10px 14px;
  }

  /* Ajusta tamanho do timestamp para tablet */
  .message-timestamp {
    font-size: 10px;
    margin-top: 3px;
  }

  .input-controls {
    gap: 6px;
  }

  .control-btn {
    width: 40px;
    height: 40px;
  }

  .context-input-wrapper,
  .transcription-display {
    padding: 12px;
  }
}

/* ===== MOBILE LANDSCAPE ===== */
@media (max-width: 768px) and (orientation: landscape) {
  .chat-messages {
    padding: 12px;
  }
  
  .main-input-wrapper {
    padding: 8px 12px;
  }
  
  .message-bubble-wrapper {
    max-width: 80%;
  }
}

/* ===== MOBILE PORTRAIT ===== */
@media (max-width: 480px) {
  .conversational-chat {
    border-radius: 12px;
  }
  
  .chat-messages {
    padding: 12px 8px;
  }

  .main-input-wrapper {
    padding: 8px 12px;
  }

  /* Mobile specific message styling */
  .message-bubble-wrapper {
    max-width: 90%;
  }

  .message {
    gap: 4px;
  }

  .user-message,
  .system-message {
    margin: 0 0 12px 0;
  }

  .message-content {
    padding: 8px 12px;
    font-size: 14px;
  }

  .message-text {
    font-size: 14px;
    line-height: 1.5;
  }

  .message-timestamp {
    font-size: 9px;
    margin-top: 2px;
  }

  /* Avatar adjustments for mobile */
  .message-avatar {
    width: 28px;
    height: 28px;
  }

  .message-avatar svg {
    width: 20px;
    height: 20px;
  }

  /* Control buttons for mobile */
  .input-controls {
    gap: 4px;
    margin-left: 8px;
  }

  .control-btn {
    width: 36px;
    height: 36px;
  }

  /* Context and transcription for mobile */
  .context-input-wrapper,
  .transcription-display {
    padding: 8px;
    margin-bottom: 8px;
  }

  .context-input {
    padding: 8px;
    min-height: 50px;
    font-size: 13px;
  }

  .transcription-text {
    font-size: 13px;
  }

  /* Welcome message adjustments */
  .welcome-message {
    padding: 20px 12px;
  }

  .welcome-message h3 {
    font-size: 18px;
  }

  .welcome-message p {
    font-size: 13px;
  }

  /* Scroll button for mobile */
  .scroll-to-bottom-btn {
    width: 40px;
    height: 40px;
    bottom: 16px;
    margin-right: 8px;
  }
}

/* ===== VERY SMALL SCREENS ===== */
@media (max-width: 320px) {
  .chat-messages {
    padding: 8px 6px;
  }
  
  .main-input-wrapper {
    padding: 6px 8px;
  }
  
  .message-bubble-wrapper {
    max-width: 95%;
  }
  
  .message-content {
    padding: 6px 10px;
  }
  
  .control-btn {
    width: 32px;
    height: 32px;
  }
  
  .input-controls {
    gap: 2px;
    margin-left: 4px;
  }
}

/* ===== PRINT STYLES ===== */
@media print {
  .conversational-chat {
    background: white !important;
    border: 1px solid #000 !important;
    box-shadow: none !important;
  }
  
  .chat-input-area,
  .input-controls,
  .scroll-to-bottom-btn {
    display: none !important;
  }
  
  .message-content {
    background: #f5f5f5 !important;
    border: 1px solid #ddd !important;
    color: #000 !important;
  }
  
  .message-timestamp {
    color: #666 !important;
  }
} import { ChatMessage } from "../hooks/usePersistentMessages";

export interface ChatConversation {
  id: string;
  title: string;
  lastMessage: string;
  lastMessageTime: Date;
  createdAt: Date;
  messages: ChatMessage[];
  isActive: boolean;
}

export interface ChatHistoryState {
  conversations: ChatConversation[];
  currentConversationId: string | null;
}

export interface UseChatHistoryReturn {
  conversations: ChatConversation[];
  currentConversation: ChatConversation | null;
  currentConversationId: string | null;
  createNewConversation: () => string;
  selectConversation: (id: string) => void;
  deleteConversation: (id: string) => void;
  updateConversationTitle: (id: string, title: string) => void;
  addMessageToConversation: (
    conversationId: string,
    message: ChatMessage
  ) => void;
  searchConversations: (query: string) => ChatConversation[];
  clearConversationMessages: (conversationId: string) => void;
}
import { MicrophoneState, SelectedDevices } from "../../../../../context";
import { ChatMessage as PersistentChatMessage } from "../hooks/usePersistentMessages";
import { ChatConversation } from "./ChatHistoryTypes";

/**
 * Core chat message interface representing a single message in the chat
 */
export interface ChatMessage {
  /** Unique identifier for the message */
  id: string;
  /** Type of message - user input, system message, or error */
  type: "user" | "system" | "error";
  /** The actual message content */
  content: string;
  /** When the message was created */
  timestamp: Date;
  /** Whether this message has additional context */
  hasContext?: boolean;
  /** The context content if hasContext is true */
  contextContent?: string;
}

// Component props interfaces following Interface Segregation Principle
export interface ChatInputProps {
  value: string;
  onChange: (value: string) => void;
  onSend: () => void;
  onKeyPress: (e: React.KeyboardEvent) => void;
  disabled?: boolean;
  placeholder?: string;
}

export interface ContextInputProps {
  value: string;
  onChange: (value: string) => void;
  onClose: () => void;
  show: boolean;
}

export interface TranscriptionDisplayProps {
  text: string;
  onClear: () => void;
  transcriptions?: Array<{
    text: string;
    timestamp: string;
    speaker: string;
    sent?: boolean;
  }>;
}

export interface ChatControlsProps {
  microphoneState: MicrophoneState;
  onToggleRecording: () => void;
  onSend: () => void;
  onToggleContext: () => void;
  canSend: boolean;
  showContext: boolean;
  onToggleAudioSettings?: () => void;
  showAudioSettings?: boolean;
  audioSettingsButtonRef?: React.RefObject<HTMLElement>;
}

export interface DebugControlsProps {
  onAddTestMessage: () => void;
  onAddTestAI: () => void;
  onRestore: () => void;
  onClearAll: () => void;
  hasBackup: boolean;
}

export interface ScrollButtonProps {
  show: boolean;
  onClick: () => void;
}

export interface ChatMessagesContainerProps {
  messages: ChatMessage[];
  isProcessing: boolean;
  onScrollChange: (isNearBottom: boolean) => void;
  scrollRef: React.RefObject<HTMLDivElement | null>;
  showScrollButton: boolean;
  onScrollToBottom: () => void;
  newMessageCount?: number;
  hasNewMessages?: boolean;
  onAddTestMessage?: () => void;
  onResetState?: () => void;
  onClearMessages?: () => void;
}

/**
 * Main props interface for the ConversationalChat component
 */
export interface ConversationalChatProps {
  // Transcription handling
  /** Current transcription text from speech-to-text */
  transcriptionText: string;
  /** Handler for transcription text changes */
  onTranscriptionChange: (value: string) => void;
  /** Handler to clear transcription */
  onClearTranscription: () => void;

  // AI Response handling
  /** Current AI response text */
  aiResponseText: string;
  /** Handler for AI response text changes */
  onAiResponseChange: (value: string) => void;
  /** Handler to clear AI response */
  onClearAiResponse: () => void;

  // Context handling
  /** Temporary context for the current conversation */
  temporaryContext: string;
  /** Handler for temporary context changes */
  onTemporaryContextChange: (value: string) => void;

  // Recording controls
  /** Current microphone state */
  microphoneState: MicrophoneState;
  /** Handler to toggle recording on/off */
  onToggleRecording: () => void;
  /** Handler to send a prompt to the AI */
  onSendPrompt: (messageContent?: string, contextContent?: string) => void;

  // Audio settings props (optional)
  /** Current selected language */
  language?: string;
  /** Handler to change language */
  setLanguage?: (value: string) => void;
  /** Whether microphone is enabled */
  isMicrophoneOn?: boolean;
  /** Handler to toggle microphone */
  setIsMicrophoneOn?: (value: boolean) => void;
  /** Whether system audio is enabled */
  isSystemAudioOn?: boolean;
  /** Handler to toggle system audio */
  setIsSystemAudioOn?: (value: boolean) => void;
  /** Available audio devices */
  audioDevices?: MediaDeviceInfo[];
  /** Currently selected audio devices */
  selectedDevices?: SelectedDevices;
  /** Handler for audio device changes */
  handleDeviceChange?: (deviceId: string, isSystemAudio: boolean) => void;

  // Chat History props (optional)
  /** Current active conversation */
  currentConversation?: ChatConversation | null;
  /** Handler to add a message to a conversation */
  onAddMessageToConversation?: (
    conversationId: string,
    message: PersistentChatMessage
  ) => void;

  // Processing state callback (optional)
  /** Handler called when processing state changes */
  onProcessingChange?: (isProcessing: boolean) => void;
}

/**
 * Chat state management interface used by internal hooks
 */
export interface ChatState {
  /** Current input message being typed */
  inputMessage: string;
  /** Setter for input message */
  setInputMessage: (value: string) => void;
  /** Current context text */
  currentContext: string;
  /** Setter for context text */
  setCurrentContext: (value: string) => void;
  /** Whether to show the context input field */
  showContextField: boolean;
  /** Toggle context field visibility */
  setShowContextField: (show: boolean) => void;
  /** Whether a message is currently being processed */
  isProcessing: boolean;
  /** Set processing state */
  setIsProcessing: (processing: boolean) => void;
  /** Reference to processing timeout for cleanup */
  processingTimeoutRef: React.MutableRefObject<NodeJS.Timeout | null>;
}

/**
 * Scroll state management interface for chat messages container
 */
export interface ScrollState {
  /** Whether to show the scroll-to-bottom button */
  showScrollButton: boolean;
  /** Function to scroll to the bottom of messages */
  scrollToBottom: () => void;
  /** Handler for scroll events */
  handleScroll: () => void;
  /** Reference to the messages container element */
  messagesRef: React.RefObject<HTMLDivElement | null>;
  /** Count of new messages since last scroll */
  newMessageCount: number;
  /** Whether there are new messages to notify about */
  hasNewMessages: boolean;
  /** Clear new message notification */
  clearNotification: () => void;
}

// Add new interface for ChatInputAreaProps
export interface ChatInputAreaProps extends ConversationalChatProps {
  chatState: ChatState;
  onSendMessage: () => void;
  onKeyPress: (e: React.KeyboardEvent) => void;
  onToggleContext: () => void;
  onAddTestMessage: () => void;
  onAddTestAI: () => void;
  onRestore: () => void;
  onClearAll: () => void;
  hasBackup: boolean;
  onToggleAudioSettings?: () => void;
  showAudioSettings?: boolean;
  audioSettingsButtonRef?: React.RefObject<HTMLElement>;
}
import { nanoid } from "nanoid";
import { ChatMessage } from "../hooks/usePersistentMessages";
import { ChatConversation } from "../types/ChatHistoryTypes";

const OLD_STORAGE_KEY = "orch-chat-messages";
const NEW_STORAGE_KEY = "orch-chat-history";
const MIGRATION_FLAG_KEY = "orch-chat-migration-completed";

/**
 * Migrates old chat messages to the new conversation format
 */
export const migrateOldChatMessages = (): boolean => {
  try {
    // Check if migration has already been completed
    const migrationCompleted = localStorage.getItem(MIGRATION_FLAG_KEY);
    if (migrationCompleted === "true") {
      return false;
    }

    // Check if there's already data in the new format
    const newData = localStorage.getItem(NEW_STORAGE_KEY);
    if (newData) {
      // Mark migration as completed if new data exists
      localStorage.setItem(MIGRATION_FLAG_KEY, "true");
      return false;
    }

    // Try to load old messages
    const oldData = localStorage.getItem(OLD_STORAGE_KEY);
    if (!oldData) {
      // No old data to migrate
      localStorage.setItem(MIGRATION_FLAG_KEY, "true");
      return false;
    }

    console.log("[MIGRATION] Starting chat history migration...");

    // Parse old messages
    const oldMessages: ChatMessage[] = JSON.parse(oldData).map((msg: any) => ({
      ...msg,
      timestamp: new Date(msg.timestamp),
    }));

    if (oldMessages.length === 0) {
      localStorage.setItem(MIGRATION_FLAG_KEY, "true");
      return false;
    }

    // Create a new conversation with the old messages
    const now = new Date();
    const migrationConversation: ChatConversation = {
      id: nanoid(),
      title: "Conversas Anteriores (Migradas)",
      lastMessage: oldMessages[oldMessages.length - 1]?.content || "",
      lastMessageTime: oldMessages[oldMessages.length - 1]?.timestamp || now,
      createdAt: oldMessages[0]?.timestamp || now,
      messages: oldMessages,
      isActive: true,
    };

    // Save in new format
    const newHistoryData = {
      conversations: [migrationConversation],
      currentId: migrationConversation.id,
    };

    localStorage.setItem(NEW_STORAGE_KEY, JSON.stringify(newHistoryData));

    // Mark migration as completed
    localStorage.setItem(MIGRATION_FLAG_KEY, "true");

    console.log(
      `[MIGRATION] Successfully migrated ${oldMessages.length} messages to new format`
    );

    // Optionally backup old data before removing
    localStorage.setItem(OLD_STORAGE_KEY + "-backup", oldData);

    return true;
  } catch (error) {
    console.error("[MIGRATION] Error during chat history migration:", error);
    return false;
  }
};

/**
 * Clears migration flag (useful for testing)
 */
export const resetMigration = () => {
  localStorage.removeItem(MIGRATION_FLAG_KEY);
  console.log("[MIGRATION] Migration flag reset");
};

/**
 * Checks if migration is needed
 */
export const isMigrationNeeded = (): boolean => {
  const migrationCompleted = localStorage.getItem(MIGRATION_FLAG_KEY);
  const hasOldData = !!localStorage.getItem(OLD_STORAGE_KEY);
  const hasNewData = !!localStorage.getItem(NEW_STORAGE_KEY);

  return !migrationCompleted && hasOldData && !hasNewData;
};
/* ================================================================
 * CONVERSATIONAL CHAT - MAIN STYLES
 * ================================================================
 * Modular CSS organization for better maintainability
 * Each module handles a specific aspect of the chat interface
 */

/* Import all CSS modules */
@import './styles/ConversationalChat.base.css';
@import './styles/ConversationalChat.messages.css';
@import './styles/ConversationalChat.input.css';
@import './styles/ConversationalChat.animations.css';
@import './styles/ConversationalChat.components.css';
@import './styles/ConversationalChat.responsive.css';

/* Main Container - Core Layout */
.conversational-chat {
  display: flex;
  flex-direction: column;
  height: 100%;
  width: 100%;
  background: linear-gradient(
    135deg,
    rgba(0, 10, 20, 0.95) 0%,
    rgba(10, 0, 20, 0.95) 100%
  );
  border-radius: 16px;
  -webkit-backdrop-filter: blur(20px);
  backdrop-filter: blur(20px);
  border: 1px solid rgba(0, 250, 255, 0.2);
  overflow: hidden; /* Garante que os cantos arredondados cortem o conteúdo */
  position: relative;
}

/* Welcome Message */
.welcome-message {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  text-align: center;
  padding: 40px 20px;
  color: rgba(255, 255, 255, 0.7);
  height: 100%;
}

/* Message Styles */
.message {
  display: flex;
  gap: 8px;
  max-width: 100%;
  animation: messageSlideIn 0.3s ease-out;
}

@keyframes messageSlideIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.message-avatar {
  flex-shrink: 0;
  width: 32px;
  height: 32px;
  border-radius: 50%;
  margin-top: 4px;
}

.message-bubble-wrapper {
  display: flex;
  flex-direction: column;
  gap: 4px;
  max-width: 75%;
}

.user-message {
  flex-direction: row-reverse;
}

.user-message .message-bubble-wrapper {
  align-items: flex-end;
}

.system-message {
  justify-content: flex-start;
}

.system-message .message-bubble-wrapper {
  align-items: flex-start;
}

.message-content {
  padding: 12px 16px;
  word-wrap: break-word;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
}

.user-message .message-content {
  background: linear-gradient(135deg, rgba(0, 250, 255, 0.3) 0%, rgba(0, 102, 204, 0.3) 100%);
  border: 1px solid rgba(0, 250, 255, 0.3);
  border-radius: 18px 18px 4px 18px;
}

.system-message .message-content {
  background: linear-gradient(135deg, rgba(255, 77, 210, 0.15) 0%, rgba(124, 77, 255, 0.15) 100%);
  border: 1px solid rgba(255, 77, 210, 0.3);
  border-radius: 18px 18px 18px 4px;
}

.message-text {
  font-size: 15px;
  line-height: 1.6;
  color: rgba(255, 255, 255, 0.9);
}

.message-timestamp {
  font-size: 11px;
  color: rgba(255, 255, 255, 0.4);
  padding: 0 4px;
}import React, { useCallback, useEffect, useRef, useState } from "react";
import { useSettingsState } from "../settings/useSettingsState";
import { AudioSettingsPopover } from "./components/AudioSettingsPopover";
import { ChatInputArea } from "./components/ChatInputArea";
import { ChatMessagesContainer } from "./components/ChatMessagesContainer";
import "./ConversationalChat.css";
import { useChatScroll } from "./hooks/useChatScroll";
import { useChatState } from "./hooks/useChatState";
import { useConversationMessages } from "./hooks/useConversationMessages";
import { usePersistentMessages } from "./hooks/usePersistentMessages";
import { ConversationalChatProps } from "./types/ChatTypes";

/**
 * Refactored Conversational Chat Component
 *
 * Applies SOLID, DRY, KISS, and YAGNI principles:
 *
 * SOLID:
 * - Single Responsibility: Each component has one clear purpose
 * - Open/Closed: Components are open for extension, closed for modification
 * - Liskov Substitution: Components can be replaced with compatible implementations
 * - Interface Segregation: Components only depend on interfaces they use
 * - Dependency Inversion: Depends on abstractions (hooks) not concrete implementations
 *
 * DRY: Shared logic in hooks, reusable components, no code duplication
 * KISS: Simple, focused components that are easy to understand
 * YAGNI: Debug functionality separated and only included in development
 */
const ConversationalChatRefactored: React.FC<ConversationalChatProps> = ({
  transcriptionText,
  onTranscriptionChange,
  onClearTranscription,
  aiResponseText,
  onAiResponseChange,
  onClearAiResponse,
  temporaryContext,
  onTemporaryContextChange,
  microphoneState,
  onToggleRecording,
  onSendPrompt,
  // Audio settings props
  language,
  setLanguage,
  isMicrophoneOn,
  setIsMicrophoneOn,
  isSystemAudioOn,
  setIsSystemAudioOn,
  audioDevices,
  selectedDevices,
  handleDeviceChange,
  // Chat History props
  currentConversation,
  onAddMessageToConversation,
  onProcessingChange,
}) => {
  // Component lifecycle tracking
  const componentId = useRef(
    `chat-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
  );
  const mountTime = useRef(Date.now());

  // Log prop changes
  useEffect(() => {
    console.log("[CHAT_PROPS] Conversation prop changed:", {
      conversationId: currentConversation?.id,
      title: currentConversation?.title,
      messageCount: currentConversation?.messages.length,
    });
  }, [currentConversation]);

  // Custom hooks for state management (Dependency Inversion Principle)
  // Use conversation messages if available, otherwise fall back to persistent messages
  const persistentMessagesHook = usePersistentMessages();
  const conversationMessagesHook = useConversationMessages({
    currentConversation: currentConversation || null,
    onAddMessage: onAddMessageToConversation || (() => {}),
    onClearConversation: () => {}, // Will be implemented later
  });

  // Clear input state when conversation changes
  useEffect(() => {
    if (currentConversation) {
      // Clear any input state when switching conversations
      chatState.setInputMessage("");
      chatState.setCurrentContext("");
      chatState.setShowContextField(false);
      chatState.setIsProcessing(false);

      // Clear any ongoing processing timeouts
      if (chatState.processingTimeoutRef.current) {
        clearTimeout(chatState.processingTimeoutRef.current);
        chatState.processingTimeoutRef.current = null;
      }

      // Clear any pending AI responses
      if (
        aiResponseText &&
        (aiResponseText === "Processing..." ||
          aiResponseText === "Processando...")
      ) {
        onClearAiResponse();
      }

      // Reset response tracking
      isReceivingResponse.current = false;
      lastProcessedResponse.current = "";

      console.log("[CHAT] Conversation changed, clearing all state");
    }
  }, [currentConversation?.id]); // chatState and other deps intentionally omitted to prevent infinite loops

  // Select the appropriate hook based on whether we have a conversation system
  const useConversationSystem = !!(
    currentConversation && onAddMessageToConversation
  );
  const {
    messages: chatMessages,
    addMessage,
    clearMessages,
  } = useConversationSystem ? conversationMessagesHook : persistentMessagesHook;

  // Recovery is only available with persistent messages
  const recovery = useConversationSystem
    ? {
        hasBackup: false,
        restoreFromBackup: () => {},
        clearBackup: () => {},
        integrityCheck: () => false,
        lastSaveTime: 0,
      }
    : persistentMessagesHook.recovery;

  const chatState = useChatState();

  // Notify parent when processing state changes
  useEffect(() => {
    if (onProcessingChange) {
      onProcessingChange(chatState.isProcessing);
    }
  }, [chatState.isProcessing, onProcessingChange]);

  // Audio settings state
  const [showAudioSettings, setShowAudioSettings] = useState(false);
  const audioSettingsButtonRef = useRef<HTMLElement>(null!);
  const audioSettings = useSettingsState(showAudioSettings);

  // Refs for scroll management
  const messagesContainerRef = useRef<HTMLDivElement>(null!);

  // Use the updated scroll hook
  const scrollState = useChatScroll({
    messages: chatMessages,
    messagesContainerRef,
  });

  // Log component lifecycle (only in development)
  useEffect(() => {
    // Clear state on mount
    chatState.setInputMessage("");
    chatState.setCurrentContext("");
    chatState.setShowContextField(false);
    chatState.setIsProcessing(false);

    if (process.env.NODE_ENV !== "production") {
      console.log("🔄 [CHAT_LIFECYCLE] Component MOUNTED:", {
        componentId: componentId.current,
        mountTime: new Date(mountTime.current).toISOString(),
        conversationId: currentConversation?.id,
      });

      return () => {
        console.log("🔄 [CHAT_LIFECYCLE] Component UNMOUNTING:", {
          componentId: componentId.current,
          lifespan: Date.now() - mountTime.current,
        });
      };
    }
  }, []); // chatState omitted intentionally - we want this to run only on mount

  // Track if we're currently receiving a response
  const isReceivingResponse = useRef(false);
  const lastProcessedResponse = useRef<string>("");
  const responseDebounceTimer = useRef<NodeJS.Timeout | null>(null);

  // Handle AI response processing with debounce and better state management
  useEffect(() => {
    if (!aiResponseText || aiResponseText.trim() === "") {
      return;
    }

    // Handle processing state
    if (
      aiResponseText === "Processing..." ||
      aiResponseText === "Processando..."
    ) {
      chatState.setIsProcessing(true);
      isReceivingResponse.current = true;
      return;
    }

    // Clear any existing debounce timer
    if (responseDebounceTimer.current) {
      clearTimeout(responseDebounceTimer.current);
    }

    // Debounce the response processing to avoid rapid updates
    responseDebounceTimer.current = setTimeout(() => {
      // Check if this is truly a new response
      if (aiResponseText === lastProcessedResponse.current) {
        console.log("⚠️ [CHAT] Same response, skipping");
        return;
      }

      // Check for duplicates in existing messages
      const isDuplicate = chatMessages.some(
        (msg) => msg.type === "system" && msg.content === aiResponseText
      );

      if (isDuplicate) {
        console.log("⚠️ [CHAT] Duplicate AI response in messages, skipping");
        return;
      }

      // Check if this looks like a final response (not partial)
      // A final response typically doesn't end with "..." and has reasonable length
      const looksLikeFinalResponse =
        !aiResponseText.endsWith("...") &&
        aiResponseText.length > 10 &&
        !aiResponseText.includes("Processando") &&
        !aiResponseText.includes("Processing");

      if (looksLikeFinalResponse) {
        console.log(
          "✅ [CHAT] Adding final AI response:",
          aiResponseText.substring(0, 50)
        );

        // Add AI response
        addMessage({
          type: "system",
          content: aiResponseText,
        });

        // Update last processed response
        lastProcessedResponse.current = aiResponseText;

        // Clear processing state
        chatState.setIsProcessing(false);
        isReceivingResponse.current = false;

        if (chatState.processingTimeoutRef.current) {
          clearTimeout(chatState.processingTimeoutRef.current);
          chatState.processingTimeoutRef.current = null;
        }

        // Only clear AI response after successfully adding the message
        // Add a small delay to ensure the message is properly saved
        setTimeout(() => {
          onClearAiResponse();
        }, 100);
      } else {
        console.log("🔄 [CHAT] Partial response detected, waiting for more...");
        // For partial responses, just update the processing state
        isReceivingResponse.current = true;
      }
    }, 300); // 300ms debounce

    // Cleanup function
    return () => {
      if (responseDebounceTimer.current) {
        clearTimeout(responseDebounceTimer.current);
      }
    };
  }, [aiResponseText, chatMessages, addMessage, onClearAiResponse, chatState]);

  // Handle send message (KISS principle - simple and clear)
  const handleSendMessage = useCallback(() => {
    const messageContent = chatState.inputMessage.trim();

    if (!messageContent && !transcriptionText.trim()) {
      return;
    }

    const finalContent = messageContent || transcriptionText.trim();

    // Add user message with context info
    addMessage({
      type: "user",
      content: finalContent,
      hasContext: !!chatState.currentContext,
      contextContent: chatState.currentContext || undefined,
    });

    // Clear inputs
    chatState.setInputMessage("");
    chatState.setCurrentContext("");
    chatState.setShowContextField(false);
    onTemporaryContextChange("");

    // IMPORTANT: Clear transcription after sending
    if (transcriptionText.trim()) {
      onClearTranscription();
    }

    // Set processing state with timeout
    chatState.setIsProcessing(true);
    if (chatState.processingTimeoutRef.current) {
      clearTimeout(chatState.processingTimeoutRef.current);
    }

    // Send prompt - pass message and context separately
    setTimeout(() => {
      // Pass the message as first parameter and context as second
      onSendPrompt(finalContent, chatState.currentContext || undefined);
    }, 0);
  }, [
    chatState,
    transcriptionText,
    addMessage,
    onTemporaryContextChange,
    onSendPrompt,
    onClearTranscription,
  ]);

  // Handle key press (Enter to send, Shift+Enter for new line)
  const handleKeyPress = useCallback(
    (e: React.KeyboardEvent) => {
      if (e.key === "Enter" && !e.shiftKey) {
        e.preventDefault();
        handleSendMessage();
      }
    },
    [handleSendMessage]
  );

  // Handle context toggle
  const handleToggleContext = useCallback(() => {
    if (chatState.showContextField) {
      chatState.setCurrentContext("");
      chatState.setShowContextField(false);
    } else {
      chatState.setShowContextField(true);
    }
  }, [chatState]);

  // Handle audio settings toggle
  const handleToggleAudioSettings = useCallback(() => {
    setShowAudioSettings(!showAudioSettings);
  }, [showAudioSettings]);

  // Debug functions (YAGNI principle - only in development)
  const debugFunctions = React.useMemo(() => {
    if (process.env.NODE_ENV === "production") {
      return {};
    }

    return {
      addTestMessage: () => {
        addMessage({
          type: "user",
          content: "Test message",
        });
      },
      addTestAIResponse: () => {
        addMessage({
          type: "system",
          content:
            "This is a test AI response to verify the chat is working correctly.",
        });
      },
      resetChatState: () => {
        chatState.setIsProcessing(false);
        if (chatState.processingTimeoutRef.current) {
          clearTimeout(chatState.processingTimeoutRef.current);
          chatState.processingTimeoutRef.current = null;
        }
      },
    };
  }, [addMessage, chatState]);

  // Force scroll to bottom on mount and when messages are loaded
  useEffect(() => {
    // Small delay to ensure DOM is fully rendered
    const scrollTimer = setTimeout(() => {
      if (messagesContainerRef.current && chatMessages.length > 0) {
        // Force multiple scroll attempts to ensure we reach the absolute bottom
        const forceScroll = () => {
          if (!messagesContainerRef.current) return;

          const element = messagesContainerRef.current;
          const maxScroll = element.scrollHeight - element.clientHeight;

          // Try different methods to ensure scroll
          element.scrollTop = element.scrollHeight;
          element.scrollTo(0, element.scrollHeight);

          // Verify and retry if needed
          requestAnimationFrame(() => {
            if (element.scrollTop < maxScroll - 2) {
              element.scrollTop = maxScroll + 100; // Overshoot to ensure bottom
            }
          });
        };

        // Execute multiple times with delays
        forceScroll();
        setTimeout(forceScroll, 100);
        setTimeout(forceScroll, 300);
      }
    }, 50);

    return () => clearTimeout(scrollTimer);
  }, []); // Only run once on mount

  return (
    <div className="conversational-chat">
      {/* Chat Messages Container */}
      <ChatMessagesContainer
        messages={chatMessages}
        isProcessing={chatState.isProcessing}
        onScrollChange={() => {}} // Not needed anymore
        scrollRef={messagesContainerRef}
        showScrollButton={scrollState.showScrollButton}
        onScrollToBottom={scrollState.scrollToBottom}
        onAddTestMessage={debugFunctions.addTestMessage}
        onResetState={debugFunctions.resetChatState}
        onClearMessages={clearMessages}
      />

      {/* Chat Input Area */}
      <ChatInputArea
        transcriptionText={transcriptionText}
        onTranscriptionChange={onTranscriptionChange}
        onClearTranscription={onClearTranscription}
        aiResponseText={aiResponseText}
        onAiResponseChange={onAiResponseChange}
        onClearAiResponse={onClearAiResponse}
        temporaryContext={temporaryContext}
        onTemporaryContextChange={onTemporaryContextChange}
        microphoneState={microphoneState}
        onToggleRecording={onToggleRecording}
        onSendPrompt={onSendPrompt}
        chatState={chatState}
        onSendMessage={handleSendMessage}
        onKeyPress={handleKeyPress}
        onToggleContext={handleToggleContext}
        onAddTestMessage={debugFunctions.addTestMessage || (() => {})}
        onAddTestAI={debugFunctions.addTestAIResponse || (() => {})}
        onRestore={recovery.restoreFromBackup}
        onClearAll={() => {
          clearMessages();
          recovery.clearBackup();
        }}
        hasBackup={recovery.hasBackup}
        onToggleAudioSettings={handleToggleAudioSettings}
        showAudioSettings={showAudioSettings}
        audioSettingsButtonRef={audioSettingsButtonRef}
      />

      {/* Audio Settings Popover */}
      <AudioSettingsPopover
        show={showAudioSettings}
        onClose={() => setShowAudioSettings(false)}
        anchorRef={audioSettingsButtonRef}
        settings={{
          // Language
          language: language || "pt-BR",
          setLanguage: setLanguage || (() => {}),

          // Device selection
          isMicrophoneOn: isMicrophoneOn || false,
          setIsMicrophoneOn: setIsMicrophoneOn || (() => {}),
          isSystemAudioOn: isSystemAudioOn || false,
          setIsSystemAudioOn: setIsSystemAudioOn || (() => {}),
          audioDevices: audioDevices || [],
          selectedDevices: selectedDevices || {
            microphone: null,
            systemAudio: null,
          },
          handleDeviceChange: handleDeviceChange || (() => {}),
        }}
      />
    </div>
  );
};

// Custom comparison to ignore frequent transcriptionText updates
function areEqual(
  prev: ConversationalChatProps,
  next: ConversationalChatProps
) {
  const isEqual =
    prev.aiResponseText === next.aiResponseText &&
    prev.temporaryContext === next.temporaryContext &&
    prev.microphoneState === next.microphoneState &&
    prev.onAiResponseChange === next.onAiResponseChange &&
    prev.onClearAiResponse === next.onClearAiResponse &&
    prev.onClearTranscription === next.onClearTranscription &&
    prev.onSendPrompt === next.onSendPrompt &&
    prev.onToggleRecording === next.onToggleRecording &&
    prev.onTranscriptionChange === next.onTranscriptionChange &&
    // Audio settings comparisons (only check if props changed, not device arrays)
    prev.language === next.language &&
    prev.setLanguage === next.setLanguage &&
    prev.isMicrophoneOn === next.isMicrophoneOn &&
    prev.setIsMicrophoneOn === next.setIsMicrophoneOn &&
    prev.isSystemAudioOn === next.isSystemAudioOn &&
    prev.setIsSystemAudioOn === next.setIsSystemAudioOn &&
    prev.handleDeviceChange === next.handleDeviceChange;
  // transcriptionText, audioDevices and selectedDevices deliberately ignored

  if (!isEqual && process.env.NODE_ENV !== "production") {
    console.log(
      "🔄 [CHAT_RERENDER] ConversationalChat re-rendering due to prop change:",
      {
        aiResponseChanged: prev.aiResponseText !== next.aiResponseText,
        aiResponseText: {
          prev: prev.aiResponseText?.substring(0, 50),
          next: next.aiResponseText?.substring(0, 50),
        },
        temporaryContextChanged:
          prev.temporaryContext !== next.temporaryContext,
        microphoneStateChanged: prev.microphoneState !== next.microphoneState,
      }
    );
  }

  return isEqual;
}

// Export component with memo to prevent unnecessary re-renders
export const ConversationalChat = React.memo(
  ConversationalChatRefactored,
  areEqual
);
// Export main component
export { ConversationalChat } from "./ConversationalChat";

// Export types
export * from "./types/ChatTypes";

// Export hooks
export { useChatScroll } from "./hooks/useChatScroll";
export { useChatState } from "./hooks/useChatState";
export { usePersistentMessages } from "./hooks/usePersistentMessages";

// Individual components (for potential reuse)
export { ChatInputArea } from "./components/ChatInputArea";
export { ChatMessagesContainer } from "./components/ChatMessagesContainer";
export { ContextInput } from "./components/ContextInput";
export { MessageInput } from "./components/MessageInput";
export { ScrollToBottomButton } from "./components/ScrollToBottomButton";
export { TranscriptionDisplay } from "./components/TranscriptionDisplay";
# ConversationalChat - Refatoração SOLID, DRY, KISS, YAGNI

Esta refatoração aplica os princípios fundamentais de desenvolvimento de software conforme descrito nos artigos de referência sobre [SOLID, Clean Code, DRY, KISS, YAGNI](https://medium.com/javascript-render/solid-clean-code-dry-kiss-yagni-principles-react-97fe92da25cd) e [Common Sense Refactoring](https://alexkondov.com/refactoring-a-messy-react-component/).

## 🎯 Princípios Aplicados

### SOLID

#### **S - Single Responsibility Principle (SRP)**
- **Antes**: Um componente monolítico com 964 linhas fazendo tudo
- **Depois**: Componentes focados com responsabilidades únicas:
  - `MessageInput` - apenas input de mensagem
  - `ContextInput` - apenas input de contexto  
  - `ChatControls` - apenas controles do chat
  - `ChatMessagesContainer` - apenas exibição de mensagens
  - `useChatState` - apenas gerenciamento de estado
  - `useChatScroll` - apenas gerenciamento de scroll

#### **O - Open/Closed Principle (OCP)**
- Componentes são **abertos para extensão** através de props
- **Fechados para modificação** - funcionalidade core protegida
- Novos tipos de mensagem podem ser adicionados sem modificar componentes existentes

#### **L - Liskov Substitution Principle (LSP)**
- Componentes podem ser substituídos por implementações compatíveis
- Interfaces bem definidas garantem substituibilidade

#### **I - Interface Segregation Principle (ISP)**
- Cada componente recebe **apenas as props que precisa**
- Interfaces específicas para cada responsabilidade
- Exemplo: `ChatInputProps` vs `ChatControlsProps`

#### **D - Dependency Inversion Principle (DIP)**
- Componentes dependem de **abstrações** (hooks) não implementações
- `usePersistentMessages`, `useChatState`, `useChatScroll` são abstrações
- Facilita testes e substituição de implementações

### DRY (Don't Repeat Yourself)

- **Hooks customizados** eliminam duplicação de lógica
- **Componentes reutilizáveis** evitam código repetido
- **Tipos compartilhados** em `ChatTypes.ts`
- **Estilos consistentes** através de classes CSS

### KISS (Keep It Simple, Stupid)

- **Componentes pequenos** e focados (< 100 linhas cada)
- **Lógica clara** e fácil de entender
- **Nomes descritivos** para funções e variáveis
- **Estrutura simples** de arquivos

### YAGNI (You Aren't Gonna Need It)

- **Funcionalidades de debug** separadas em `DebugControls`
- **Removidas automaticamente** em produção (`process.env.NODE_ENV`)
- **Fácil remoção** sem afetar código principal

## 📁 Estrutura de Arquivos

```
ConversationalChat/
├── README.md                           # Esta documentação
├── index.ts                           # Exports centralizados
├── ConversationalChatRefactored.tsx   # Componente principal
├── types/
│   └── ChatTypes.ts                   # Tipos compartilhados
├── hooks/
│   ├── usePersistentMessages.ts       # Gerenciamento de mensagens
│   ├── useChatState.ts               # Estado do chat
│   └── useChatScroll.ts              # Comportamento de scroll
└── components/
    ├── MessageInput.tsx              # Input de mensagem
    ├── ContextInput.tsx              # Input de contexto
    ├── TranscriptionDisplay.tsx      # Exibição de transcrição
    ├── ChatControls.tsx              # Controles principais
    ├── DebugControls.tsx             # Controles de debug
    ├── ScrollToBottomButton.tsx      # Botão de scroll
    ├── ChatInputArea.tsx             # Área de input completa
    └── ChatMessagesContainer.tsx     # Container de mensagens
```

## 🔄 Comparação: Antes vs Depois

### Antes (Monolítico)
```typescript
// 964 linhas em um arquivo
// Múltiplas responsabilidades
// Lógica misturada
// Difícil de testar
// Difícil de manter
```

### Depois (Modular)
```typescript
// 12 arquivos focados
// Responsabilidade única por arquivo
// Lógica separada em hooks
// Fácil de testar individualmente
// Fácil de manter e estender
```

## 🚀 Benefícios da Refatoração

### **Manutenibilidade**
- Código mais fácil de entender e modificar
- Mudanças isoladas não afetam outros componentes
- Debugging mais simples

### **Testabilidade**
- Componentes pequenos são mais fáceis de testar
- Hooks podem ser testados independentemente
- Mocking mais simples

### **Reutilização**
- Componentes podem ser reutilizados em outros contextos
- Hooks podem ser compartilhados entre componentes
- Lógica não duplicada

### **Performance**
- `React.memo` aplicado estrategicamente
- Re-renders minimizados
- Componentes otimizados individualmente

### **Desenvolvimento**
- Funcionalidades de debug separadas
- Fácil adição de novas funcionalidades
- Onboarding mais rápido para novos desenvolvedores

## 🛠️ Como Usar

### Importação Simples
```typescript
import { ConversationalChat } from './ConversationalChat';
```

### Importação de Componentes Individuais
```typescript
import { 
  MessageInput, 
  ChatControls, 
  useChatState 
} from './ConversationalChat';
```

### Uso em Produção
```typescript
// Debug controls são automaticamente removidos em produção
// Logs de desenvolvimento não aparecem em produção
```

## 🔧 Extensibilidade

### Adicionando Novo Tipo de Mensagem
1. Atualizar `ChatTypes.ts`
2. Modificar `MessageItem` em `ChatMessagesContainer.tsx`
3. Nenhuma outra mudança necessária

### Adicionando Nova Funcionalidade
1. Criar novo hook se necessário
2. Criar novo componente focado
3. Compor no componente principal

### Customizando Comportamento
1. Substituir hooks por implementações customizadas
2. Manter interfaces compatíveis
3. Funcionalidade mantida

## 📚 Referências

- [SOLID, Clean Code, DRY, KISS, YAGNI Principles + React](https://medium.com/javascript-render/solid-clean-code-dry-kiss-yagni-principles-react-97fe92da25cd)
- [Common Sense Refactoring of a Messy React Component](https://alexkondov.com/refactoring-a-messy-react-component/)
- [SOLID, YAGNI, DRY, KISS principles in React](https://www.it-justice.com/blog/tech/solid-yagni-dry-kiss-principles-in-react/)

---

**Resultado**: Código mais limpo, manutenível, testável e extensível seguindo as melhores práticas da indústria. // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Hook para gerenciar compatibilidade entre modelos Deepgram e idiomas
 * Aplica princípios de Responsabilidade Única e extrai lógica de negócio do componente
 * Otimizado para evitar cálculos repetidos e logs excessivos
 */
export const useDeepgramLanguageCompatibility = () => {
  // Cache de resultados para evitar recálculos desnecessários
  const modelLanguageCache = new Map<string, string[]>();
  // Dicionário de idiomas disponíveis por família de modelo
  const modelLanguageCompatibility: { [key: string]: string[] } = {
    // Nova-3 suporta apenas inglês na versão inicial
    'nova-3': ['en', 'en-US', 'en-GB', 'en-AU', 'en-IN', 'en-NZ'],
    'nova-3-medical': ['en', 'en-US', 'en-GB', 'en-AU', 'en-IN', 'en-NZ'],
    
    // Nova-2 tem suporte extenso a idiomas
    'nova-2': [
      'multi', 
      'en', 'en-US', 'en-GB', 'en-AU', 'en-IN', 'en-NZ',
      'pt', 'pt-BR', 'pt-PT',
      'es', 'es-419',
      'fr', 'fr-CA',
      'de', 'de-CH',
      'it', 'ja', 'ko', 'ko-KR',
      'zh-CN', 'zh-TW', 'zh-HK',
      'ru', 'hi', 'nl', 'nl-BE',
      'bg', 'ca', 'cs', 'da', 'da-DK',
      'el', 'et', 'fi', 'hu', 'id',
      'lv', 'lt', 'ms', 'no', 'pl', 'ro',
      'sk', 'sv', 'sv-SE', 'th', 'th-TH',
      'tr', 'uk', 'vi'
    ],
    'nova-2-meeting': [
      'multi', 
      'en', 'en-US', 'en-GB', 'en-AU', 'en-IN', 'en-NZ',
      'pt', 'pt-BR', 'pt-PT',
      'es', 'es-419', 'fr', 'de', 'it', 'ja'
    ],
    'nova-2-phonecall': [
      'multi', 
      'en', 'en-US', 'en-GB', 'en-AU', 'en-IN', 'en-NZ',
      'pt', 'pt-BR', 'pt-PT',
      'es', 'es-419', 'fr', 'de', 'it', 'ja'
    ],
    'nova-2-video': [
      'multi', 
      'en', 'en-US', 'en-GB', 'en-AU', 'en-IN', 'en-NZ',
      'pt', 'pt-BR', 'pt-PT',
      'es', 'es-419', 'fr', 'de', 'it', 'ja'
    ],
    
    // Nova modelos padrão
    'nova': ['en', 'en-US', 'en-GB', 'en-AU', 'en-IN', 'en-NZ', 'pt', 'pt-BR', 'es', 'fr', 'de', 'it', 'ja', 'ko', 'nl', 'pl', 'ru'],
    'nova-phonecall': ['en', 'en-US', 'en-GB', 'pt', 'pt-BR', 'es', 'fr', 'de', 'it', 'ja'],
    
    // Enhanced models
    'enhanced': ['en', 'en-US', 'en-GB', 'en-AU', 'pt', 'pt-BR', 'es', 'es-419', 'fr', 'de', 'it', 'ja', 'hi', 'nl', 'id', 'zh-CN', 'ko', 'ru'],
    'enhanced-meeting': ['en', 'en-US', 'en-GB', 'pt', 'pt-BR', 'es', 'fr', 'de', 'it', 'ja'],
    'enhanced-phonecall': ['en', 'en-US', 'en-GB', 'pt', 'pt-BR', 'es', 'fr', 'de', 'it', 'ja', 'ko'],
    'enhanced-finance': ['en', 'en-US', 'en-GB'],
    
    // Base models
    'base': ['en', 'en-US', 'en-GB', 'pt', 'pt-BR', 'es', 'fr', 'de', 'it', 'ja', 'hi', 'nl', 'id', 'zh-CN', 'ko'],
    'base-meeting': ['en', 'en-US', 'en-GB', 'pt', 'pt-BR', 'es', 'fr', 'de', 'it', 'ja'],
    'base-phonecall': ['en', 'en-US', 'en-GB', 'pt', 'pt-BR', 'es', 'fr', 'de', 'it', 'ja', 'ko'],
    'base-finance': ['en', 'en-US', 'en-GB']
  };

  // Definições de idiomas para exibição
  const languageDisplay: { [key: string]: string } = {
    'multi': 'Multilingual (Auto-detect)',
    'en': 'English (Global)',
    'en-US': 'English (US)',
    'en-GB': 'English (UK)',
    'en-AU': 'English (Australia)',
    'en-IN': 'English (India)',
    'en-NZ': 'English (New Zealand)',
    'pt': 'Portuguese',
    'pt-BR': 'Portuguese (Brazil)',
    'pt-PT': 'Portuguese (Portugal)',
    'es': 'Spanish',
    'es-419': 'Spanish (Latin America)',
    'fr': 'French',
    'fr-CA': 'French (Canada)',
    'de': 'German',
    'de-CH': 'German (Switzerland)',
    'it': 'Italian',
    'ja': 'Japanese',
    'ko': 'Korean',
    'ko-KR': 'Korean',
    'zh-CN': 'Chinese (Simplified)',
    'zh-TW': 'Chinese (Traditional)',
    'zh-HK': 'Chinese (Cantonese)',
    'ru': 'Russian',
    'hi': 'Hindi',
    'nl': 'Dutch',
    'nl-BE': 'Dutch (Belgium)/Flemish',
    'bg': 'Bulgarian',
    'ca': 'Catalan',
    'cs': 'Czech',
    'da': 'Danish',
    'da-DK': 'Danish',
    'el': 'Greek',
    'et': 'Estonian',
    'fi': 'Finnish',
    'hu': 'Hungarian',
    'id': 'Indonesian',
    'lv': 'Latvian',
    'lt': 'Lithuanian',
    'ms': 'Malay',
    'no': 'Norwegian',
    'pl': 'Polish',
    'ro': 'Romanian',
    'sk': 'Slovak',
    'sv': 'Swedish',
    'sv-SE': 'Swedish',
    'th': 'Thai',
    'th-TH': 'Thai',
    'tr': 'Turkish',
    'uk': 'Ukrainian',
    'vi': 'Vietnamese'
  };

  /**
   * Retorna os idiomas compatíveis para um determinado modelo
   * Com tratamento robusto para variações na forma como o modelo está salvo
   * Otimizado com cache e logs reduzidos
   */
  const getCompatibleLanguages = (model: string): string[] => {
    // Tratamento para casos undefined/null
    if (!model) {
      return ['en', 'en-US'];
    }
    
    // Verificar cache primeiro
    if (modelLanguageCache.has(model)) {
      return modelLanguageCache.get(model)!;
    }
    
    // Criar cópia normalizada para consulta (lowercase)
    const normalizedModel = model.toLowerCase();
    
    // Estratégia de resolução:
    let result: string[];
    
    // 1. Tentar encontrar o modelo exato ou normalizado
    if (modelLanguageCompatibility[model]) {
      result = modelLanguageCompatibility[model];
    } else {
      // 2. Tentar encontrar o modelo normalizado (case insensitive)
      const exactMatchKey = Object.keys(modelLanguageCompatibility).find(
        key => key.toLowerCase() === normalizedModel
      );
      
      if (exactMatchKey) {
        result = modelLanguageCompatibility[exactMatchKey];
      } else {
        // 3. Tentar extrair família do modelo (ex: 'nova-2-meeting' -> 'nova-2')
        const modelFamily = model.split('-').slice(0, 2).join('-');
        
        if (modelLanguageCompatibility[modelFamily]) {
          result = modelLanguageCompatibility[modelFamily];
        } else {
          // 4. Tentar obter o modelo base (ex: 'nova-2-meeting' -> 'nova')
          const modelPrefix = model.split('-')[0];
          
          if (modelLanguageCompatibility[modelPrefix]) {
            result = modelLanguageCompatibility[modelPrefix];
          } else {
            // 5. Fallback para inglês (sempre disponível)
            console.warn(`⚠️ [Deepgram] Modelo não reconhecido: '${model}', usando fallback para inglês`);
            result = ['en', 'en-US'];
          }
        }
      }
    }
    
    // Salvar no cache
    modelLanguageCache.set(model, result);
    return result;
  };

  /**
   * Retorna o nome de exibição para um código de idioma
   */
  const getLanguageDisplay = (code: string): string => {
    return languageDisplay[code] || code;
  };

  return {
    getCompatibleLanguages,
    getLanguageDisplay,
    languageDisplay,
    modelLanguageCompatibility
  };
};

export default useDeepgramLanguageCompatibility;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";

/**
 * Componente de navegação entre seções de API
 * Implementa princípio de Responsabilidade Única (SRP) do SOLID
 * Simplificado para usar apenas Ollama
 */
interface ApiNavigationProps {
  openSection: "ollama" | null;
  setOpenSection: (section: "ollama" | null) => void;
}

export const ApiNavigation: React.FC<ApiNavigationProps> = ({
  openSection,
  setOpenSection,
}) => {
  return (
    <div className="mb-6 border-b border-cyan-900/40">
      <div className="flex space-x-6">
        <button
          type="button"
          className={`px-3 py-2 font-medium transition-colors border-b-2 ${
            openSection === "ollama"
              ? "text-cyan-300 border-cyan-500/70"
              : "text-cyan-500/60 border-transparent hover:text-cyan-400/80 hover:border-cyan-600/30"
          }`}
          onClick={() =>
            setOpenSection(openSection === "ollama" ? null : "ollama")
          }
        >
          🦙 Ollama Local Models
        </button>
      </div>
    </div>
  );
};

export default ApiNavigation;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useEffect, useState } from "react";
import { OrchOSModeEnum } from "../../../../../../services/ModeService";
import {
  getOption,
  setOption,
  STORAGE_KEYS,
} from "../../../../../../services/StorageService";
import { getModelDimensions } from "../../../../../../utils/EmbeddingUtils";
import { HuggingFaceSettingsProps } from "./types";

const HF_EMBEDDING_MODELS = ["Xenova/all-MiniLM-L6-v2"];

/**
 * Componente para configurações do modo básico
 * Implementa princípio de Responsabilidade Única (SRP) do SOLID
 * Symbolic: Neurônios de interface para configuração cognitiva local
 */
export const BasicModeSettings: React.FC<HuggingFaceSettingsProps> = ({
  setApplicationMode,
  hfModel,
  setHfModel,
  hfEmbeddingModel,
  setHfEmbeddingModel,
  hfModelOptions = [],
  hfEmbeddingModelOptions = [],
}) => {
  // Estado para o caminho do DuckDB
  const [duckDbPath, setDuckDbPath] = useState<string>(
    () => getOption<string>(STORAGE_KEYS.DUCKDB_PATH) || "./orch-os-memory"
  );

  // Estado para ferramentas habilitadas
  const [toolsEnabled, setToolsEnabled] = useState<boolean>(
    () => getOption<boolean>(STORAGE_KEYS.TOOLS_ENABLED) ?? true
  );

  // Carregar configuração salva na inicialização
  useEffect(() => {
    const savedPath = getOption<string>(STORAGE_KEYS.DUCKDB_PATH);
    if (savedPath) {
      setDuckDbPath(savedPath);
    }
  }, []);

  // Handler para seleção de diretório
  const handleBrowseDirectory = async () => {
    try {
      // Verificar se estamos no Electron
      if (typeof window !== "undefined" && (window as any).electronAPI) {
        const result = await (window as any).electronAPI.selectDirectory();

        if (result.success && result.path) {
          const newPath = result.path;
          setDuckDbPath(newPath);
          setOption(STORAGE_KEYS.DUCKDB_PATH, newPath);
          console.log("📁 [SETTINGS] DuckDB path updated:", newPath);

          // Reinicializar DuckDB com o novo caminho
          try {
            const reinitResult = await (
              window as any
            ).electronAPI.reinitializeDuckDB(newPath);
            if (reinitResult.success) {
              console.log(
                "✅ [SETTINGS] DuckDB successfully reinitialized with new path"
              );
            } else {
              console.error(
                "❌ [SETTINGS] Failed to reinitialize DuckDB:",
                reinitResult.error
              );
            }
          } catch (reinitError) {
            console.error(
              "❌ [SETTINGS] Error reinitializing DuckDB:",
              reinitError
            );
          }
        } else if (!result.canceled) {
          console.error(
            "❌ [SETTINGS] Failed to select directory:",
            result.error
          );
        }
      } else {
        console.warn(
          "⚠️ [SETTINGS] Directory selection not available in web mode"
        );
      }
    } catch (error) {
      console.error("❌ [SETTINGS] Error selecting directory:", error);
    }
  };

  // Handler para mudança manual do caminho
  const handlePathChange = async (newPath: string) => {
    setDuckDbPath(newPath);
    setOption(STORAGE_KEYS.DUCKDB_PATH, newPath);

    // Reinicializar DuckDB apenas se estamos no Electron e o caminho não está vazio
    if (
      typeof window !== "undefined" &&
      (window as any).electronAPI &&
      newPath.trim()
    ) {
      try {
        const reinitResult = await (
          window as any
        ).electronAPI.reinitializeDuckDB(newPath);
        if (reinitResult.success) {
          console.log(
            "✅ [SETTINGS] DuckDB successfully reinitialized with new path"
          );
        } else {
          console.error(
            "❌ [SETTINGS] Failed to reinitialize DuckDB:",
            reinitResult.error
          );
        }
      } catch (reinitError) {
        console.error(
          "❌ [SETTINGS] Error reinitializing DuckDB:",
          reinitError
        );
      }
    }
  };
  return (
    <div className="p-3 rounded-lg bg-black/30 border border-cyan-500/20">
      <div className="flex items-center mb-2">
        <div className="w-8 h-8 flex items-center justify-center rounded-full bg-gradient-to-r from-cyan-500/30 to-blue-500/30 mr-3">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="16"
            height="16"
            fill="currentColor"
            viewBox="0 0 16 16"
            className="text-cyan-300"
          >
            <path d="M8 15A7 7 0 1 1 8 1a7 7 0 0 1 0 14zm0 1A8 8 0 1 0 8 0a8 8 0 0 0 0 16z" />
            <path d="m8.93 6.588-2.29.287-.082.38.45.083c.294.07.352.176.288.469l-.738 3.468c-.194.897.105 1.319.808 1.319.545 0 1.178-.252 1.465-.598l.088-.416c-.2.176-.492.246-.686.246-.275 0-.375-.193-.304-.533L8.93 6.588zM9 4.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0z" />
          </svg>
        </div>
        <h3 className="text-lg font-medium text-cyan-300">
          Basic Mode Services
        </h3>
      </div>

      <div className="space-y-3">
        {/* Reutilizando o componente HuggingFaceSettings aqui seria mais elegante,
            mas para manter a exata aparência do componente original, mantive a implementação direta */}
        <div className="bg-black/40 p-2 rounded-md">
          <h4 className="text-cyan-400 font-medium mb-1">
            Hugging Face Text Models
          </h4>
          <p className="text-white/70 text-sm">
            Select a local text-generation model for your Orch-OS instance. Only
            browser-compatible models are shown.
          </p>
          <div className="mt-2">
            <select
              className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
              title="Select HuggingFace Model"
              aria-label="Select HuggingFace Model"
              value={hfModel}
              onChange={(e) => {
                setHfModel(e.target.value);
              }}
            >
              {hfModelOptions.map((model) => (
                <option key={model.id} value={model.id}>
                  {model.label}
                </option>
              ))}
            </select>
          </div>
        </div>

        <div className="bg-black/40 p-2 rounded-md">
          <h4 className="text-cyan-400 font-medium mb-1">
            Hugging Face Embedding Models{" "}
            <span className="text-xs text-cyan-500/70">(Basic Mode)</span>
          </h4>
          <p className="text-white/70 text-sm">
            Select a local model for generating vector embeddings in the neural
            memory database.
          </p>
          <div className="mt-2">
            <select
              className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
              title="Select HuggingFace Embedding Model"
              aria-label="Select HuggingFace Embedding Model"
              value={hfEmbeddingModel}
              onChange={(e) => {
                setHfEmbeddingModel(e.target.value);
              }}
            >
              {hfEmbeddingModelOptions.map((model) => (
                <option key={model.id} value={model.id}>
                  {model.label} ({getModelDimensions(model.id)}d)
                </option>
              ))}
            </select>
            <p className="text-xs text-cyan-400/60 mt-1">
              Modelo utilizado para gerar embeddings e busca semântica na
              memória no modo básico.
            </p>
          </div>
        </div>

        <div className="bg-black/40 p-2 rounded-md">
          <h4 className="text-cyan-400 font-medium mb-1">
            Memory Storage Location
          </h4>
          <p className="text-white/70 text-sm">
            Directory where your neural memory database will be stored locally.
          </p>
          <div className="mt-2 flex">
            <input
              type="text"
              className="flex-1 p-2 rounded-l bg-black/40 text-white/90 border border-cyan-500/30"
              value={duckDbPath}
              onChange={(e) => handlePathChange(e.target.value)}
              title="Memory storage directory"
              aria-label="Memory storage directory"
              placeholder="Enter or browse for directory path"
            />
            <button
              className="bg-cyan-600/30 hover:bg-cyan-500/40 text-cyan-300 rounded-r px-3 border border-cyan-500/30 transition-colors"
              onClick={handleBrowseDirectory}
              title="Browse for directory"
            >
              Browse
            </button>
          </div>
          <p className="text-xs text-cyan-400/60 mt-1">
            DuckDB will automatically manage the vector database at this
            location.
          </p>
        </div>
      </div>

      <div className="mt-3 flex justify-end">
        <button
          type="button"
          className="w-full bg-gradient-to-r from-purple-500/20 to-cyan-500/20 text-purple-300 border border-purple-500/30 rounded-lg py-2 mt-3 hover:from-purple-500/30 hover:to-cyan-500/30 transition-all shadow-[0_0_10px_rgba(200,0,255,0.2)] backdrop-blur-sm"
          onClick={() =>
            setApplicationMode && setApplicationMode(OrchOSModeEnum.ADVANCED)
          }
        >
          🦙 Switch to Advanced Mode (Ollama)
        </button>
      </div>
    </div>
  );
};

export default BasicModeSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { OrchOSModeEnum } from "../../../../../../services/ModeService";
import {
  setOption,
  STORAGE_KEYS,
} from "../../../../../../services/StorageService";
import { ChatGPTSettingsProps } from "./types";

// Modelos de embedding OpenAI suportados
const SUPPORTED_OPENAI_EMBEDDING_MODELS = [
  "text-embedding-ada-002",
  "text-embedding-3-small",
  "text-embedding-3-large",
];

/**
 * Componente para configuração da integração com ChatGPT/OpenAI
 * Implementa princípio de Responsabilidade Única (SRP) do SOLID
 */
export const ChatGPTSettings: React.FC<ChatGPTSettingsProps> = ({
  applicationMode,
  chatgptApiKey,
  setChatgptApiKey,
  chatgptModel,
  setChatgptModel,
  openaiEmbeddingModel,
  setOpenaiEmbeddingModel,
}) => {
  return (
    <div className="p-4 rounded-md bg-black/20 mb-4 animate-fade-in">
      <h3 className="text-lg text-cyan-300 mb-4">ChatGPT Integration</h3>
      <div className="space-y-4">
        <div>
          <label
            htmlFor="chatgptApiKey"
            className="block mb-1 text-sm text-cyan-200/80"
          >
            ChatGPT API Key
          </label>
          <input
            type="password"
            id="chatgptApiKey"
            className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
            value={chatgptApiKey}
            onChange={(e) => {
              setChatgptApiKey(e.target.value);
              setOption(STORAGE_KEYS.OPENAI_API_KEY, e.target.value);
            }}
            placeholder="Enter your ChatGPT API key"
          />
        </div>

        <div>
          <label
            htmlFor="chatgptModel"
            className="block mb-1 text-sm text-cyan-200/80"
          >
            ChatGPT Model
          </label>
          <select
            id="chatgptModel"
            className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
            value={chatgptModel}
            onChange={(e) => {
              setChatgptModel(e.target.value);
              setOption(STORAGE_KEYS.CHATGPT_MODEL, e.target.value);
            }}
            title="Select ChatGPT Model"
          >
            {/* Modelos ChatGPT em ordem cronológica de lançamento */}
            <option value="gpt-3.5-turbo">GPT-3.5 Turbo</option>
            <option value="gpt-4">GPT-4</option>
            <option value="gpt-4-turbo">GPT-4 Turbo</option>
            <option value="gpt-4o-mini">GPT-4o Mini</option>
            <option value="gpt-4o">GPT-4o</option>
            <option value="gpt-4.1-nano">GPT-4.1 Nano</option>
            <option value="gpt-4.1-mini">GPT-4.1 Mini</option>
            <option value="gpt-4.1">GPT-4.1</option>
            <option value="gpt-4.5-preview">GPT-4.5 Preview</option>
          </select>
        </div>

        {/* OpenAI Embedding Model - apenas no modo avançado */}
        {applicationMode === OrchOSModeEnum.ADVANCED && (
          <div>
            <label
              htmlFor="openaiEmbeddingModel"
              className="block mb-1 text-sm text-cyan-200/80"
            >
              OpenAI Embedding Model{" "}
              <span className="text-xs text-cyan-500/70">(Advanced Mode)</span>
            </label>
            <select
              id="openaiEmbeddingModel"
              className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
              value={openaiEmbeddingModel}
              onChange={(e) => {
                setOpenaiEmbeddingModel(e.target.value);
                setOption(STORAGE_KEYS.OPENAI_EMBEDDING_MODEL, e.target.value);
              }}
              title="Select OpenAI Embedding Model"
            >
              {SUPPORTED_OPENAI_EMBEDDING_MODELS.map((model) => (
                <option key={model} value={model}>
                  {model}
                </option>
              ))}
            </select>
            <p className="text-xs text-cyan-400/60 mt-1">
              Modelo utilizado para gerar embeddings e busca semântica na
              memória no modo avançado.
            </p>
          </div>
        )}
      </div>
    </div>
  );
};

export default ChatGPTSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useEffect } from 'react';
import { setOption, STORAGE_KEYS } from '../../../../../../services/StorageService';
import { useDeepgramLanguageCompatibility } from './hooks/useDeepgramLanguageCompatibility';
import { DeepgramSettingsProps } from './types';

/**
 * Componente para configuração da integração com Deepgram
 * Implementa princípio de Responsabilidade Única (SRP) do SOLID
 * Extrai lógica de compatibilidade para um hook customizado
 * 
 * DEBUG: Adicionados logs temporários para diagnóstico de persistência
 */
const DeepgramSettings: React.FC<DeepgramSettingsProps> = React.memo(({
  deepgramApiKey,
  setDeepgramApiKey,
  deepgramModel,
  setDeepgramModel,
  deepgramLanguage,
  setDeepgramLanguage
}) => {
  // Hook para gerenciar compatibilidade entre modelo e idioma
  const { getCompatibleLanguages, getLanguageDisplay } = useDeepgramLanguageCompatibility();
  const compatibleLanguages = getCompatibleLanguages(deepgramModel);

  // Efeito para verificar e ajustar idioma quando o modelo muda
  useEffect(() => {
    // Só ajuste se realmente necessário
    const isCurrentLanguageCompatible = compatibleLanguages.includes(deepgramLanguage);
    const fallbackLanguage = compatibleLanguages[0];

    if (
      !isCurrentLanguageCompatible &&
      fallbackLanguage &&
      deepgramLanguage !== fallbackLanguage
    ) {
      console.log(
        `🌐 Modelo ${deepgramModel} não suporta idioma ${deepgramLanguage}, alterando para ${fallbackLanguage}`
      );
      setDeepgramLanguage(fallbackLanguage);
    }
    // Se já está compatível ou já é o fallback, não faz nada!
  }, [deepgramModel, deepgramLanguage, compatibleLanguages, setDeepgramLanguage]);

  // Move os logs para um useEffect para evitar logs em cada render
  useEffect(() => {
    console.log('[DeepgramSettings] deepgramModel alterado:', deepgramModel);
  }, [deepgramModel]);

  useEffect(() => {
    console.log('[DeepgramSettings] deepgramLanguage alterado:', deepgramLanguage);
  }, [deepgramLanguage]);

  return (
    <div className="p-3 rounded-md bg-black/20 mb-3 animate-fade-in">
      <h3 className="text-lg text-cyan-300 mb-2">Deepgram Voice Transcription</h3>
      <div className="space-y-3">
        <div>
          <label htmlFor="deepgramApiKey" className="block mb-1 text-sm text-cyan-200/80">Deepgram API Key</label>
          <input 
            type="password"
            id="deepgramApiKey"
            className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
            value={deepgramApiKey}
            onChange={e => {
              setDeepgramApiKey(e.target.value);
              setOption(STORAGE_KEYS.DEEPGRAM_API_KEY, e.target.value);
            }}
            placeholder="Enter your Deepgram API key"
          />
        </div>
        
        <div>
          <label htmlFor="deepgramModel" className="block mb-1 text-sm text-cyan-200/80">Deepgram Model</label>
          <select
            id="deepgramModel"
            className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
            value={deepgramModel}
            onChange={e => {
              console.log('Salvando modelo Deepgram:', e.target.value);
              setDeepgramModel(e.target.value);
              setOption(STORAGE_KEYS.DEEPGRAM_MODEL, e.target.value);
            }}
            title="Select Deepgram Model"
          >
            {/* Nova-3 - Latest and most advanced */}
            <optgroup label="Nova-3 Models">
              <option value="nova-3">Nova-3 General</option>
              <option value="nova-3-medical">Nova-3 Medical</option>
            </optgroup>
            
            {/* Nova-2 - Second generation */}
            <optgroup label="Nova-2 Models">
              <option value="nova-2">Nova-2 General (Recommended)</option>
              <option value="nova-2-meeting">Nova-2 Meeting</option>
              <option value="nova-2-phonecall">Nova-2 Phone Call</option>
              <option value="nova-2-video">Nova-2 Video</option>
            </optgroup>
            
            {/* Nova - First generation */}
            <optgroup label="Nova Models">
              <option value="nova">Nova General</option>
              <option value="nova-phonecall">Nova Phone Call</option>
            </optgroup>
            
            {/* Enhanced - Legacy models */}
            <optgroup label="Enhanced Models">
              <option value="enhanced">Enhanced General</option>
              <option value="enhanced-meeting">Enhanced Meeting</option>
              <option value="enhanced-phonecall">Enhanced Phone Call</option>
              <option value="enhanced-finance">Enhanced Finance</option>
            </optgroup>
            
            {/* Base - Basic models */}
            <optgroup label="Base Models">
              <option value="base">Base General</option>
              <option value="base-meeting">Base Meeting</option>
              <option value="base-phonecall">Base Phone Call</option>
              <option value="base-finance">Base Finance</option>
            </optgroup>
          </select>
        </div>
        
        {/* Compatibilidade modelo-idioma */}
        <div>
          <label htmlFor="deepgramLanguage" className="block mb-1 text-sm text-cyan-200/80">Transcription Language</label>
            
          {/* Seletor de idioma filtrado por compatibilidade com o modelo */}
          <select
            id="deepgramLanguage"
            className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
            value={deepgramLanguage}
            onChange={e => {
              const newValue = e.target.value;
              setDeepgramLanguage(newValue);
              setOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE, newValue);
            }}
            title="Select Transcription Language"
          >
            {compatibleLanguages.map(langCode => (
              <option key={langCode} value={langCode}>
                {getLanguageDisplay(langCode)}
              </option>
            ))}
          </select>
          
          {/* Exibir informações de compatibilidade */}
          {compatibleLanguages.length === 1 && compatibleLanguages[0] === 'en' && (
            <p className="text-xs text-amber-400 mt-1">
              Este modelo suporta apenas inglês.
            </p>
          )}
          {compatibleLanguages.includes('multi') && (
            <p className="text-xs text-cyan-400/60 mt-1">
              Este modelo suporta detecção automática de idioma.
            </p>
          )}
        </div>
      </div>
    </div>
  );
});

export default DeepgramSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import {
  setOption,
  STORAGE_KEYS,
} from "../../../../../../services/StorageService";
import { HuggingFaceSettingsProps } from "./types";

/**
 * Componente para configuração de modelos do HuggingFace
 * Implementa princípio de Responsabilidade Única (SRP) do SOLID
 */
export const HuggingFaceSettings: React.FC<HuggingFaceSettingsProps> = ({
  hfModel,
  setHfModel,
  hfEmbeddingModel,
  setHfEmbeddingModel,
}) => {
  // AI model options for HuggingFace
  const HF_MODELS = [
    {
      id: "Xenova/llama2.c-stories15M",
      label: "Llama2.c Stories (~15MB) - Ultra pequeno",
    },
    {
      id: "Xenova/distilgpt2",
      label: "DistilGPT-2 (~353MB) - Otimizado",
    },
    {
      id: "Xenova/gpt2",
      label: "GPT-2 Base (~548MB) - Estável",
    },
    {
      id: "Xenova/TinyLlama-1.1B-Chat-v1.0",
      label: "TinyLlama Chat (~1.1B) - Modelo de chat",
    },
  ];

  const HF_EMBEDDING_MODELS = [
    {
      id: "Xenova/all-MiniLM-L6-v2",
      label: "all-MiniLM-L6-v2 (MiniLM 384d) — Recomendado",
    },
  ];

  return (
    <div className="space-y-4">
      <div className="bg-black/40 p-4 rounded-md">
        <h4 className="text-cyan-400 font-medium mb-2">
          Hugging Face Text Models
        </h4>
        <p className="text-white/70 text-sm">
          Select a local text-generation model for your Orch-OS instance. Only
          browser-compatible models are shown.
        </p>
        <div className="mt-3">
          <select
            className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
            title="Select HuggingFace Model"
            aria-label="Select HuggingFace Model"
            value={hfModel}
            onChange={(e) => {
              setHfModel(e.target.value);
              setOption(STORAGE_KEYS.HF_MODEL, e.target.value);
            }}
          >
            {HF_MODELS.map((m) => (
              <option key={m.id} value={m.id}>
                {m.label}
              </option>
            ))}
          </select>
        </div>
      </div>

      <div className="bg-black/40 p-4 rounded-md">
        <h4 className="text-cyan-400 font-medium mb-2">
          Hugging Face Embedding Models{" "}
          <span className="text-xs text-cyan-500/70">(Basic Mode)</span>
        </h4>
        <p className="text-white/70 text-sm">
          Select a local model for generating vector embeddings in the neural
          memory database.
        </p>
        <div className="mt-3">
          <select
            className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
            title="Select HuggingFace Embedding Model"
            aria-label="Select HuggingFace Embedding Model"
            value={hfEmbeddingModel}
            onChange={(e) => {
              setHfEmbeddingModel(e.target.value);
              setOption(STORAGE_KEYS.HF_EMBEDDING_MODEL, e.target.value);
            }}
          >
            {HF_EMBEDDING_MODELS.map((m) => (
              <option key={m.id} value={m.id}>
                {m.label}
              </option>
            ))}
          </select>
          <p className="text-xs text-cyan-400/60 mt-1">
            Modelo utilizado para gerar embeddings e busca semântica na memória
            no modo básico.
          </p>
        </div>
      </div>

      <div className="bg-black/40 p-4 rounded-md">
        <h4 className="text-cyan-400 font-medium mb-2">Local Storage</h4>
        <p className="text-white/70 text-sm">
          Storage location for your neural memory database.
        </p>
        <div className="mt-3 flex">
          <input
            type="text"
            className="flex-1 p-2 rounded-l bg-black/40 text-white/90 border border-cyan-500/30"
            value="./orch-os-memory"
            readOnly
            title="Local storage location"
            aria-label="Local storage location"
          />
          <button className="bg-cyan-600/30 hover:bg-cyan-500/40 text-cyan-300 rounded-r px-3 border border-cyan-500/30">
            Browse
          </button>
        </div>
      </div>
    </div>
  );
};

export default HuggingFaceSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Componentes principais para Ollama
export { default as ApiNavigation } from "./ApiNavigation";
export { default as BasicModeSettings } from "./BasicModeSettings";
export { default as OllamaSettings } from "./OllamaSettings";

// Componentes legados mantidos para compatibilidade (podem ser removidos gradualmente)
export { default as ChatGPTSettings } from "./ChatGPTSettings";
export { default as DeepgramSettings } from "./DeepgramSettings";
export { default as HuggingFaceSettings } from "./HuggingFaceSettings";
export { default as PineconeSettings } from "./PineconeSettings";

// Tipos
export * from "./types";
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  ArrowDownTrayIcon,
  CheckCircleIcon,
  ChevronDownIcon,
  ExclamationTriangleIcon,
  TrashIcon,
  XMarkIcon,
} from "@heroicons/react/24/outline";
import React, { useCallback, useEffect, useState } from "react";

/* ------------------------------------------------------------------
 * Local helper types & globals
 * ------------------------------------------------------------------ */
// Lightweight copy of the vLLM status type (avoids deep relative import)
type VllmStatus = {
  state:
    | "idle"
    | "downloading"
    | "pulling_image"
    | "starting"
    | "ready"
    | "error";
  progress?: number;
  message?: string;
  modelId?: string;
};

// Static model definitions as requested by the user
const AVAILABLE_MODELS: OllamaModel[] = [
  // Main models that support tools/function calling
  {
    id: "qwen3:4b",
    name: "Qwen3 4B",
    description: "Advanced reasoning model with tools support",
    size: "2.6GB",
    category: "main",
    isDownloaded: false,
    isDownloading: false,
  },
  {
    id: "mistral:latest",
    name: "Mistral Latest",
    description: "Fast and efficient model with tools support",
    size: "4.1GB",
    category: "main",
    isDownloaded: false,
    isDownloading: false,
  },
  {
    id: "mistral-nemo:latest",
    name: "Mistral Nemo",
    description: "Optimized Mistral variant for tools and function calling",
    size: "5.5GB",
    category: "main",
    isDownloaded: false,
    isDownloading: false,
  },
  {
    id: "llama3.2:latest",
    name: "Llama 3.2 Latest",
    description: "Meta's latest model with excellent tools integration",
    size: "3.8GB",
    category: "main",
    isDownloaded: false,
    isDownloading: false,
  },
  // Embedding models (keeping as they might be needed for embeddings)
  {
    id: "bge-m3:latest",
    name: "BGE-M3 Latest",
    description:
      "Advanced multilingual embedding model (dense + sparse + multi-vector)",
    size: "2.4GB",
    category: "embedding",
    isDownloaded: false,
    isDownloading: false,
  },
  {
    id: "nomic-embed-text:latest",
    name: "Nomic Embed Text",
    description: "High-quality text embeddings",
    size: "274MB",
    category: "embedding",
    isDownloaded: false,
    isDownloading: false,
  },
  {
    id: "mxbai-embed-large:latest",
    name: "MxBai Embed Large",
    description: "Large embedding model for better accuracy",
    size: "670MB",
    category: "embedding",
    isDownloaded: false,
    isDownloading: false,
  },
];

interface OllamaModel {
  id: string;
  name: string;
  description: string;
  size?: string;
  isDownloaded?: boolean;
  isDownloading?: boolean;
  downloadProgress?: number;
  downloadSpeed?: string;
  downloadETA?: string;
  category?: "main" | "embedding";
}

interface OllamaSettingsProps {
  ollamaModel: string;
  setOllamaModel: (value: string) => void;
  ollamaEmbeddingModel: string;
  setOllamaEmbeddingModel: (value: string) => void;
  ollamaEnabled: boolean;
  setOllamaEnabled: (value: boolean) => void;
  storagePath?: string;
  setStoragePath?: (path: string) => void;
}

export const OllamaSettings: React.FC<OllamaSettingsProps> = ({
  ollamaModel,
  setOllamaModel,
  ollamaEmbeddingModel,
  setOllamaEmbeddingModel,
  ollamaEnabled,
  setOllamaEnabled,
  storagePath = "./orch-os-memory",
  setStoragePath,
}) => {
  const [availableModels, setAvailableModels] =
    useState<OllamaModel[]>(AVAILABLE_MODELS);
  const [installedModels, setInstalledModels] = useState<string[]>([]);
  const [isLoadingModels, setIsLoadingModels] = useState(false);
  const [isLoadingAvailable, setIsLoadingAvailable] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [downloadingModels, setDownloadingModels] = useState<
    Map<string, { progress: number; speed: string; eta: string }>
  >(new Map());

  // Dropdown open/close state
  const [mainDropdownOpen, setMainDropdownOpen] = useState(false);
  const [embeddingDropdownOpen, setEmbeddingDropdownOpen] = useState(false);

  /* ------------------------------------------------------------------
   * Estado de carregamento dos modelos (vLLM) & polling
   * ------------------------------------------------------------------ */
  const [modelStatus, setModelStatus] = useState<VllmStatus | null>(null);

  useEffect(() => {
    let interval: NodeJS.Timeout;
    const fetchStatus = async () => {
      try {
        if (window.electronAPI?.vllmModelStatus) {
          const res = await window.electronAPI.vllmModelStatus();
          if (res?.success && res.status) {
            setModelStatus(res.status as VllmStatus);
          }
        }
      } catch {
        /* silêncio */
      }
    };
    fetchStatus();
    interval = setInterval(fetchStatus, 2000);
    return () => clearInterval(interval);
  }, []);

  // Função para buscar modelos instalados
  const fetchInstalledModels = useCallback(async () => {
    try {
      setIsLoadingModels(true);
      setError(null);

      // Tentar usar a API do Electron (Ollama) primeiro
      if (window.electronAPI?.listModels) {
        const models = await window.electronAPI.listModels();
        if (models && Array.isArray(models)) {
          const installed = models.map((m) => m.name || m.id);
          setInstalledModels(installed);
          return;
        }
      }

      // Fallback para API HTTP local do Ollama
      const response = await fetch("http://localhost:11434/api/tags");
      if (response.ok) {
        const data = await response.json();
        if (data.models && Array.isArray(data.models)) {
          setInstalledModels(data.models.map((m: any) => m.name));
        }
      } else {
        throw new Error("Ollama não está rodando ou não está acessível");
      }
    } catch (error) {
      console.error("Erro ao carregar modelos instalados:", error);
      setError(
        "Não foi possível conectar ao Ollama. Verifique se está rodando."
      );
      setInstalledModels([]);
    } finally {
      setIsLoadingModels(false);
    }
  }, []);

  // Função para buscar modelos disponíveis (agora usa lista estática)
  const fetchAvailableModels = useCallback(async () => {
    try {
      setIsLoadingAvailable(true);
      setError(null);

      // Usar lista estática de modelos
      setAvailableModels(AVAILABLE_MODELS);
    } catch (error) {
      console.error("Erro ao carregar modelos disponíveis:", error);
      setError("Erro ao carregar lista de modelos disponíveis");
    } finally {
      setIsLoadingAvailable(false);
    }
  }, []);

  // Atualizar status dos modelos com base nos instalados
  useEffect(() => {
    const updatedModels = availableModels.map((model) => {
      const downloadInfo = downloadingModels.get(model.id);
      return {
        ...model,
        isDownloaded: installedModels.includes(model.id),
        isDownloading: downloadingModels.has(model.id),
        downloadProgress: downloadInfo?.progress || 0,
        downloadSpeed: downloadInfo?.speed || "",
        downloadETA: downloadInfo?.eta || "",
      };
    });
    setAvailableModels(updatedModels);
  }, [installedModels, downloadingModels]);

  // Carregar dados ao montar o componente
  useEffect(() => {
    const loadData = async () => {
      await Promise.all([fetchInstalledModels(), fetchAvailableModels()]);
    };

    loadData();
  }, [fetchInstalledModels, fetchAvailableModels]);

  // Função para baixar modelo com progress via Ollama
  const downloadModel = async (modelId: string) => {
    try {
      // Iniciar download
      setDownloadingModels((prev) =>
        new Map(prev).set(modelId, {
          progress: 0,
          speed: "0 MB/s",
          eta: "Calculando...",
        })
      );

      // Tentar usar a API do Electron (Ollama) primeiro
      if (window.electronAPI?.downloadModel) {
        const success = await window.electronAPI.downloadModel(modelId);
        if (success) {
          // Recarregar lista ao concluir
          await fetchInstalledModels();
          setDownloadingModels((prev) => {
            const newMap = new Map(prev);
            newMap.delete(modelId);
            return newMap;
          });
          return;
        }
      }

      // Fallback para API HTTP do Ollama com simulação de progresso
      const response = await fetch("http://localhost:11434/api/pull", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          name: modelId,
          stream: false,
        }),
      });

      if (response.ok) {
        // Simular progresso enquanto baixa
        const progressInterval = setInterval(() => {
          setDownloadingModels((prev) => {
            const current = prev.get(modelId);
            if (!current) return prev;

            const newProgress = Math.min(
              current.progress + Math.random() * 10,
              100
            );
            const newMap = new Map(prev);

            if (newProgress >= 100) {
              clearInterval(progressInterval);
              newMap.delete(modelId);
              // Adicionar à lista de instalados
              setInstalledModels((prev) => [...prev, modelId]);
            } else {
              newMap.set(modelId, {
                progress: newProgress,
                speed: `${(Math.random() * 10 + 1).toFixed(1)} MB/s`,
                eta: `${Math.ceil((100 - newProgress) / 10)} min`,
              });
            }

            return newMap;
          });
        }, 1000);
      } else {
        throw new Error("Falha ao baixar modelo");
      }
    } catch (error) {
      console.error("Erro ao baixar modelo:", error);
      setDownloadingModels((prev) => {
        const newMap = new Map(prev);
        newMap.delete(modelId);
        return newMap;
      });
      setError(`Erro ao baixar modelo ${modelId}`);
    }
  };

  // Função para cancelar download
  const cancelDownload = async (modelId: string) => {
    setDownloadingModels((prev) => {
      const newMap = new Map(prev);
      newMap.delete(modelId);
      return newMap;
    });
  };

  // Função para remover modelo
  const removeModel = async (modelId: string) => {
    try {
      // Tentar usar a API do Electron (Ollama) primeiro
      if (window.electronAPI?.removeModel) {
        await window.electronAPI.removeModel(modelId);
        await fetchInstalledModels();
        return;
      }

      // Fallback para API HTTP do Ollama
      const response = await fetch("http://localhost:11434/api/delete", {
        method: "DELETE",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          name: modelId,
        }),
      });

      if (response.ok) {
        await fetchInstalledModels();
      } else {
        throw new Error("Falha ao remover modelo");
      }
    } catch (error) {
      console.error("Erro ao remover modelo:", error);
      setError(`Erro ao remover modelo ${modelId}`);
    }
  };

  // Função para recarregar dados
  const refreshData = async () => {
    await Promise.all([fetchInstalledModels(), fetchAvailableModels()]);
  };

  // Função para abrir seletor de diretório
  const handleBrowseStoragePath = async () => {
    try {
      if (window.electronAPI?.selectDirectory) {
        const result = await window.electronAPI.selectDirectory();
        if (result && result.success && result.path && setStoragePath) {
          setStoragePath(result.path);
        }
      } else {
        // Fallback para web - usar input file com webkitdirectory
        const input = document.createElement("input");
        input.type = "file";
        input.webkitdirectory = true;
        input.onchange = (e) => {
          const files = (e.target as HTMLInputElement).files;
          if (files && files.length > 0 && setStoragePath) {
            // Pegar o caminho do primeiro arquivo e extrair o diretório
            const firstFile = files[0];
            const pathParts = firstFile.webkitRelativePath.split("/");
            pathParts.pop(); // Remove o nome do arquivo
            const dirPath = pathParts.join("/");
            setStoragePath(dirPath || "./orch-os-memory");
          }
        };
        input.click();
      }
    } catch (error) {
      console.error("Erro ao selecionar diretório:", error);
    }
  };

  // Função para obter o nome do modelo selecionado
  const getModelDisplayName = (modelId: string, modelsList: OllamaModel[]) => {
    const model = modelsList.find((m) => m.id === modelId);
    return model
      ? `${model.name} (${model.size})`
      : modelId || "Selecione um modelo...";
  };

  // Computed lists for dropdowns
  const mainModelsList = availableModels.filter(
    (m) => m.category !== "embedding"
  );
  const embeddingModelsList = availableModels.filter(
    (m) => m.category === "embedding"
  );

  // Computed lists for the interface
  const downloadedMainModels = mainModelsList.filter((m) => m.isDownloaded);
  const downloadedEmbeddingModels = embeddingModelsList.filter(
    (m) => m.isDownloaded
  );
  const downloadedModels = availableModels.filter((m) => m.isDownloaded);
  const availableForDownload = availableModels.filter((m) => !m.isDownloaded);

  // Indicadores de loading para os spinners
  const mainModelLoading = !!(
    modelStatus &&
    modelStatus.modelId === ollamaModel &&
    modelStatus.state !== "ready" &&
    modelStatus.state !== "idle" &&
    modelStatus.state !== "error"
  );

  const embeddingModelLoading = !!(
    modelStatus &&
    modelStatus.modelId === ollamaEmbeddingModel &&
    modelStatus.state !== "ready" &&
    modelStatus.state !== "idle" &&
    modelStatus.state !== "error"
  );

  // Função para selecionar modelo principal
  const handleSelectMain = async (model: OllamaModel) => {
    setOllamaModel(model.id);
    if (!ollamaEnabled) setOllamaEnabled(true);
    setMainDropdownOpen(false);
    if (!model.isDownloaded && !model.isDownloading) {
      await downloadModel(model.id);
    }
  };

  // Função para selecionar modelo de embedding
  const handleSelectEmbedding = async (model: OllamaModel) => {
    setOllamaEmbeddingModel(model.id);
    setEmbeddingDropdownOpen(false);
    if (!model.isDownloaded && !model.isDownloading) {
      await downloadModel(model.id);
    }
  };

  return (
    <div className="space-y-3">
      {/* Compact Header */}
      <div className="flex items-center justify-between">
        <div className="flex items-center space-x-2">
          <span className="text-sm">🦙</span>
          <h3 className="text-sm font-medium text-cyan-300">Ollama Models</h3>
        </div>
        <button
          onClick={refreshData}
          disabled={isLoadingModels || isLoadingAvailable}
          className="bg-cyan-600/30 hover:bg-cyan-500/40 text-cyan-300 rounded px-2 py-1 transition-colors disabled:opacity-50 text-xs"
          title="Refresh models"
        >
          <span
            className={`${
              isLoadingModels || isLoadingAvailable ? "animate-spin" : ""
            }`}
          >
            🔄
          </span>
        </button>
      </div>

      {/* Error Message */}
      {error && (
        <div className="bg-red-500/10 border border-red-500/30 rounded p-2">
          <div className="flex items-center">
            <ExclamationTriangleIcon className="w-3 h-3 text-red-400 mr-1" />
            <div>
              <h4 className="text-red-400 font-medium text-[10px]">Erro</h4>
              <p className="text-red-400/70 text-[10px]">{error}</p>
            </div>
          </div>
        </div>
      )}

      {/* Storage Path - Inline */}
      <div className="flex items-center space-x-2">
        <span className="text-xs text-purple-400">🧠 Storage:</span>
        <input
          type="text"
          value={storagePath}
          onChange={(e) => setStoragePath?.(e.target.value)}
          className="flex-1 bg-black/30 text-purple-300 rounded px-2 py-1 text-xs border border-purple-500/20 focus:outline-none focus:border-purple-400/50"
          placeholder="Storage path..."
        />
        <button
          onClick={handleBrowseStoragePath}
          className="bg-blue-600/30 hover:bg-blue-500/40 text-blue-300 rounded px-2 py-1 text-xs transition-colors"
        >
          📁
        </button>
      </div>

      {/* Model Selection - Horizontal Layout */}
      <div className="grid grid-cols-2 gap-2">
        {/* Main Model */}
        <div className="relative">
          <label className="block text-xs text-cyan-400 mb-1">Main Model</label>
          <button
            onClick={() => setMainDropdownOpen((prev) => !prev)}
            disabled={isLoadingAvailable}
            className="flex items-center justify-between w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30 focus:outline-none hover:bg-black/50 transition-colors text-xs"
          >
            <span className="text-left truncate">
              {ollamaModel ? ollamaModel.split(":")[0] : "Select..."}
            </span>
            <ChevronDownIcon
              className={`w-3 h-3 transition-transform ${
                mainDropdownOpen ? "rotate-180" : ""
              }`}
            />
          </button>

          {mainDropdownOpen && (
            <div className="absolute z-10 w-full mt-1 bg-black/90 border border-cyan-500/30 rounded shadow-lg max-h-40 overflow-y-auto">
              {mainModelsList.map((model) => (
                <button
                  key={model.id}
                  onClick={() => handleSelectMain(model)}
                  className="w-full px-2 py-2 text-left hover:bg-cyan-500/20 transition-colors border-b border-cyan-500/10 last:border-b-0"
                >
                  <div className="flex items-center justify-between">
                    <div className="flex-1 truncate">
                      <div className="text-white/90 font-medium text-xs truncate">
                        {model.name}
                      </div>
                      <div className="text-cyan-400/60 text-[10px]">
                        {model.size}
                      </div>
                    </div>
                    {model.isDownloaded && (
                      <CheckCircleIcon className="w-3 h-3 text-green-400 ml-2" />
                    )}
                  </div>
                </button>
              ))}
            </div>
          )}
        </div>

        {/* Embedding Model */}
        <div className="relative">
          <label className="block text-xs text-cyan-400 mb-1">
            Embedding Model
          </label>
          <button
            onClick={() => setEmbeddingDropdownOpen((prev) => !prev)}
            disabled={isLoadingAvailable}
            className="flex items-center justify-between w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30 focus:outline-none hover:bg-black/50 transition-colors text-xs"
          >
            <span className="text-left truncate">
              {ollamaEmbeddingModel
                ? ollamaEmbeddingModel.split(":")[0]
                : "Select..."}
            </span>
            <ChevronDownIcon
              className={`w-3 h-3 transition-transform ${
                embeddingDropdownOpen ? "rotate-180" : ""
              }`}
            />
          </button>

          {embeddingDropdownOpen && (
            <div className="absolute z-10 w-full mt-1 bg-black/90 border border-cyan-500/30 rounded shadow-lg max-h-40 overflow-y-auto">
              {embeddingModelsList.map((model) => (
                <button
                  key={model.id}
                  onClick={() => handleSelectEmbedding(model)}
                  className="w-full px-2 py-2 text-left hover:bg-cyan-500/20 transition-colors border-b border-cyan-500/10 last:border-b-0"
                >
                  <div className="flex items-center justify-between">
                    <div className="flex-1 truncate">
                      <div className="text-white/90 font-medium text-xs truncate">
                        {model.name}
                      </div>
                      <div className="text-cyan-400/60 text-[10px]">
                        {model.size}
                      </div>
                    </div>
                    {model.isDownloaded && (
                      <CheckCircleIcon className="w-3 h-3 text-green-400 ml-2" />
                    )}
                  </div>
                </button>
              ))}
            </div>
          )}
        </div>
      </div>

      {/* Downloaded Models - Compact List */}
      {downloadedModels.length > 0 && (
        <div>
          <h4 className="text-xs font-medium text-green-400 mb-1">
            ✅ Downloaded ({downloadedModels.length})
          </h4>
          <div className="space-y-1 max-h-32 overflow-y-auto">
            {downloadedModels.map((model) => (
              <div
                key={model.id}
                className="flex items-center justify-between bg-green-500/10 border border-green-500/30 rounded p-2"
              >
                <div className="flex items-center space-x-2 flex-1 truncate">
                  <CheckCircleIcon className="w-3 h-3 text-green-400" />
                  <span className="text-green-300 text-xs truncate">
                    {model.name}
                  </span>
                  <span className="text-green-400/60 text-[10px]">
                    {model.size}
                  </span>
                  {model.category === "embedding" && (
                    <span className="px-1 py-0.5 bg-blue-500/20 text-blue-300 text-[9px] rounded">
                      Embed
                    </span>
                  )}
                </div>
                <button
                  onClick={() => removeModel(model.id)}
                  className="bg-red-600/20 hover:bg-red-500/30 text-red-400 rounded p-1 transition-colors"
                  title="Remove Model"
                >
                  <TrashIcon className="w-3 h-3" />
                </button>
              </div>
            ))}
          </div>
        </div>
      )}

      {/* Available Models - Compact List */}
      {availableForDownload.length > 0 && (
        <div>
          <h4 className="text-xs font-medium text-cyan-400 mb-1">
            📥 Available ({availableForDownload.length})
          </h4>
          <div className="space-y-1 max-h-32 overflow-y-auto">
            {availableForDownload.map((model) => (
              <div
                key={model.id}
                className="bg-black/30 border border-cyan-500/20 rounded p-2"
              >
                <div className="flex items-center justify-between">
                  <div className="flex items-center space-x-2 flex-1 truncate">
                    <span className="text-cyan-300 text-xs truncate">
                      {model.name}
                    </span>
                    <span className="text-cyan-400/60 text-[10px]">
                      {model.size}
                    </span>
                    {model.category === "embedding" && (
                      <span className="px-1 py-0.5 bg-blue-500/20 text-blue-300 text-[9px] rounded">
                        Embed
                      </span>
                    )}
                  </div>
                  <div className="flex items-center space-x-1">
                    {model.isDownloading ? (
                      <button
                        onClick={() => cancelDownload(model.id)}
                        className="bg-red-600/30 hover:bg-red-500/40 text-red-300 rounded p-1 transition-colors"
                        title="Cancel Download"
                      >
                        <XMarkIcon className="w-3 h-3" />
                      </button>
                    ) : (
                      <button
                        onClick={() => downloadModel(model.id)}
                        className="bg-cyan-600/30 hover:bg-cyan-500/40 text-cyan-300 rounded p-1 transition-colors"
                        title="Download Model"
                      >
                        <ArrowDownTrayIcon className="w-3 h-3" />
                      </button>
                    )}
                  </div>
                </div>

                {model.isDownloading && (
                  <div className="mt-2 space-y-1">
                    <div className="flex justify-between text-[10px]">
                      <span className="text-yellow-400">Downloading...</span>
                      <span className="text-yellow-300">
                        {model.downloadProgress?.toFixed(1)}%
                      </span>
                    </div>
                    <div className="w-full bg-gray-700 rounded-full h-1">
                      <div
                        className="bg-gradient-to-r from-yellow-400 to-orange-400 h-1 rounded-full transition-all duration-300"
                        style={{ width: `${model.downloadProgress || 0}%` }}
                      ></div>
                    </div>
                    <div className="flex justify-between text-[9px] text-yellow-400/70">
                      <span>{model.downloadSpeed}</span>
                      <span>ETA: {model.downloadETA}</span>
                    </div>
                  </div>
                )}
              </div>
            ))}
          </div>
        </div>
      )}
    </div>
  );
};

export default OllamaSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { PineconeSettingsProps } from './types';
import { setOption, STORAGE_KEYS } from '../../../../../../services/StorageService';

/**
 * Componente para configuração da integração com Pinecone Vector Database
 * Implementa princípio de Responsabilidade Única (SRP) do SOLID
 */
export const PineconeSettings: React.FC<PineconeSettingsProps> = ({
  pineconeApiKey,
  setPineconeApiKey
}) => {
  return (
    <div className="p-4 rounded-md bg-black/20 mb-4 animate-fade-in">
      <h3 className="text-lg text-cyan-300 mb-4">Pinecone Vector Database</h3>
      <div className="space-y-4">
        <div>
          <label htmlFor="pineconeApiKey" className="block mb-1 text-sm text-cyan-200/80">Pinecone API Key</label>
          <input 
            type="password"
            id="pineconeApiKey"
            className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
            value={pineconeApiKey}
            onChange={e => {
              setPineconeApiKey(e.target.value);
              setOption(STORAGE_KEYS.PINECONE_API_KEY, e.target.value);
            }}
            placeholder="Enter your Pinecone API key"
          />
          <p className="text-xs text-cyan-400/60 mt-1">Used for long-term memory storage.</p>
        </div>
      </div>
    </div>
  );
};

export default PineconeSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { OrchOSMode } from "../../../../../../services/ModeService";

/**
 * Tipos para os componentes de configuração de APIs
 * Seguindo princípio de Interface Segregation do SOLID
 * Interface unificada sem navegação por abas
 */

// Props base para todos componentes de API settings
export interface BaseApiSettingsProps {
  applicationMode: OrchOSMode;
}

// Props específicas para Ollama
export interface OllamaSettingsProps {
  ollamaModel: string;
  setOllamaModel: (value: string) => void;
  ollamaEmbeddingModel: string;
  setOllamaEmbeddingModel: (value: string) => void;
  ollamaEnabled: boolean;
  setOllamaEnabled: (value: boolean) => void;
  storagePath?: string;
  setStoragePath?: (path: string) => void;
}

// Props completas para o componente ApiSettings (interface unificada)
export interface ApiSettingsProps {
  applicationMode: OrchOSMode;
  setApplicationMode: (mode: OrchOSMode) => void;
  // Ollama
  ollamaModel: string;
  setOllamaModel: (value: string) => void;
  ollamaEmbeddingModel: string;
  setOllamaEmbeddingModel: (value: string) => void;
  ollamaEnabled: boolean;
  setOllamaEnabled: (value: boolean) => void;
}

// Tipos legados mantidos para compatibilidade (podem ser removidos gradualmente)
export interface PineconeSettingsProps {
  pineconeApiKey: string;
  setPineconeApiKey: (value: string) => void;
}

export interface ChatGPTSettingsProps extends BaseApiSettingsProps {
  chatgptApiKey: string;
  setChatgptApiKey: (value: string) => void;
  chatgptModel: string;
  setChatgptModel: (value: string) => void;
  openaiEmbeddingModel: string;
  setOpenaiEmbeddingModel: (value: string) => void;
}

export interface HuggingFaceSettingsProps extends BaseApiSettingsProps {
  hfModel: string;
  setHfModel: (value: string) => void;
  hfEmbeddingModel: string;
  setHfEmbeddingModel: (value: string) => void;
  hfModelOptions?: Array<{ id: string; label: string }>;
  hfEmbeddingModelOptions?: Array<{ id: string; label: string }>;
  setApplicationMode?: (mode: OrchOSMode) => void;
}

export interface DeepgramSettingsProps {
  deepgramApiKey: string;
  setDeepgramApiKey: (value: string) => void;
  deepgramModel: string;
  setDeepgramModel: (value: string) => void;
  deepgramLanguage: string;
  setDeepgramLanguage: (value: string) => void;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useEffect, useMemo, useState } from "react";
import {
  getOption,
  setOption,
  STORAGE_KEYS,
  subscribeToStorageChanges,
} from "../../../../../../services/StorageService";

/**
 * Hook para gerenciamento de configurações de APIs externas
 * Interface unificada sem navegação por abas
 * Neurônio de infraestrutura responsável pelas credenciais e parâmetros de serviços
 */
export const useApiSettings = () => {
  // Ollama - configurações principais
  const [ollamaModel, setOllamaModel] = useState<string>(
    () => getOption<string>(STORAGE_KEYS.OLLAMA_MODEL) || ""
  );
  const [ollamaEmbeddingModel, setOllamaEmbeddingModel] = useState<string>(
    () => getOption<string>(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL) || ""
  );
  const [ollamaEnabled, setOllamaEnabled] = useState<boolean>(
    () => getOption<boolean>(STORAGE_KEYS.OLLAMA_ENABLED) || false
  );

  // Sincroniza mudanças do storage para o estado local
  useEffect(() => {
    const handleStorageChange = (key: string, value: any) => {
      switch (key) {
        // Ollama
        case STORAGE_KEYS.OLLAMA_MODEL:
          if (value !== ollamaModel) setOllamaModel(value);
          break;
        case STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL:
          if (value !== ollamaEmbeddingModel) setOllamaEmbeddingModel(value);
          break;
        case STORAGE_KEYS.OLLAMA_ENABLED:
          if (value !== ollamaEnabled) setOllamaEnabled(value);
          break;
      }
    };

    const unsubscribe = subscribeToStorageChanges(handleStorageChange);
    return () => unsubscribe();
  }, [ollamaModel, ollamaEmbeddingModel, ollamaEnabled]);

  // Salva as configurações de API no storage
  const saveApiSettings = () => {
    // Ollama
    setOption(STORAGE_KEYS.OLLAMA_MODEL, ollamaModel);
    setOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL, ollamaEmbeddingModel);
    setOption(STORAGE_KEYS.OLLAMA_ENABLED, ollamaEnabled);
  };

  // Memoize all API settings to ensure stable reference and avoid unnecessary renders
  return useMemo(
    () => ({
      // Ollama
      ollamaModel,
      setOllamaModel,
      ollamaEmbeddingModel,
      setOllamaEmbeddingModel,
      ollamaEnabled,
      setOllamaEnabled,
      // Salvar tudo
      saveApiSettings,
    }),
    [
      ollamaModel,
      setOllamaModel,
      ollamaEmbeddingModel,
      setOllamaEmbeddingModel,
      ollamaEnabled,
      setOllamaEnabled,
      saveApiSettings,
    ]
  );
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useState, useEffect, useRef, useMemo } from "react";
import { getOption, setOption, STORAGE_KEYS, subscribeToStorageChanges } from '../../../../../../services/StorageService';

/**
 * Hook para gerenciamento de configurações de áudio
 * Cortex neural para processamento e qualidade do sinal de áudio
 */
export const useAudioSettings = (show: boolean) => {
  // Estado neural-simbólico para opções de áudio
  const [audioQuality, setAudioQuality] = useState<number>(() => getOption<number>(STORAGE_KEYS.AUDIO_QUALITY) ?? 80);
  const [autoGainControl, setAutoGainControl] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.AUTO_GAIN_CONTROL) ?? true);
  const [noiseSuppression, setNoiseSuppression] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.NOISE_SUPPRESSION) ?? true);
  const [echoCancellation, setEchoCancellation] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.ECHO_CANCELLATION) ?? true);
  const [enhancedPunctuation, setEnhancedPunctuation] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.ENHANCED_PUNCTUATION) ?? true);
  const [speakerDiarization, setSpeakerDiarization] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.SPEAKER_DIARIZATION) ?? true);
  
  // Transcrição
  const [transcriptionEnabled, setTranscriptionEnabled] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.TRANSCRIPTION_ENABLED) ?? true);
  const [speakerIdentification, setSpeakerIdentification] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.SPEAKER_IDENTIFICATION) ?? true);
  
  // Sincroniza mudanças do storage para o estado local
  useEffect(() => {
    const handleStorageChange = (key: string, value: any) => {
      switch(key) {
        case STORAGE_KEYS.AUDIO_QUALITY: setAudioQuality(value); break;
        case STORAGE_KEYS.AUTO_GAIN_CONTROL: setAutoGainControl(value); break;
        case STORAGE_KEYS.NOISE_SUPPRESSION: setNoiseSuppression(value); break;
        case STORAGE_KEYS.ECHO_CANCELLATION: setEchoCancellation(value); break;
        case STORAGE_KEYS.ENHANCED_PUNCTUATION: setEnhancedPunctuation(value); break;
        case STORAGE_KEYS.SPEAKER_DIARIZATION: setSpeakerDiarization(value); break;
        case STORAGE_KEYS.TRANSCRIPTION_ENABLED: setTranscriptionEnabled(value); break;
        case STORAGE_KEYS.SPEAKER_IDENTIFICATION: setSpeakerIdentification(value); break;
      }
    };
    
    const unsubscribe = subscribeToStorageChanges(handleStorageChange);
    return () => unsubscribe();
  }, []);
  
  // Salva as configurações de áudio no storage
  const saveAudioSettings = () => {
    setOption(STORAGE_KEYS.AUDIO_QUALITY, audioQuality);
    setOption(STORAGE_KEYS.AUTO_GAIN_CONTROL, autoGainControl);
    setOption(STORAGE_KEYS.NOISE_SUPPRESSION, noiseSuppression);
    setOption(STORAGE_KEYS.ECHO_CANCELLATION, echoCancellation);
    setOption(STORAGE_KEYS.ENHANCED_PUNCTUATION, enhancedPunctuation);
    setOption(STORAGE_KEYS.SPEAKER_DIARIZATION, speakerDiarization);
    setOption(STORAGE_KEYS.TRANSCRIPTION_ENABLED, transcriptionEnabled);
    setOption(STORAGE_KEYS.SPEAKER_IDENTIFICATION, speakerIdentification);
  };
  
  return useMemo(() => ({
    // Valores
    audioQuality,
    setAudioQuality,
    autoGainControl,
    setAutoGainControl,
    noiseSuppression,
    setNoiseSuppression,
    echoCancellation,
    setEchoCancellation,
    enhancedPunctuation,
    setEnhancedPunctuation,
    speakerDiarization,
    setSpeakerDiarization,
    transcriptionEnabled,
    setTranscriptionEnabled,
    speakerIdentification,
    setSpeakerIdentification,
    // Ações
    saveAudioSettings
  }), [
    audioQuality,
    setAudioQuality,
    autoGainControl,
    setAutoGainControl,
    noiseSuppression,
    setNoiseSuppression,
    echoCancellation,
    setEchoCancellation,
    enhancedPunctuation,
    setEnhancedPunctuation,
    speakerDiarization,
    setSpeakerDiarization,
    transcriptionEnabled,
    setTranscriptionEnabled,
    speakerIdentification,
    setSpeakerIdentification,
    saveAudioSettings
  ]);
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useState, useEffect, useMemo } from "react";
import { getOption, setOption, STORAGE_KEYS, subscribeToStorageChanges } from '../../../../../../services/StorageService';

/**
 * Hook para gerenciamento de configurações de depuração
 * Neurônio metacognitivo de diagnóstico e instrumentação
 */
export const useDebugSettings = () => {
  // Estado neural-simbólico para opções de debug
  const [debugMode, setDebugMode] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.DEBUG_MODE) ?? false);
  const [logLevel, setLogLevel] = useState<string>(() => getOption<string>(STORAGE_KEYS.LOG_LEVEL) || 'info');
  
  // Sincroniza mudanças do storage para o estado local
  useEffect(() => {
    const handleStorageChange = (key: string, value: any) => {
      switch(key) {
        case STORAGE_KEYS.DEBUG_MODE: setDebugMode(value); break;
        case STORAGE_KEYS.LOG_LEVEL: setLogLevel(value); break;
      }
    };
    
    const unsubscribe = subscribeToStorageChanges(handleStorageChange);
    return () => unsubscribe();
  }, []);
  
  // Salva as configurações de debug no storage
  const saveDebugSettings = () => {
    setOption(STORAGE_KEYS.DEBUG_MODE, debugMode);
    setOption(STORAGE_KEYS.LOG_LEVEL, logLevel);
  };
  
  return useMemo(() => ({
    // Valores
    debugMode,
    setDebugMode,
    logLevel,
    setLogLevel,
    // Ações
    saveDebugSettings
  }), [
    debugMode,
    setDebugMode,
    logLevel,
    setLogLevel,
    saveDebugSettings
  ]);
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useState, useEffect, useMemo } from "react";
import { getOption, getUserName, setOption, STORAGE_KEYS, subscribeToStorageChanges } from '../../../../../../services/StorageService';
import { ModeService, OrchOSMode } from '../../../../../../services/ModeService';

/**
 * Hook para gerenciamento de configurações gerais
 * Componente neural-simbólico para configurações básicas do sistema
 */
export const useGeneralSettings = () => {
  // Estado neural-simbólico para opções gerais
  const [name, setName] = useState<string>(() => getUserName() || 'User');
  const [enableMatrix, setEnableMatrix] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.ENABLE_MATRIX) ?? true);
  const [matrixDensity, setMatrixDensity] = useState<number>(() => getOption<number>(STORAGE_KEYS.MATRIX_DENSITY) ?? 60);
  const [enableEffects, setEnableEffects] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.ENABLE_EFFECTS) ?? true);
  const [enableAnimations, setEnableAnimations] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.ENABLE_ANIMATIONS) ?? true);
  
  // Orch-OS Mode Cortex: single source of truth for mode
  const [applicationMode, setApplicationMode] = useState<OrchOSMode>(
    () => ModeService.getMode()
  );
  
  // Sincroniza mudanças do storage para o estado local
  useEffect(() => {
    const handleStorageChange = (key: string, value: any) => {
      switch(key) {
        case STORAGE_KEYS.USER_NAME: setName(value); break;
        case STORAGE_KEYS.APPLICATION_MODE: 
          setApplicationMode(value);
          break;
        case STORAGE_KEYS.ENABLE_MATRIX: setEnableMatrix(value); break;
        case STORAGE_KEYS.MATRIX_DENSITY: setMatrixDensity(value); break;
        case STORAGE_KEYS.ENABLE_EFFECTS: setEnableEffects(value); break;
        case STORAGE_KEYS.ENABLE_ANIMATIONS: setEnableAnimations(value); break;
      }
    };
    
    const unsubscribe = subscribeToStorageChanges(handleStorageChange);
    return () => unsubscribe();
  }, []);
  
  // Salva as configurações gerais no storage
  const saveGeneralSettings = () => {
    setOption(STORAGE_KEYS.USER_NAME, name);
    if (applicationMode) {
      setOption(STORAGE_KEYS.APPLICATION_MODE, applicationMode);
      ModeService.setMode(applicationMode); // Atualiza o modo no serviço
    }
    setOption(STORAGE_KEYS.ENABLE_MATRIX, enableMatrix);
    setOption(STORAGE_KEYS.MATRIX_DENSITY, matrixDensity);
    setOption(STORAGE_KEYS.ENABLE_EFFECTS, enableEffects);
    setOption(STORAGE_KEYS.ENABLE_ANIMATIONS, enableAnimations);
  };
  
  return useMemo(() => ({
    // Valores
    name,
    setName,
    applicationMode,
    setApplicationMode,
    enableMatrix,
    setEnableMatrix,
    matrixDensity,
    setMatrixDensity,
    enableEffects,
    setEnableEffects,
    enableAnimations,
    setEnableAnimations,
    // Ações
    saveGeneralSettings
  }), [
    name,
    setName,
    applicationMode,
    setApplicationMode,
    enableMatrix,
    setEnableMatrix,
    matrixDensity,
    setMatrixDensity,
    enableEffects,
    setEnableEffects,
    enableAnimations,
    setEnableAnimations,
    saveGeneralSettings
  ]);
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useState, useEffect, useMemo } from "react";
import { getOption, setOption, STORAGE_KEYS, subscribeToStorageChanges } from '../../../../../../services/StorageService';

/**
 * Hook para gerenciamento de configurações de interface
 * Componente neural-simbólico especializado na experiência visual do usuário
 */
export const useInterfaceSettings = () => {
  // Estado neural-simbólico para opções de interface
  const [darkMode, setDarkMode] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.DARK_MODE) ?? true);
  const [enableNeumorphism, setEnableNeumorphism] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.ENABLE_NEUMORPHISM) ?? true);
  const [enableGlassmorphism, setEnableGlassmorphism] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.ENABLE_GLASSMORPHISM) ?? true);
  const [panelTransparency, setPanelTransparency] = useState<number>(() => getOption<number>(STORAGE_KEYS.PANEL_TRANSPARENCY) ?? 70);
  const [colorTheme, setColorTheme] = useState<string>(() => getOption<string>(STORAGE_KEYS.COLOR_THEME) || 'quantum-blue');
  const [theme, setTheme] = useState<string>(() => getOption<string>(STORAGE_KEYS.THEME) || 'auto');
  const [uiDensity, setUiDensity] = useState<string>(() => getOption<string>(STORAGE_KEYS.UI_DENSITY) || 'normal');
  const [showAdvancedSettings, setShowAdvancedSettings] = useState<boolean>(() => getOption<boolean>(STORAGE_KEYS.SHOW_ADVANCED_SETTINGS) ?? false);
  
  // Sincroniza mudanças do storage para o estado local
  useEffect(() => {
    const handleStorageChange = (key: string, value: any) => {
      switch(key) {
        case STORAGE_KEYS.DARK_MODE: setDarkMode(value); break;
        case STORAGE_KEYS.ENABLE_NEUMORPHISM: setEnableNeumorphism(value); break;
        case STORAGE_KEYS.ENABLE_GLASSMORPHISM: setEnableGlassmorphism(value); break;
        case STORAGE_KEYS.PANEL_TRANSPARENCY: setPanelTransparency(value); break;
        case STORAGE_KEYS.COLOR_THEME: setColorTheme(value); break;
        case STORAGE_KEYS.THEME: setTheme(value); break;
        case STORAGE_KEYS.UI_DENSITY: setUiDensity(value); break;
        case STORAGE_KEYS.SHOW_ADVANCED_SETTINGS: setShowAdvancedSettings(value); break;
      }
    };
    
    const unsubscribe = subscribeToStorageChanges(handleStorageChange);
    return () => unsubscribe();
  }, []);
  
  // Salva as configurações de interface no storage
  const saveInterfaceSettings = () => {
    setOption(STORAGE_KEYS.DARK_MODE, darkMode);
    setOption(STORAGE_KEYS.ENABLE_NEUMORPHISM, enableNeumorphism);
    setOption(STORAGE_KEYS.ENABLE_GLASSMORPHISM, enableGlassmorphism);
    setOption(STORAGE_KEYS.PANEL_TRANSPARENCY, panelTransparency);
    setOption(STORAGE_KEYS.COLOR_THEME, colorTheme);
    setOption(STORAGE_KEYS.THEME, theme);
    setOption(STORAGE_KEYS.UI_DENSITY, uiDensity);
    setOption(STORAGE_KEYS.SHOW_ADVANCED_SETTINGS, showAdvancedSettings);
  };
  
  return useMemo(() => ({
    // Valores
    darkMode,
    setDarkMode,
    enableNeumorphism,
    setEnableNeumorphism,
    enableGlassmorphism,
    setEnableGlassmorphism,
    panelTransparency,
    setPanelTransparency,
    colorTheme,
    setColorTheme,
    theme,
    setTheme,
    uiDensity,
    setUiDensity,
    showAdvancedSettings,
    setShowAdvancedSettings,
    
    // Ações
    saveInterfaceSettings
  }), [
    darkMode,
    setDarkMode,
    enableNeumorphism,
    setEnableNeumorphism,
    enableGlassmorphism,
    setEnableGlassmorphism,
    panelTransparency,
    setPanelTransparency,
    colorTheme,
    setColorTheme,
    theme,
    setTheme,
    uiDensity,
    setUiDensity,
    showAdvancedSettings,
    setShowAdvancedSettings,
    saveInterfaceSettings
  ]);
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useState, useMemo } from "react";
import { OpenSectionType, TabType } from '../types';

/**
 * Hook para gerenciamento de estado de navegação do modal de configurações
 * Implementa o princípio de responsabilidade única gerenciando apenas a navegação
 */
export const useNavigationState = () => {
  const [openSection, setOpenSection] = useState<OpenSectionType>(null);
  const [activeTab, setActiveTab] = useState<TabType>('general');

  return useMemo(() => ({
    openSection,
    setOpenSection,
    activeTab,
    setActiveTab
  }), [
    openSection,
    setOpenSection,
    activeTab,
    setActiveTab
  ]);
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { memo, useState } from "react";
import { OrchOSModeEnum } from "../../../../../services/ModeService";
import {
  getOption,
  setOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";

import { BasicModeSettings, OllamaSettings } from "./api";
import { ApiSettingsProps } from "./api/types";

/**
 * Componente orquestrador para configurações de APIs externas
 * Arquitetura de modos:
 * - Modo Basic: HuggingFace (modelos leves via Transformers.js)
 * - Modo Advanced: Ollama + configurações avançadas (interface unificada)
 *
 * Refatorado seguindo princípios SOLID:
 * - Single Responsibility Principle: cada subcomponente tem responsabilidade única
 * - Open/Closed Principle: aberto para extensão, fechado para modificação
 * - Liskov Substitution: interfaces consistentes entre componentes
 * - Interface Segregation: interfaces específicas para cada tipo de configuração
 * - Dependency Inversion: depende de abstrações, não implementações concretas
 */
const ApiSettings: React.FC<ApiSettingsProps> = memo(
  ({
    applicationMode,
    setApplicationMode,
    ollamaModel,
    setOllamaModel,
    ollamaEmbeddingModel,
    setOllamaEmbeddingModel,
    ollamaEnabled,
    setOllamaEnabled,
  }) => {
    // Estado para configurações avançadas
    const [duckDbPath, setDuckDbPath] = useState<string>(
      () => getOption<string>(STORAGE_KEYS.DUCKDB_PATH) || "./orch-os-memory"
    );

    const [toolsEnabled, setToolsEnabled] = useState<boolean>(
      () => getOption<boolean>(STORAGE_KEYS.TOOLS_ENABLED) ?? true
    );

    // AI model options for HuggingFace (modo básico)
    const HF_MODELS = [
      {
        id: "Xenova/llama2.c-stories15M",
        label: "Llama2.c Stories (~15MB) - Ultra pequeno",
      },
      {
        id: "Xenova/distilgpt2",
        label: "DistilGPT-2 (~353MB) - Otimizado",
      },
      {
        id: "Xenova/gpt2",
        label: "GPT-2 Base (~548MB) - Estável",
      },
      {
        id: "Xenova/TinyLlama-1.1B-Chat-v1.0",
        label: "TinyLlama Chat (~1.1B) - Modelo de chat",
      },
    ];

    // Lista de modelos de embedding compatíveis (modo básico)
    const HF_EMBEDDING_MODELS = [
      {
        id: "Xenova/all-MiniLM-L6-v2",
        label: "all-MiniLM-L6-v2 (MiniLM 384d) — Recomendado",
      },
    ];

    // Handler para seleção de diretório DuckDB
    const handleBrowseDirectory = async () => {
      try {
        if (typeof window !== "undefined" && (window as any).electronAPI) {
          const result = await (window as any).electronAPI.selectDirectory();

          if (result.success && result.path) {
            const newPath = result.path;
            setDuckDbPath(newPath);
            setOption(STORAGE_KEYS.DUCKDB_PATH, newPath);
            console.log("📁 [SETTINGS] DuckDB path updated:", newPath);

            // Reinicializar DuckDB com o novo caminho
            try {
              const reinitResult = await (
                window as any
              ).electronAPI.reinitializeDuckDB(newPath);
              if (reinitResult.success) {
                console.log(
                  "✅ [SETTINGS] DuckDB successfully reinitialized with new path"
                );
              } else {
                console.error(
                  "❌ [SETTINGS] Failed to reinitialize DuckDB:",
                  reinitResult.error
                );
              }
            } catch (reinitError) {
              console.error(
                "❌ [SETTINGS] Error reinitializing DuckDB:",
                reinitError
              );
            }
          } else if (!result.canceled) {
            console.error(
              "❌ [SETTINGS] Failed to select directory:",
              result.error
            );
          }
        } else {
          console.warn(
            "⚠️ [SETTINGS] Directory selection not available in web mode"
          );
        }
      } catch (error) {
        console.error("❌ [SETTINGS] Error selecting directory:", error);
      }
    };

    // Handler para mudança manual do caminho DuckDB
    const handlePathChange = async (newPath: string) => {
      setDuckDbPath(newPath);
      setOption(STORAGE_KEYS.DUCKDB_PATH, newPath);

      // Reinicializar DuckDB apenas se estamos no Electron e o caminho não está vazio
      if (
        typeof window !== "undefined" &&
        (window as any).electronAPI &&
        newPath.trim()
      ) {
        try {
          const reinitResult = await (
            window as any
          ).electronAPI.reinitializeDuckDB(newPath);
          if (reinitResult.success) {
            console.log(
              "✅ [SETTINGS] DuckDB successfully reinitialized with new path"
            );
          } else {
            console.error(
              "❌ [SETTINGS] Failed to reinitialize DuckDB:",
              reinitResult.error
            );
          }
        } catch (reinitError) {
          console.error(
            "❌ [SETTINGS] Error reinitializing DuckDB:",
            reinitError
          );
        }
      }
    };

    // Handler para atualização do modelo HuggingFace (modo básico)
    const handleHfModelChange = (value: string) => {
      setOption(STORAGE_KEYS.HF_MODEL, value);
    };

    // Handler para atualização do modelo de embedding HuggingFace (modo básico)
    const handleHfEmbeddingModelChange = (value: string) => {
      setOption(STORAGE_KEYS.HF_EMBEDDING_MODEL, value);
    };

    // Renderização condicional baseada no modo da aplicação
    if (applicationMode === OrchOSModeEnum.BASIC) {
      return (
        <div className="space-y-4">
          {/* Header explicativo para modo básico */}
          <div className="bg-gradient-to-r from-green-500/10 to-blue-500/10 border border-green-500/30 rounded-lg p-4">
            <div className="flex items-center space-x-3">
              <div className="w-8 h-8 flex items-center justify-center rounded-full bg-green-500/20">
                <span className="text-green-400 text-sm">🤗</span>
              </div>
              <div>
                <h3 className="text-green-400 font-medium">
                  Modo Basic - HuggingFace
                </h3>
                <p className="text-green-400/70 text-sm">
                  Modelos leves executados via Transformers.js no navegador
                </p>
              </div>
            </div>
          </div>

          <BasicModeSettings
            applicationMode={applicationMode}
            setApplicationMode={setApplicationMode}
            hfModel=""
            setHfModel={handleHfModelChange}
            hfEmbeddingModel=""
            setHfEmbeddingModel={handleHfEmbeddingModelChange}
            hfModelOptions={HF_MODELS}
            hfEmbeddingModelOptions={HF_EMBEDDING_MODELS}
          />
        </div>
      );
    }

    return (
      <div className="flex flex-col w-full space-y-6">
        {/* Header explicativo para modo avançado */}
        <div className="bg-gradient-to-r from-cyan-500/10 to-purple-500/10 border border-cyan-500/30 rounded-lg p-4">
          <div className="flex items-center space-x-3">
            <div className="w-8 h-8 flex items-center justify-center rounded-full bg-cyan-500/20">
              <span className="text-cyan-400 text-sm">🦙</span>
            </div>
            <div>
              <h3 className="text-cyan-400 font-medium">
                Modo Advanced - Configurações Completas
              </h3>
              <p className="text-cyan-400/70 text-sm">
                Ollama, banco de dados e configurações avançadas
              </p>
            </div>
          </div>
        </div>

        {/* Seção Ollama - Modelos Locais */}
        <OllamaSettings
          ollamaModel={ollamaModel}
          setOllamaModel={(value) => {
            setOllamaModel(value);
            setOption(STORAGE_KEYS.OLLAMA_MODEL, value);
          }}
          ollamaEmbeddingModel={ollamaEmbeddingModel}
          setOllamaEmbeddingModel={(value) => {
            setOllamaEmbeddingModel(value);
            setOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL, value);
          }}
          ollamaEnabled={ollamaEnabled}
          setOllamaEnabled={(value) => {
            setOllamaEnabled(value);
            setOption(STORAGE_KEYS.OLLAMA_ENABLED, value);
          }}
          storagePath={duckDbPath}
          setStoragePath={(path) => {
            handlePathChange(path);
          }}
        />

        {/* Botão para voltar ao modo básico */}
        <div className="flex justify-end">
          <button
            type="button"
            className="bg-gradient-to-r from-green-500/20 to-blue-500/20 text-green-300 border border-green-500/30 rounded-lg px-6 py-2 hover:from-green-500/30 hover:to-blue-500/30 transition-all shadow-[0_0_10px_rgba(0,255,100,0.2)] backdrop-blur-sm"
            onClick={() => setApplicationMode(OrchOSModeEnum.BASIC)}
          >
            🤗 Switch to Basic Mode (HuggingFace)
          </button>
        </div>
      </div>
    );
  }
);

export default ApiSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';

/**
 * Componente para configurações de áudio e transcrição
 * Responsável por configurações relacionadas a entrada de áudio e processamento
 */
interface AudioSettingsProps {
  enhancedPunctuation: boolean;
  setEnhancedPunctuation: (value: boolean) => void;
  speakerDiarization: boolean;
  setSpeakerDiarization: (value: boolean) => void;
  audioQuality: number;
  setAudioQuality: (value: number) => void;
  autoGainControl: boolean;
  setAutoGainControl: (value: boolean) => void;
  noiseSuppression: boolean;
  setNoiseSuppression: (value: boolean) => void;
  echoCancellation: boolean;
  setEchoCancellation: (value: boolean) => void;
}

const AudioSettings: React.FC<AudioSettingsProps> = ({
  enhancedPunctuation,
  setEnhancedPunctuation,
  speakerDiarization,
  setSpeakerDiarization,
  audioQuality,
  setAudioQuality,
  autoGainControl,
  setAutoGainControl,
  noiseSuppression,
  setNoiseSuppression,
  echoCancellation,
  setEchoCancellation
}) => {
  return (
    <div className="space-y-4">
      {/* Configurações de transcrição */}
      <div>
        <h3 className="text-cyan-300 mb-2">Transcription Options</h3>
        
        <div className="flex items-center mb-2">
          <input
            type="checkbox"
            id="enhancedPunctuation"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={enhancedPunctuation}
            onChange={e => setEnhancedPunctuation(e.target.checked)}
          />
          <label htmlFor="enhancedPunctuation" className="text-cyan-100/80">Enhanced Punctuation</label>
        </div>
        
        <div className="flex items-center mb-2">
          <input
            type="checkbox"
            id="speakerDiarization"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={speakerDiarization}
            onChange={e => setSpeakerDiarization(e.target.checked)}
          />
          <label htmlFor="speakerDiarization" className="text-cyan-100/80">Speaker Diarization</label>
        </div>
      </div>

      {/* Qualidade de áudio */}
      <div>
        <label className="block mb-2 text-sm text-cyan-200/70">Audio Quality</label>
        <div className="flex items-center gap-3">
          <span className="text-xs text-cyan-400/60">Low</span>
          <input
            title="Audio Quality"
            type="range"
            min="0"
            max="100"
            value={audioQuality}
            onChange={e => setAudioQuality(parseInt(e.target.value))}
            className="flex-1 accent-cyan-400"
          />
          <span className="text-xs text-cyan-400/60">High</span>
        </div>
      </div>

      {/* Processamento de áudio */}
      <div>
        <h3 className="text-cyan-300 mb-2">Audio Processing</h3>
        
        <div className="flex items-center mb-2">
          <input
            type="checkbox"
            id="autoGainControl"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={autoGainControl}
            onChange={e => setAutoGainControl(e.target.checked)}
          />
          <label htmlFor="autoGainControl" className="text-cyan-100/80">Auto Gain Control</label>
        </div>
        
        <div className="flex items-center mb-2">
          <input
            type="checkbox"
            id="noiseSuppression"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={noiseSuppression}
            onChange={e => setNoiseSuppression(e.target.checked)}
          />
          <label htmlFor="noiseSuppression" className="text-cyan-100/80">Noise Suppression</label>
        </div>
        
        <div className="flex items-center">
          <input
            type="checkbox"
            id="echoCancellation"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={echoCancellation}
            onChange={e => setEchoCancellation(e.target.checked)}
          />
          <label htmlFor="echoCancellation" className="text-cyan-100/80">Echo Cancellation</label>
        </div>
      </div>
    </div>
  );
};

export default AudioSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { SelectedDevices } from "../../../../context";
import AudioControls from "../AudioControls";
import LanguageSelector from "../LanguageSelector";

/**
 * Componente simplificado de configurações de áudio
 * Apenas: Language e Device selection
 */
interface AudioSettingsSimpleProps {
  // Language
  language: string;
  setLanguage: (value: string) => void;

  // Device selection
  isMicrophoneOn: boolean;
  setIsMicrophoneOn: (value: boolean) => void;
  isSystemAudioOn: boolean;
  setIsSystemAudioOn: (value: boolean) => void;
  audioDevices: MediaDeviceInfo[];
  selectedDevices: SelectedDevices;
  handleDeviceChange: (deviceId: string, isSystemAudio: boolean) => void;
}

const AudioSettingsSimple: React.FC<AudioSettingsSimpleProps> = ({
  // Language
  language,
  setLanguage,

  // Device selection
  isMicrophoneOn,
  setIsMicrophoneOn,
  isSystemAudioOn,
  setIsSystemAudioOn,
  audioDevices,
  selectedDevices,
  handleDeviceChange,
}) => {
  return (
    <div className="space-y-6">
      {/* Language Selection */}
      <div className="pb-5 border-b border-cyan-500/20">
        <h4 className="text-cyan-300 mb-3 font-semibold text-base">Language</h4>
        <LanguageSelector language={language} setLanguage={setLanguage} />
      </div>

      {/* Device Controls */}
      <div>
        <h4 className="text-cyan-300 mb-4 font-semibold text-base">
          Audio Devices
        </h4>
        <AudioControls
          isMicrophoneOn={isMicrophoneOn}
          setIsMicrophoneOn={setIsMicrophoneOn}
          isSystemAudioOn={isSystemAudioOn}
          setIsSystemAudioOn={setIsSystemAudioOn}
          audioDevices={audioDevices}
          selectedDevices={selectedDevices}
          handleDeviceChange={handleDeviceChange}
        />
      </div>
    </div>
  );
};

export default AudioSettingsSimple;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import ModeToggle from './ModeToggle';

/**
 * Componente para configurações gerais do Orch-OS
 * Segue os princípios neurais-simbólicos (Single Responsibility)
 */
import { OrchOSMode, OrchOSModeEnum } from '../../../../../services/ModeService';

interface GeneralSettingsProps {
  name: string;
  setName: (value: string) => void;
  applicationMode: OrchOSMode;
  setApplicationMode: (mode: OrchOSMode) => void;
  enableMatrix: boolean;
  setEnableMatrix: (value: boolean) => void;
  matrixDensity: number;
  setMatrixDensity: (value: number) => void;
  enableEffects: boolean;
  setEnableEffects: (value: boolean) => void;
  enableAnimations: boolean;
  setEnableAnimations: (value: boolean) => void;
}

const GeneralSettings: React.FC<GeneralSettingsProps> = ({
  name,
  setName,
  applicationMode,
  setApplicationMode,
  enableMatrix,
  setEnableMatrix,
  matrixDensity,
  setMatrixDensity,
  enableEffects,
  setEnableEffects,
  enableAnimations,
  setEnableAnimations,
}) => {
  return (
    <div className="space-y-4">
      {/* Nome de usuário - identidade simbólica */}
      <div>
        <label htmlFor="userName" className="block text-cyan-200/80 mb-1">User Name</label>
        <input
          type="text"
          id="userName"
          className="w-full p-2 rounded-lg bg-black/30 border-2 border-cyan-400/40 text-white focus:outline-none focus:border-cyan-400"
          value={name}
          onChange={e => setName(e.target.value)}
          placeholder="Enter your name"
        />
      </div>

      {/* Toggle de modo Neural-Simbólico */}
      <div className="pt-2">
        <h3 className="text-cyan-300 mb-2">Neural Processing Mode</h3>
        <ModeToggle mode={applicationMode} onChange={setApplicationMode} />
      </div>

      {/* Quantum Consciousness Matrix */}
      <div className="pt-1">
        <h3 className="text-cyan-300 mb-2">Quantum Consciousness Matrix</h3>
        <div className="flex items-center mb-2">
          <input
            type="checkbox"
            id="enableMatrix"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={enableMatrix}
            onChange={e => setEnableMatrix(e.target.checked)}
          />
          <label htmlFor="enableMatrix" className="text-cyan-100/80">Enable Quantum Visualization</label>
        </div>
        
        <div className="mb-2">
          <label className="block mb-2 text-sm text-cyan-200/70">Particle Density</label>
          <input
            title="Particle Density"
            type="range"
            min="10"
            max="100"
            value={matrixDensity}
            onChange={e => setMatrixDensity(parseInt(e.target.value))}
            className="w-full accent-cyan-400"
            disabled={!enableMatrix}
          />
        </div>
      </div>

      {/* System Performance */}
      <div className="pt-2">
        <h3 className="text-cyan-300 mb-2">System Performance</h3>
        <div className="flex items-center mb-2">
          <input
            type="checkbox"
            id="enableEffects"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={enableEffects}
            onChange={e => setEnableEffects(e.target.checked)}
          />
          <label htmlFor="enableEffects" className="text-cyan-100/80">Enable Visual Effects</label>
        </div>
        
        <div className="flex items-center">
          <input
            type="checkbox"
            id="enableAnimations"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={enableAnimations}
            onChange={e => setEnableAnimations(e.target.checked)}
          />
          <label htmlFor="enableAnimations" className="text-cyan-100/80">Enable Animations</label>
        </div>
      </div>
    </div>
  );
};

export default GeneralSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

export { default as GeneralSettings } from './GeneralSettings';
export { default as InterfaceSettings } from './InterfaceSettings';
export { default as AudioSettings } from './AudioSettings';

export { default as ModeToggle } from './ModeToggle';
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';

/**
 * Componente para configurações de interface do quantum dashboard
 * Segue os princípios SOLID e Clean Architecture do Orch-OS
 */
interface InterfaceSettingsProps {
  darkMode: boolean;
  setDarkMode: (value: boolean) => void;
  enableNeumorphism: boolean;
  setEnableNeumorphism: (value: boolean) => void;
  enableGlassmorphism: boolean;
  setEnableGlassmorphism: (value: boolean) => void;
  panelTransparency: number;
  setPanelTransparency: (value: number) => void;
  colorTheme: string;
  setColorTheme: (value: string) => void;
}

const InterfaceSettings: React.FC<InterfaceSettingsProps> = ({
  darkMode,
  setDarkMode,
  enableNeumorphism,
  setEnableNeumorphism,
  enableGlassmorphism,
  setEnableGlassmorphism,
  panelTransparency,
  setPanelTransparency,
  colorTheme,
  setColorTheme
}) => {
  return (
    <div className="space-y-4">
      {/* Appearance */}
      <div>
        <h3 className="text-cyan-300 mb-2">Visual Appearance</h3>
        
        <div className="flex items-center mb-3">
          <input
            type="checkbox"
            id="darkMode"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={darkMode}
            onChange={e => setDarkMode(e.target.checked)}
          />
          <label htmlFor="darkMode" className="text-cyan-100/80">Dark Mode</label>
        </div>
        
        <div className="flex items-center mb-3">
          <input
            type="checkbox"
            id="enableGlassmorphism"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={enableGlassmorphism}
            onChange={e => setEnableGlassmorphism(e.target.checked)}
          />
          <label htmlFor="enableGlassmorphism" className="text-cyan-100/80">Glassmorphic Effects</label>
        </div>
        
        <div className="flex items-center mb-2">
          <input
            type="checkbox"
            id="enableNeumorphism"
            className="mr-2 h-5 w-5 rounded-sm accent-cyan-400 bg-black/50"
            checked={enableNeumorphism}
            onChange={e => setEnableNeumorphism(e.target.checked)}
          />
          <label htmlFor="enableNeumorphism" className="text-cyan-100/80">Neumorphic Controls</label>
        </div>
      </div>

      {/* Panel Transparency */}
      <div>
        <label className="block mb-2 text-sm text-cyan-200/70">Panel Transparency</label>
        <div className="flex items-center gap-3">
          <span className="text-xs text-cyan-400/60">Solid</span>
          <input
            title="Panel Transparency"
            type="range"
            min="0"
            max="90"
            value={panelTransparency}
            onChange={e => setPanelTransparency(parseInt(e.target.value))}
            className="flex-1 accent-cyan-400"
          />
          <span className="text-xs text-cyan-400/60">Clear</span>
        </div>
      </div>

      {/* Color Theme Selection */}
      <div>
        <label htmlFor="colorTheme" className="block mb-2 text-sm text-cyan-200/70">Color Theme</label>
        <select
          id="colorTheme"
          className="w-full p-2 rounded bg-black/40 text-white/90 border border-cyan-500/30"
          value={colorTheme}
          onChange={e => setColorTheme(e.target.value)}
          title="Select color theme"
        >
          <option value="quantum-blue">Quantum Blue (Default)</option>
          <option value="neural-purple">Neural Purple</option>
          <option value="cosmic-green">Cosmic Green</option>
          <option value="plasma-orange">Plasma Orange</option>
        </select>
      </div>
    </div>
  );
};

export default InterfaceSettings;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';

/**
 * Neural-Symbolic Mode Toggle Component
 * Controla a seleção entre o modo básico e avançado do Orch-OS
 */
import { OrchOSMode, OrchOSModeEnum } from '../../../../../services/ModeService';

interface ModeToggleProps {
  mode: OrchOSMode;
  onChange: (mode: OrchOSMode) => void;
}

const ModeToggle: React.FC<ModeToggleProps> = ({ mode, onChange }) => {
  // Symbolic: All mode checks use enum for clarity and type safety
  const isBasic = mode === OrchOSModeEnum.BASIC;
  const isAdvanced = mode === OrchOSModeEnum.ADVANCED;

  return (
    <div className="flex flex-col items-center">
      <div className="w-full max-w-xs backdrop-blur-md bg-black/40 rounded-full p-1 flex relative overflow-hidden border border-cyan-500/30 shadow-[0_0_15px_rgba(0,200,255,0.2)]">
        {/* Symbolic: Glow indicator moves according to mode */}
        <div 
          className={`absolute inset-y-1 w-1/2 rounded-full transition-transform duration-500 ease-quantum ${
            isBasic
              ? 'left-1 bg-gradient-to-r from-cyan-500/40 to-blue-500/40 shadow-[0_0_20px_5px_rgba(56,189,248,0.35)]'
              : 'right-1 left-auto bg-gradient-to-r from-blue-500/40 to-purple-600/40 shadow-[0_0_20px_5px_rgba(147,51,234,0.35)]'
          }`}
        />
        
        {/* Symbolic: Use enum for mode selection */}
        <button 
          className={`flex-1 py-2 px-2 rounded-full z-10 transition-colors duration-300 ${
            isBasic ? 'text-white font-medium' : 'text-white/60'
          }`}
          onClick={() => onChange(OrchOSModeEnum.BASIC)}
        >
          Basic Mode
        </button>
        
        <button 
          className={`flex-1 py-2 px-2 rounded-full z-10 transition-colors duration-300 ${
            isAdvanced ? 'text-white font-medium' : 'text-white/60'
          }`}
          onClick={() => onChange(OrchOSModeEnum.ADVANCED)}
        >
          Advanced Mode
        </button>
      </div>
      
      {/* Symbolic: Mode description uses enum for clarity */}
      <p className="text-xs text-cyan-300/70 mt-2 text-center max-w-xs">
        {isBasic
          ? 'Using HuggingFace models and local database storage.' 
          : 'Using Deepgram, OpenAI and Pinecone neural infrastructure.'}
      </p>
    </div>
  );
};

export default ModeToggle;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { SettingsFooterProps } from "./types";

/**
 * Componente para o rodapé do modal de configurações
 * Implementa o princípio de responsabilidade única isolando
 * a interface de ações do usuário
 */
const SettingsFooter: React.FC<SettingsFooterProps> = ({ onClose, saveSettings }) => {
  // Handler para salvar configurações e fechar
  const handleApplyChanges = () => {
    saveSettings();
    onClose();
  };
  
  return (
    <div className="flex justify-end space-x-4 mt-8">
      <button 
        className="px-6 py-2 bg-black/40 text-cyan-400/80 rounded hover:bg-black/60 hover:text-cyan-300 transition-all"
        onClick={onClose}
      >
        Cancel
      </button>
      <button 
        className="px-6 py-2 bg-gradient-to-r from-cyan-500/30 to-blue-500/30 text-cyan-300 rounded hover:from-cyan-500/40 hover:to-blue-500/40 transition-all shadow-[0_0_10px_rgba(0,200,255,0.2)] backdrop-blur-sm"
        onClick={handleApplyChanges}
      >
        Apply Changes
      </button>
    </div>
  );
};

export default SettingsFooter;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { SettingsHeaderProps } from "./types";

/**
 * Componente para o cabeçalho do modal de configurações
 * Separado seguindo o princípio de responsabilidade única (SRP)
 */
const SettingsHeader: React.FC<SettingsHeaderProps> = ({ onClose }) => {
  return (
    <>
      <button
        className="orchos-btn-circle absolute top-4 right-4"
        onClick={onClose}
        title="Close"
      >
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M4.646 4.646a.5.5 0 0 1 .708 0L8 7.293l2.646-2.647a.5.5 0 0 1 .708.708L8.707 8l2.647 2.646a.5.5 0 0 1-.708.708L8 8.707l-2.646 2.647a.5.5 0 0 1-.708-.708L7.293 8 4.646 5.354a.5.5 0 0 1 0-.708z" />
        </svg>
      </button>
      
      <h2 className="text-2xl font-bold mb-4 text-center tracking-wide bg-gradient-to-r from-cyan-400 via-blue-500 to-purple-500 bg-clip-text text-transparent drop-shadow-[0_0_12px_rgba(0,240,255,0.5)] orchos-title">
        Quantum System Settings
      </h2>
    </>
  );
};

export default SettingsHeader;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { SettingsNavigationProps } from "./types";

/**
 * Componente para a navegação entre as abas do modal de configurações
 * Simbolicamente representa o cortex de navegação entre contextos neurais de configuração
 */
const SettingsNavigation: React.FC<SettingsNavigationProps> = ({ 
  activeTab, 
  setActiveTab 
}) => {
  return (
    <div className="flex space-x-2 mb-6 border-b border-cyan-400/30 pb-2">
      <button 
        className={`px-4 py-2 rounded-t-lg ${activeTab === 'general' ? 'bg-cyan-500/20 text-cyan-300' : 'text-cyan-400/60 hover:text-cyan-300'} transition-all duration-200`}
        onClick={() => setActiveTab('general')}
      >
        General
      </button>
      <button 
        className={`px-4 py-2 rounded-t-lg ${activeTab === 'interface' ? 'bg-cyan-500/20 text-cyan-300' : 'text-cyan-400/60 hover:text-cyan-300'} transition-all duration-200`}
        onClick={() => setActiveTab('interface')}
      >
        Interface
      </button>
      <button 
        className={`px-4 py-2 rounded-t-lg ${activeTab === 'audio' ? 'bg-cyan-500/20 text-cyan-300' : 'text-cyan-400/60 hover:text-cyan-300'} transition-all duration-200`}
        onClick={() => setActiveTab('audio')}
      >
        Audio
      </button>
      <button 
        className={`px-4 py-2 rounded-t-lg ${activeTab === 'advanced' ? 'bg-cyan-500/20 text-cyan-300' : 'text-cyan-400/60 hover:text-cyan-300'} transition-all duration-200`}
        onClick={() => setActiveTab('advanced')}
      >
        Advanced
      </button>
    </div>
  );
};

export default SettingsNavigation;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { OrchOSMode } from "../../../../../services/ModeService";

// Interfaces para props compartilhados entre componentes

export interface SettingsModalProps {
  show: boolean;
  onClose: () => void;
}

export interface SettingsNavigationProps {
  activeTab: TabType;
  setActiveTab: (tab: TabType) => void;
}

export interface SettingsHeaderProps {
  onClose: () => void;
}

export interface SettingsFooterProps {
  onClose: () => void;
  saveSettings: () => void;
}

export type TabType = "general" | "interface" | "audio" | "advanced";

export type OpenSectionType =
  | "pinecone"
  | "chatgpt"
  | "deepgram"
  | "ollama"
  | null;

// Interface para o state completo das configurações
export interface SettingsState {
  // Estado de navegação e seções
  openSection: OpenSectionType;
  setOpenSection: (section: OpenSectionType) => void;
  activeTab: TabType;
  setActiveTab: (tab: TabType) => void;

  // General
  name: string;
  setName: (name: string) => void;
  enableMatrix: boolean;
  setEnableMatrix: (enable: boolean) => void;
  matrixDensity: number;
  setMatrixDensity: (density: number) => void;
  enableEffects: boolean;
  setEnableEffects: (enable: boolean) => void;
  enableAnimations: boolean;
  setEnableAnimations: (enable: boolean) => void;

  // Interface
  darkMode: boolean;
  setDarkMode: (enable: boolean) => void;
  enableNeumorphism: boolean;
  setEnableNeumorphism: (enable: boolean) => void;
  enableGlassmorphism: boolean;
  setEnableGlassmorphism: (enable: boolean) => void;
  panelTransparency: number;
  setPanelTransparency: (transparency: number) => void;
  colorTheme: string;
  setColorTheme: (theme: string) => void;

  // Audio
  audioQuality: number;
  setAudioQuality: (quality: number) => void;
  autoGainControl: boolean;
  setAutoGainControl: (enable: boolean) => void;
  noiseSuppression: boolean;
  setNoiseSuppression: (enable: boolean) => void;
  echoCancellation: boolean;
  setEchoCancellation: (enable: boolean) => void;
  enhancedPunctuation: boolean;
  setEnhancedPunctuation: (enable: boolean) => void;
  speakerDiarization: boolean;
  setSpeakerDiarization: (enable: boolean) => void;

  // ChatGPT, Deepgram & Pinecone
  chatgptApiKey: string;
  setChatgptApiKey: (key: string) => void;
  chatgptModel: string;
  setChatgptModel: (model: string) => void;
  chatgptTemperature: number;
  setChatgptTemperature: (temp: number) => void;
  chatgptMaxTokens: number;
  setChatgptMaxTokens: (tokens: number) => void;
  openaiEmbeddingModel: string;
  setOpenaiEmbeddingModel: (model: string) => void;

  // HuggingFace
  hfModel: string;
  setHfModel: (model: string) => void;
  hfEmbeddingModel: string;
  setHfEmbeddingModel: (model: string) => void;

  // Deepgram
  deepgramApiKey: string;
  setDeepgramApiKey: (key: string) => void;
  deepgramModel: string;
  setDeepgramModel: (model: string) => void;
  deepgramLanguage: string;
  setDeepgramLanguage: (language: string) => void;
  deepgramTier: string;
  setDeepgramTier: (tier: string) => void;

  // Ollama
  ollamaModel: string;
  setOllamaModel: (model: string) => void;
  ollamaEmbeddingModel: string;
  setOllamaEmbeddingModel: (model: string) => void;
  ollamaEnabled: boolean;
  setOllamaEnabled: (enabled: boolean) => void;

  // Transcrição
  transcriptionEnabled: boolean;
  setTranscriptionEnabled: (enable: boolean) => void;
  speakerIdentification: boolean;
  setSpeakerIdentification: (enable: boolean) => void;

  // Pinecone
  pineconeApiKey: string;
  setPineconeApiKey: (key: string) => void;
  pineconeEnvironment: string;
  setPineconeEnvironment: (env: string) => void;
  pineconeIndex: string;
  setPineconeIndex: (index: string) => void;

  // Interface adicional
  theme: string;
  setTheme: (theme: string) => void;
  uiDensity: string;
  setUiDensity: (density: string) => void;
  showAdvancedSettings: boolean;
  setShowAdvancedSettings: (show: boolean) => void;

  // Debug
  debugMode: boolean;
  setDebugMode: (enable: boolean) => void;
  logLevel: string;
  setLogLevel: (level: string) => void;

  // Orch-OS Mode
  applicationMode: OrchOSMode;
  setApplicationMode: (mode: OrchOSMode) => void;

  // Métodos
  saveSettings: () => void;
}

// Props específicos para ApiSettings (mantido para compatibilidade)
export interface ApiSettingsProps {
  applicationMode: OrchOSMode;
  setApplicationMode: (mode: OrchOSMode) => void;
  pineconeApiKey: string;
  setPineconeApiKey: (key: string) => void;
  chatgptApiKey: string;
  setChatgptApiKey: (key: string) => void;
  chatgptModel: string;
  setChatgptModel: (model: string) => void;
  openaiEmbeddingModel: string;
  setOpenaiEmbeddingModel: (model: string) => void;
  hfModel: string;
  setHfModel: (model: string) => void;
  hfEmbeddingModel: string;
  setHfEmbeddingModel: (model: string) => void;
  deepgramApiKey: string;
  setDeepgramApiKey: (key: string) => void;
  deepgramModel: string;
  setDeepgramModel: (model: string) => void;
  deepgramLanguage: string;
  setDeepgramLanguage: (language: string) => void;
  // Ollama
  ollamaModel: string;
  setOllamaModel: (model: string) => void;
  ollamaEmbeddingModel: string;
  setOllamaEmbeddingModel: (model: string) => void;
  ollamaEnabled: boolean;
  setOllamaEnabled: (enabled: boolean) => void;
  openSection: OpenSectionType;
  setOpenSection: (section: OpenSectionType) => void;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useState, useMemo } from "react";
import { SettingsState } from './types';
import { useNavigationState } from './hooks/useNavigationState';
import { useGeneralSettings } from './hooks/useGeneralSettings';
import { useInterfaceSettings } from './hooks/useInterfaceSettings';
import { useAudioSettings } from './hooks/useAudioSettings';
import { useApiSettings } from './hooks/useApiSettings';
import { useDebugSettings } from './hooks/useDebugSettings';
import { OrchOSMode, ModeService } from '../../../../../services/ModeService';

/**
 * Hook orquestrador para gerenciamento de estado neural-simbólico das configurações
 * Implementa o princípio de responsabilidade única e composição
 * agregando os hooks especializados por domínio
 */
export const useSettingsState = (show: boolean): SettingsState => {
  // Composição de hooks especializados por domínio
  const navigation = useNavigationState();
  const general = useGeneralSettings();
  const interfaceSettings = useInterfaceSettings();
  const audio = useAudioSettings(show);
  const api = useApiSettings();
  const debug = useDebugSettings();
  
  // Modo de aplicação (Básico/Avançado)
  const [applicationMode, setApplicationModeState] = useState<OrchOSMode>(() => ModeService.getMode());
  
  // Handler para alteração do modo com persistência
  const setApplicationMode = (mode: OrchOSMode) => {
    ModeService.setMode(mode);
    setApplicationModeState(mode);
  };
  
  // Função unificada para salvar todas as configurações
  const saveSettings = () => {
    // Salva as configurações de cada domínio
    general.saveGeneralSettings();
    interfaceSettings.saveInterfaceSettings();
    audio.saveAudioSettings();
    api.saveApiSettings();
    debug.saveDebugSettings();
  };
  
  // Combina todos os estados e funções dos hooks especializados
  // Memoiza o objeto para evitar renders desnecessários
  return useMemo(() => ({
    // Navegação
    ...navigation,
    // General
    ...general,
    // Interface
    ...interfaceSettings,
    // Audio e Transcrição
    ...audio,
    // API (OpenAI, Deepgram, HuggingFace, Pinecone)
    ...api,
    // Debug
    ...debug,
    // Modo da aplicação
    applicationMode,
    setApplicationMode,
    // Ação unificada
    saveSettings
  }), [
    // Dependências: todos os objetos retornados pelos hooks especializados e applicationMode
    navigation,
    general,
    interfaceSettings,
    audio,
    api,
    debug,
    applicationMode,
    setApplicationMode,
    saveSettings
  ]);
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { AudioControlsProps } from "../types/interfaces";
import DeviceSelector from "./DeviceSelector";
import ToggleSwitch from "./ToggleSwitch";

const AudioControls: React.FC<AudioControlsProps> = ({
  isMicrophoneOn,
  setIsMicrophoneOn,
  isSystemAudioOn,
  setIsSystemAudioOn,
  audioDevices,
  selectedDevices,
  handleDeviceChange,
}) => {
  return (
    <div className="space-y-4">
      {/* Microphone Section */}
      <div className="space-y-2">
        <ToggleSwitch
          label="Microphone"
          isOn={isMicrophoneOn}
          onChange={() => setIsMicrophoneOn(!isMicrophoneOn)}
          title="Toggle microphone"
        />

        {isMicrophoneOn && (
          <div className="ml-0 animate-fadeIn">
            <DeviceSelector
              devices={audioDevices}
              selectedId={selectedDevices.microphone ?? ""}
              onChange={(deviceId) => handleDeviceChange(deviceId, false)}
              title="Select microphone"
              isSystemAudio={false}
            />
          </div>
        )}
      </div>

      {/* System Audio Section */}
      <div className="space-y-2">
        <ToggleSwitch
          label="System Audio"
          isOn={isSystemAudioOn}
          onChange={() => setIsSystemAudioOn(!isSystemAudioOn)}
          title="Toggle system audio"
        />

        {isSystemAudioOn && (
          <div className="ml-0 animate-fadeIn">
            <DeviceSelector
              devices={audioDevices}
              selectedId={selectedDevices.systemAudio ?? ""}
              onChange={(deviceId) => handleDeviceChange(deviceId, true)}
              title="Select system audio source"
              isSystemAudio={true}
            />
          </div>
        )}
      </div>
    </div>
  );
};

export default AudioControls;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import CognitionTimeline from '../../CognitionTimeline/CognitionTimeline';
import { CognitionEvent } from '../../../context/deepgram/types/CognitionEvent';
import styles from '../components/TextControls.module.css';

interface CognitionLogSectionProps {
  cognitionEvents: CognitionEvent[];
  exporters: { label: string }[];
  exportEvents: (label: string) => void;
  clearEvents: () => void;
}

const CognitionLogSection: React.FC<CognitionLogSectionProps> = ({
  cognitionEvents,
  exporters,
  exportEvents,
  clearEvents
}) => {
  return (
    <div className="flex flex-col h-full">
      {/* Button row centralizada Vision Pro Style */}
      <div className="orchos-cognition-logs-buttons justify-center items-center gap-2 mb-2.5 flex flex-wrap w-full" style={{ position: 'relative', zIndex: 10 }}>
        <button
          className={`${styles['orchos-btn-glass']} ${styles['orchos-btn-glow']} ${styles['orchos-btn-action']} px-3 py-1 flex items-center justify-center mx-[0.18rem]`}
          onClick={() => exportEvents('Export cognitive log (JSON)')}
          title="Export as JSON"
          style={{ minWidth: '72px', height: '32px', fontSize: '0.92rem', fontWeight: 500 }}
        >
          <span className="align-middle">JSON</span>
        </button>
        <button
          className={`${styles['orchos-btn-glass']} ${styles['orchos-btn-glow']} ${styles['orchos-btn-action']} px-3 py-1 flex items-center justify-center`}
          onClick={() => exportEvents('Export cognitive log (TXT)')}
          title="Export as TXT"
          style={{ minWidth: '72px', height: '32px', fontSize: '0.92rem', fontWeight: 500 }}
        >
          <span className="align-middle">TXT</span>
        </button>
        <button
          className={`${styles['orchos-btn-glass']} ${styles['orchos-btn-glow']} ${styles['orchos-btn-action']} px-3 py-1 flex items-center justify-center`}
          onClick={clearEvents}
          title="Clear all logs"
          style={{ minWidth: '72px', height: '32px', fontSize: '0.92rem', fontWeight: 500 }}
        >
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none"><ellipse cx="8" cy="8" rx="6.5" ry="4.5" stroke="#ff4dd2" strokeWidth="1.2"/><path d="M6 6l4 4M10 6l-4 4" stroke="#ff4dd2" strokeWidth="1.3" strokeLinecap="round"/></svg>
          <span className="hidden md:inline ml-1 align-middle">Clear</span>
        </button>
      </div>
      <div className="orchos-cognition-logs-list">
        <CognitionTimeline events={cognitionEvents} />
      </div>
    </div>
  );
};

export default CognitionLogSection;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { ConnectionDiagnosticsProps } from '../types/interfaces';

const ConnectionDiagnostics: React.FC<ConnectionDiagnosticsProps> = ({
  connectionDetails,
  setConnectionDetails,
  getConnectionStatus,
  showToast,
  disconnectFromDeepgram,
  connectToDeepgram,
  waitForConnectionState,
  hasActiveConnection,
  ConnectionState
}) => {
  const checkConnectionStatus = () => {
    if (getConnectionStatus) {
      const status = getConnectionStatus();
      setConnectionDetails(status);

      if (status.active) {
        showToast("Diagnostics", "Connection is active and ready to use", "success");
      } else if (status.hasConnectionObject && status.readyState !== 1) {
        showToast("Diagnostics", `Connection exists but ReadyState = ${status.readyState} (expected: 1)`, "error");
      } else if (!status.hasConnectionObject && status.stateRef === 'OPEN') {
        showToast("Diagnostics", "State inconsistent: marked as OPEN but connection is null", "error");
      } else {
        showToast("Diagnostics", `Connection is not active (${status.stateRef})`, "error");
      }
    }
  };

  const forceReconnect = async () => {
    try {
      disconnectFromDeepgram();
      await new Promise(resolve => setTimeout(resolve, 1000));
      await connectToDeepgram();
      const connected = await waitForConnectionState(ConnectionState.OPEN, 5000);
      if (connected && hasActiveConnection()) {
        showToast("Diagnostics", "Reconnection successful", "success");
      } else {
        showToast("Diagnostics", "Reconnection failed", "error");
      }
    } catch (error) {
      console.error("Error forcing reconnection:", error);
      showToast("Diagnostics", "Error forcing reconnection", "error");
    }
  };

  return (
    <div className="mt-4 p-3 bg-gray-800 rounded-md text-xs">
      <div className="flex justify-between items-center mb-2">
        <div className="font-medium">Connection Diagnostics</div>
        <button
          onClick={checkConnectionStatus}
          className="text-xs bg-blue-600 px-2 py-1 rounded hover:bg-blue-700"
        >
          Check Now
        </button>
      </div>
      {(connectionDetails && typeof connectionDetails === "object" && connectionDetails !== null && "state" in connectionDetails) ? (
        <div className="space-y-2">
          <div className="grid grid-cols-2 gap-x-2">
            <div>Current state:</div>
            <div className={
              typeof connectionDetails.state === 'string' && connectionDetails.state === 'OPEN' ? 'text-green-400' :
                typeof connectionDetails.state === 'string' && connectionDetails.state === 'CONNECTING' ? 'text-yellow-400' :
                  'text-red-400'
            }>
              {typeof connectionDetails.state === 'string' ? connectionDetails.state : ''}
            </div>
            <div>State (ref):</div>
            <div className={
              typeof connectionDetails.stateRef === 'string' && connectionDetails.stateRef === 'OPEN' ? 'text-green-400' :
                typeof connectionDetails.stateRef === 'string' && connectionDetails.stateRef === 'CONNECTING' ? 'text-yellow-400' :
                  'text-red-400'
            }>
              {typeof connectionDetails.stateRef === 'string' ? connectionDetails.stateRef : ''}
            </div>
            <div>Connection object:</div>
            <div className={connectionDetails.hasConnectionObject ? 'text-green-400' : 'text-red-400'}>
              {connectionDetails.hasConnectionObject ? 'Available' : 'Not available'}
            </div>
            <div>WebSocket ReadyState:</div>
            <div className={
              connectionDetails.readyState === 1 ? 'text-green-400' :
                connectionDetails.readyState === 0 ? 'text-yellow-400' :
                  'text-red-400'
            }>
              {connectionDetails.readyState === null ? 'N/A' :
                connectionDetails.readyState === 0 ? '0 (CONNECTING)' :
                  connectionDetails.readyState === 1 ? '1 (OPEN)' :
                    connectionDetails.readyState === 2 ? '2 (CLOSING)' :
                      connectionDetails.readyState === 3 ? '3 (CLOSED)' : 'Unknown'}
            </div>
            <div>Connection active:</div>
            <div className={connectionDetails.active ? 'text-green-400' : 'text-red-400'}>
              {connectionDetails.active ? 'Yes ✅' : 'No ❌'}
            </div>
          </div>
          <div className="pt-2 flex space-x-2">
            <button
              onClick={disconnectFromDeepgram}
              className="flex-1 bg-red-700 p-2 rounded hover:bg-red-800"
            >
              Disconnect
            </button>
            <button
              onClick={forceReconnect}
              className="flex-1 bg-green-700 p-2 rounded hover:bg-green-800"
            >
              Force Reconnect
            </button>
          </div>
        </div>
      ) : (
        <div className="text-gray-400">Click on &quot;Check Now&quot; to view connection details</div>
      )}
    </div>
  );
};

export default ConnectionDiagnostics;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";

interface DeviceSelectorProps {
  devices: MediaDeviceInfo[];
  selectedId?: string;
  onChange: (id: string) => void;
  title: string;
  isSystemAudio: boolean;
}

const DeviceSelector: React.FC<DeviceSelectorProps> = ({
  devices,
  selectedId,
  onChange,
  title,
  isSystemAudio,
}) => {
  // Filter devices based on type
  const filteredDevices = devices.filter((device) => {
    const label = device.label.toLowerCase();
    const isSystemDevice =
      label.includes("blackhole") || label.includes("dipper");
    return isSystemAudio ? isSystemDevice : !isSystemDevice;
  });

  return (
    <select
      title={title}
      className="w-full p-2.5 rounded-lg bg-black/40 text-white/90 text-sm border border-cyan-500/20 focus:border-cyan-500/50 focus:outline-none focus:ring-1 focus:ring-cyan-500/30 transition-all"
      value={selectedId || ""}
      onChange={(e) => onChange(e.target.value)}
    >
      {filteredDevices.length === 0 && (
        <option value="" disabled>
          No {isSystemAudio ? "system audio" : "microphone"} devices found
        </option>
      )}
      {filteredDevices.map((device) => (
        <option key={device.deviceId} value={device.deviceId}>
          {device.label || `Device ${device.deviceId.substring(0, 5)}...`}
        </option>
      ))}
    </select>
  );
};

export default DeviceSelector;
/* SPDX-License-Identifier: MIT OR Apache-2.0
   Copyright (c) 2025 Guilherme Ferrari Brescia */

/* Painel Principal de Diagnóstico - neural-simbólico Orch-OS refinado */
.diagnosticsPanel {
  background: rgba(11, 20, 38, 0.98);
  border-radius: 12px;
  box-shadow: 0 0 18px 2px #00faff22, inset 0 0 0 1px #00faff10;
  padding: 12px 16px 8px 16px;
  width: 100%;
  max-width: 340px;
  color: #eafcff;
  -webkit-backdrop-filter: blur(8px);
  backdrop-filter: blur(8px);
  border: 1px solid #00faff;
  animation: panelFadeIn 0.3s ease-out, borderGlow 4s infinite alternate;
  position: relative;
  overflow: hidden;
}

@keyframes borderGlow {
  0% { border-color: #00faff80; }
  50% { border-color: #00faff; }
  100% { border-color: #00faff80; }
}

/* Efeito de borda brilhante */
.diagnosticsPanel::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  border-radius: 16px;
  pointer-events: none;
  z-index: -1;
  box-shadow: 0 0 15px rgba(0, 250, 255, 0.5);
}

@keyframes panelFadeIn {
  from { opacity: 0; transform: translateY(-10px); }
  to { opacity: 1; transform: translateY(0); }
}

/* Título do Painel */
.panelTitle {
  font-size: 1.25rem;
  font-weight: 700;
  color: #00faff;
  margin-bottom: 0.5rem;
  padding-bottom: 0.25rem;
  border-bottom: 1px solid;
  border-image: linear-gradient(90deg, #00faff11, #00faff66, #00faff11) 1;
  text-align: center;
  font-family: 'Orbitron', 'Inter', -apple-system, sans-serif;
  letter-spacing: 0.02em;
  text-shadow: 0 0 6px #00faff44;
  position: relative;
  display: flex;
  align-items: center;
  justify-content: center;
}

.panelTitle::after {
  content: '';
  position: absolute;
  bottom: -1px;
  left: 25%;
  right: 25%;
  height: 1px;
  background: linear-gradient(90deg, transparent, #00faff, transparent);
  opacity: 0.6;
  animation: glowLine 3s infinite alternate;
}

@keyframes glowLine {
  0% { opacity: 0.3; }
  100% { opacity: 0.8; }
}

/* Tabela de Status */
.statusTable {
  width: 100%;
  border-spacing: 0 3px;
  border-collapse: separate;
  margin-bottom: 0.6rem;
  table-layout: fixed;
}

.statusRow {
  height: 24px;
  position: relative;
}

.statusRow:not(:last-child)::after {
  content: '';
  position: absolute;
  bottom: -1px;
  left: 8%;
  right: 8%;
  height: 1px;
  background: linear-gradient(90deg, transparent, #00faff22, transparent);
}

.statusCell {
  padding: 0.15rem 0.1rem;
  vertical-align: middle;
}

.statusCell:first-child {
  text-align: left;
  padding-right: 0.5rem;
  width: 45%;
}

.statusCell:last-child {
  text-align: right;
  width: 55%;
}

/* Labels */
.statusLabel {
  display: inline-block;
  white-space: nowrap;
  text-align: left;
  font-size: 0.88rem;
  color: #00faff;
  font-weight: 500;
  letter-spacing: 0.01em;
  opacity: 0.9;
  transition: color 0.2s ease;
}

.statusLabel:hover {
  color: #fff;
  text-shadow: 0 0 4px rgba(0, 250, 255, 0.5);
}

/* Valores dos Estados */
.statusValue {
  display: inline-block;
  font-size: 0.86rem;
  font-weight: 700;
  padding: 0.12rem 0.65rem;
  border-radius: 4px;
  min-width: 78px;
  text-align: center;
  text-transform: uppercase;
  letter-spacing: 0.02em;
  border-width: 1px;
  border-style: solid;
  background-color: rgba(11, 20, 38, 0.5);
  font-family: 'Orbitron', 'Inter', -apple-system, sans-serif;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.3);
  white-space: nowrap;
  transform: translateY(0);
  transition: transform 0.15s, filter 0.15s, background-color 0.15s;
}

/* Status CLOSED (amarelo neural) */
.statusOpen, .statusReady, .statusUnavailable {
  color: #ffe066;
  border-color: #ffe066;
  background-color: rgba(255, 224, 102, 0.06);
  transition: all 0.3s;
}

.statusOpen:hover, .statusReady:hover, .statusUnavailable:hover {
  background-color: rgba(255, 224, 102, 0.1);
  box-shadow: 0 0 8px rgba(255, 224, 102, 0.3);
}

/* Status AVAILABLE (ciano neural) */
.statusYes {
  color: #00faff;
  border-color: #00faff;
  background-color: rgba(0, 250, 255, 0.06);
  animation: statusPulse 2s infinite alternate;
}

@keyframes statusPulse {
  0% {
    box-shadow: 0 0 4px rgba(0, 250, 255, 0.2);
    text-shadow: 0 0 0 transparent;
  }
  100% {
    box-shadow: 0 0 6px rgba(0, 250, 255, 0.5);
    text-shadow: 0 0 3px rgba(0, 250, 255, 0.3);
  }
}

.statusYes:hover {
  background-color: rgba(0, 250, 255, 0.1);
  filter: brightness(1.1);
}

/* Status NO ✗ (vermelho neural) */
.statusNo, .statusError {
  color: #ff4455;
  border-color: #ff4455;
  background-color: rgba(255, 68, 85, 0.06);
  transition: all 0.3s;
}

.statusNo:hover, .statusError:hover {
  background-color: rgba(255, 68, 85, 0.1);
  box-shadow: 0 0 8px rgba(255, 68, 85, 0.3);
}

/* Container de Botões */
.buttonContainer {
  display: flex;
  flex-direction: row;
  gap: 0.5rem;
  margin-top: 0.8rem;
  justify-content: flex-end;
  padding: 0.1rem 0;
}

/* Botões de Ação neural-simbólico refinados */
.button {
  min-width: 124px;
  max-width: 170px;
  height: 36px;
  padding: 0 1em;
  border-radius: 10px;
  font-size: 0.92rem;
  font-weight: 700;
  cursor: pointer;
  transition: all 0.25s ease;
  text-align: center;
  border: 1px solid transparent;
  background: #101a2a;
  color: #00faff;
  box-shadow: 0 2px 8px 0 #00faff22;
  display: flex;
  align-items: center;
  justify-content: center;
  letter-spacing: 0.01em;
  position: relative;
  overflow: hidden;
  white-space: nowrap;
}

.button::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: linear-gradient(120deg, transparent 20%, rgba(255,255,255,0.1) 40%, transparent 60%);
  transform: translateX(-100%);
  transition: transform 0.5s ease;
}

.button:hover::before {
  transform: translateX(100%);
}

.button:active, .button:focus {
  filter: brightness(1.06) drop-shadow(0 0 4px #00faff);
  transform: translateY(-1px);
}
.disconnectButton {
  border: 1px solid #ff4455;
  color: #ff4455;
  background: #17172a;
}
.disconnectButton:hover {
  background: rgba(255, 68, 85, 0.15);
  color: #fff;
  border-color: #ff4455;
  box-shadow: 0 2px 12px rgba(255, 68, 85, 0.3);
}
.disconnectButton:active {
  background: rgba(255, 68, 85, 0.25);
  transform: translateY(1px);
}
.reconnectButton {
  border: 1px solid #00faff;
  color: #fff;
  background: linear-gradient(90deg, #00faff 0%, #1e90ff 100%);
  min-width: 146px; /* Específico para garantir que caiba o texto 'Force Reconnect' */
}
.reconnectButton:hover {
  filter: brightness(1.1) drop-shadow(0 0 8px #00faff60);
  background: linear-gradient(90deg, #00e1f0 0%, #007cf0 100%);
  color: #fff;
  border-color: #00faff;
}
.reconnectButton:active {
  background: linear-gradient(90deg, #00d1e0 0%, #006ce0 100%);
  transform: translateY(1px);
}

// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { ConnectionState, MicrophoneState } from '../../../context';
import styles from './DiagnosticsPanel.module.css';

// Mantendo o arquivo mais limpo sem os ícones

/**
 * DiagnosticsPanel - Interface cortical para monitoramento de estados neurais
 * Representa os estados de conexão dos componentes de input neural (Deepgram e Microfone)
 * seguindo a estética neural-simbólica do Orch-OS
 */
interface DiagnosticsPanelProps {
  connectionState: ConnectionState;
  microphoneState: MicrophoneState;
  onDisconnect?: () => void;
  onReconnect?: () => void;
}

const DiagnosticsPanel: React.FC<DiagnosticsPanelProps> = ({
  connectionState, 
  microphoneState,
  onDisconnect,
  onReconnect
}) => {
  // Determina o estado geral do sistema baseado nos estados dos componentes
  const isFullyConnected = 
    connectionState === ConnectionState.OPEN && 
    (microphoneState === MicrophoneState.Open || microphoneState === MicrophoneState.Ready);

  // Mapeia valor de enum para string legível
  const getConnectionStateText = (state: ConnectionState): string => {
    switch (state) {
      case ConnectionState.OPEN:
        return 'OPEN';
      case ConnectionState.CLOSED:
        return 'CLOSED';
      case ConnectionState.CONNECTING:
        return 'CONNECTING';
      case ConnectionState.ERROR:
        return 'ERROR';
      default:
        return 'UNKNOWN';
    }
  };

  const getMicStateText = (state: MicrophoneState): string => {
    switch (state) {
      case MicrophoneState.Open:
        return 'OPEN';
      case MicrophoneState.Ready:
        return 'READY';
      case MicrophoneState.Opening:
        return 'OPENING';
      case MicrophoneState.Error:
        return 'ERROR';
      case MicrophoneState.NotSetup:
        return 'NO';
      default:
        return 'UNKNOWN';
    }
  };

  // Mapeia estado para classe CSS neural-simbólica
  const getConnectionStateClass = (state: ConnectionState): string => {
    switch (state) {
      case ConnectionState.OPEN:
        return styles.statusYes;
      case ConnectionState.CLOSED:
        return styles.statusUnavailable; // Fechado deve usar o amarelo (warning)
      case ConnectionState.CONNECTING:
        return styles.statusUnavailable;
      case ConnectionState.ERROR:
        return styles.statusError;
      default:
        return styles.statusUnavailable;
    }
  };

  const getMicStateClass = (state: MicrophoneState): string => {
    switch (state) {
      case MicrophoneState.Open:
      case MicrophoneState.Ready:
        return styles.statusYes;
      case MicrophoneState.Opening:
        return styles.statusUnavailable;
      case MicrophoneState.Error:
        return styles.statusError;
      case MicrophoneState.NotSetup:
        return styles.statusNo;
      default:
        return styles.statusUnavailable;
    }
  };

  // Manipuladores de eventos para ações neurais
  const handleDisconnect = () => {
    if (onDisconnect) onDisconnect();
  };

  const handleReconnect = () => {
    if (onReconnect) onReconnect();
  };

  return (
    <div className={styles.diagnosticsPanel}>
      <h3 className={styles.panelTitle}>Connection Diagnostics</h3>
      
      <table className={styles.statusTable}>
        <tbody>
          <tr className={styles.statusRow}>
            <td className={styles.statusCell}>
              <span className={styles.statusLabel}>Conn. state:</span>
            </td>
            <td className={styles.statusCell}>
              <span className={`${styles.statusValue} ${getConnectionStateClass(connectionState)}`}>
                {getConnectionStateText(connectionState)}
              </span>
            </td>
          </tr>
          <tr className={styles.statusRow}>
            <td className={styles.statusCell}>
              <span className={styles.statusLabel}>Self state:</span>
            </td>
            <td className={styles.statusCell}>
              <span className={`${styles.statusValue} ${getConnectionStateClass(connectionState)}`}>
                {getConnectionStateText(connectionState)}
              </span>
            </td>
          </tr>
          <tr className={styles.statusRow}>
            <td className={styles.statusCell}>
              <span className={styles.statusLabel}>Conn. obj.:</span>
            </td>
            <td className={styles.statusCell}>
              <span className={`${styles.statusValue} ${styles.statusYes}`}>
                {connectionState !== ConnectionState.ERROR ? 'AVAILABLE' : 'ERROR'}
              </span>
            </td>
          </tr>
          <tr className={styles.statusRow}>
            <td className={styles.statusCell}>
              <span className={styles.statusLabel}>Mic state:</span>
            </td>
            <td className={styles.statusCell}>
              <span className={`${styles.statusValue} ${getMicStateClass(microphoneState)}`}>
                {getMicStateText(microphoneState)}
              </span>
            </td>
          </tr>
          <tr className={styles.statusRow}>
            <td className={styles.statusCell}>
              <span className={styles.statusLabel}>Conn. active:</span>
            </td>
            <td className={styles.statusCell}>
              <span className={`${styles.statusValue} ${isFullyConnected ? styles.statusYes : styles.statusNo}`}>
                {isFullyConnected ? 'YES' : 'NO ✗'}
              </span>
            </td>
          </tr>
        </tbody>
      </table>
      
      <div className={styles.buttonContainer}>
        <button 
          className={`${styles.button} ${styles.disconnectButton}`}
          onClick={handleDisconnect}
        >
          Disconnect
        </button>
        <button 
          className={`${styles.button} ${styles.reconnectButton}`}
          onClick={handleReconnect}
        >
          Force Reconnect
        </button>
      </div>
    </div>
  );
};

export default DiagnosticsPanel;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { ImportModalProps } from "../types/interfaces";

const ImportModal: React.FC<ImportModalProps> = ({
  show,
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  onClose,
  importFile,
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  setImportFile, 
  importUserName,
  setImportUserName,
  importMode,
  setImportMode,
  importProgress,
  importStage,
  importSummary,
  isImporting,
  handleFileChange,
  handleStartImport,
  handleCloseImportModal
}) => {
  if (!show) return null;
  return (
    <div className="fixed inset-0 z-50 flex items-center justify-center bg-black/60">
      <div className="bg-gray-900/90 rounded-2xl shadow-2xl p-8 w-full max-w-md relative backdrop-blur-lg ring-2 ring-cyan-400/10">
        <button
          className="orchos-btn-circle absolute top-2 right-2"
          onClick={handleCloseImportModal}
          title="Fechar"
        >
          <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
            <path d="M4.646 4.646a.5.5 0 0 1 .708 0L8 7.293l2.646-2.647a.5.5 0 0 1 .708.708L8.707 8l2.647 2.646a.5.5 0 0 1-.708.708L8 8.707l-2.646 2.647a.5.5 0 0 1-.708-.708L7.293 8 4.646 5.354a.5.5 0 0 1 0-.708z" />
          </svg>
        </button>
        <h2 className="text-2xl font-bold mb-6 text-center tracking-wide bg-gradient-to-r from-cyan-400 via-blue-500 to-purple-500 bg-clip-text text-transparent drop-shadow-[0_0_12px_rgba(0,240,255,0.5)]" style={{fontFamily:'Orbitron, Inter, sans-serif'}}>Import Neural Data</h2>
        <input
          data-testid="import-user-name"
          type="text"
          className="mb-4 w-full text-white bg-black/30 border-2 border-cyan-400/40 rounded-full p-3 focus:outline-none focus:ring-2 focus:ring-cyan-400/60 placeholder:text-cyan-200/60 text-lg shadow-inner backdrop-blur"
          placeholder="Main user name"
          disabled={isImporting}
          value={importUserName || ''}
          onChange={e => setImportUserName(e.target.value)}
        />
        <label className="block mb-6">
          <span className="block text-cyan-200/80 mb-2 font-medium">Select file</span>
          <div className="flex items-center gap-3">
            <input
              data-testid="import-user-input"
              type="file"
              accept="application/json"
              onChange={handleFileChange}
              className="hidden"
              id="orchos-upload-neural"
              disabled={isImporting}
            />
            <label htmlFor="orchos-upload-neural" className="flex items-center gap-2 px-4 py-2 rounded-full bg-gradient-to-r from-cyan-500 via-blue-700 to-purple-600 text-white font-semibold shadow-lg cursor-pointer hover:scale-105 transition-all duration-150">
              <svg width="18" height="18" viewBox="0 0 20 20" fill="none"><circle cx="10" cy="10" r="8" stroke="#00F0FF" strokeWidth="1.5" /><path d="M10 6v5m0 0l2.5-2.5M10 11l-2.5-2.5" stroke="#8F00FF" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round"/></svg>
              {importFile ? importFile.name : 'Choose file'}
            </label>
          </div>
        </label>
        <div className="mb-6 px-4 py-3 bg-black/30 rounded-xl flex flex-col items-center gap-2 ring-1 ring-cyan-400/10">
          <span className="text-cyan-200/90 font-medium mb-1">Import Mode:</span>
          <div className="flex gap-4">
            <label className="flex items-center gap-2 cursor-pointer text-white/90 hover:text-cyan-300 transition-all">
              <input
                type="radio"
                name="importMode"
                value="overwrite"
                checked={importMode === 'overwrite'}
                onChange={() => setImportMode('overwrite')}
                disabled={isImporting}
                className="accent-cyan-400 w-4 h-4"
              />
              <span>Overwrite all</span>
            </label>
            <label className="flex items-center gap-2 cursor-pointer text-white/90 hover:text-cyan-300 transition-all">
              <input
                type="radio"
                name="importMode"
                value="increment"
                checked={importMode === 'increment'}
                onChange={() => setImportMode('increment')}
                disabled={isImporting}
                className="accent-purple-400 w-4 h-4"
              />
              <span>Increment <span className="text-xs text-cyan-200/60">(add only new)</span></span>
            </label>
          </div>
        </div>
        <button
          type="button"
          className="flex items-center gap-2 justify-center w-full py-3 mt-2 rounded-full font-bold text-lg bg-gradient-to-r from-cyan-400 via-blue-700 to-purple-600 shadow-[0_0_18px_2px_rgba(0,240,255,0.18)] hover:shadow-cyan-400/70 hover:scale-105 transition-all duration-200 ring-2 ring-cyan-400/20 backdrop-blur text-white focus:outline-none focus:ring-4 focus:ring-cyan-400/60 disabled:opacity-60 disabled:cursor-not-allowed mb-4"
          onClick={() => {
            handleStartImport(importUserName);
          }}
          disabled={!importFile || isImporting}
        >
          <svg width="18" height="18" viewBox="0 0 20 20" fill="none" aria-hidden="true"><circle cx="10" cy="10" r="8" stroke="#00F0FF" strokeWidth="1.5" /><path d="M10 6v5m0 0l2.5-2.5M10 11l-2.5-2.5" stroke="#8F00FF" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round"/></svg>
          {isImporting ? 'Importing...' : 'Start Import'}
        </button>
        {isImporting && (
          <div className="w-full flex flex-col items-center mb-6">
            <div className="relative w-full h-9 rounded-full bg-gradient-to-r from-cyan-900/40 via-blue-900/30 to-purple-900/40 shadow-inner overflow-hidden mt-2 mb-3 ring-1 ring-cyan-400/10">
              <div
                className="absolute top-0 left-0 h-full bg-gradient-to-r from-cyan-400 via-blue-700 to-purple-600 shadow-[0_0_18px_2px_rgba(0,240,255,0.18)] transition-all duration-300"
                style={{ width: `${importProgress}%`, minWidth: importProgress > 0 ? '2.5rem' : 0, borderRadius: '9999px' }}
              >
              </div>
              <div className="absolute top-0 left-0 h-full w-full pointer-events-none"></div>
            </div>
            <span className="text-cyan-300 text-sm font-semibold mt-3 drop-shadow">{importProgress}%</span>
          </div>
        )}
        {isImporting && (
          <div className="mb-2 flex items-center justify-center text-base">
            <span className="mr-2 text-white/70 font-medium">Current stage:</span>
            <span className="font-bold bg-gradient-to-r from-cyan-400 via-blue-500 to-purple-500 bg-clip-text text-transparent drop-shadow-[0_0_8px_rgba(0,240,255,0.3)] animate-pulse">
              {(() => {
                switch (importStage) {
                  case 'parsing': return 'Reading messages';
                  case 'deduplicating': return 'Checking duplicates';
                  case 'generating_embeddings': return 'Generating embeddings';
                  case 'saving': return 'Saving to database';
                  default: return importStage ? importStage : 'Preparing...';
                }
              })()}
            </span>
          </div>
        )}
        {importSummary && (
          <div className="text-green-400 text-sm mt-2" data-testid="import-summary">{importSummary}</div>
        )}

        {importSummary && (
          <div className="text-white/80 text-xs mt-2" data-testid="import-result-details">
            {/* Detailed success case with stats */}
            {importSummary.includes('Imported:') && importSummary.includes('Skipped:') && (
              <>
                Total messages processed: {(() => {
                  const matches = importSummary.match(/Imported: (\d+), Skipped: (\d+)/);
                  if (matches) {
                    return Number(matches[1]) + Number(matches[2]);
                  }
                  return 0;
                })()}<br />
                {importSummary.replace('Import complete! ', '')}
              </>
            )}
            
            {/* Simple success case without stats */}
            {importSummary === 'Import complete!' && (
              <>Import completed successfully with no details available.</>
            )}
            
            {/* Error case */}
            {importSummary.includes('Error:') && (
              <span className="text-red-400">{importSummary}</span>
            )}
          </div>
        )}
      </div>
    </div>
  );
};

export default ImportModal;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useEffect } from "react";
import {
  setOption,
  STORAGE_KEYS,
  subscribeToStorageChanges,
} from "../../../../services/StorageService";

interface LanguageSelectorProps {
  language: string;
  setLanguage: (language: string) => void;
}

const LanguageSelector: React.FC<LanguageSelectorProps> = ({
  language,
  setLanguage,
}) => {
  // Sincroniza com as configurações globais
  useEffect(() => {
    // Handler para mudanças no storage
    const handleStorageChange = (key: string, value: any) => {
      if (
        key === STORAGE_KEYS.DEEPGRAM_LANGUAGE &&
        value &&
        value !== language
      ) {
        console.log(
          "🌐 LanguageSelector: Sincronizando com configurações globais:",
          value
        );
        setLanguage(value);
      }
    };

    // Inscrição no sistema de eventos
    const unsubscribe = subscribeToStorageChanges(handleStorageChange);

    // Limpeza ao desmontar
    return unsubscribe;
  }, [language, setLanguage]);

  return (
    <div className="language-selector-wrapper">
      <label className="text-sm text-cyan-200/70 block mb-2">
        Transcription Language
      </label>
      <select
        title="Transcription Language"
        className="w-full p-2.5 rounded-lg bg-black/40 text-white/90 text-sm border border-cyan-500/20 focus:border-cyan-500/50 focus:outline-none focus:ring-1 focus:ring-cyan-500/30 transition-all"
        value={language}
        onChange={(e) => {
          const newLanguage = e.target.value;
          console.log("Language selector changed to:", newLanguage);

          // Atualiza o estado local
          setLanguage(newLanguage);

          // Salva automaticamente no storage do sistema
          setOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE, newLanguage);
          console.log(
            "💾 Idioma salvo automaticamente no sistema:",
            newLanguage
          );
        }}
      >
        <option value="pt-BR">Portuguese (Brazil)</option>
        <option value="pt-PT">Portuguese (Portugal)</option>
        <option value="en-US">English (United States)</option>
        <option value="en">English (Global)</option>
        <option value="es">Spanish</option>
        <option value="fr">French</option>
        <option value="de">German</option>
        <option value="it">Italian</option>
        <option value="ja">Japanese</option>
        <option value="zh">Chinese</option>
      </select>
    </div>
  );
};

export default LanguageSelector;
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

.quantum-spin {
  animation: quantumSpin 4.5s linear infinite;
}

@keyframes quantumSpin {
  100% {
    transform: rotate(360deg);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { ConnectionState, MicrophoneState } from '../../../context';
import styles from './PanelHeader.module.css';
import QuantumSettingsIcon from './QuantumSettingsIcon';
import WifiStatusConnection from './WifiStatusConnection';

/**
 * Interface cortical para o cabeçalho do painel de transcrição
 * Representa os controles principais de interface com usuário
 * seguindo a estética neural-simbólica do Orch-OS
 */
interface PanelHeaderProps {
  onClose: () => void;
  onToggleDiagnostics: () => void;
  onShowImportModal: () => void;
  onShowLogsModal: () => void; // Abre o modal de logs de cognição
  onShowSettings?: () => void; // Abre as configurações gerais do sistema
  onShowDebugModal?: () => void; // Abre o modal de debug DuckDB
  onMinimize?: () => void;
  connectionState: ConnectionState;
  microphoneState: MicrophoneState;
  hasActiveConnection?: () => boolean;
  onDisconnect?: () => void;
  onReconnect?: () => void;
}

const PanelHeader: React.FC<PanelHeaderProps> = ({
  onClose,
  onToggleDiagnostics,
  onShowImportModal,
  onShowLogsModal,
  onShowSettings,
  onShowDebugModal,
  onMinimize,
  connectionState,
  microphoneState,
  hasActiveConnection = () => false,
  onDisconnect,
  onReconnect
}) => {
  return (
    <div className="orchos-header-glass flex justify-between items-center mb-4 h-14 px-4">
      {/* Logo e Título */}
      <h3 className="orchos-title font-bold text-xl tracking-wider flex items-center ml-2 orchos-header-gradient-text">
        <span className="mr-2 text-base">
          <svg width="20" height="20" viewBox="0 0 20 20" fill="none">
            <circle cx="10" cy="10" r="8" stroke="#00F0FF" strokeWidth="1.5" />
            <circle cx="10" cy="10" r="3" fill="#8F00FF" />
          </svg>
        </span>
        Orch-OS
      </h3>
      
      {/* Espaço flex para empurrar os elementos para a direita */}
      <div className="flex-1"></div>
      
      {/* Área de todos os controles agrupados */}
      <div className="flex items-center gap-4 mr-2">
        {/* Botão Import Neural Data */}
        <button
          title="Import Neural Data"
          onClick={onShowImportModal}
          className="flex items-center gap-2 px-4 py-1.5 rounded-full font-bold text-base bg-gradient-to-r from-cyan-400 via-blue-700 to-purple-600 shadow-[0_0_24px_4px_rgba(0,240,255,0.25)] hover:shadow-cyan-400/80 hover:scale-105 transition-all duration-200 ring-2 ring-cyan-400/30 backdrop-blur text-white focus:outline-none focus:ring-4 focus:ring-cyan-400/60 h-10"
        >
          <svg width="18" height="18" viewBox="0 0 20 20" fill="none" aria-hidden="true">
            <circle cx="10" cy="10" r="8" stroke="#00F0FF" strokeWidth="1.5" />
            <path d="M10 6v5m0 0l2.5-2.5M10 11l-2.5-2.5" stroke="#8F00FF" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round"/>
          </svg>
          Import Neural Data
        </button>
        
        {/* Botão Logs */}
        <button
          title="Cognition Logs"
          aria-label="Open Cognition Logs"
          onClick={onShowLogsModal}
          className="flex items-center gap-2 px-4 py-1.5 rounded-full font-bold text-base bg-gradient-to-r from-cyan-400 via-blue-700 to-purple-600 shadow-[0_0_24px_4px_rgba(0,240,255,0.25)] hover:shadow-cyan-400/80 hover:scale-105 transition-all duration-200 ring-2 ring-cyan-400/30 backdrop-blur text-white focus:outline-none focus:ring-4 focus:ring-cyan-400/60 h-10"
        >
          <svg width="18" height="18" viewBox="0 0 20 20" fill="none" aria-hidden="true" style={{ display: 'inline', marginRight: 2, verticalAlign: 'middle', filter: 'drop-shadow(0 0 4px #00faff88)' }}>
            <ellipse cx="10" cy="10" rx="8" ry="6" stroke="#00F0FF" strokeWidth="2"/>
            <circle cx="10" cy="10" r="3" fill="#8F00FF"/>
          </svg>
          Logs
        </button>
        
        {/* Botão Debug DuckDB - apenas em desenvolvimento */}
        {onShowDebugModal && process.env.NODE_ENV === 'development' && (
          <button
            title="DuckDB Debug (Dev Only)"
            aria-label="Open DuckDB Debug"
            onClick={onShowDebugModal}
            className="flex items-center gap-2 px-4 py-1.5 rounded-full font-bold text-base bg-gradient-to-r from-orange-400 via-red-500 to-purple-600 shadow-[0_0_24px_4px_rgba(255,165,0,0.25)] hover:shadow-orange-400/80 hover:scale-105 transition-all duration-200 ring-2 ring-orange-400/30 backdrop-blur text-white focus:outline-none focus:ring-4 focus:ring-orange-400/60 h-10"
          >
            <svg width="18" height="18" viewBox="0 0 20 20" fill="none" aria-hidden="true">
              <rect x="3" y="4" width="14" height="12" rx="2" stroke="#FFA500" strokeWidth="1.5" />
              <path d="M7 8h6M7 11h6M7 14h4" stroke="#FF4500" strokeWidth="1.5" strokeLinecap="round"/>
            </svg>
            Debug
          </button>
        )}
        
        {/* Botão de Configurações */}
        {onShowSettings && (
          <button
            title="Settings"
            onClick={onShowSettings}
            className={`flex items-center justify-center w-10 h-10 rounded-full bg-black/40 border border-cyan-500/40 hover:border-cyan-400/70 transition-all duration-200 ${styles['quantum-spin']}`}
          >
            <QuantumSettingsIcon size={26} />
          </button>
        )}
        
        {/* Indicador WiFi */}
        <WifiStatusConnection 
          connectionState={connectionState}
          microphoneState={microphoneState}
          signalStrength={hasActiveConnection() ? 'strong' : 'none'}
          onStatusClick={onToggleDiagnostics}
          showDetailedText={false}
          onDisconnect={onDisconnect}
          onReconnect={onReconnect}
        />
        
        {/* Área dos botões de controle de janela com flexbox para alinhamento perfeito */}
        <div className="flex items-center gap-2 ml-4">
          {onMinimize && (
            <button
              title="Minimize"
              onClick={onMinimize}
              className="flex items-center justify-center w-10 h-10 rounded-full bg-black/40 border border-cyan-500/40 hover:border-cyan-400/70 transition-all duration-200"
            >
              <svg width="18" height="18" viewBox="0 0 22 22" fill="none">
                <circle cx="11" cy="11" r="8" stroke="#00faff" strokeWidth="1.5" strokeOpacity="0.8" />
                <rect x="6" y="10.5" width="10" height="1.5" rx="0.75" fill="#00faff" fillOpacity="0.9" />
              </svg>
            </button>
          )}
          <button
            title="Close"
            onClick={onClose}
            className="flex items-center justify-center w-10 h-10 rounded-full bg-black/40 border border-red-500/40 hover:border-red-400/70 transition-all duration-200"
          >
            <svg width="18" height="18" viewBox="0 0 22 22" fill="none">
              <circle cx="11" cy="11" r="8" stroke="#ff4455" strokeWidth="1.5" strokeOpacity="0.8" />
              <path d="M7.5 7.5L14.5 14.5M14.5 7.5L7.5 14.5" stroke="#ff4455" strokeWidth="1.5" strokeLinecap="round" strokeOpacity="0.9" />
            </svg>
          </button>
        </div>
      </div>
    </div>
  );
};

export default PanelHeader;
// SPDX-License-Identifier: MIT OR Apache-2.0
// QuantumSettingsIcon.tsx - Epic quantum/futuristic settings icon for Orch-OS
// Inspired by Icons8 Futuristic Neon Gear, adapted for React

import React from "react";

interface QuantumSettingsIconProps {
  size?: number;
  className?: string;
}

const QuantumSettingsIcon: React.FC<QuantumSettingsIconProps> = ({ size = 28, className = "" }) => (
  <svg
    width={size}
    height={size}
    viewBox="0 0 64 64"
    fill="none"
    className={className}
    xmlns="http://www.w3.org/2000/svg"
    style={{
      filter: 'drop-shadow(0 0 6px #0ff) drop-shadow(0 0 18px #0ff)',
      background: 'transparent',
    }}
  >
    <circle cx="32" cy="32" r="30" stroke="#0ff" strokeWidth="2.5" fill="rgba(10,20,30,0.5)" />
    <g>
      <path
        d="M32 22a10 10 0 1 1 0 20 10 10 0 0 1 0-20zm0-8v4m0 28v4m12-12h4m-32 0h4m19.8-13.8l2.8-2.8m-23.6 23.6l2.8-2.8m0-18l-2.8-2.8m23.6 23.6-2.8-2.8"
        stroke="#0ff"
        strokeWidth="2.5"
        strokeLinecap="round"
        strokeLinejoin="round"
        style={{ filter: 'drop-shadow(0 0 4px #0ff)' }}
      />
    </g>
    <circle cx="32" cy="32" r="6.5" fill="#0ff" fillOpacity="0.18" />
    <circle cx="32" cy="32" r="4" fill="#0ff" fillOpacity="0.5" />
  </svg>
);

export default QuantumSettingsIcon;
/* SPDX-License-Identifier: MIT OR Apache-2.0
   Copyright (c) 2025 Guilherme Ferrari Brescia */

/* Base container - Neural activation container */

/* Neural activation button - Recording control */

/* Active recording state */

/* Inactive recording state */
.recordButtonInactive {
  border: 3px solid #00faff;
  box-shadow: 0 0 32px 8px rgba(0, 250, 255, 0.2);
}

/* Recording status indicator */

/* Pulse animation for active recording */
@keyframes pulse {
  0% {
    opacity: 1;
  }
  50% {
    opacity: 0.7;
  }
  100% {
    opacity: 1;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { MicrophoneState } from '../../../context';
import styles from './RecordingControl.module.css';

/**
 * Neural activation interface for audio recording control
 */
interface RecordingControlProps {
  microphoneState: MicrophoneState;
  toggleRecording: () => Promise<void>;
}

/**
 * Recording control component - Neural activation interface
 * Symbolic intent: Neural impulse control for audio signal capture
 */
const RecordingControl: React.FC<RecordingControlProps> = ({
  microphoneState,
  toggleRecording
}) => {
  return (
    <div className={styles.recordingContainer}>
      <button
        className={`orchos-btn-circular orchos-btn-fab orchos-btn-glass orchos-btn-ripple orchos-btn-rel flex items-center justify-center shadow-lg transition-all duration-200 ${styles.recordButton} ${
          microphoneState === MicrophoneState.Open 
            ? `${styles.recordButtonActive} animate-pulse` 
            : styles.recordButtonInactive
        }`}
        style={{
          border: microphoneState === MicrophoneState.Open ? '3px solid #ff4455' : '3px solid #00faff',
          boxShadow: microphoneState === MicrophoneState.Open ? '0 0 32px 8px rgba(255, 68, 85, 0.33)' : '0 0 32px 8px rgba(0, 250, 255, 0.2)'
        }}
        onClick={e => {
          // Create ripple effect - neural activation visualization
          const btn = e.currentTarget;
          const ripple = document.createElement('span');
          ripple.className = 'ripple orchos-ripple-electric';
          ripple.style.left = `${e.nativeEvent.offsetX}px`;
          ripple.style.top = `${e.nativeEvent.offsetY}px`;
          btn.appendChild(ripple);
          setTimeout(() => ripple.remove(), 600);
          toggleRecording();
        }}
        title={microphoneState === MicrophoneState.Open ? 'Stop Recording' : 'Start Recording'}
        aria-label={microphoneState === MicrophoneState.Open ? 'Stop Recording' : 'Start Recording'}
      >
        {microphoneState === MicrophoneState.Open ? (
          // Ícone de STOP neural (quadrado com pulso)
          <svg width="38" height="38" viewBox="0 0 38 38" fill="none" className="orchos-icon-glow">
            <ellipse cx="19" cy="19" rx="16" ry="12" stroke="#ff4455" strokeWidth="2.5"/>
            <rect x="13" y="13" width="12" height="12" rx="3" fill="#ff4455"/>
            <path d="M19 7 L19 3" stroke="#ff4455" strokeWidth="2" strokeLinecap="round"/>
            <circle cx="19" cy="3" r="1.3" fill="#ff4455"/>
            <path d="M17 19 Q19 23 21 19" stroke="#ff4455" strokeWidth="1.5" fill="none"/>
          </svg>
        ) : (
          // Ícone de microfone neural (START)
          <svg width="38" height="38" viewBox="0 0 38 38" fill="none" className="orchos-icon-glow">
            <ellipse cx="19" cy="19" rx="16" ry="12" stroke="#00faff" strokeWidth="2.5"/>
            <rect x="16" y="11" width="6" height="14" rx="3" fill="#00faff"/>
            <rect x="17.5" y="25" width="3" height="3" rx="1.5" fill="#00faff"/>
            <path d="M19 7 L19 3" stroke="#00faff" strokeWidth="2" strokeLinecap="round"/>
            <circle cx="19" cy="3" r="1.3" fill="#00faff"/>
            <path d="M17 19 Q19 23 21 19" stroke="#00faff" strokeWidth="1.5" fill="none"/>
          </svg>
        )}
      </button>
      {/* Neural state indicator - active recording status */}
      {microphoneState === MicrophoneState.Open && (
        <div className={styles.recordingLabel}>
          <svg width="18" height="18" viewBox="0 0 18 18" fill="none" className="inline mr-1 align-text-bottom">
            <ellipse cx="9" cy="9" rx="7" ry="5" stroke="#ff4455" strokeWidth="1.5"/>
            <circle cx="9" cy="9" r="2" fill="#ff4455"/>
          </svg>
          Recording...
        </div>
      )}
    </div>
  );
};

export default RecordingControl;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { AudioSettings, GeneralSettings, InterfaceSettings } from "./settings";
import ApiSettings from "./settings/ApiSettings";
import SettingsFooter from "./settings/SettingsFooter";
import SettingsHeader from "./settings/SettingsHeader";
import SettingsNavigation from "./settings/SettingsNavigation";
import { SettingsModalProps } from "./settings/types";
import { useSettingsState } from "./settings/useSettingsState";

/**
 * Modal de configurações do Orch-OS
 * Refatorado seguindo os princípios neural-simbólicos:
 * - Clean Architecture (separação de responsabilidades)
 * - SOLID (componentes com responsabilidade única)
 * - KISS (simplificação da lógica)
 */
const SettingsModal: React.FC<SettingsModalProps> = ({ show, onClose }) => {
  // Usando o hook customizado para gerenciar todo o estado
  const settings = useSettingsState(show);

  // Se não for exibido, não renderizar nada
  if (!show) return null;

  return (
    <div className="fixed inset-0 z-50 flex items-center justify-center bg-black/60">
      <div className="bg-gray-900/90 rounded-2xl shadow-2xl p-6 w-full max-w-2xl relative backdrop-blur-lg ring-2 ring-cyan-400/10">
        {/* Cabeçalho */}
        <SettingsHeader onClose={onClose} />

        {/* Navegação */}
        <SettingsNavigation
          activeTab={settings.activeTab}
          setActiveTab={settings.setActiveTab}
        />

        {/* Conteúdo das abas */}
        <div className="mb-4">
          {/* General Tab */}
          {settings.activeTab === "general" && (
            <GeneralSettings
              name={settings.name}
              setName={settings.setName}
              applicationMode={settings.applicationMode}
              setApplicationMode={settings.setApplicationMode}
              enableMatrix={settings.enableMatrix}
              setEnableMatrix={settings.setEnableMatrix}
              matrixDensity={settings.matrixDensity}
              setMatrixDensity={settings.setMatrixDensity}
              enableEffects={settings.enableEffects}
              setEnableEffects={settings.setEnableEffects}
              enableAnimations={settings.enableAnimations}
              setEnableAnimations={settings.setEnableAnimations}
            />
          )}

          {/* Interface Tab */}
          {settings.activeTab === "interface" && (
            <InterfaceSettings
              darkMode={settings.darkMode}
              setDarkMode={settings.setDarkMode}
              enableNeumorphism={settings.enableNeumorphism}
              setEnableNeumorphism={settings.setEnableNeumorphism}
              enableGlassmorphism={settings.enableGlassmorphism}
              setEnableGlassmorphism={settings.setEnableGlassmorphism}
              panelTransparency={settings.panelTransparency}
              setPanelTransparency={settings.setPanelTransparency}
              colorTheme={settings.colorTheme}
              setColorTheme={settings.setColorTheme}
            />
          )}

          {/* Audio Tab */}
          {settings.activeTab === "audio" && (
            <AudioSettings
              enhancedPunctuation={settings.enhancedPunctuation}
              setEnhancedPunctuation={settings.setEnhancedPunctuation}
              speakerDiarization={settings.speakerDiarization}
              setSpeakerDiarization={settings.setSpeakerDiarization}
              audioQuality={settings.audioQuality}
              setAudioQuality={settings.setAudioQuality}
              autoGainControl={settings.autoGainControl}
              setAutoGainControl={settings.setAutoGainControl}
              noiseSuppression={settings.noiseSuppression}
              setNoiseSuppression={settings.setNoiseSuppression}
              echoCancellation={settings.echoCancellation}
              setEchoCancellation={settings.setEchoCancellation}
            />
          )}

          {/* Advanced Tab */}
          {settings.activeTab === "advanced" && (
            <ApiSettings
              applicationMode={settings.applicationMode}
              setApplicationMode={settings.setApplicationMode}
              ollamaModel={settings.ollamaModel}
              setOllamaModel={settings.setOllamaModel}
              ollamaEmbeddingModel={settings.ollamaEmbeddingModel}
              setOllamaEmbeddingModel={settings.setOllamaEmbeddingModel}
              ollamaEnabled={settings.ollamaEnabled}
              setOllamaEnabled={settings.setOllamaEnabled}
            />
          )}
        </div>

        {/* Footer com botões de ação */}
        <SettingsFooter
          onClose={onClose}
          saveSettings={settings.saveSettings}
        />
      </div>
    </div>
  );
};

export default SettingsModal;
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

.orchos-btn-glass {
  background: rgba(24, 24, 40, 0.55);
  border: 2px solid #00faff;
  border-radius: 999px !important;
  box-shadow: 0 0 12px 2px #00faff44, 0 0 0 1.5px #00faff22;
  color: #00faff;
  transition: box-shadow 0.18s, background 0.18s, border-color 0.18s;
}
.orchos-btn-glass:focus,
.orchos-btn-glass:hover {
  background: rgba(0, 245, 255, 0.12);
  box-shadow: 0 0 16px 4px #00faff77, 0 0 0 2.5px #00faff55;
  border-color: #00faff;
}

.orchos-btn-glow {
  box-shadow: 0 0 8px 2px #00faff77;
}

.orchos-btn-action {
  font-weight: 500;
  font-size: 0.9rem;
  border-radius: 999px !important;
  padding-left: 0.9em;
  padding-right: 0.9em;
  min-height: 32px;
  min-width: 32px;
  display: flex;
  align-items: center;
  justify-content: center;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import styles from './TextControls.module.css';

interface TextControlsProps {
  label: string;
  onClear: () => void;
  onExpand?: () => void;
  isExpanded?: boolean;
}

const TextControls: React.FC<TextControlsProps> = ({ label, onClear, onExpand, isExpanded }) => (
  <div className="flex justify-between items-center py-1 mb-2 flex-wrap">
    <h4 className="text-sm font-medium">{label}</h4>
    <div className="flex gap-2 flex-wrap">

      {onExpand && (
        <button
          className={`${styles['orchos-btn-glass']} ${styles['orchos-btn-glow']} ${styles['orchos-btn-action']} px-2.5 py-1.5 transition-all duration-150 flex items-center justify-center group`}
          onClick={(e) => {
            e.preventDefault();
            e.stopPropagation();
            onExpand();
          }}
          title={isExpanded ? "Collapse" : "Expand"}
          aria-label={isExpanded ? "Collapse" : "Expand"}
        >
          <svg width="17" height="17" viewBox="0 0 17 17" fill="none"><ellipse cx="8.5" cy="8.5" rx="7.5" ry="5.5" stroke="#00faff" strokeWidth="1.3"/><path d="M5 6l3.5 3.5L12 6" stroke="#00faff" strokeWidth="1.2" fill="none"/></svg>
          <span className="hidden md:inline ml-1 align-middle">{isExpanded ? "Collapse" : "Expand"}</span>
        </button>
      )}
      <button
        className={`${styles['orchos-btn-glass']} ${styles['orchos-btn-glow']} ${styles['orchos-btn-action']} px-2.5 py-1.5 transition-all duration-150 flex items-center justify-center group`}
        onClick={onClear}
        title="Clear text"
        aria-label="Clear text"
      >
        <svg width="16" height="16" viewBox="0 0 16 16" fill="none"><ellipse cx="8" cy="8" rx="6.5" ry="4.5" stroke="#ff4dd2" strokeWidth="1.2"/><path d="M6 6l4 4M10 6l-4 4" stroke="#ff4dd2" strokeWidth="1.3" strokeLinecap="round"/></svg>
        <span className="hidden md:inline ml-1 align-middle">Clear</span>
      </button>
    </div>
  </div>
);

export default TextControls;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import TextControls from './TextControls';


interface TextEditorProps {
  label: string;
  value: string;
  onChange: (value: string) => void;
  onClear: () => void;
  rows?: number;
  placeholder?: string;
  isExpanded?: boolean;
  toggleExpand?: () => void;
  forwardedRef?: React.RefObject<HTMLTextAreaElement>;
  useAutosize?: boolean;
  readOnly?: boolean;
}

const TextEditor: React.FC<TextEditorProps> = ({
  label,
  value,
  onChange,
  onClear,
  rows = 5,
  placeholder = '',
  isExpanded,
  toggleExpand,
  forwardedRef,
  useAutosize = true,
  readOnly = false
}) => {

  // Classes comuns para todos os textareas, adaptando-se ao contexto neural
  const commonClasses = `w-full p-4 rounded-xl bg-black/40 text-white leading-relaxed font-medium shadow-inner neural-adaptive-container`;

  return (
    <div className="flex-1 flex flex-col min-h-0 neural-text-container">
      <TextControls
        label={label}
        onClear={onClear}
        onExpand={toggleExpand}
      />
      
      {useAutosize ? (
        <textarea
          className={`${commonClasses} orchos-textarea-neural resize-none flex-1 h-full min-h-0 ${isExpanded ? 'max-h-96' : ''} overflow-y-auto`}
          value={value}
          onChange={(e) => { if (!readOnly) onChange(e.target.value); }}
          readOnly={readOnly}
          rows={isExpanded ? 10 : rows}
          ref={forwardedRef}
          placeholder={placeholder}
          title={readOnly ? "Transcription text (read-only)" : label}
          aria-label={readOnly ? "Transcription text (read-only)" : label}
          style={{ height: '100%', minHeight: '60px' }}
        />
      ) : (
        <textarea
          className={`${commonClasses} orchos-textarea-neural resize-none flex-1 h-full min-h-0`}
          value={value}
          onChange={(e) => { if (!readOnly) onChange(e.target.value); }}
          readOnly={readOnly}
          rows={rows}
          ref={forwardedRef}
          placeholder={placeholder}
          title={readOnly ? "Transcription text (read-only)" : label}
          aria-label={readOnly ? "Transcription text (read-only)" : label}
          style={{ height: '100%', minHeight: '60px' }}
        />
      )}
    </div>
  );
};

export default TextEditor;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";

interface ToggleSwitchProps {
  label: string;
  isOn: boolean;
  onChange: () => void;
  title: string;
}

const ToggleSwitch: React.FC<ToggleSwitchProps> = ({
  label,
  isOn,
  onChange,
  title,
}) => (
  <div className="flex items-center justify-between py-3">
    <span className="text-sm text-gray-300 font-medium">{label}</span>
    <button
      type="button"
      title={title}
      onClick={onChange}
      className="relative h-7 w-12 rounded-full transition-all duration-200 ease-in-out focus:outline-none focus:ring-2 focus:ring-cyan-400 focus:ring-offset-2 focus:ring-offset-gray-900"
      style={{
        backgroundColor: isOn ? "#00D9FF" : "#374151",
        boxShadow: isOn
          ? "inset 0 0 0 2px rgba(255, 255, 255, 0.5), inset 0 0 0 1px rgba(0, 217, 255, 1), 0 0 12px rgba(0, 217, 255, 0.5), 0 0 4px rgba(0, 217, 255, 0.8)"
          : "inset 0 0 0 2px rgba(255, 255, 255, 0.1), inset 0 0 0 1px rgba(156, 163, 175, 0.5), 0 1px 2px rgba(0, 0, 0, 0.1)",
      }}
      aria-label={`${label}: ${isOn ? "on" : "off"}`}
    >
      <div
        className="absolute top-[2px] h-[22px] w-[22px] rounded-full transition-transform duration-200 ease-in-out"
        style={{
          transform: isOn ? "translateX(22px)" : "translateX(2px)",
          backgroundColor: "#FFFFFF",
          boxShadow: isOn
            ? "inset 0 0 0 2px rgba(0, 217, 255, 0.6), 0 3px 5px rgba(0, 0, 0, 0.3), 0 0 8px rgba(0, 217, 255, 0.4)"
            : "inset 0 0 0 1px rgba(156, 163, 175, 0.3), 0 3px 5px rgba(0, 0, 0, 0.2), 0 0 0 0.5px rgba(0, 0, 0, 0.08)",
        }}
      />
    </button>
  </div>
);

export default ToggleSwitch;
/* SPDX-License-Identifier: MIT OR Apache-2.0
   Copyright (c) 2025 Guilherme Ferrari Brescia */

/* Base Styles - Neural Signal Connectivity Interface */
.wifiStatusConnection {
  display: inline-flex;
  align-items: center;
  cursor: pointer;
  position: relative;
}

/* Neural Icon Container */
.wifiIconContainer {
  position: relative;
  display: flex;
  align-items: center;
  justify-content: center;
  height: 28px;
  width: 28px;
  margin-right: 0.5rem;
  margin-left: 0.25rem;
}

/* Container para o painel de diagnósticos */
.diagnosticsPanelContainer {
  position: absolute;
  top: 100%;
  right: -40px;
  margin-top: 0.5rem;
  z-index: 50;
  filter: drop-shadow(0 0 10px rgba(0, 0, 0, 0.5));
  animation: fadeInPanel 0.2s ease-out;
}

@keyframes fadeInPanel {
  from { opacity: 0; transform: translateY(-10px); }
  to { opacity: 1; transform: translateY(0); }
}

/* Neural Signal Icon Base */
.neuralSignalIcon {
  overflow: visible; /* Allow glow effects to extend beyond SVG boundaries */
  transform: scale(1.05); /* Slightly larger to enhance visibility */
}

/* Neural Field Animation */
@keyframes neuralFieldPulse {
  0% {
    opacity: 0.05;
    transform: scale(1);
  }
  50% {
    opacity: 0.15;
    transform: scale(1.05);
  }
  100% {
    opacity: 0.05;
    transform: scale(1);
  }
}

.neuralField {
  transform-origin: center;
}

.neuralFieldActive {
  animation: neuralFieldPulse 3s infinite ease-in-out;
}

/* Neural Connection States - Color Definitions */
:global(.wifi-status-fully-connected) .neuralSignalIcon {
  color: #00faff; /* Cyan - Neural connection fully active */
  filter: drop-shadow(0 0 5px rgba(0, 250, 255, 0.9)) drop-shadow(0 0 10px rgba(0, 250, 255, 0.5));
}

:global(.wifi-status-partially-connected) .neuralSignalIcon {
  color: #00e1f0; /* Ciano mais vibrante - Neural connection partially active */
  filter: drop-shadow(0 0 4px rgba(0, 225, 240, 0.7)) drop-shadow(0 0 8px rgba(0, 225, 240, 0.4));
}

:global(.wifi-status-connecting) .neuralSignalIcon {
  color: #ffe066; /* Yellow - Neural connection forming */
  filter: drop-shadow(0 0 4px rgba(255, 224, 102, 0.7)) drop-shadow(0 0 8px rgba(255, 224, 102, 0.4));
}

:global(.wifi-status-error) .neuralSignalIcon {
  color: #ff4455; /* Red - Neural connection disrupted */
  filter: drop-shadow(0 0 4px rgba(255, 68, 85, 0.7)) drop-shadow(0 0 8px rgba(255, 68, 85, 0.4));
}

:global(.wifi-status-disconnected) .neuralSignalIcon,
:global(.wifi-status-unknown) .neuralSignalIcon {
  color: #aaaaaa; /* Cinza mais claro para melhor visibilidade quando desconectado */
  filter: drop-shadow(0 0 3px rgba(170, 170, 170, 0.6)) drop-shadow(0 0 5px rgba(170, 170, 170, 0.3));
}

/* Neural Ring Animation */
@keyframes neuralRingPulse {
  0% {
    stroke-opacity: 0.7;
    stroke-width: 1.5;
  }
  50% {
    stroke-opacity: 1;
    stroke-width: 2;
  }
  100% {
    stroke-opacity: 0.7;
    stroke-width: 1.5;
  }
}

.neuralRing {
  transition: all 0.3s ease-in-out;
  transform-origin: center center;
}

.neuralRingActive {
  animation: neuralRingPulse 3s infinite ease-in-out;
}

/* Neural Wave Animation */
@keyframes neuralWavePulse {
  0% {
    stroke-opacity: 0.7;
    transform: translateY(0);
  }
  50% {
    stroke-opacity: 1;
    transform: translateY(-0.5px);
  }
  100% {
    stroke-opacity: 0.7;
    transform: translateY(0);
  }
}

.neuralWave {
  stroke-opacity: 0.7;
  transform-origin: center;
}

:global(.wifi-status-fully-connected) .neuralWave {
  animation: neuralWavePulse 2.5s infinite ease-in-out;
}

/* Neural Core Animation */
@keyframes neuralCorePulse {
  0% {
    opacity: 0.8;
    transform: scale(1);
  }
  50% {
    opacity: 1;
    transform: scale(1.1);
  }
  100% {
    opacity: 0.8;
    transform: scale(1);
  }
}

.neuralCore {
  transform-origin: center;
}

.neuralCoreActive {
  animation: neuralCorePulse 2s infinite ease-in-out;
}

/* Inner Core Animation */
@keyframes neuralInnerCorePulse {
  0% {
    opacity: 0.8;
    transform: scale(1);
  }
  50% {
    opacity: 1;
    transform: scale(1.2);
  }
  100% {
    opacity: 0.8;
    transform: scale(1);
  }
}

.neuralInnerCore {
  transform-origin: center;
  animation: neuralInnerCorePulse 1.5s infinite ease-in-out;
}

/* Status Text Styling */
.statusText {
  margin-left: 0.25rem;
  font-size: 0.75rem;
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
  letter-spacing: 0.02em;
  transition: color 0.3s ease;
}

/* Status text styling */
.statusText {
  font-size: 0.75rem;
}

.statusConnected {
  color: var(--color-green-400);
}

.statusConnecting {
  color: var(--color-yellow-400);
}

.statusDisconnected {
  color: var(--color-red-400);
}

.statusUnknown {
  color: var(--color-gray-400);
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useState, useRef, useEffect } from 'react';
import { ConnectionState, MicrophoneState } from '../../../context';
import styles from './WifiStatusConnection.module.css';
import DiagnosticsPanel from './DiagnosticsPanel';

interface WifiStatusConnectionProps {
  connectionState: ConnectionState;
  microphoneState: MicrophoneState;
  signalStrength?: 'strong' | 'medium' | 'weak' | 'none';
  onStatusClick?: () => void;
  showDetailedText?: boolean;
  className?: string;
  onDisconnect?: () => void;
  onReconnect?: () => void;
}

/**
 * Neural signal visualization component for connection state
 * Symbolic intent: Interface neuron for connectivity visualization
 */
const WifiStatusConnection: React.FC<WifiStatusConnectionProps> = ({ 
  connectionState, 
  microphoneState, 
  signalStrength = 'medium',
  onStatusClick,
  showDetailedText = false,
  className = '',
  onDisconnect,
  onReconnect
}) => {
  // Estado para controlar a visibilidade do painel de diagnósticos
  const [showDiagnostics, setShowDiagnostics] = useState(false);
  const diagnosticsPanelRef = useRef<HTMLDivElement>(null);
  const iconRef = useRef<HTMLDivElement>(null);
  
  // Gerencia cliques fora do painel para fechá-lo
  useEffect(() => {
    const handleClickOutside = (event: MouseEvent) => {
      if (
        showDiagnostics && 
        diagnosticsPanelRef.current && 
        !diagnosticsPanelRef.current.contains(event.target as Node) &&
        iconRef.current &&
        !iconRef.current.contains(event.target as Node)
      ) {
        setShowDiagnostics(false);
      }
    };

    document.addEventListener('mousedown', handleClickOutside);
    return () => {
      document.removeEventListener('mousedown', handleClickOutside);
    };
  }, [showDiagnostics]);
  
  // Alterna a visibilidade do painel de diagnósticos
  const toggleDiagnosticsPanel = () => {
    setShowDiagnostics(prev => !prev);
  };

  // Determinar se o sistema está completamente conectado (Deepgram + microfone)
  const isFullyConnected = () => {
    return (
      connectionState === ConnectionState.OPEN && 
      (microphoneState === MicrophoneState.Open || microphoneState === MicrophoneState.Ready)
    );
  };

  // Determinar se há erros em qualquer subsistema
  const hasErrors = () => {
    return (
      connectionState === ConnectionState.ERROR || 
      microphoneState === MicrophoneState.Error
    );
  };

  // Determinar se está em processo de conexão
  const isConnecting = () => {
    return (
      connectionState === ConnectionState.CONNECTING ||
      microphoneState === MicrophoneState.Opening ||
      microphoneState === MicrophoneState.SettingUp
    );
  };

  // Convert connection state to user-friendly message
  const getConnectionStatusText = () => {
    if (isFullyConnected()) {
      return 'Fully Connected';
    } else if (hasErrors()) {
      return 'Connection Error';
    } else if (isConnecting()) {
      return 'Connecting...';
    } else if (connectionState === ConnectionState.OPEN) {
      return 'Deepgram Connected';
    } else if (microphoneState === MicrophoneState.Ready || microphoneState === MicrophoneState.Open) {
      return 'Microphone Ready';
    } else if (connectionState === ConnectionState.CLOSED || connectionState === ConnectionState.STOPPED) {
      return 'Disconnected';
    } else {
      return 'Unknown State';
    }
  };

  // Determine colors based on combined connection state
  const getConnectionColors = () => {
    if (isFullyConnected()) {
      // Ambos conectados - verde ciano
      return {
        primary: '#00faff',
        secondary: 'rgba(0, 250, 255, 0.8)',
        glow: 'rgba(0, 250, 255, 0.5)',
        textColor: 'text-green-400'
      };
    } else if (hasErrors()) {
      // Erros - vermelho
      return {
        primary: '#ff4455',
        secondary: 'rgba(255, 68, 85, 0.8)',
        glow: 'rgba(255, 68, 85, 0.5)',
        textColor: 'text-red-400'
      };
    } else if (isConnecting()) {
      // Conectando - amarelo
      return {
        primary: '#ffe066',
        secondary: 'rgba(255, 224, 102, 0.8)',
        glow: 'rgba(255, 224, 102, 0.5)',
        textColor: 'text-yellow-400'
      };
    } else if (connectionState === ConnectionState.OPEN || 
               microphoneState === MicrophoneState.Ready || 
               microphoneState === MicrophoneState.Open) {
      // Parcialmente conectado - ciano mais fraco
      return {
        primary: '#00c8d4',
        secondary: 'rgba(0, 200, 212, 0.8)',
        glow: 'rgba(0, 200, 212, 0.5)',
        textColor: 'text-cyan-400'
      };
    } else {
      // Desconectado ou estado desconhecido - cinza
      return {
        primary: '#888888',
        secondary: 'rgba(136, 136, 136, 0.8)',
        glow: 'rgba(136, 136, 136, 0.5)',
        textColor: 'text-gray-400'
      };
    }
  };

  // Get color variables for the component
  const colors = getConnectionColors();
  const statusText = getConnectionStatusText();

  // Determine which bars should be active based on connection status and signal strength
  const getSignalBars = () => {
    // Se ambos estiverem conectados, use a força do sinal fornecida
    if (isFullyConnected()) {
      switch (signalStrength) {
        case 'strong':
          return [true, true, true];
        case 'medium':
          return [true, true, false];
        case 'weak':
          return [true, false, false];
        default:
          return [true, true, true]; // Por padrão, mostra sinal forte quando conectado
      }
    } 
    // Se estiver conectando, mostra sinal médio
    else if (isConnecting()) {
      return [true, true, false];
    }
    // Se apenas um dos sistemas estiver conectado, mostra sinal fraco
    else if (connectionState === ConnectionState.OPEN || 
             microphoneState === MicrophoneState.Ready || 
             microphoneState === MicrophoneState.Open) {
      return [true, false, false];
    }
    // Se estiver desconectado ou com erro, não mostra sinal
    else {
      return [false, false, false];
    }
  };

  const signalBars = getSignalBars();

  // Generate dynamic classNames based on combined connection state
  const getConnectionStateClass = () => {
    if (isFullyConnected()) {
      return 'wifi-status-fully-connected';
    } else if (hasErrors()) {
      return 'wifi-status-error';
    } else if (isConnecting()) {
      return 'wifi-status-connecting';
    } else if (connectionState === ConnectionState.OPEN || 
               microphoneState === MicrophoneState.Ready || 
               microphoneState === MicrophoneState.Open) {
      return 'wifi-status-partially-connected';
    } else if (connectionState === ConnectionState.CLOSED || connectionState === ConnectionState.STOPPED) {
      return 'wifi-status-disconnected';
    } else {
      return 'wifi-status-unknown';
    }
  };

  // Add dynamic classes based on connection state
  const connectionStateClass = getConnectionStateClass();
  
  // Determine CSS classes for status text
  const getStatusTextClass = () => {
    switch (connectionState) {
      case ConnectionState.OPEN:
        return styles.statusConnected;
      case ConnectionState.CONNECTING:
        return styles.statusConnecting;
      case ConnectionState.CLOSED:
      case ConnectionState.ERROR:
      case ConnectionState.STOPPED:
        return styles.statusDisconnected;
      default:
        return styles.statusUnknown;
    }
  };

  return (
    <div 
      className={`${styles.wifiStatusConnection} ${connectionStateClass} ${className}`}
      title={statusText}
      ref={iconRef}
      onClick={toggleDiagnosticsPanel}
    >
      {/* Painel de diagnósticos flutuante */}
      {showDiagnostics && (
        <div 
          className={styles.diagnosticsPanelContainer} 
          ref={diagnosticsPanelRef}
          onClick={(e) => e.stopPropagation()}
        >
          <DiagnosticsPanel 
            connectionState={connectionState} 
            microphoneState={microphoneState}
            onDisconnect={onDisconnect}
            onReconnect={onReconnect}
          />
        </div>
      )}
      <div className={styles.wifiIconContainer}>
        {/* WiFi icon with neural aesthetic for Orch-OS */}
        <svg width="26" height="26" viewBox="0 0 26 26" fill="none" className={styles.neuralSignalIcon}>
          {/* Outer ring with neural aesthetic */}
          <circle 
            cx="13" 
            cy="13" 
            r="11" 
            className={`${styles.neuralRing} ${isFullyConnected() ? styles.neuralRingActive : ''}`} 
            stroke="currentColor" 
            strokeWidth="1.5" 
            strokeOpacity={isFullyConnected() ? "1" : isConnecting() ? "0.8" : "0.6"}
            fill="transparent"
          />
          
          {/* WiFi signal wave - upper arc - always visible but with varied opacity */}
          <path 
            d="M6.5 13 A9 9 0 0 1 19.5 13" 
            className={styles.neuralWave} 
            stroke="currentColor" 
            strokeWidth="1.5" 
            strokeLinecap="round"
            strokeOpacity={signalBars[0] ? "1" : "0.25"}
          />
          
          {/* WiFi signal wave - lower arc - always visible but with varied opacity */}
          <path 
            d="M9 16.5 A6 6 0 0 1 17 16.5" 
            className={styles.neuralWave} 
            stroke="currentColor" 
            strokeWidth="1.5" 
            strokeLinecap="round"
            strokeOpacity={signalBars[1] ? "1" : "0.25"}
          />
          
          {/* Central signal point - always visible but with varied opacity */}
          <circle 
            cx="13" 
            cy="19.5" 
            r="1.8" 
            className={`${styles.neuralCore} ${isFullyConnected() ? styles.neuralCoreActive : ''}`} 
            fill="currentColor" 
            fillOpacity={signalBars[2] ? "1" : "0.25"}
          />
        </svg>
      </div>
      
      {showDetailedText && (
        <span className={`${styles.statusText} ${getStatusTextClass()}`}>
          {statusText}
        </span>
      )}
    </div>
  );
};

export default WifiStatusConnection;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useState } from 'react';
import { ModeService } from '../../../../services/ModeService';
import { ToastVariant } from '../../../ui/toast';
import { ImportMode } from '../types/interfaces';

// Custom hook following Single Responsibility and Open/Closed principles
export const useChatGptImport = (
  showToast: (title: string, description: string, variant: ToastVariant) => void
) => {
  const [importFile, setImportFile] = useState<File | null>(null);
  const [importUserName, setImportUserName] = useState<string>("");
  const [importMode, setImportMode] = useState<ImportMode>('increment');
  const [importProgress, setImportProgress] = useState<number>(0);
  const [importStage, setImportStage] = useState<string>("");
  const [importSummary, setImportSummary] = useState<string>("");
  const [isImporting, setIsImporting] = useState(false);
  const [showImportModal, setShowImportModal] = useState(false);

  // Handler for file selection
  const handleFileChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    if (e.target.files && e.target.files.length > 0) {
      setImportFile(e.target.files[0]);
      setImportSummary("");
    }
  };

  // Handler for starting the import process
  const handleStartImport = async (userName: string) => {
    console.log('[useChatGptImport] handleStartImport chamado. importMode:', importMode, '| importUserName:', userName, '| importFile:', !!importFile);

    if (!importFile) return;
    
    setIsImporting(true);
    setImportProgress(0);
    setImportStage(""); // Clear stage on new import
    setImportSummary("");
    
    try {
      // Read file as buffer
      const fileBuffer = await importFile.arrayBuffer();
      
      type ProgressData = { processed: number; total: number; percentage?: number; stage?: string };

      // Vamos usar a solução original com uma pequena modificação
      // para evitar pulos bruscos
      
      // Armazenar o último percentual para evitar regressão
      let lastPercent = 0;
      
      // Get current application mode to pass to import process
      const applicationMode = ModeService.getMode();
      console.log('[useChatGptImport] Using applicationMode:', applicationMode);
      
      const result = await window.electronAPI.importChatHistory({
        fileBuffer,
        mode: importMode,
        user: userName,
        applicationMode,
        onProgress: (data: ProgressData) => {
          // Usar o percentual enviado ou calcular
          let percent = data.percentage !== undefined 
            ? data.percentage 
            : Math.round((data.processed / Math.max(1, data.total)) * 100);
          
          // Garantir que o progresso nunca regride
          if (percent < lastPercent && lastPercent < 95) {
            percent = lastPercent;
          } else {
            lastPercent = percent;
          }
          
          // Atualizar o estado
          setImportProgress(percent);
          if (data.stage) setImportStage(data.stage);
          
          // Atualizar o título e log
          document.title = `Importing... ${percent}%`;
          console.log(`[RENDERER] Progress: ${percent}% (${data.processed}/${data.total}) | Stage: ${data.stage || ''}`);
        },
      });
      
      // Ensure progress bar is visible for at least a minimum duration
      const minDisplay = process.env.NODE_ENV === 'test' ? 1000 : 200;
      await new Promise(res => setTimeout(res, minDisplay));
      
      setIsImporting(false);
      setImportProgress(100);
      
      // Update summary based on result
      if (result?.imported !== undefined && result?.skipped !== undefined) {
        setImportSummary(`Import complete! Imported: ${result.imported}, Skipped: ${result.skipped}`);
        showToast("Import complete", `Imported: ${result.imported}, Ignored: ${result.skipped}`, "success");
      } else if (result?.success) {
        setImportSummary('Import complete!');
        showToast("Import complete", "Process completed successfully", "success");
      } else {
        setImportSummary(`Error: ${result?.error || "Unknown failure"}`);
        showToast("Error", result?.error || "Unknown failure", "error");
      }
    } catch (err: unknown) {
      setIsImporting(false);
      let errorMsg = "Import failed";
      if (err instanceof Error) {
        errorMsg = err.message;
      } else if (typeof err === "string") {
        errorMsg = err;
      }
      setImportSummary(`Error: ${errorMsg}`);
      showToast("Error", errorMsg, "error");
    }
  };

  // Handler to close import modal
  const handleCloseImportModal = () => {
    setShowImportModal(false);
    setImportFile(null);
    setImportProgress(0);
    setImportSummary("");
    setIsImporting(false);
  };

  return {
    // State
    importFile,
    setImportFile,
    importUserName,
    setImportUserName,
    importMode,
    setImportMode,
    importProgress,
    importStage,
    importSummary,
    isImporting,
    showImportModal,
    setShowImportModal,
    
    // Methods
    handleFileChange,
    handleStartImport,
    handleCloseImportModal
  };
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useCallback, useContext, useEffect, useRef, useState } from "react";
import { useToast } from "../../../../App";
import {
  ConnectionState,
  MicrophoneState,
  useDeepgram,
  useMicrophone,
  useTranscription,
} from "../../../context";
import { LanguageContext } from "../../../context/LanguageContext";
// We're directly using the transcription context's texts object
// so no need to import the TranscriptionTextsState interface

export const useTranscriptionManager = () => {
  const transcriptionContext = useTranscription();
  if (!transcriptionContext) return null;

  const { texts, setTexts } = transcriptionContext;

  // Always use LanguageContext, which now syncs with storage
  const { language, setLanguage } = useContext(LanguageContext);
  const { showToast } = useToast();

  const {
    microphoneState,
    getCurrentMicrophoneState,
    startMicrophone,
    stopMicrophone,
    audioDevices,
    selectedDevices,
    handleDeviceChange,
    isMicrophoneOn,
    isSystemAudioOn,
    setIsMicrophoneOn,
    setIsSystemAudioOn,
  } = useMicrophone();

  const {
    connectionState,
    sendTranscriptionPrompt,
    sendDirectMessage,
    connectToDeepgram,
    disconnectFromDeepgram,
    waitForConnectionState,
    getConnectionStatus,
    hasActiveConnection,
    flushTranscriptionsToUI,
    clearTranscriptionData,
    transcriptionService,
  } = useDeepgram();

  // Memoized utility functions to prevent recreating on every render
  const clearTranscription = useCallback(() => {
    console.log("🧹 [CLEAR_TRANSCRIPTION] Clearing all transcription data");

    // Clear UI state
    setTexts((prev) => ({ ...prev, transcription: "" }));

    // Clear service-level data if available
    if (clearTranscriptionData) {
      clearTranscriptionData();
    }

    // Also clear the TranscriptionStorageService data
    if (transcriptionService?.clearTranscriptionData) {
      console.log("🧹 Clearing TranscriptionStorageService data");
      transcriptionService.clearTranscriptionData();
    }
  }, [setTexts, clearTranscriptionData, transcriptionService]);

  const clearAiResponse = useCallback(
    () => setTexts((prev) => ({ ...prev, aiResponse: "" })),
    [setTexts]
  );

  const [showDetailedDiagnostics, setShowDetailedDiagnostics] = useState(false);
  const [connectionDetails, setConnectionDetails] = useState<Record<
    string,
    unknown
  > | null>(null);
  const [temporaryContext, setTemporaryContext] = useState<string>("");
  const [isExpanded, setIsExpanded] = useState<boolean>(false);
  const [isLocalProcessing, setIsLocalProcessing] = useState<boolean>(false);
  const temporaryContextRef = useRef<string>("");
  const transcriptionRef = useRef<HTMLTextAreaElement>(null);

  useEffect(() => {
    temporaryContextRef.current = temporaryContext;
  }, [temporaryContext]);

  useEffect(() => {
    if (transcriptionRef.current) {
      transcriptionRef.current.scrollTop =
        transcriptionRef.current.scrollHeight;
    }
  }, [texts.transcription]);

  useEffect(() => {
    if (!showDetailedDiagnostics) return;

    const intervalId = setInterval(() => {
      if (getConnectionStatus) {
        setConnectionDetails(getConnectionStatus());
      }
    }, 1000);

    return () => clearInterval(intervalId);
  }, [showDetailedDiagnostics, getConnectionStatus]);

  // Setup electron listeners for transcription events
  useEffect(() => {
    if (typeof window !== "undefined" && window.electronAPI) {
      const removeListener = window.electronAPI.onRealtimeTranscription(
        (text) => {
          setTexts((prev) => {
            const newTranscription = prev.transcription
              ? `${prev.transcription}\n${text}`
              : text;

            return {
              ...prev,
              transcription: newTranscription,
            };
          });
        }
      );

      return () => {
        removeListener();
      };
    }
  }, []);

  // Memoized toggleRecording to prevent recreating on every render
  const toggleRecording = useCallback(async () => {
    const currentState = getCurrentMicrophoneState();
    console.log("🔊 Button clicked! Microphone state:", currentState);

    if (currentState === MicrophoneState.Open) {
      console.log("🚫 Stopping recording via button...");
      stopMicrophone();

      // Clear transcriptions when stopping recording
      console.log("🧹 Clearing transcriptions after stopping recording");
      clearTranscription();

      // Also clear the TranscriptionStorageService data
      if (transcriptionService?.clearTranscriptionData) {
        console.log("🧹 Clearing TranscriptionStorageService data");
        transcriptionService.clearTranscriptionData();
      }
    } else {
      console.log("🎤 Starting recording via button...");

      // Clear both UI and service data before starting a new recording
      console.log(
        "🧹 Clearing all previous transcription data for a fresh start."
      );
      clearTranscription(); // Clears UI state from TranscriptionContext
      if (clearTranscriptionData) {
        clearTranscriptionData(); // Clears service-level data from DeepgramContext
      }

      // Also clear the TranscriptionStorageService data
      if (transcriptionService?.clearTranscriptionData) {
        console.log(
          "🧹 Clearing TranscriptionStorageService data before starting"
        );
        transcriptionService.clearTranscriptionData();
      }

      // If no audio source is active, enable system audio by default before recording
      if (!isMicrophoneOn && !isSystemAudioOn) {
        setIsSystemAudioOn(true);
        setTimeout(() => startMicrophone(), 100);
      } else {
        startMicrophone();
      }
    }
  }, [
    getCurrentMicrophoneState,
    stopMicrophone,
    startMicrophone,
    isMicrophoneOn,
    isSystemAudioOn,
    setIsSystemAudioOn,
    clearTranscriptionData,
    clearTranscription,
    transcriptionService,
  ]);

  // Memoize handleSendPrompt to prevent recreating on every render
  const handleSendPrompt = useCallback(
    async (messageContent?: string, contextContent?: string) => {
      // Proteção de duplicação agora é feita no DeepgramContext (global)

      try {
        // Set state for UI updates (e.g., disabling button visually)
        setIsLocalProcessing(true);

        // Check if there's any content to send
        const currentTranscription = texts.transcription?.trim();
        if (!messageContent && !currentTranscription) {
          console.warn("⚠️ [SEND_PROMPT] No content to send");
          showToast("Aviso", "Nenhuma mensagem para enviar", "neutral");
          setIsLocalProcessing(false);
          return;
        }

        console.log("🚀 [SEND_PROMPT] Starting prompt send:", {
          messageContent,
          contextContent,
          temporaryContext: temporaryContextRef.current,
          hasDirectMessage: !!messageContent,
          hasTranscription: !!currentTranscription,
          timestamp: new Date().toISOString(),
        });

        // If it's a direct message (from chat input), use sendDirectMessage
        if (messageContent) {
          console.log("💬 [SEND_PROMPT] Sending as direct message");
          await sendDirectMessage(
            messageContent,
            contextContent || temporaryContextRef.current
          );
        } else {
          // Otherwise, use the transcription-based prompt
          // Only flush transcriptions when we have actual transcriptions
          if (flushTranscriptionsToUI && currentTranscription) {
            console.log("📤 [SEND_PROMPT] Flushing transcriptions before send");
            flushTranscriptionsToUI();
          }

          console.log("🎯 [SEND_PROMPT] Sending as transcription prompt");
          await sendTranscriptionPrompt(temporaryContextRef.current);
        }

        console.log("✅ [SEND_PROMPT] Prompt send completed successfully");
      } catch (error) {
        const errorMessage =
          error instanceof Error
            ? error.message === "PROCESSING_IN_PROGRESS"
              ? "Um processamento já está em andamento. Aguarde a conclusão."
              : error.message
            : "Erro desconhecido ao processar o prompt";

        console.error("❌ [SEND_PROMPT] Error sending prompt:", error);
        showToast("Erro", errorMessage, "error");
      } finally {
        // Always clear state
        setIsLocalProcessing(false);
      }
    },
    [
      sendTranscriptionPrompt,
      sendDirectMessage,
      showToast,
      flushTranscriptionsToUI,
      texts.transcription,
      clearTranscriptionData,
      clearTranscription,
    ]
  );

  // Setup electron listeners for AI prompt responses (FIXED: No dependencies)
  useEffect(() => {
    if (typeof window !== "undefined" && window.electronAPI) {
      // Track response accumulation
      let responseBuffer = "";
      let isAccumulatingResponse = false;

      // Create stable callbacks that don't depend on changing state
      const handlePartialResponse = (partialResponse: string) => {
        console.log("🔄 [IPC] Partial response received:", {
          response: partialResponse.substring(0, 50),
          length: partialResponse.length,
          timestamp: new Date().toISOString(),
        });

        // Mark that we're accumulating a response
        isAccumulatingResponse = true;
        responseBuffer = partialResponse;

        // Update with the partial response
        // Don't clear it - let the component decide when to clear
        setTexts((prev) => ({
          ...prev,
          aiResponse: partialResponse,
        }));
      };

      const handleSuccessResponse = (finalResponse: string) => {
        console.log("✅ [IPC] Final response received:", {
          response: finalResponse.substring(0, 50),
          length: finalResponse.length,
          timestamp: new Date().toISOString(),
        });

        // Clear accumulation state
        isAccumulatingResponse = false;
        responseBuffer = "";

        // Set the final response WITHOUT clearing transcription
        // This allows new transcriptions to continue appearing
        setTexts((prev) => ({
          ...prev,
          aiResponse: finalResponse,
        }));
      };

      const handleErrorResponse = (error: string) => {
        console.log("❌ [IPC] Error response received:", {
          error,
          timestamp: new Date().toISOString(),
        });

        // Clear accumulation state
        isAccumulatingResponse = false;
        responseBuffer = "";

        setTexts((prev) => ({
          ...prev,
          aiResponse: `Erro: ${error}`,
        }));
        showToast("Erro", error, "error");
      };

      const handleSendingState = () => {
        console.log("🔄 [IPC] Sending state received:", {
          timestamp: new Date().toISOString(),
        });

        // Reset accumulation state when starting a new request
        isAccumulatingResponse = false;
        responseBuffer = "";

        setTexts((prev) => ({
          ...prev,
          aiResponse: "Processando...",
        }));
      };

      const removePartialListener = window.electronAPI.onPromptPartialResponse(
        handlePartialResponse
      );
      const removeSuccessListener = window.electronAPI.onPromptSuccess(
        handleSuccessResponse
      );
      const removeErrorListener =
        window.electronAPI.onPromptError(handleErrorResponse);
      const removeSendingListener =
        window.electronAPI.onPromptSending(handleSendingState);

      return () => {
        removePartialListener();
        removeSuccessListener();
        removeErrorListener();
        removeSendingListener();
      };
    }
  }, [setTexts, showToast]);

  // Update Deepgram language when UI language changes
  useEffect(() => {
    if (language) {
      console.log(
        `🌐 Language changed to ${language}, will use on next connection`
      );
    }
  }, [language]);

  useEffect(() => {
    if (microphoneState === MicrophoneState.Error) {
      showToast(
        "Error",
        "Failed to access audio. Check your microphone and permissions.",
        "error"
      );
    }
  }, [microphoneState]);

  const toggleExpand = useCallback(() => {
    setIsExpanded(!isExpanded);
  }, [isExpanded, setIsExpanded]);

  return {
    language,
    setLanguage,
    microphoneState,
    connectionState,
    toggleRecording,
    handleSendPrompt,
    clearTranscription,
    clearAiResponse,
    toggleExpand,
    isExpanded,
    temporaryContext,
    setTemporaryContext,
    texts,
    setTexts,
    audioDevices,
    selectedDevices,
    handleDeviceChange,
    isMicrophoneOn,
    isSystemAudioOn,
    setIsMicrophoneOn,
    setIsSystemAudioOn,
    showDetailedDiagnostics,
    setShowDetailedDiagnostics,
    connectionDetails,
    setConnectionDetails,
    transcriptionRef,
    getConnectionStatus,
    disconnectFromDeepgram,
    connectToDeepgram,
    waitForConnectionState,
    hasActiveConnection,
    ConnectionState,
    clearTranscriptionData,
    isLocalProcessing,
  };
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { EventEmitter } from 'events';
import { ModeService } from '../../../../services/ModeService';
import { IChatGptImportService, ImportMode, ImportProgressData } from '../types/interfaces';

// Implementation of the import service (Single Responsibility Principle)
export class ChatGptImportService implements IChatGptImportService {
  private progressEmitter: EventEmitter;

  constructor() {
    this.progressEmitter = new EventEmitter();
  }

  async importChatHistory(options: {
    fileBuffer: ArrayBuffer | Buffer;
    mode: ImportMode;
    user: string;
    onProgress?: (data: ImportProgressData) => void;
  }): Promise<{ success: boolean; error?: string; imported?: number; skipped?: number }> {
    try {
      // Call the actual electronAPI import method
      if (typeof window !== 'undefined' && window.electronAPI) {
        // Get current application mode from ModeService
        const currentApplicationMode = ModeService.getMode();
        
        // Debug logging for mode detection
        console.log('🔍 [ChatGptImportService] Raw ModeService.getMode():', currentApplicationMode);
        console.log('🔍 [ChatGptImportService] typeof currentApplicationMode:', typeof currentApplicationMode);
        
        // Convert enum to string if needed
        const applicationModeString = currentApplicationMode === 'basic' ? 'basic' : 
                                      currentApplicationMode === 'advanced' ? 'advanced' :
                                      String(currentApplicationMode);
        
        console.log('🔍 [ChatGptImportService] Final applicationMode string:', applicationModeString);
        
        const result = await window.electronAPI.importChatHistory({
          fileBuffer: options.fileBuffer,
          mode: options.mode,
          user: options.user,
          applicationMode: currentApplicationMode,
          onProgress: options.onProgress,
        });
        
        // Return the result obtained from the API
        return {
          success: result.success ?? false,
          imported: result.imported ?? 0,
          skipped: result.skipped ?? 0,
          error: result.error
        };
      } else {
        throw new Error("Electron API not available");
      }
    } catch (error) {
      console.error("Error in ChatGptImportService:", error);
      let errorMsg = "Import failed";
      if (error instanceof Error) {
        errorMsg = error.message;
      } else if (typeof error === "string") {
        errorMsg = error;
      }
      // Return object with basic fields
      return { 
        error: errorMsg, 
        success: false,
        imported: 0,
        skipped: 0
      };
    }
  }
}

// Singleton instance for the service (Single Responsibility Principle)
const chatGptImportService = new ChatGptImportService();
export default chatGptImportService;
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

.orchos-quantum-dashboard::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-image: 
    radial-gradient(circle at 20% 50%, rgba(0, 250, 255, 0.1) 0%, transparent 2%),
    radial-gradient(circle at 80% 80%, rgba(255, 0, 128, 0.08) 0%, transparent 2%),
    radial-gradient(circle at 40% 80%, rgba(120, 119, 198, 0.1) 0%, transparent 2%);
  background-size: 400px 400px;
  animation: quantumDrift 20s ease-in-out infinite;
  pointer-events: none;
  opacity: 0.4;
}

@keyframes quantumDrift {
  0%, 100% { transform: translate(0, 0) scale(1); opacity: 0.4; }
  25% { transform: translate(30px, -20px) scale(1.1); opacity: 0.5; }
  50% { transform: translate(-20px, 30px) scale(0.9); opacity: 0.3; }
  75% { transform: translate(-30px, -10px) scale(1.05); opacity: 0.45; }
}

.quantum-visualization-zone::after {
  content: '';
  position: absolute;
  top: -50%;
  left: -50%;
  width: 200%;
  height: 200%;
  background: linear-gradient(
    45deg,
    transparent 30%,
    rgba(0, 250, 255, 0.03) 50%,
    transparent 70%
  );
  animation: holographicShimmer 8s linear infinite;
  pointer-events: none;
}

@keyframes holographicShimmer {
  0% { transform: translateX(-100%) translateY(-100%) rotate(0deg); }
  100% { transform: translateX(100%) translateY(100%) rotate(360deg); }
}

.quantum-visualization-zone::before {
  content: '';
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  width: 300px;
  height: 300px;
  background: radial-gradient(circle, rgba(0,250,255,0.15) 0%, transparent 70%);
  filter: blur(60px);
  animation: quantumPulse 4s ease-in-out infinite;
  pointer-events: none;
}

@keyframes quantumPulse {
  0%, 100% { opacity: 0.3; transform: translate(-50%, -50%) scale(1); }
  50% { opacity: 0.6; transform: translate(-50%, -50%) scale(1.2); }
}

@keyframes recordingPulse {
  0%, 100% { transform: scale(1); }
  50% { transform: scale(1.1); }
}

.ripple {
  position: absolute;
  border-radius: 50%;
  background: radial-gradient(circle, rgba(0, 250, 255, 0.3) 0%, transparent 70%);
  transform: scale(0);
  animation: rippleEffect 0.6s ease-out;
  pointer-events: none;
}

@keyframes rippleEffect {
  to {
    transform: scale(4);
    opacity: 0;
  }
}

.orchos-tooltip::after {
  content: '';
  position: absolute;
  width: 100%;
  height: 100%;
  left: 0;
  top: 0;
  border-radius: inherit;
  background: linear-gradient(135deg, 
    rgba(255, 77, 210, 0.1) 0%, 
    rgba(0, 250, 255, 0.1) 50%,
    rgba(124, 77, 255, 0.1) 100%);
  opacity: 0;
  z-index: -1;
  animation: neuralGradient 8s ease-in-out infinite;
}

@keyframes neuralGradient {
  0%, 100% { opacity: 0.1; transform: rotate(0deg); }
  50% { opacity: 0.3; transform: rotate(180deg); }
}

.orchos-quantum-btn {
  position: relative;
  width: 48px;
  height: 48px;
  min-width: 48px;
  min-height: 48px;
  padding: 0;
  background: var(--quantum-bg-dark);
  border: 2px solid transparent;
  border-radius: 50%;
  cursor: pointer;
  overflow: hidden;
  transition: all 0.3s var(--quantum-cubic);
  -webkit-backdrop-filter: blur(20px) saturate(180%);
  backdrop-filter: blur(20px) saturate(180%);
  box-shadow: 
    0 0 30px rgba(0, 250, 255, 0.2),
    inset 0 0 20px rgba(0, 250, 255, 0.1),
    0 4px 20px rgba(0, 0, 0, 0.5);
  animation: neuralBreathing 6s ease-in-out infinite;
}

@keyframes neuralBreathing {
  0%, 100% { transform: scale(0.98); }
  50% { transform: scale(1.02); }
}

.orchos-quantum-btn:hover {
  animation: none;
  transform: scale(1.08) translateZ(var(--quantum-depth-2));
  box-shadow: 
    0 0 50px rgba(0, 250, 255, 0.4),
    inset 0 0 30px rgba(0, 250, 255, 0.2),
    0 8px 30px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-btn::before {
  content: '';
  position: absolute;
  inset: -2px;
  border-radius: 50%;
  padding: 2px;
  background: linear-gradient(135deg, 
    #ff4dd2 0%, 
    #00faff 33%, 
    #7c4dff 66%, 
    #ff4dd2 100%
  );
  

/* Cross-browser mask support */


  -webkit-mask: linear-gradient(#fff 0 0) content-box, linear-gradient(#fff 0 0);
  -webkit-mask-composite: xor;
  mask: linear-gradient(#fff 0 0) content-box, linear-gradient(#fff 0 0);
  mask-composite: exclude;
  opacity: 0.7;
  animation: quantumBorderRotate 4s linear infinite;
}

@keyframes quantumBorderRotate {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

.orchos-quantum-particles::before,
.orchos-quantum-particles::after {
  content: '';
  position: absolute;
  width: 2px;
  height: 2px;
  background: var(--active-color, #00faff);
  border-radius: 50%;
  box-shadow: 
    0 0 4px var(--active-color, #00faff),
    0 0 8px var(--active-color, #00faff);
  animation: quantumFloat 4s ease-in-out infinite;
  opacity: 0.7;
}

.orchos-quantum-particles::before {
  top: 25%;
  left: 30%;
  animation-delay: 0s;
}

.orchos-quantum-particles::after {
  bottom: 30%;
  right: 25%;
  animation-delay: 2s;
}

.orchos-quantum-btn::after {
  content: '';
  position: absolute;
  top: 60%;
  left: 15%;
  width: 2px;
  height: 2px;
  background: var(--active-color, #00faff);
  border-radius: 50%;
  box-shadow: 
    0 0 4px var(--active-color, #00faff),
    0 0 8px var(--active-color, #00faff);
  animation: quantumFloat 4.5s ease-in-out infinite;
  animation-delay: 1s;
  z-index: 2;
  opacity: 0.7;
}

.orchos-quantum-btn::before {
  content: '';
  position: absolute;
  top: 20%;
  right: 25%;
  width: 2px;
  height: 2px;
  background: var(--active-color, #00faff);
  border-radius: 50%;
  box-shadow: 
    0 0 4px var(--active-color, #00faff),
    0 0 8px var(--active-color, #00faff);
  animation: quantumFloat 5s ease-in-out infinite;
  animation-delay: 2.5s;
  z-index: 2;
  opacity: 0.7;
}

@keyframes quantumFloat {
  0%, 100% {
    transform: translate(0, 0) scale(1);
    opacity: 0.3;
  }
  50% {
    transform: translate(10px, -10px) scale(1.5);
    opacity: 1;
  }
}

@keyframes neuralParticleActive {
  0%, 100% {
    transform: translate(0, 0) scale(1);
    opacity: 0.6;
  }
  25% {
    transform: translate(15px, -5px) scale(2);
    opacity: 1;
  }
  50% {
    transform: translate(5px, -15px) scale(1.2);
    opacity: 0.8;
  }
  75% {
    transform: translate(-10px, -8px) scale(1.8);
    opacity: 0.9;
  }
}

.orchos-neural-ring {
  position: absolute;
  inset: 3px;
  border-radius: 50%;
  background: radial-gradient(circle at center,
    transparent 0%,
    transparent 40%,
    rgba(0, 250, 255, 0.1) 50%,
    transparent 60%
  );
  animation: neuralPulse 2s ease-in-out infinite;
}

@keyframes neuralPulse {
  0%, 100% {
    transform: scale(1);
    opacity: 0.5;
  }
  50% {
    transform: scale(1.2);
    opacity: 1;
  }
}

@keyframes recordingPulse {
  0%, 100% {
    box-shadow: 
      0 0 30px rgba(255, 77, 210, 0.4),
      inset 0 0 20px rgba(255, 77, 210, 0.2),
      0 4px 20px rgba(0, 0, 0, 0.5);
  }
  50% {
    box-shadow: 
      0 0 50px rgba(255, 77, 210, 0.8),
      inset 0 0 30px rgba(255, 77, 210, 0.5),
      0 4px 20px rgba(0, 0, 0, 0.5);
    transform: scale(1.05);
  }
}

.orchos-quantum-btn:active::after,
.orchos-quantum-btn:active::before {
  animation: synapticBurst 0.5s ease-out forwards;
}

@keyframes synapticBurst {
  0% { transform: scale(1); opacity: 0.7; }
  100% { transform: scale(6); opacity: 0; }
}

.orchos-btn-record:hover .neural-wave {
  animation-duration: 4s;
  opacity: 0.9;
}

.orchos-btn-send:hover .neural-send-wave {
  animation-duration: 3s;
  opacity: 0.9;
}

.orchos-btn-send:hover .neural-send-arrow {
  animation-duration: 1.5s;
  opacity: 1;
}

.settings-btn:hover svg circle:nth-of-type(1) {
  animation: qNodePulse 2s infinite alternate ease-in-out;
}

.settings-btn:hover svg circle:nth-of-type(2) {
  animation: orbitalSpin 8s infinite linear;
}

.settings-btn:hover svg circle:nth-of-type(3),
.settings-btn:hover svg circle:nth-of-type(4),
.settings-btn:hover svg circle:nth-of-type(5),
.settings-btn:hover svg circle:nth-of-type(6) {
  animation: connectionPulse 3s infinite alternate ease-in-out;
}

@keyframes qNodePulse {
  0% { r: 3; fill: rgba(0, 250, 255, 0.2); }
  100% { r: 3.5; fill: rgba(0, 250, 255, 0.4); }
}

@keyframes orbitalSpin {
  0% { transform: rotate(0deg) translate(10px, 10px); }
  100% { transform: rotate(360deg) translate(10px, 10px); }
}

@keyframes connectionPulse {
  0% { r: 1.5; fill: rgba(0, 250, 255, 0.7); }
  100% { r: 1.8; fill: #00faff; }
}

.neural-settings-popup {
  animation: settingsPopupReveal 0.3s ease-out forwards;
  transform-origin: top right;
}

@keyframes settingsPopupReveal {
  0% { opacity: 0; transform: scale(0.9); }
  100% { opacity: 1; transform: scale(1); }
}

.neural-wave {
  opacity: 0.7;
  stroke-dasharray: 10;
  animation: neuralWaveConverge 6s ease-in-out infinite;
}

.wave-1 { animation-delay: 0s; }

.wave-2 { animation-delay: 0.7s; }

.wave-3 { animation-delay: 1.4s; }

.wave-4 { animation-delay: 2.1s; }

.wave-5 { animation-delay: 2.8s; }

.wave-6 { animation-delay: 3.5s; }

.wave-7 { animation-delay: 4.2s; }

.wave-8 { animation-delay: 4.9s; }

@keyframes neuralWaveConverge {
  0% {
    stroke-dashoffset: 40;
    opacity: 0.3;
    stroke-width: 1px;
  }
  50% {
    opacity: 0.9;
    stroke-width: 1.75px;
  }
  100% {
    stroke-dashoffset: 0;
    opacity: 0.3;
    stroke-width: 1px;
  }
}

.neural-send-wave {
  opacity: 0.7;
  stroke-dasharray: 12;
  animation: neuralWaveDiverge 5s ease-in-out infinite;
}

.neural-send-arrow {
  opacity: 0.8;
  animation: neuralSendPulse 3s ease-in-out infinite;
}

@keyframes neuralWaveDiverge {
  0% {
    stroke-dashoffset: 0;
    opacity: 0.3;
    stroke-width: 1px;
  }
  50% {
    opacity: 0.9;
    stroke-width: 1.75px;
  }
  100% {
    stroke-dashoffset: 40;
    opacity: 0.3;
    stroke-width: 1px;
  }
}

@keyframes neuralSendPulse {
  0%, 100% {
    opacity: 0.6;
    transform: translateX(0);
  }
  50% {
    opacity: 1;
    transform: translateX(2px);
  }
}

.neural-core-record {
  animation: coreRecordPulse 4s ease-in-out infinite;
}

.neural-core-send {
  animation: coreSendPulse 4s ease-in-out infinite;
}

@keyframes coreRecordPulse {
  0%, 100% {
    r: 5;
    opacity: 0.8;
  }
  50% {
    r: 5.5;
    opacity: 1;
  }
}

@keyframes coreSendPulse {
  0%, 100% {
    r: 3;
    opacity: 0.8;
  }
  50% {
    r: 3.3;
    opacity: 1;
  }
}

@keyframes activeRecordPulse {
  0%, 100% {
    r: 5;
    opacity: 0.9;
  }
  50% {
    r: 6;
    opacity: 1;
    filter: drop-shadow(0 0 8px var(--quantum-record-active));
  }
}

.orchos-quantum-ripple {
  position: absolute;
  border-radius: 50%;
  transform: translate(-50%, -50%);
  background: radial-gradient(circle, 
    rgba(0, 250, 255, 0.4) 0%, 
    rgba(124, 77, 255, 0.2) 50%, 
    transparent 70%
  );
  pointer-events: none;
  animation: quantumRippleExpand 1s ease-out forwards;
}

@keyframes quantumRippleExpand {
  0% {
    width: 0;
    height: 0;
    opacity: 1;
  }
  100% {
    width: 200px;
    height: 200px;
    opacity: 0;
  }
}

@media (prefers-reduced-motion: reduce) {
  .orchos-quantum-btn,
  .orchos-quantum-btn::before,
  .orchos-neural-ring,
  .orchos-quantum-particles::before,
  .orchos-quantum-particles::after {
    animation: none !important;
    transition: none !important;
  }
  
  .orchos-quantum-btn {
    transition: transform 0.2s ease;
  }
}

/* Card entrance animation */
@keyframes slideInUp {
  from {
    opacity: 0;
    transform: translateY(20px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Transcription display animation */
@keyframes transcriptionFadeIn {
  from {
    opacity: 0;
    transform: translateY(-10px);
    max-height: 0;
  }
  to {
    opacity: 1;
    transform: translateY(0);
    max-height: 150px;
  }
}

@keyframes transcriptionFadeOut {
  from {
    opacity: 1;
    transform: translateY(0);
    max-height: 150px;
  }
  to {
    opacity: 0;
    transform: translateY(-10px);
    max-height: 0;
  }
}

.transcription-display {
  animation: transcriptionFadeIn 0.3s ease-out forwards;
}

.transcription-display.hiding {
  animation: transcriptionFadeOut 0.3s ease-out forwards;
}

/* Quantum glow effect */
@keyframes quantumGlow {
  0%, 100% {
    filter: drop-shadow(0 0 20px rgba(0, 250, 255, 0.5));
  }
  50% {
    filter: drop-shadow(0 0 40px rgba(0, 250, 255, 0.8));
  }
}

/* Button hover animations */
@keyframes buttonPulse {
  0% {
    box-shadow: 0 0 0 0 rgba(0, 250, 255, 0.7);
  }
  70% {
    box-shadow: 0 0 0 10px rgba(0, 250, 255, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(0, 250, 255, 0);
  }
}

/* Loading spinner */
@keyframes spin {
  to {
    transform: rotate(360deg);
  }
}

/* Text shimmer effect */
@keyframes shimmer {
  0% {
    background-position: -200% center;
  }
  100% {
    background-position: 200% center;
  }
}

/* Connection state animations */
@keyframes connectionPulse {
  0%, 100% {
    opacity: 1;
  }
  50% {
    opacity: 0.5;
  }
}

/* Card glow animation */
.neural-card {
  animation: slideInUp 0.3s ease-out;
}

/* Button active state */
.control-btn:active {
  animation: buttonPulse 0.3s ease-out;
}

/* Loading state */
.loading {
  animation: spin 1s linear infinite;
}

/* Connection indicator */
.connection-indicator {
  animation: connectionPulse 2s ease-in-out infinite;
}

/* Quantum visualization container animation */
.quantum-visualization-zone {
  animation: quantumGlow 3s ease-in-out infinite;
}

/* Text animation for AI responses */
.ai-response-text {
  background: linear-gradient(
    90deg,
    #00faff 0%,
    #7c4dff 50%,
    #00faff 100%
  );
  background-size: 200% auto;
  -webkit-background-clip: text;
  background-clip: text;
  -webkit-text-fill-color: transparent;
  animation: shimmer 3s linear infinite;
}

/* Smooth transitions for all interactive elements */
.control-btn,
.neural-card,
.transcription-display,
.settings-popup {
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

/* Hover lift effect */
.neural-card:hover {
  transform: translateY(-2px);
  box-shadow: 
    0 12px 24px rgba(0, 250, 255, 0.15),
    0 0 0 1px rgba(0, 250, 255, 0.3);
}

/* Recording animation */
@keyframes recordingPulse {
  0%, 100% {
    box-shadow: 0 0 0 0 rgba(255, 68, 85, 0.7);
  }
  50% {
    box-shadow: 0 0 0 20px rgba(255, 68, 85, 0);
  }
}

.mic-btn.recording {
  animation: recordingPulse 1.5s ease-in-out infinite;
}

/* Settings popup animation */
@keyframes settingsSlideIn {
  from {
    opacity: 0;
    transform: translateX(20px);
  }
  to {
    opacity: 1;
    transform: translateX(0);
  }
}

.settings-popup {
  animation: settingsSlideIn 0.2s ease-out;
}

/* Tooltip animation */
@keyframes tooltipFadeIn {
  from {
    opacity: 0;
    transform: translateY(5px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.tooltip {
  animation: tooltipFadeIn 0.2s ease-out;
}
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

.orchos-btn-circular, .orchos-btn-fab, .orchos-btn-glass {
  border-radius: 999px !important;
}

.orchos-btn-fab {
  background: rgba(24,24,40,0.82);
  border: 3px solid #00faff;
  box-shadow: 0 0 24px 8px #00faff33, 0 0 0 0 #fff0;
  color: #00faff;
  font-weight: 600;
  font-size: 1.1rem;
  transition: box-shadow 0.22s, background 0.22s, border-color 0.22s;
}

.orchos-btn-fab:focus, .orchos-btn-fab:hover {
  background: rgba(0, 245, 255, 0.12);
  box-shadow: 0 0 36px 12px #00faff77, 0 0 0 3px #00faff55;
  border-color: #00faff;
}

.orchos-btn-circular {
  background: rgba(24, 24, 40, 0.60);
  border: 2px solid #00faff;
  box-shadow: 0 2px 16px 0 rgba(0,245,255,0.12);
  color: #00faff;
  min-width: 48px;
  min-height: 48px;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: box-shadow 0.18s, background 0.18s, border-color 0.18s;
}

.orchos-btn-circular:focus, .orchos-btn-circular:hover {
  background: rgba(0, 245, 255, 0.12);
  box-shadow: 0 0 16px 4px #00faff77;
  border-color: #00faff;
}

/* Header button styling */

/* Enhanced Button Styles - Quantum Theme */

.neural-control-grid button {
  padding: 0.32rem 0.9rem !important;
  font-size: 0.74rem !important;
  min-height: 1.7rem !important;
  height: auto !important;
  background: 
    linear-gradient(
      135deg, 
      rgba(255, 255, 255, 0.03) 0%, 
      rgba(255, 255, 255, 0.01) 100%
    );
  border: 1px solid rgba(255, 255, 255, 0.07);
  color: rgba(255, 255, 255, 0.88);
  font-weight: 500;
  letter-spacing: 0.02em;
  transition: all 0.3s cubic-bezier(0.23, 1, 0.32, 1);
  position: relative;
  overflow: hidden;
  border-radius: 0.6rem;
  -webkit-backdrop-filter: blur(16px);
  backdrop-filter: blur(16px);
  transform-style: preserve-3d;
}

.neural-control-grid button::before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background: linear-gradient(90deg, transparent, rgba(0, 250, 255, 0.2), transparent);
  transition: left 0.5s ease;
}

.neural-control-grid button:hover {
  background: linear-gradient(135deg, rgba(0, 250, 255, 0.2) 0%, rgba(0, 250, 255, 0.1) 100%);
  border-color: rgba(0, 250, 255, 0.5);
  box-shadow: 0 0 20px rgba(0, 250, 255, 0.3);
  transform: translateY(-1px);
}

.neural-control-grid button:hover::before {
  left: 100%;
}

/* Recording button special styling */

/* Transcription card button layout */

/* Microinteractions - Click Ripple Effect */

.neural-control-grid button {
  position: relative;
  overflow: hidden;
}

.ripple {
  position: absolute;
  border-radius: 50%;
  background: radial-gradient(circle, rgba(0, 250, 255, 0.3) 0%, transparent 70%);
  transform: scale(0);
  animation: rippleEffect 0.6s ease-out;
  pointer-events: none;
}

.neural-control-grid button:focus,
.neural-control-grid input:focus,
.neural-control-grid textarea:focus {
  outline: none;
  box-shadow: 
    0 0 0 2px rgba(0, 250, 255, 0.3),
    0 0 20px rgba(0, 250, 255, 0.2);
}

@media (max-height: 800px) {
  .orchos-quantum-dashboard {
    padding: 0.2rem;
    gap: 0.2rem;
    height: calc(100vh - 2.5rem);
    padding-top: 2.5rem;
  }
  
  .neural-control-grid {
    gap: 0.2rem;
  }
  
  .neural-control-grid button {
    padding: 0.15rem 0.4rem !important;
    font-size: 0.65rem !important;
    min-height: 1.4rem !important;
  }
}

/* Quantum Action Buttons - LEGENDARY */

.orchos-quantum-btn:hover .orchos-tooltip,
.orchos-quantum-btn:focus .orchos-tooltip,
.orchos-quantum-btn:focus-visible .orchos-tooltip {
  opacity: 1;
  transform: translateX(-50%) scale(1);
  pointer-events: auto;
  box-shadow: 
    0 8px 32px 0 rgba(0,250,255,0.15), 
    0 2px 8px 0 rgba(124,77,255,0.15),
    inset 0 0 8px 0 rgba(0,250,255,0.1);
}

.orchos-quantum-btn {
  margin-top: -5px; /* Ajuste fino para alinhamento */
  position: relative;
  width: 48px;
  height: 48px;
  min-width: 48px;
  min-height: 48px;
  padding: 0;
  background: var(--quantum-bg-dark);
  border: 2px solid transparent;
  border-radius: 50%;
  cursor: pointer;
  overflow: hidden;
  transition: all 0.3s var(--quantum-cubic);
  -webkit-backdrop-filter: blur(20px) saturate(180%);
  backdrop-filter: blur(20px) saturate(180%);
  box-shadow: 
    0 0 30px rgba(0, 250, 255, 0.2),
    inset 0 0 20px rgba(0, 250, 255, 0.1),
    0 4px 20px rgba(0, 0, 0, 0.5);
  animation: neuralBreathing 6s ease-in-out infinite;
}

.orchos-quantum-btn:hover {
  animation: none;
  transform: scale(1.08) translateZ(var(--quantum-depth-2));
  box-shadow: 
    0 0 50px rgba(0, 250, 255, 0.4),
    inset 0 0 30px rgba(0, 250, 255, 0.2),
    0 8px 30px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-btn:active {
  transform: scale(0.92);
  transition: all 0.1s var(--quantum-cubic);
}

.orchos-quantum-btn::before {
  content: '';
  position: absolute;
  inset: -2px;
  border-radius: 50%;
  padding: 2px;
  background: linear-gradient(135deg, 
    #ff4dd2 0%, 
    #00faff 33%, 
    #7c4dff 66%, 
    #ff4dd2 100%
  );
  

/* Cross-browser mask support */


  -webkit-mask: linear-gradient(#fff 0 0) content-box, linear-gradient(#fff 0 0);
  -webkit-mask-composite: xor;
  mask: linear-gradient(#fff 0 0) content-box, linear-gradient(#fff 0 0);
  mask-composite: exclude;
  opacity: 0.7;
  animation: quantumBorderRotate 4s linear infinite;
}

.orchos-quantum-btn:hover {
  transform: scale(1.1);
  box-shadow: 
    0 0 50px rgba(0, 250, 255, 0.4),
    inset 0 0 30px rgba(0, 250, 255, 0.2),
    0 8px 30px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-btn:active {
  transform: scale(0.95);
}

.orchos-quantum-btn-inner {
  position: relative;
  width: 100%;
  height: 100%;
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 2;
}

.orchos-quantum-btn::after {
  content: '';
  position: absolute;
  top: 60%;
  left: 15%;
  width: 2px;
  height: 2px;
  background: var(--active-color, #00faff);
  border-radius: 50%;
  box-shadow: 
    0 0 4px var(--active-color, #00faff),
    0 0 8px var(--active-color, #00faff);
  animation: quantumFloat 4.5s ease-in-out infinite;
  animation-delay: 1s;
  z-index: 2;
  opacity: 0.7;
}

.orchos-quantum-btn:hover::after {
  width: 3px;
  height: 3px;
  opacity: 1;
}

.orchos-quantum-btn::before {
  content: '';
  position: absolute;
  top: 20%;
  right: 25%;
  width: 2px;
  height: 2px;
  background: var(--active-color, #00faff);
  border-radius: 50%;
  box-shadow: 
    0 0 4px var(--active-color, #00faff),
    0 0 8px var(--active-color, #00faff);
  animation: quantumFloat 5s ease-in-out infinite;
  animation-delay: 2.5s;
  z-index: 2;
  opacity: 0.7;
}

/* Record Button Specific */

.orchos-quantum-btn.orchos-btn-record {
  --active-color: var(--quantum-record-primary);
}

.orchos-quantum-btn.orchos-btn-send:hover {
  box-shadow: 
    0 0 40px rgba(0, 250, 255, 0.5),
    inset 0 0 25px rgba(0, 250, 255, 0.3),
    0 8px 25px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-btn.orchos-btn-record:hover {
  box-shadow: 
    0 0 40px rgba(255, 77, 210, 0.5),
    inset 0 0 25px rgba(255, 77, 210, 0.3),
    0 8px 25px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-btn:active::after,
.orchos-quantum-btn:active::before {
  animation: synapticBurst 0.5s ease-out forwards;
}

/* Send Button Specific */

.orchos-quantum-btn.orchos-btn-send {
  --active-color: var(--quantum-send-primary);
}

.orchos-quantum-btn.orchos-btn-send:hover .orchos-quantum-icon {
  filter: drop-shadow(0 0 8px var(--quantum-send-active));
  transform: translateX(2px);
  transition: transform 0.3s var(--quantum-cubic);
}

.orchos-btn-record:hover .neural-wave {
  animation-duration: 4s;
  opacity: 0.9;
}

.orchos-btn-send:hover .neural-send-wave {
  animation-duration: 3s;
  opacity: 0.9;
}

.orchos-btn-send:hover .neural-send-arrow {
  animation-duration: 1.5s;
  opacity: 1;
}

.settings-btn {
  width: 34px !important;
  height: 34px !important;
  padding: 0 !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  color: var(--theme-cyan) !important;
  cursor: pointer !important;
  border-radius: 50% !important;
  background-color: rgba(0, 15, 30, 0.7) !important;
  border: 2px solid rgba(0, 250, 255, 0.5) !important;
  box-shadow: 0 0 8px rgba(0, 250, 255, 0.4), inset 0 0 10px rgba(0, 250, 255, 0.1) !important;
  transition: all 0.3s ease !important;
  -webkit-backdrop-filter: blur(2px) !important;
  backdrop-filter: blur(2px) !important;
}

.settings-btn:hover {
  transform: scale(1.05);
  background-color: rgba(0, 40, 70, 0.9) !important;
  box-shadow: 0 0 15px rgba(0, 250, 255, 0.7), inset 0 0 12px rgba(0, 250, 255, 0.2) !important;
  border-color: rgba(0, 250, 255, 0.8) !important;
}

.settings-btn svg {
  width: 20px !important;
  height: 20px !important;
  filter: drop-shadow(0 0 3px rgba(0, 250, 255, 0.6)) !important;
}

.settings-btn {
  background-color: rgba(0, 15, 30, 0.7) !important;
  border: 2px solid rgba(0, 250, 255, 0.5) !important;
  box-shadow: 0 0 8px rgba(0, 250, 255, 0.4), inset 0 0 10px rgba(0, 250, 255, 0.1) !important;
}

.settings-btn:hover {
  background-color: rgba(0, 30, 60, 0.9) !important;
  border-color: rgba(0, 250, 255, 0.8) !important;
  box-shadow: 0 0 15px rgba(0, 250, 255, 0.7), inset 0 0 12px rgba(0, 250, 255, 0.2) !important;
}

.settings-btn svg {
  filter: drop-shadow(0 0 3px #00faff);
}

.settings-btn:hover svg circle:nth-of-type(1) {
  animation: qNodePulse 2s infinite alternate ease-in-out;
}

.settings-btn:hover svg circle:nth-of-type(2) {
  animation: orbitalSpin 8s infinite linear;
}

.settings-btn:hover svg circle:nth-of-type(3),
.settings-btn:hover svg circle:nth-of-type(4),
.settings-btn:hover svg circle:nth-of-type(5),
.settings-btn:hover svg circle:nth-of-type(6) {
  animation: connectionPulse 3s infinite alternate ease-in-out;
}

.orchos-quantum-btn {
  margin: 0 3px;
}

.orchos-btn-circle {
  box-sizing: border-box;
}

/* Record Button - Converging Neural Waves */

/* Send Button - Diverging Neural Waves */

/* Quantum Ripple Effect */

.orchos-quantum-ripple {
  position: absolute;
  border-radius: 50%;
  transform: translate(-50%, -50%);
  background: radial-gradient(circle, 
    rgba(0, 250, 255, 0.4) 0%, 
    rgba(124, 77, 255, 0.2) 50%, 
    transparent 70%
  );
  pointer-events: none;
  animation: quantumRippleExpand 1s ease-out forwards;
}

.orchos-quantum-btn:focus {
  outline: none;
  box-shadow: 
    0 0 50px rgba(0, 250, 255, 0.5),
    inset 0 0 30px rgba(0, 250, 255, 0.2),
    0 8px 30px rgba(0, 0, 0, 0.6),
    0 0 0 3px rgba(0, 250, 255, 0.3);
}

.orchos-quantum-btn:focus-visible {
  outline: 2px solid #00faff;
  outline-offset: 4px;
}

@media (prefers-reduced-motion: reduce) {
  .orchos-quantum-btn,
  .orchos-quantum-btn::before,
  .orchos-neural-ring,
  .orchos-quantum-particles::before,
  .orchos-quantum-particles::after {
    animation: none !important;
    transition: none !important;
  }
  
  .orchos-quantum-btn {
    transition: transform 0.2s ease;
  }
}
/* Chat History Sidebar Styles */
.chat-history-sidebar {
  height: 100%;
  background: rgba(9, 9, 11, 0.95);
  -webkit-backdrop-filter: blur(20px);
  backdrop-filter: blur(20px);
  border-right: 1px solid rgba(124, 77, 255, 0.2);
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

/* Sidebar Header */
.sidebar-header {
  padding: 1.5rem 1rem;
  border-bottom: 1px solid rgba(124, 77, 255, 0.1);
  display: flex;
  align-items: center;
  justify-content: space-between;
  flex-shrink: 0;
}

.sidebar-title {
  margin: 0;
  font-size: 1.25rem;
  font-weight: 600;
  color: #ffffff;
  background: linear-gradient(135deg, #7c4dff 0%, #00faff 100%);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}

.new-chat-button {
  width: 36px;
  height: 36px;
  border-radius: 50%;
  background: linear-gradient(135deg, #7c4dff 0%, #00faff 100%);
  border: none;
  color: #ffffff;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 0.3s ease;
  position: relative;
  overflow: hidden;
}

.new-chat-button::before {
  content: '';
  position: absolute;
  top: 50%;
  left: 50%;
  width: 0;
  height: 0;
  border-radius: 50%;
  background: rgba(255, 255, 255, 0.2);
  transform: translate(-50%, -50%);
  transition: width 0.3s, height 0.3s;
}

.new-chat-button:hover::before {
  width: 100%;
  height: 100%;
}

.new-chat-button:hover {
  transform: scale(1.05);
  box-shadow: 0 4px 20px rgba(124, 77, 255, 0.4);
}

/* Disabled state for new chat button */
.new-chat-button:disabled,
.new-chat-button.disabled {
  opacity: 0.5;
  cursor: not-allowed;
  background: rgba(124, 77, 255, 0.3);
}

.new-chat-button:disabled:hover,
.new-chat-button.disabled:hover {
  transform: none;
  box-shadow: none;
}

.new-chat-button:disabled::before,
.new-chat-button.disabled::before {
  display: none;
}

/* Search Section */
.sidebar-search {
  padding: 1rem;
  position: relative;
  flex-shrink: 0;
}

.search-input {
  width: 100%;
  padding: 0.75rem 2.5rem 0.75rem 1rem;
  background: rgba(124, 77, 255, 0.05);
  border: 1px solid rgba(124, 77, 255, 0.2);
  border-radius: 12px;
  color: #ffffff;
  font-size: 0.875rem;
  transition: all 0.3s ease;
  outline: none;
}

.search-input::placeholder {
  color: rgba(255, 255, 255, 0.4);
}

.search-input:focus {
  background: rgba(124, 77, 255, 0.1);
  border-color: #7c4dff;
  box-shadow: 0 0 0 3px rgba(124, 77, 255, 0.1);
}

.search-icon {
  position: absolute;
  right: 1.75rem;
  top: 50%;
  transform: translateY(-50%);
  color: rgba(255, 255, 255, 0.4);
  pointer-events: none;
}

/* Conversations List */
.conversations-list {
  flex: 1;
  overflow-y: auto;
  padding: 0.5rem;
}

.conversations-list::-webkit-scrollbar {
  width: 6px;
}

.conversations-list::-webkit-scrollbar-track {
  background: transparent;
}

.conversations-list::-webkit-scrollbar-thumb {
  background: rgba(124, 77, 255, 0.3);
  border-radius: 3px;
}

.conversations-list::-webkit-scrollbar-thumb:hover {
  background: rgba(124, 77, 255, 0.5);
}

/* Conversation Item */
.conversation-item {
  padding: 1rem;
  margin-bottom: 0.5rem;
  background: rgba(124, 77, 255, 0.05);
  border: 1px solid transparent;
  border-radius: 12px;
  cursor: pointer;
  transition: all 0.3s ease;
  position: relative;
  overflow: hidden;
}

.conversation-item::before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background: linear-gradient(90deg, 
    transparent,
    rgba(124, 77, 255, 0.1),
    transparent
  );
  transition: left 0.5s;
}

.conversation-item:hover::before {
  left: 100%;
}

.conversation-item:hover {
  background: rgba(124, 77, 255, 0.1);
  border-color: rgba(124, 77, 255, 0.2);
  transform: translateX(4px);
}

.conversation-item.active {
  background: linear-gradient(135deg, rgba(124, 77, 255, 0.15) 0%, rgba(0, 250, 255, 0.1) 100%);
  border-color: rgba(124, 77, 255, 0.3);
  box-shadow: 0 0 20px rgba(124, 77, 255, 0.2);
}

.conversation-content {
  position: relative;
  z-index: 1;
  padding-right: 2rem;
}

.conversation-title {
  margin: 0 0 0.25rem 0;
  font-size: 0.9rem;
  font-weight: 500;
  color: #ffffff;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.conversation-preview {
  margin: 0 0 0.5rem 0;
  font-size: 0.8rem;
  color: rgba(255, 255, 255, 0.6);
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.conversation-time {
  font-size: 0.75rem;
  color: rgba(255, 255, 255, 0.4);
}

/* Delete Button */
.delete-button {
  position: absolute;
  top: 1rem;
  right: 1rem;
  width: 28px;
  height: 28px;
  border-radius: 50%;
  background: rgba(239, 68, 68, 0.1);
  border: 1px solid rgba(239, 68, 68, 0.2);
  color: #ef4444;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 0.3s ease;
  z-index: 2;
}

.delete-button:hover {
  background: rgba(239, 68, 68, 0.2);
  border-color: #ef4444;
  transform: scale(1.1);
}

/* No Conversations */
.no-conversations {
  text-align: center;
  padding: 3rem 1rem;
  color: rgba(255, 255, 255, 0.4);
  font-size: 0.875rem;
}

/* Main Dashboard Layout Update */
.orchos-quantum-dashboard.with-sidebar {
  display: grid;
  grid-template-columns: 280px 1.618fr 1fr;
  gap: 0;
  height: 100%;
}

.orchos-quantum-dashboard.with-sidebar.single-column {
  grid-template-columns: 280px 1fr;
}

/* Mobile Responsiveness */
@media (max-width: 768px) {
  .orchos-quantum-dashboard.with-sidebar {
    grid-template-columns: 1fr;
  }
  
  .chat-history-sidebar {
    position: fixed;
    left: -280px;
    top: 0;
    width: 280px;
    height: 100vh;
    z-index: 1000;
    transition: left 0.3s ease;
  }
  
  .chat-history-sidebar.mobile-open {
    left: 0;
    box-shadow: 2px 0 20px rgba(0, 0, 0, 0.5);
  }
  
  /* Mobile toggle button */
  .mobile-sidebar-toggle {
    position: fixed;
    left: 1rem;
    bottom: 1rem;
    width: 56px;
    height: 56px;
    border-radius: 50%;
    background: linear-gradient(135deg, #7c4dff 0%, #00faff 100%);
    border: none;
    color: #ffffff;
    cursor: pointer;
    display: none;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 20px rgba(124, 77, 255, 0.4);
    z-index: 999;
  }
  
  @media (max-width: 768px) {
    .mobile-sidebar-toggle {
      display: flex;
    }
  }
} /* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

.orchos-header-glass {
  position: relative;
  z-index: 30;
  background: linear-gradient(90deg, #191a2b 80%, #232447 100%);
  box-shadow: 0 8px 48px 0 #00f0ff22;
  -webkit-backdrop-filter: blur(3.5px) saturate(1.15);
  backdrop-filter: blur(3.5px) saturate(1.15);
}

/* Modal de Logs de Cognição - layout robusto e responsivo */
.orchos-cognition-logs-modal {
  display: flex;
  flex-direction: column;
  width: 100%;
  min-width: 350px;
  max-width: 600px;
  max-height: 92vh;
  background: linear-gradient(135deg, rgba(24,24,40,0.97) 70%, rgba(124,77,255,0.08) 100%);
  border-radius: 1.3rem;
  box-shadow: 0 12px 48px 0 #7c4dff33, 0 0 0 2.5px #7c4dffcc, 0 0 48px 6px #7c4dff44;
  border: 2.5px solid #7c4dffcc;
  padding: 1.5rem 1.5rem 1.2rem 1.5rem;
  overflow: hidden;
  position: relative;
  animation: modalFadeIn 0.33s cubic-bezier(.33,1.4,.44,1) both;
}

@keyframes modalFadeIn {
  0% { opacity: 0; transform: scale(0.96); }
  100% { opacity: 1; transform: scale(1); }
}

.orchos-cognition-logs-header button {
  box-shadow: 0 0 8px #7c4dff88, 0 0 0 2px #7c4dff44;
  transition: box-shadow 0.18s, background 0.18s;
}
.orchos-cognition-logs-header button:hover {
  background: #7c4dff22;
  box-shadow: 0 0 16px #7c4dffcc, 0 0 0 3px #7c4dff88;
}

.orchos-cognition-logs-buttons button {
  position: relative;
  overflow: hidden;
}
.orchos-cognition-logs-buttons button::after {
  content: '';
  display: block;
  position: absolute;
  left: 50%;
  top: 50%;
  width: 0;
  height: 0;
  background: radial-gradient(circle, #00fff7bb 0%, #7c4dff44 80%, transparent 100%);
  opacity: 0.34;
  border-radius: 50%;
  transform: translate(-50%, -50%) scale(0.7);
  transition: width 0.28s cubic-bezier(.33,1.4,.44,1), height 0.28s cubic-bezier(.33,1.4,.44,1), opacity 0.22s;
  pointer-events: none;
}
.orchos-cognition-logs-buttons button:active::after {
  width: 180%;
  height: 180%;
  opacity: 0.17;
  transition: width 0.12s, height 0.12s, opacity 0.12s;
}

@media (max-width: 800px) {
  .orchos-cognition-logs-modal {
    max-width: 96vw;
    padding: 1.1rem 0.7rem 1rem 0.7rem;
  }
}


.orchos-cognition-logs-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 1.5rem;
  gap: 1rem;
}

.orchos-cognition-logs-buttons {
  display: flex;
  gap: 1rem;
  flex-wrap: wrap;
  margin-bottom: 1.3rem;
}

.orchos-cognition-logs-list {
  flex: 1 1 auto;
  min-height: 120px;
  max-height: 60vh;
  overflow-y: auto;
  padding: 1.2rem 0.7rem 0.2rem 0.7rem;
  background: transparent;
  border-radius: 0.9rem;
  box-shadow: none;
  border: none;
  -webkit-backdrop-filter: none;
  backdrop-filter: none;
}

@media (max-width: 600px) {
  .orchos-cognition-logs-modal {
    padding: 1.1rem 0.7rem 1rem 0.7rem;
    min-width: 0;
    max-width: 99vw;
  }
  .orchos-cognition-logs-list {
    padding: 0.5rem 0.1rem 0.1rem 0.1rem;
    max-height: 46vh;
  }
}

/* Neural text inputs with rounded borders */

textarea.orchos-textarea-neural,
.orchos-textarea-neural {
  min-height: 60px;
  max-height: 60px;
  border-radius: 1.1rem !important;
  background: rgba(10, 20, 40, 0.88);
  border: 2px solid #00faff33;
  color: #e3f6ff;
  font-size: 1.08rem;
  padding: 0.7rem 1rem;
  margin-bottom: 0;
  transition: border-color 0.18s, box-shadow 0.18s;
  box-shadow: 0 2px 12px 0 #00faff22;
  outline: none;
  resize: none;
}


.orchos-textarea-neural:focus {
  border-color: #00faff;
  box-shadow: 0 0 0 2px #00faff44;
}

.orchos-btn-circular {
  background: rgba(24, 24, 40, 0.60);
  border: 2px solid #00faff;
  box-shadow: 0 2px 16px 0 rgba(0,245,255,0.12);
  color: #00faff;
  min-width: 48px;
  min-height: 48px;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: box-shadow 0.18s, background 0.18s, border-color 0.18s;
}

/* Quantum Dashboard Layout - Enhanced */

.orchos-quantum-dashboard {
  display: grid;
  grid-template-columns: 1.618fr 1fr; /* Golden ratio split */
  gap: 1.2rem; /* Gap uniforme entre visualização e cards */
  width: 100%;
  height: 100%;
  flex: 1 1 auto;
  max-width: 100%;
  max-height: 100%;
  margin: 0;
  padding: 0.5rem;
  background: 
    radial-gradient(ellipse at 20% 30%, rgba(0, 250, 255, 0.03) 0%, transparent 50%),
    radial-gradient(ellipse at 80% 70%, rgba(255, 0, 128, 0.02) 0%, transparent 50%),
    radial-gradient(ellipse at center, #0a0f1a 0%, #000511 100%);
  overflow: hidden;
  position: relative;
  box-sizing: border-box;
}

/* Override background when showing only chat (single column) */
.orchos-quantum-dashboard.single-column {
  background: #0a0f1a !important;
}

/* Hide pseudo-element when showing only chat */
.orchos-quantum-dashboard.single-column::before {
  display: none !important;
}

/* Ensure all child elements have solid background in single-column mode */
.orchos-quantum-dashboard.single-column * {
  background-color: transparent !important;
}

.orchos-quantum-dashboard.single-column .neural-chat-zone {
  background: #0a0f1a !important;
}

.orchos-quantum-dashboard::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-image: 
    radial-gradient(circle at 20% 50%, rgba(0, 250, 255, 0.1) 0%, transparent 2%),
    radial-gradient(circle at 80% 80%, rgba(255, 0, 128, 0.08) 0%, transparent 2%),
    radial-gradient(circle at 40% 80%, rgba(120, 119, 198, 0.1) 0%, transparent 2%);
  background-size: 400px 400px;
  animation: quantumDrift 20s ease-in-out infinite;
  pointer-events: none;
  opacity: 0.4;
}

/* Quantum Visualization Zone - Holographic Container */

.quantum-visualization-zone {
  position: relative;
  background: 
    linear-gradient(135deg, rgba(0,20,40,0.2) 0%, transparent 40%),
    radial-gradient(circle at center, rgba(0,20,40,0.4) 0%, transparent 70%);
  border-radius: 1rem;
  padding: 0;
  display: flex;
  align-items: center;
  justify-content: center;
  overflow: hidden;
  border: 1px solid rgba(0,250,255,0.2);
  box-shadow: 
    inset 0 0 40px rgba(0,250,255,0.08),
    0 0 60px rgba(0,250,255,0.05),
    0 8px 32px rgba(0,0,0,0.4);
  height: 100%;
  -webkit-backdrop-filter: blur(10px);
  backdrop-filter: blur(10px);
}

/* Card horizontal para resposta AI (ocupa duas colunas do grid) */
.neural-control-grid [data-type="ai"] {
  grid-column: 1 / span 2;
  width: 100%;
}

/* Neural Control Grid - Advanced Glassmorphism */

.neural-control-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  grid-template-rows: min-content min-content;
  gap: 0.5rem;
  height: auto;
  padding: 0;
}

/* Enhanced Glassmorphic Cards */

/* Card hover state - elevated glassmorphism */

/* Card headers - futuristic design */

/* Add scanning line effect to headers */

/* Card body - improved spacing */

/* Custom scrollbar - quantum style */

/* Ensure main body doesn't scroll */

body.orchos-active {
  overflow: hidden;
}

/* Header positioning - ultra compact */

/* Header title with glow */

/* Header button styling */

.neural-control-grid button {
  padding: 0.32rem 0.9rem !important;
  font-size: 0.74rem !important;
  min-height: 1.7rem !important;
  height: auto !important;
  background: 
    linear-gradient(
      135deg, 
      rgba(255, 255, 255, 0.03) 0%, 
      rgba(255, 255, 255, 0.01) 100%
    );
  border: 1px solid rgba(255, 255, 255, 0.07);
  color: rgba(255, 255, 255, 0.88);
  font-weight: 500;
  letter-spacing: 0.02em;
  transition: all 0.3s cubic-bezier(0.23, 1, 0.32, 1);
  position: relative;
  overflow: hidden;
  border-radius: 0.6rem;
  -webkit-backdrop-filter: blur(16px);
  backdrop-filter: blur(16px);
  transform-style: preserve-3d;
}

.neural-control-grid button::before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background: linear-gradient(90deg, transparent, rgba(0, 250, 255, 0.2), transparent);
  transition: left 0.5s ease;
}

.neural-control-grid button:hover {
  background: linear-gradient(135deg, rgba(0, 250, 255, 0.2) 0%, rgba(0, 250, 255, 0.1) 100%);
  border-color: rgba(0, 250, 255, 0.5);
  box-shadow: 0 0 20px rgba(0, 250, 255, 0.3);
  transform: translateY(-1px);
}

.neural-control-grid button:hover::before {
  left: 100%;
}

/* Typography Enhancements */

.neural-control-grid .text-sm {
  font-size: 0.8rem !important;
  line-height: 1.4 !important;
  color: rgba(255, 255, 255, 0.8);
}

.neural-control-grid .text-xs {
  font-size: 0.7rem !important;
  line-height: 1.3 !important;
  color: rgba(255, 255, 255, 0.6);
  font-family: 'Monaco', 'Consolas', monospace;
}

/* Input and Textarea Styling */

.neural-control-grid textarea,
.neural-control-grid input {
  font-size: 0.8rem !important;
  line-height: 1.4 !important;
  background: rgba(0, 0, 0, 0.3);
  border: 1px solid rgba(0, 250, 255, 0.2);
  color: rgba(255, 255, 255, 0.9);
  padding: 0.5rem;
  border-radius: 0.5rem;
  transition: all 0.3s ease;
  font-family: 'Monaco', 'Consolas', monospace;
}

.neural-control-grid textarea:focus,
.neural-control-grid input:focus {
  outline: none;
  border-color: rgba(0, 250, 255, 0.5);
  box-shadow: 0 0 0 2px rgba(0, 250, 255, 0.1);
  background: rgba(0, 0, 0, 0.5);
}

/* Legacy grid - keeping for backwards compatibility */

/* Ensure quantum visualization takes full space */

.quantum-visualization-zone > div {
  width: 100% !important;
  height: 100% !important;
}

/* Neural Chat Zone - Isolated background */
.neural-chat-zone {
  position: relative;
  background: #0a0f1a; /* Solid background matching the dashboard center color */
  isolation: isolate; /* Create new stacking context */
}

/* Add a pseudo-element to ensure complete coverage */
.neural-chat-zone::before {
  content: '';
  position: absolute;
  top: -10px;
  left: -10px;
  right: -10px;
  bottom: -10px;
  background: #0a0f1a;
  z-index: -1;
  pointer-events: none;
}

/* Recording control inside card */

/* Transcription card button layout */

/* Data Visualization Elements */

.neural-control-grid button {
  position: relative;
  overflow: hidden;
}

/* Enhanced Focus States */

.neural-control-grid button:focus,
.neural-control-grid input:focus,
.neural-control-grid textarea:focus {
  outline: none;
  box-shadow: 
    0 0 0 2px rgba(0, 250, 255, 0.3),
    0 0 20px rgba(0, 250, 255, 0.2);
}

/* Loading States */

/* Ensure cards use all available space */

.neural-control-grid > * {
  min-height: 0;
  min-width: 0;
  overflow: hidden;
  min-height: 0;
  overflow: hidden;
}

/* Responsive adjustments */

@media (max-height: 800px) {
  .orchos-quantum-dashboard {
    padding: 0.2rem;
    gap: 0.2rem;
    height: calc(100vh - 2.5rem);
    padding-top: 2.5rem;
  }
  
  .neural-control-grid {
    gap: 0.2rem;
  }
  
  .neural-control-grid button {
    padding: 0.15rem 0.4rem !important;
    font-size: 0.65rem !important;
    min-height: 1.4rem !important;
  }
}

/* ============================= */

/* ============================= */

/* Sistema Neural-Quântico de Profundidade */

.orchos-tooltip {
  position: absolute;
  left: 50%;
  bottom: 110%;
  transform: translateX(-50%) scale(0.95);
  min-width: 80px;
  max-width: 140px;
  background: rgba(18, 24, 38, 0.85);
  -webkit-backdrop-filter: blur(8px) saturate(120%);
  backdrop-filter: blur(8px) saturate(120%);
  color: #fff;
  font-size: 1rem;
  font-family: inherit;
  text-align: center;
  padding: 0.4em 1em;
  border-radius: 1.2em;
  box-shadow: 0 4px 24px 0 rgba(0,250,255,0.10), 0 1.5px 4px 0 rgba(124,77,255,0.09);
  opacity: 0;
  pointer-events: none;
  z-index: 10;
  transition: opacity 0.3s var(--quantum-cubic), transform 0.3s var(--quantum-cubic);
  white-space: pre-line;
  border: 1px solid rgba(124, 77, 255, 0.2);
  letter-spacing: 0.03em;
}

/* Sistema Neural Biomimético - Respiração */

/* Interação Neural-Magnética */

.orchos-quantum-btn-inner {
  position: relative;
  width: 100%;
  height: 100%;
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 2;
}

/* Sistema Neural de Partículas Quânticas */

/* Adiciona 6 partículas quânticas com posições e animações variadas */

/* Geração de partículas neurais adicionais */

/* Duplica as partículas em hover */

/* Animação para partículas neurais ativas */

/* Aplicar animação mais intensa quando gravando */

/* Efeito neural para botão de envio (hover/focus) */

/* Efeito neural para botão de gravação (hover/focus) */

/* Efeito sináptico ao clicar */

/* Hover Animations for Neural Wave Patterns */

/* Container do botão de configurações */

.settings-container {
  display: flex;
  align-items: center;
  height: 100%;
  margin-right: -4px;
}

/* Estilos para o botão de configurações neural-quântico */

.settings-btn {
  width: 34px !important;
  height: 34px !important;
  padding: 0 !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  color: var(--theme-cyan) !important;
  cursor: pointer !important;
  border-radius: 50% !important;
  background-color: rgba(0, 15, 30, 0.7) !important;
  border: 2px solid rgba(0, 250, 255, 0.5) !important;
  box-shadow: 0 0 8px rgba(0, 250, 255, 0.4), inset 0 0 10px rgba(0, 250, 255, 0.1) !important;
  transition: all 0.3s ease !important;
  -webkit-backdrop-filter: blur(2px) !important;
  backdrop-filter: blur(2px) !important;
}

/* Estilização para o popup de configurações neural-quântico */

.neural-settings-popup {
  background-color: rgba(0, 15, 30, 0.95);
  border: 2px solid rgba(0, 250, 255, 0.5);
  box-shadow: 0 5px 25px rgba(0, 0, 0, 0.7), 0 0 20px rgba(0, 250, 255, 0.3);
  -webkit-backdrop-filter: blur(15px);
  backdrop-filter: blur(15px);
  min-width: 250px;
  z-index: 100;
  position: absolute;
  top: calc(100% + 10px);
  right: 0;
  padding: 14px 16px;
  max-height: 60vh;
  overflow-y: auto;

  /* Usando display: flex em vez de block para permitir alinhamento vertical */
  display: flex;
  flex-direction: column;
  border-radius: 12px;
  margin-top: 0;
}


/* Animações do botão de configurações */

/* Animação do círculo central */

/* Animação do orbital */

/* Animação dos pontos de conexão */

/* Efeito de abertura do popup */

/* Estilos de controle para o posicionamento do popover de configurações */

/* Refinamentos para posicionamento global dos botões quantum */

/* Garantir que o botão de configurações não desloque o layout */

/* Icon Animations */

/* Neural Wave Pattern Animations */

/* Neural Core Animations */

/* Active State Enhanced Animations */

/* Focus States */

/* Reduced Motion */
/* ================================================================
 * OVERRIDE STYLES - Single Column Mode
 * ================================================================
 * These styles override the quantum dashboard background when
 * showing only the chat (without quantum visualization)
 */

/* Force solid background in single-column mode */
.orchos-quantum-dashboard.single-column {
  background: #0a0f1a !important;
  background-image: none !important;
}

/* Remove all pseudo-elements and animations in single-column mode */
.orchos-quantum-dashboard.single-column::before,
.orchos-quantum-dashboard.single-column::after {
  display: none !important;
  content: none !important;
  background: none !important;
  background-image: none !important;
}

/* Ensure neural-chat-zone has solid background */
.orchos-quantum-dashboard.single-column .neural-chat-zone {
  background: #0a0f1a !important;
  background-image: none !important;
}

/* Remove any background from child elements that might leak */
.orchos-quantum-dashboard.single-column .conversational-chat,
.orchos-quantum-dashboard.single-column .chat-messages-container,
.orchos-quantum-dashboard.single-column .chat-messages,
.orchos-quantum-dashboard.single-column .messages-wrapper {
  background: #0a0f1a !important;
  background-image: none !important;
} /* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

.settings-container {
  display: flex;
  align-items: center;
  height: 100%;
  margin-right: -4px;
}

.settings-btn {
  width: 34px !important;
  height: 34px !important;
  padding: 0 !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  color: var(--theme-cyan) !important;
  cursor: pointer !important;
  border-radius: 50% !important;
  background-color: rgba(0, 15, 30, 0.7) !important;
  border: 2px solid rgba(0, 250, 255, 0.5) !important;
  box-shadow: 0 0 8px rgba(0, 250, 255, 0.4), inset 0 0 10px rgba(0, 250, 255, 0.1) !important;
  transition: all 0.3s ease !important;
  -webkit-backdrop-filter: blur(2px) !important;
  backdrop-filter: blur(2px) !important;
}

.settings-btn:hover {
  transform: scale(1.05);
  background-color: rgba(0, 40, 70, 0.9) !important;
  box-shadow: 0 0 15px rgba(0, 250, 255, 0.7), inset 0 0 12px rgba(0, 250, 255, 0.2) !important;
  border-color: rgba(0, 250, 255, 0.8) !important;
}

.settings-btn svg {
  width: 20px !important;
  height: 20px !important;
  filter: drop-shadow(0 0 3px rgba(0, 250, 255, 0.6)) !important;
}

.neural-settings-popup {
  background-color: rgba(0, 15, 30, 0.95);
  border: 2px solid rgba(0, 250, 255, 0.5);
  box-shadow: 0 5px 25px rgba(0, 0, 0, 0.7), 0 0 20px rgba(0, 250, 255, 0.3);
  -webkit-backdrop-filter: blur(15px);
  backdrop-filter: blur(15px);
  min-width: 250px;
  z-index: 100;
  position: absolute;
  top: calc(100% + 10px);
  right: 0;
  padding: 14px 16px;
  

/* Usando display: flex em vez de block para permitir alinhamento vertical */


  display: flex;
  flex-direction: column;
  overflow: hidden;
  border-radius: 12px;
  margin-top: 0;
}

.settings-btn {
  background-color: rgba(0, 15, 30, 0.7) !important;
  border: 2px solid rgba(0, 250, 255, 0.5) !important;
  box-shadow: 0 0 8px rgba(0, 250, 255, 0.4), inset 0 0 10px rgba(0, 250, 255, 0.1) !important;
}

.settings-btn:hover {
  background-color: rgba(0, 30, 60, 0.9) !important;
  border-color: rgba(0, 250, 255, 0.8) !important;
  box-shadow: 0 0 15px rgba(0, 250, 255, 0.7), inset 0 0 12px rgba(0, 250, 255, 0.2) !important;
}

.settings-btn svg {
  filter: drop-shadow(0 0 3px #00faff);
}

.settings-btn:hover svg circle:nth-of-type(1) {
  animation: qNodePulse 2s infinite alternate ease-in-out;
}

.settings-btn:hover svg circle:nth-of-type(2) {
  animation: orbitalSpin 8s infinite linear;
}

.settings-btn:hover svg circle:nth-of-type(3),
.settings-btn:hover svg circle:nth-of-type(4),
.settings-btn:hover svg circle:nth-of-type(5),
.settings-btn:hover svg circle:nth-of-type(6) {
  animation: connectionPulse 3s infinite alternate ease-in-out;
}

.neural-settings-popup {
  animation: settingsPopupReveal 0.3s ease-out forwards;
  transform-origin: top right;
}
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

.orchos-tooltip {
  position: absolute;
  left: 50%;
  bottom: 110%;
  transform: translateX(-50%) scale(0.95);
  min-width: 80px;
  max-width: 140px;
  background: rgba(18, 24, 38, 0.85);
  -webkit-backdrop-filter: blur(8px) saturate(120%);
  backdrop-filter: blur(8px) saturate(120%);
  color: #fff;
  font-size: 1rem;
  font-family: inherit;
  text-align: center;
  padding: 0.4em 1em;
  border-radius: 1.2em;
  box-shadow: 0 4px 24px 0 rgba(0,250,255,0.10), 0 1.5px 4px 0 rgba(124,77,255,0.09);
  opacity: 0;
  pointer-events: none;
  z-index: 10;
  transition: opacity 0.3s var(--quantum-cubic), transform 0.3s var(--quantum-cubic);
  white-space: pre-line;
  border: 1px solid rgba(124, 77, 255, 0.2);
  letter-spacing: 0.03em;
}

/* Animação Neural da Tooltip */

.orchos-tooltip::after {
  content: '';
  position: absolute;
  width: 100%;
  height: 100%;
  left: 0;
  top: 0;
  border-radius: inherit;
  background: linear-gradient(135deg, 
    rgba(255, 77, 210, 0.1) 0%, 
    rgba(0, 250, 255, 0.1) 50%,
    rgba(124, 77, 255, 0.1) 100%);
  opacity: 0;
  z-index: -1;
  animation: neuralGradient 8s ease-in-out infinite;
}

.orchos-quantum-btn:hover .orchos-tooltip,
.orchos-quantum-btn:focus .orchos-tooltip,
.orchos-quantum-btn:focus-visible .orchos-tooltip {
  opacity: 1;
  transform: translateX(-50%) scale(1);
  pointer-events: auto;
  box-shadow: 
    0 8px 32px 0 rgba(0,250,255,0.15), 
    0 2px 8px 0 rgba(124,77,255,0.15),
    inset 0 0 8px 0 rgba(0,250,255,0.1);
}

/* Tooltip Arrow */

.orchos-tooltip::before {
  content: '';
  position: absolute;
  left: 50%;
  bottom: -5px;
  transform: translateX(-50%) rotate(45deg);
  width: 10px;
  height: 10px;
  background: inherit;
  -webkit-backdrop-filter: inherit;
  backdrop-filter: inherit;
  border-right: 1px solid rgba(124, 77, 255, 0.2);
  border-bottom: 1px solid rgba(124, 77, 255, 0.2);
  z-index: -1;
}
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

:root {
  --quantum-depth-2: 5px;
  --quantum-cubic: cubic-bezier(0.34, 1.56, 0.64, 1);
  
  

/* Paleta Neural-Quântica */


  --quantum-record-primary: rgba(255, 77, 210, 0.8);
  --quantum-record-active: rgba(255, 77, 210, 1);
  
  --quantum-send-primary: rgba(0, 250, 255, 0.8);
  --quantum-send-active: rgba(0, 250, 255, 1);
}
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

textarea.orchos-textarea-neural,
.orchos-textarea-neural {
  border-radius: 1.1rem !important;
  background: rgba(10, 20, 40, 0.88);
  border: 2px solid #00faff33;
  color: #e3f6ff;
  font-size: 1.08rem;
  padding: 1.1rem 1.2rem;
  transition: border-color 0.18s, box-shadow 0.18s;
  box-shadow: 0 2px 12px 0 #00faff22;
  outline: none;
  resize: none;
}

.orchos-textarea-neural:focus {
  border-color: #00faff;
  box-shadow: 0 0 0 2px #00faff44;
}

.orchos-btn-circular, .orchos-btn-fab, .orchos-btn-glass {
  border-radius: 999px !important;
}

.orchos-btn-fab {
  background: rgba(24,24,40,0.82);
  border: 3px solid #00faff;
  box-shadow: 0 0 24px 8px #00faff33, 0 0 0 0 #fff0;
  color: #00faff;
  font-weight: 600;
  font-size: 1.1rem;
  transition: box-shadow 0.22s, background 0.22s, border-color 0.22s;
}

.orchos-btn-fab:focus, .orchos-btn-fab:hover {
  background: rgba(0, 245, 255, 0.12);
  box-shadow: 0 0 36px 12px #00faff77, 0 0 0 3px #00faff55;
  border-color: #00faff;
}

.orchos-btn-circular {
  background: rgba(24, 24, 40, 0.60);
  border: 2px solid #00faff;
  box-shadow: 0 2px 16px 0 rgba(0,245,255,0.12);
  color: #00faff;
  min-width: 48px;
  min-height: 48px;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: box-shadow 0.18s, background 0.18s, border-color 0.18s;
}

.orchos-btn-circular:focus, .orchos-btn-circular:hover {
  background: rgba(0, 245, 255, 0.12);
  box-shadow: 0 0 16px 4px #00faff77;
  border-color: #00faff;
}

.orchos-quantum-dashboard {
  display: grid;
  grid-template-columns: 1.618fr 1fr; 

/* Golden ratio split */


  gap: 0.75rem;
  height: 100vh;
  padding: 0.5rem;
  padding-top: 0.5rem;
  background: 
    radial-gradient(ellipse at 20% 30%, rgba(0, 250, 255, 0.03) 0%, transparent 50%),
    radial-gradient(ellipse at 80% 70%, rgba(255, 0, 128, 0.02) 0%, transparent 50%),
    radial-gradient(ellipse at center, #0a0f1a 0%, #000511 100%);
  overflow: hidden;
  position: relative;
  box-sizing: border-box;
}

/* Override background when showing only chat (single column) */
.orchos-quantum-dashboard.single-column {
  background: #0a0f1a !important;
}

/* Add subtle animated particles background */

.orchos-quantum-dashboard::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-image: 
    radial-gradient(circle at 20% 50%, rgba(0, 250, 255, 0.1) 0%, transparent 2%),
    radial-gradient(circle at 80% 80%, rgba(255, 0, 128, 0.08) 0%, transparent 2%),
    radial-gradient(circle at 40% 80%, rgba(120, 119, 198, 0.1) 0%, transparent 2%);
  background-size: 400px 400px;
  animation: quantumDrift 20s ease-in-out infinite;
  pointer-events: none;
  opacity: 0.4;
}

/* Hide pseudo-element when showing only chat */
.orchos-quantum-dashboard.single-column::before {
  display: none !important;
}

/* Quantum Visualization Zone - Holographic Container */

.quantum-visualization-zone {
  position: relative;
  background: 
    linear-gradient(135deg, rgba(0,20,40,0.2) 0%, transparent 40%),
    radial-gradient(circle at center, rgba(0,20,40,0.4) 0%, transparent 70%);
  border-radius: 1rem;
  padding: 0;
  display: flex;
  align-items: center;
  justify-content: center;
  overflow: hidden;
  border: 1px solid rgba(0,250,255,0.2);
  box-shadow: 
    inset 0 0 40px rgba(0,250,255,0.08),
    0 0 60px rgba(0,250,255,0.05),
    0 8px 32px rgba(0,0,0,0.4);
  height: 100%;
  -webkit-backdrop-filter: blur(10px);
  backdrop-filter: blur(10px);
}

/* Add holographic shimmer effect */

.quantum-visualization-zone::after {
  content: '';
  position: absolute;
  top: -50%;
  left: -50%;
  width: 200%;
  height: 200%;
  background: linear-gradient(
    45deg,
    transparent 30%,
    rgba(0, 250, 255, 0.03) 50%,
    transparent 70%
  );
  animation: holographicShimmer 8s linear infinite;
  pointer-events: none;
}

/* Quantum glow effects */

.quantum-visualization-zone::before {
  content: '';
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  width: 300px;
  height: 300px;
  background: radial-gradient(circle, rgba(0,250,255,0.15) 0%, transparent 70%);
  filter: blur(60px);
  animation: quantumPulse 4s ease-in-out infinite;
  pointer-events: none;
}

/* Neural Control Grid - Advanced Glassmorphism */

/* Enhanced Glassmorphic Cards */

/* Remover qualquer glow roxo externo (outline/shadow) */

/* Card hover state - elevated glassmorphism */

/* Add subtle glow on hover */

/* Icons with glow effect */

/* Header title with glow */

.neural-control-grid button {
  padding: 0.32rem 0.9rem !important;
  font-size: 0.74rem !important;
  min-height: 1.7rem !important;
  height: auto !important;
  background: 
    linear-gradient(
      135deg, 
      rgba(255, 255, 255, 0.03) 0%, 
      rgba(255, 255, 255, 0.01) 100%
    );
  border: 1px solid rgba(255, 255, 255, 0.07);
  color: rgba(255, 255, 255, 0.88);
  font-weight: 500;
  letter-spacing: 0.02em;
  transition: all 0.3s cubic-bezier(0.23, 1, 0.32, 1);
  position: relative;
  overflow: hidden;
  border-radius: 0.6rem;
  -webkit-backdrop-filter: blur(16px);
  backdrop-filter: blur(16px);
  transform-style: preserve-3d;
}

.neural-control-grid button::before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background: linear-gradient(90deg, transparent, rgba(0, 250, 255, 0.2), transparent);
  transition: left 0.5s ease;
}

.neural-control-grid button:hover {
  background: linear-gradient(135deg, rgba(0, 250, 255, 0.2) 0%, rgba(0, 250, 255, 0.1) 100%);
  border-color: rgba(0, 250, 255, 0.5);
  box-shadow: 0 0 20px rgba(0, 250, 255, 0.3);
  transform: translateY(-1px);
}

.neural-control-grid button:hover::before {
  left: 100%;
}

.neural-control-grid textarea,
.neural-control-grid input {
  font-size: 0.8rem !important;
  line-height: 1.4 !important;
  background: rgba(0, 0, 0, 0.3);
  border: 1px solid rgba(0, 250, 255, 0.2);
  color: rgba(255, 255, 255, 0.9);
  padding: 0.5rem;
  border-radius: 0.5rem;
  transition: all 0.3s ease;
  font-family: 'Monaco', 'Consolas', monospace;
}

.neural-control-grid textarea:focus,
.neural-control-grid input:focus {
  outline: none;
  border-color: rgba(0, 250, 255, 0.5);
  box-shadow: 0 0 0 2px rgba(0, 250, 255, 0.1);
  background: rgba(0, 0, 0, 0.5);
}

.ripple {
  position: absolute;
  border-radius: 50%;
  background: radial-gradient(circle, rgba(0, 250, 255, 0.3) 0%, transparent 70%);
  transform: scale(0);
  animation: rippleEffect 0.6s ease-out;
  pointer-events: none;
}

.neural-control-grid button:focus,
.neural-control-grid input:focus,
.neural-control-grid textarea:focus {
  outline: none;
  box-shadow: 
    0 0 0 2px rgba(0, 250, 255, 0.3),
    0 0 20px rgba(0, 250, 255, 0.2);
}

:root {
  --quantum-depth-2: 5px;
  --quantum-cubic: cubic-bezier(0.34, 1.56, 0.64, 1);
  
  

/* Paleta Neural-Quântica */


  --quantum-record-primary: rgba(255, 77, 210, 0.8);
  --quantum-record-active: rgba(255, 77, 210, 1);
  
  --quantum-send-primary: rgba(0, 250, 255, 0.8);
  --quantum-send-active: rgba(0, 250, 255, 1);
}

.orchos-tooltip {
  position: absolute;
  left: 50%;
  bottom: 110%;
  transform: translateX(-50%) scale(0.95);
  min-width: 80px;
  max-width: 140px;
  background: rgba(18, 24, 38, 0.85);
  -webkit-backdrop-filter: blur(8px) saturate(120%);
  backdrop-filter: blur(8px) saturate(120%);
  color: #fff;
  font-size: 1rem;
  font-family: inherit;
  text-align: center;
  padding: 0.4em 1em;
  border-radius: 1.2em;
  box-shadow: 0 4px 24px 0 rgba(0,250,255,0.10), 0 1.5px 4px 0 rgba(124,77,255,0.09);
  opacity: 0;
  pointer-events: none;
  z-index: 10;
  transition: opacity 0.3s var(--quantum-cubic), transform 0.3s var(--quantum-cubic);
  white-space: pre-line;
  border: 1px solid rgba(124, 77, 255, 0.2);
  letter-spacing: 0.03em;
}

.orchos-tooltip::after {
  content: '';
  position: absolute;
  width: 100%;
  height: 100%;
  left: 0;
  top: 0;
  border-radius: inherit;
  background: linear-gradient(135deg, 
    rgba(255, 77, 210, 0.1) 0%, 
    rgba(0, 250, 255, 0.1) 50%,
    rgba(124, 77, 255, 0.1) 100%);
  opacity: 0;
  z-index: -1;
  animation: neuralGradient 8s ease-in-out infinite;
}

.orchos-quantum-btn:hover .orchos-tooltip,
.orchos-quantum-btn:focus .orchos-tooltip,
.orchos-quantum-btn:focus-visible .orchos-tooltip {
  opacity: 1;
  transform: translateX(-50%) scale(1);
  pointer-events: auto;
  box-shadow: 
    0 8px 32px 0 rgba(0,250,255,0.15), 
    0 2px 8px 0 rgba(124,77,255,0.15),
    inset 0 0 8px 0 rgba(0,250,255,0.1);
}

.orchos-tooltip::before {
  content: '';
  position: absolute;
  left: 50%;
  bottom: -5px;
  transform: translateX(-50%) rotate(45deg);
  width: 10px;
  height: 10px;
  background: inherit;
  -webkit-backdrop-filter: inherit;
  backdrop-filter: inherit;
  border-right: 1px solid rgba(124, 77, 255, 0.2);
  border-bottom: 1px solid rgba(124, 77, 255, 0.2);
  z-index: -1;
}

.orchos-quantum-btn {
  position: relative;
  width: 48px;
  height: 48px;
  min-width: 48px;
  min-height: 48px;
  padding: 0;
  background: var(--quantum-bg-dark);
  border: 2px solid transparent;
  border-radius: 50%;
  cursor: pointer;
  overflow: hidden;
  transition: all 0.3s var(--quantum-cubic);
  -webkit-backdrop-filter: blur(20px) saturate(180%);
  backdrop-filter: blur(20px) saturate(180%);
  box-shadow: 
    0 0 30px rgba(0, 250, 255, 0.2),
    inset 0 0 20px rgba(0, 250, 255, 0.1),
    0 4px 20px rgba(0, 0, 0, 0.5);
  animation: neuralBreathing 6s ease-in-out infinite;
}

.orchos-quantum-btn:hover {
  animation: none;
  transform: scale(1.08) translateZ(var(--quantum-depth-2));
  box-shadow: 
    0 0 50px rgba(0, 250, 255, 0.4),
    inset 0 0 30px rgba(0, 250, 255, 0.2),
    0 8px 30px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-btn::before {
  content: '';
  position: absolute;
  inset: -2px;
  border-radius: 50%;
  padding: 2px;
  background: linear-gradient(135deg, 
    #ff4dd2 0%, 
    #00faff 33%, 
    #7c4dff 66%, 
    #ff4dd2 100%
  );
  

/* Cross-browser mask support */


  -webkit-mask: linear-gradient(#fff 0 0) content-box, linear-gradient(#fff 0 0);
  -webkit-mask-composite: xor;
  mask: linear-gradient(#fff 0 0) content-box, linear-gradient(#fff 0 0);
  mask-composite: exclude;
  opacity: 0.7;
  animation: quantumBorderRotate 4s linear infinite;
}

.orchos-quantum-btn:hover {
  transform: scale(1.1);
  box-shadow: 
    0 0 50px rgba(0, 250, 255, 0.4),
    inset 0 0 30px rgba(0, 250, 255, 0.2),
    0 8px 30px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-particles {
  position: absolute;
  inset: 0;
  border-radius: 50%;
  overflow: hidden;
}

.orchos-quantum-particles::before,
.orchos-quantum-particles::after {
  content: '';
  position: absolute;
  width: 2px;
  height: 2px;
  background: var(--active-color, #00faff);
  border-radius: 50%;
  box-shadow: 
    0 0 4px var(--active-color, #00faff),
    0 0 8px var(--active-color, #00faff);
  animation: quantumFloat 4s ease-in-out infinite;
  opacity: 0.7;
}

.orchos-quantum-particles::before {
  top: 25%;
  left: 30%;
  animation-delay: 0s;
}

.orchos-quantum-particles::after {
  bottom: 30%;
  right: 25%;
  animation-delay: 2s;
}

.orchos-quantum-btn::after {
  content: '';
  position: absolute;
  top: 60%;
  left: 15%;
  width: 2px;
  height: 2px;
  background: var(--active-color, #00faff);
  border-radius: 50%;
  box-shadow: 
    0 0 4px var(--active-color, #00faff),
    0 0 8px var(--active-color, #00faff);
  animation: quantumFloat 4.5s ease-in-out infinite;
  animation-delay: 1s;
  z-index: 2;
  opacity: 0.7;
}

.orchos-quantum-btn:hover::after {
  width: 3px;
  height: 3px;
  opacity: 1;
}

.orchos-quantum-btn::before {
  content: '';
  position: absolute;
  top: 20%;
  right: 25%;
  width: 2px;
  height: 2px;
  background: var(--active-color, #00faff);
  border-radius: 50%;
  box-shadow: 
    0 0 4px var(--active-color, #00faff),
    0 0 8px var(--active-color, #00faff);
  animation: quantumFloat 5s ease-in-out infinite;
  animation-delay: 2.5s;
  z-index: 2;
  opacity: 0.7;
}

.orchos-quantum-btn[data-active="true"]::after,
.orchos-quantum-btn[data-active="true"]::before,
.orchos-quantum-btn[data-active="true"] .orchos-quantum-particles::before,
.orchos-quantum-btn[data-active="true"] .orchos-quantum-particles::after {
  animation: neuralParticleActive 3s ease-in-out infinite;
  opacity: 1;
  width: 3px;
  height: 3px;
  background: var(--quantum-record-active);
  box-shadow: 
    0 0 8px var(--quantum-record-active),
    0 0 16px var(--quantum-record-active);
}

/* Neural Glow Ring */

.orchos-neural-ring {
  position: absolute;
  inset: 3px;
  border-radius: 50%;
  background: radial-gradient(circle at center,
    transparent 0%,
    transparent 40%,
    rgba(0, 250, 255, 0.1) 50%,
    transparent 60%
  );
  animation: neuralPulse 2s ease-in-out infinite;
}

.orchos-quantum-btn.orchos-btn-record[data-active="true"] {
  border-color: var(--quantum-record-active);
  animation: recordingPulse 1.5s ease-in-out infinite;
  background: rgba(30, 14, 34, 0.9);
}

.orchos-quantum-btn.orchos-btn-record[data-active="true"] .orchos-quantum-icon {
  filter: drop-shadow(0 0 8px var(--quantum-record-active));
}

.orchos-quantum-btn.orchos-btn-record[data-active="true"] .orchos-neural-ring {
  background: radial-gradient(circle at center,
    transparent 0%,
    transparent 40%,
    rgba(255, 77, 210, 0.3) 50%,
    transparent 60%
  );
}

.orchos-quantum-btn.orchos-btn-record[data-active="true"]::before {
  background: linear-gradient(135deg, 
    #ff4dd2 0%, 
    #ff80ab 50%, 
    #ff4dd2 100%
  );
  opacity: 1;
}

.orchos-quantum-btn.orchos-btn-send:hover {
  box-shadow: 
    0 0 40px rgba(0, 250, 255, 0.5),
    inset 0 0 25px rgba(0, 250, 255, 0.3),
    0 8px 25px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-btn.orchos-btn-record:hover {
  box-shadow: 
    0 0 40px rgba(255, 77, 210, 0.5),
    inset 0 0 25px rgba(255, 77, 210, 0.3),
    0 8px 25px rgba(0, 0, 0, 0.6);
}

.orchos-quantum-btn:active::after,
.orchos-quantum-btn:active::before {
  animation: synapticBurst 0.5s ease-out forwards;
}

.orchos-quantum-btn.orchos-btn-send:hover .orchos-quantum-icon {
  filter: drop-shadow(0 0 8px var(--quantum-send-active));
  transform: translateX(2px);
  transition: transform 0.3s var(--quantum-cubic);
}

.settings-btn {
  width: 34px !important;
  height: 34px !important;
  padding: 0 !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  color: var(--theme-cyan) !important;
  cursor: pointer !important;
  border-radius: 50% !important;
  background-color: rgba(0, 15, 30, 0.7) !important;
  border: 2px solid rgba(0, 250, 255, 0.5) !important;
  box-shadow: 0 0 8px rgba(0, 250, 255, 0.4), inset 0 0 10px rgba(0, 250, 255, 0.1) !important;
  transition: all 0.3s ease !important;
  -webkit-backdrop-filter: blur(2px) !important;
  backdrop-filter: blur(2px) !important;
}

.settings-btn:hover {
  transform: scale(1.05);
  background-color: rgba(0, 40, 70, 0.9) !important;
  box-shadow: 0 0 15px rgba(0, 250, 255, 0.7), inset 0 0 12px rgba(0, 250, 255, 0.2) !important;
  border-color: rgba(0, 250, 255, 0.8) !important;
}

.settings-btn svg {
  width: 20px !important;
  height: 20px !important;
  filter: drop-shadow(0 0 3px rgba(0, 250, 255, 0.6)) !important;
}

.neural-settings-popup {
  background-color: rgba(0, 15, 30, 0.95);
  border: 2px solid rgba(0, 250, 255, 0.5);
  box-shadow: 0 5px 25px rgba(0, 0, 0, 0.7), 0 0 20px rgba(0, 250, 255, 0.3);
  -webkit-backdrop-filter: blur(15px);
  backdrop-filter: blur(15px);
  min-width: 250px;
  z-index: 100;
  position: absolute;
  top: calc(100% + 10px);
  right: 0;
  padding: 14px 16px;
  

/* Usando display: flex em vez de block para permitir alinhamento vertical */


  display: flex;
  flex-direction: column;
  overflow: hidden;
  border-radius: 12px;
  margin-top: 0;
}

.settings-btn {
  background-color: rgba(0, 15, 30, 0.7) !important;
  border: 2px solid rgba(0, 250, 255, 0.5) !important;
  box-shadow: 0 0 8px rgba(0, 250, 255, 0.4), inset 0 0 10px rgba(0, 250, 255, 0.1) !important;
}

.settings-btn:hover {
  background-color: rgba(0, 30, 60, 0.9) !important;
  border-color: rgba(0, 250, 255, 0.8) !important;
  box-shadow: 0 0 15px rgba(0, 250, 255, 0.7), inset 0 0 12px rgba(0, 250, 255, 0.2) !important;
}

.settings-btn svg {
  filter: drop-shadow(0 0 3px #00faff);
}

.orchos-quantum-icon {
  z-index: 3;
  filter: drop-shadow(0 0 4px currentColor);
}

.orchos-quantum-ripple {
  position: absolute;
  border-radius: 50%;
  transform: translate(-50%, -50%);
  background: radial-gradient(circle, 
    rgba(0, 250, 255, 0.4) 0%, 
    rgba(124, 77, 255, 0.2) 50%, 
    transparent 70%
  );
  pointer-events: none;
  animation: quantumRippleExpand 1s ease-out forwards;
}

.orchos-quantum-btn:focus {
  outline: none;
  box-shadow: 
    0 0 50px rgba(0, 250, 255, 0.5),
    inset 0 0 30px rgba(0, 250, 255, 0.2),
    0 8px 30px rgba(0, 0, 0, 0.6),
    0 0 0 3px rgba(0, 250, 255, 0.3);
}

@media (prefers-reduced-motion: reduce) {
  .orchos-quantum-btn,
  .orchos-quantum-btn::before,
  .orchos-neural-ring,
  .orchos-quantum-particles::before,
  .orchos-quantum-particles::after {
    animation: none !important;
    transition: none !important;
  }
  
  .orchos-quantum-btn {
    transition: transform 0.2s ease;
  }
}

/* Transcription status styles */
.transcription-item {
  padding: 8px 12px;
  margin: 4px 0;
  transition: all 0.3s ease;
  border-radius: 8px;
  background: rgba(0, 0, 0, 0.2);
  border: 1px solid rgba(0, 250, 255, 0.1);
  line-height: 1.5;
}

.transcription-pending {
  color: var(--text-primary, rgba(255, 255, 255, 0.9));
  opacity: 1;
  background: rgba(0, 250, 255, 0.05);
  border-color: rgba(0, 250, 255, 0.2);
}

.transcription-sent {
  color: var(--text-secondary, rgba(255, 255, 255, 0.5));
  opacity: 0.6;
  font-style: italic;
  position: relative;
  padding-left: 30px;
  background: rgba(76, 175, 80, 0.05);
  border-color: rgba(76, 175, 80, 0.2);
}

.transcription-sent::before {
  content: "✓";
  position: absolute;
  left: 10px;
  color: var(--success-color, #4caf50);
  font-size: 14px;
  opacity: 0.8;
}

/* Add visual separator between sent and pending */
.transcription-text {
  position: relative;
  max-height: 300px;
  overflow-y: auto;
  padding: 8px;
}

/* Scrollable transcription text with fixed height */
.transcription-scrollable {
  max-height: 80px !important;
  overflow-y: auto !important;
  overflow-x: hidden !important;
  padding: 8px 12px;
  background: transparent;
  border-radius: 4px;
  line-height: 1.5;
  word-wrap: break-word;
  white-space: pre-wrap;
  display: block;
  
  /* Custom scrollbar styling */
  scrollbar-width: thin;
  scrollbar-color: rgba(0, 250, 255, 0.3) transparent;
}

.transcription-scrollable::-webkit-scrollbar {
  width: 6px;
}

.transcription-scrollable::-webkit-scrollbar-track {
  background: rgba(0, 0, 0, 0.2);
  border-radius: 3px;
}

.transcription-scrollable::-webkit-scrollbar-thumb {
  background: rgba(0, 250, 255, 0.4);
  border-radius: 3px;
  transition: background 0.3s ease;
}

.transcription-scrollable::-webkit-scrollbar-thumb:hover {
  background: rgba(0, 250, 255, 0.6);
}

/* Smooth scroll behavior */
.transcription-scrollable {
  scroll-behavior: smooth;
}

.transcription-pending:first-child {
  border-top: 2px solid rgba(0, 250, 255, 0.3);
  padding-top: 12px;
  margin-top: 12px;
}

/* Highlight new transcriptions */
.transcription-pending {
  animation: fadeInHighlight 0.5s ease-out;
}

/* Style for multi-line transcriptions */
.transcription-item br {
  display: block;
  margin: 4px 0;
}

@keyframes fadeInHighlight {
  from {
    opacity: 0;
    background-color: rgba(0, 250, 255, 0.2);
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    background-color: rgba(0, 250, 255, 0.05);
    transform: translateY(0);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ConnectionState, MicrophoneState } from "../../../context";

export interface TranscriptionPanelProps {
  onClose: () => void;
  width?: string;
}

export interface ConnectionDetailsType {
  [key: string]: unknown;
  active?: boolean;
  socketStatus?: string;
  socketReadyState?: number;
  sessionId?: string;
  error?: string;
}

export interface DiagnosticsPanelProps {
  connectionState: ConnectionState;
  microphoneState: MicrophoneState;
}

export interface ConnectionDiagnosticsProps {
  connectionDetails: ConnectionDetailsType | null;
  setConnectionDetails: React.Dispatch<React.SetStateAction<ConnectionDetailsType | null>>;
  getConnectionStatus: () => ConnectionDetailsType;
  showToast: (title: string, description: string, variant: "neutral" | "success" | "error") => void;
  disconnectFromDeepgram: () => Promise<void>;
  connectToDeepgram: () => Promise<boolean>;
  waitForConnectionState: (targetState: ConnectionState, timeoutMs: number) => Promise<boolean>;
  hasActiveConnection: () => boolean;
  ConnectionState: typeof ConnectionState;
}

export interface TextControlsProps {
  label: string;
  onClear: () => void;

  onExpand?: () => void;
}

export interface DeviceSelectorProps {
  devices: MediaDeviceInfo[];
  selectedId: string;
  onChange: (deviceId: string) => void;
  title: string;
  isSystemAudio: boolean;
}

export interface ToggleSwitchProps {
  label: string;
  isOn: boolean;
  onChange: () => void;
  title: string;
}

export interface AudioControlsProps {
  isMicrophoneOn: boolean;
  setIsMicrophoneOn: (isOn: boolean) => void;
  isSystemAudioOn: boolean;
  setIsSystemAudioOn: (isOn: boolean) => void;
  audioDevices: MediaDeviceInfo[];
  selectedDevices: {
    microphone: string | null;
    systemAudio: string | null;
  };
  handleDeviceChange: (deviceId: string, isSystemAudio: boolean) => void;
}

export interface TranscriptionTextsState {
  transcription: string;
  aiResponse: string;
}

export interface ImportModalProps {
  show: boolean;
  onClose: () => void;
  importFile: File | null;
  setImportFile: React.Dispatch<React.SetStateAction<File | null>>;
  importUserName: string;
  setImportUserName: React.Dispatch<React.SetStateAction<string>>;
  importMode: ImportMode;
  setImportMode: React.Dispatch<React.SetStateAction<ImportMode>>;
  importProgress: number;
  importStage: string;
  importSummary: string;
  isImporting: boolean;
  handleFileChange: (e: React.ChangeEvent<HTMLInputElement>) => void;
  handleStartImport: (userName: string) => Promise<void>;
  handleCloseImportModal: () => void;
}

// Define interfaces for import functionality using Interface Segregation Principle
export type ImportMode = 'overwrite' | 'increment';

export interface ImportProgressData {
  processed: number;
  total: number;
  percentage?: number;
  stage?: string;
}

// This interface is used internally in the ImportModal component
export interface ImportOptions {
  fileBuffer: ArrayBuffer | Buffer;
  mode: ImportMode;
  user: string;
  onProgress: (data: ImportProgressData) => void;
}

// Interface for the import service (Dependency Inversion Principle)
export interface IChatGptImportService {
  importChatHistory(options: {
    fileBuffer: ArrayBuffer | Buffer;
    mode: ImportMode;
    user: string;
    onProgress?: (data: ImportProgressData) => void;
  }): Promise<{ success: boolean; error?: string; imported?: number; skipped?: number }>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Re-export components for easier importing
export { default as TranscriptionPanel } from './TranscriptionPanel';
export { default as AudioControls } from './components/AudioControls';
export { default as CognitionLogSection } from './components/CognitionLogSection';
export { default as ConnectionDiagnostics } from './components/ConnectionDiagnostics';
export { default as DeviceSelector } from './components/DeviceSelector';
export { default as DiagnosticsPanel } from './components/DiagnosticsPanel';
export { default as ImportModal } from './components/ImportModal';
export { default as LanguageSelector } from './components/LanguageSelector';
export { default as PanelHeader } from './components/PanelHeader';
export { default as RecordingControl } from './components/RecordingControl';
export { default as TextControls } from './components/TextControls';
export { default as TextEditor } from './components/TextEditor';
export { default as ToggleSwitch } from './components/ToggleSwitch';

// Re-export hooks
export { useTranscriptionManager } from './hooks/useTranscriptionManager';
export { useChatGptImport } from './hooks/useChatGptImport';

// Re-export types
export * from './types/interfaces';
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useCallback, useEffect, useRef, useState } from "react";
import { useToast } from "../../../App";
import { useCognitionLog } from "../../context/CognitionLogContext";
import AudioControls from "./components/AudioControls";
import CognitionLogSection from "./components/CognitionLogSection";
import ImportModal from "./components/ImportModal";
import PanelHeader from "./components/PanelHeader";
import SettingsModal from "./components/SettingsModal";
import { useChatGptImport } from "./hooks/useChatGptImport";
import { useTranscriptionManager } from "./hooks/useTranscriptionManager";
import { TranscriptionPanelProps } from "./types/interfaces";
// Import para controlar a visibilidade da visualização quântica
import { useGeneralSettings } from "./components/settings/hooks/useGeneralSettings";
// Módulo cortical para cards simples
// Importação dos arquivos CSS modulares - estrutura neural-simbólica
import "./styles/TranscriptionPanel.animations.css"; // Animações e keyframes
import "./styles/TranscriptionPanel.buttons.css"; // Botões e controles interativos
import "./styles/TranscriptionPanel.chathistory.css"; // Histórico de chats
import "./styles/TranscriptionPanel.layout.css"; // Layout, grid e estrutura espacial
import "./styles/TranscriptionPanel.overrides.css"; // Overrides para single-column mode
import "./styles/TranscriptionPanel.settings.css"; // Componentes de configuração
import "./styles/TranscriptionPanel.tooltip.css"; // Tooltips e ajudas contextuais
import "./styles/TranscriptionPanel.variables.css"; // Variáveis globais e propriedades customizadas
import "./styles/TranscriptionPanel.visual.css"; // Efeitos visuais e glassmorfismo
// Quantum consciousness visualization import
import { QuantumVisualizationContainer } from "../QuantumVisualization/QuantumVisualizationContainer";
// Conversational Chat import
import { ConversationalChat } from "./components/ConversationalChat";
// Chat History imports
import { ChatHistorySidebar } from "./components/ConversationalChat/components/ChatHistorySidebar";
import { useChatHistory } from "./components/ConversationalChat/hooks/useChatHistory";
// Brain visualization is now handled in a separate module

const TranscriptionPanel: React.FC<TranscriptionPanelProps> = ({
  onClose,
  width,
}) => {
  const transcriptionManager = useTranscriptionManager();
  const { showToast } = useToast();

  // Hook para acessar as configurações gerais, incluindo enableMatrix
  const { enableMatrix } = useGeneralSettings();

  // Chat History Hook
  const chatHistory = useChatHistory();

  // Track processing state
  const [isProcessing, setIsProcessing] = useState(false);

  if (!transcriptionManager) return null;

  // Move stable callbacks here when setTexts is already defined

  const {
    language,
    setLanguage,
    microphoneState,
    connectionState,
    toggleRecording,
    handleSendPrompt,
    clearTranscription,
    clearAiResponse,
    toggleExpand,
    isExpanded,
    temporaryContext,
    setTemporaryContext,
    texts,
    setTexts,
    audioDevices,
    selectedDevices,
    handleDeviceChange,
    isMicrophoneOn,
    isSystemAudioOn,
    setIsMicrophoneOn,
    setIsSystemAudioOn,
    showDetailedDiagnostics,
    setShowDetailedDiagnostics,
    connectionDetails,
    setConnectionDetails,
    transcriptionRef,
    getConnectionStatus,
    disconnectFromDeepgram,
    connectToDeepgram,
    waitForConnectionState,
    hasActiveConnection,
    ConnectionState,
  } = transcriptionManager;

  const {
    events: cognitionEvents,
    exporters,
    clearEvents,
    exportEvents,
  } = useCognitionLog();

  const {
    importFile,
    setImportFile,
    importUserName,
    setImportUserName,
    importMode,
    setImportMode,
    importProgress,
    importStage,
    importSummary,
    isImporting,
    showImportModal,
    setShowImportModal,
    handleFileChange,
    handleStartImport,
    handleCloseImportModal,
  } = useChatGptImport(showToast);

  // Stable callbacks defined AFTER setTexts is available to avoid TS errors
  const handleTranscriptionChange = useCallback(
    (value: string) => {
      console.log("🔄 [PANEL] Transcription change:", value.substring(0, 50));
      setTexts((prev) => ({ ...prev, transcription: value }));
    },
    [setTexts]
  );

  const handleAiResponseChange = useCallback(
    (value: string) => {
      console.log("🔄 [PANEL] AI response change:", value.substring(0, 50));
      setTexts((prev) => ({ ...prev, aiResponse: value }));
    },
    [setTexts]
  );

  const handleTemporaryContextChange = useCallback(
    (value: string) => {
      console.log(
        "🔄 [PANEL] Temporary context change:",
        value.substring(0, 50)
      );
      setTemporaryContext(value);
    },
    [setTemporaryContext]
  );

  // Brain state and logic has been moved to BrainVisualization module

  // Brain visualization components have been moved to BrainVisualization module

  // --- Configurações simples para Audio/Language Controls ---
  const [showSettings, setShowSettings] = useState(false);
  const settingsContainerRef = useRef<HTMLDivElement>(null);
  const settingsBtnRef = useRef<HTMLButtonElement>(null);
  const popupRef = useRef<HTMLDivElement>(null);
  const [settingsPopupPosition, setSettingsPopupPosition] = useState({
    top: 0,
    left: 0,
  });

  // Função para alternar a visibilidade das configurações
  const toggleSettings = () => {
    setShowSettings(!showSettings);
  };

  // Fechar configurações ao clicar fora
  useEffect(() => {
    function handleClickOutside(event: MouseEvent) {
      if (
        settingsBtnRef.current &&
        !settingsBtnRef.current.contains(event.target as Node) &&
        popupRef.current &&
        !popupRef.current.contains(event.target as Node)
      ) {
        setShowSettings(false);
      }
    }
    if (showSettings) {
      document.addEventListener("mousedown", handleClickOutside);
    }
    return () => {
      document.removeEventListener("mousedown", handleClickOutside);
    };
  }, [showSettings]);

  useEffect(() => {
    if (showSettings && settingsBtnRef.current) {
      const rect = settingsBtnRef.current.getBoundingClientRect();
      const POPUP_WIDTH = 350;
      let left = rect.right + window.scrollX - POPUP_WIDTH;
      // Garante que não vaze para fora da viewport
      left = Math.max(8, Math.min(left, window.innerWidth - POPUP_WIDTH - 8));
      setSettingsPopupPosition({
        top: rect.bottom + window.scrollY + 8,
        left,
      });
    }
  }, [showSettings]);

  // Prevent body scroll when dashboard is active
  useEffect(() => {
    document.body.classList.add("orchos-active");
    return () => {
      document.body.classList.remove("orchos-active");
    };
  }, []);

  // Estados para modais
  const [showLogsModal, setShowLogsModal] = useState(false);
  const [showSettingsModal, setShowSettingsModal] = useState(false);
  const [mobileSidebarOpen, setMobileSidebarOpen] = useState(false);

  // Settings content for the chat component
  const settingsContent = (
    <div className="neural-settings-popup">
      <div className="mb-2 pb-2">
        <h3 className="orchos-title flex items-center gap-2">
          <svg
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="#00faff"
            strokeWidth="2"
          >
            <circle cx="12" cy="12" r="3"></circle>
            <path d="M19.4 15a1.65 1.65 0 0 0 .33 1.82l.06.06a2 2 0 0 1 0 2.83 2 2 0 0 1-2.83 0l-.06-.06a1.65 1.65 0 0 0-1.82-.33 1.65 1.65 0 0 0-1 1.51V21a2 2 0 0 1-2 2 2 2 0 0 1-2-2v-.09A1.65 1.65 0 0 0 9 19.4a1.65 1.65 0 0 0-1.82.33l-.06.06a2 2 0 0 1-2.83 0 2 2 0 0 1 0-2.83l.06-.06a1.65 1.65 0 0 0 .33-1.82 1.65 1.65 0 0 0-1.51-1H3a2 2 0 0 1-2-2 2 2 0 0 1 2-2h.09A1.65 1.65 0 0 0 4.6 9a1.65 1.65 0 0 0-.33-1.82l-.06-.06a2 2 0 0 1 0-2.83 2 2 0 0 1 2.83 0l.06.06a1.65 1.65 0 0 0 1.82.33H9a1.65 1.65 0 0 0 1-1.51V3a2 2 0 0 1 2-2 2 2 0 0 1 2 2v.09a1.65 1.65 0 0 0 1 1.51 1.65 1.65 0 0 0 1.82-.33l.06-.06a2 2 0 0 1 2.83 0 2 2 0 0 1 0 2.83l-.06.06a1.65 1.65 0 0 0-.33 1.82V9a1.65 1.65 0 0 0 1.51 1H21a2 2 0 0 1 2 2 2 2 0 0 1-2 2h-.09a1.65 1.65 0 0 0-1.51 1z"></path>
          </svg>
          Neural Settings
        </h3>
      </div>
      <div className="flex flex-col gap-4 px-1">
        <AudioControls
          isMicrophoneOn={isMicrophoneOn}
          setIsMicrophoneOn={setIsMicrophoneOn}
          isSystemAudioOn={isSystemAudioOn}
          setIsSystemAudioOn={setIsSystemAudioOn}
          audioDevices={audioDevices}
          selectedDevices={selectedDevices}
          handleDeviceChange={handleDeviceChange}
        />
      </div>
    </div>
  );

  // Generate a key based on current conversation ID to force remount when conversation changes
  const chatKey = React.useMemo(
    () => chatHistory.currentConversationId || "default-chat",
    [chatHistory.currentConversationId]
  );

  // --- Render ---
  return (
    <div
      style={{
        height: "100vh",
        width: "100vw",
        display: "flex",
        flexDirection: "column",
        overflow: "hidden",
        maxHeight: "100vh",
        maxWidth: "100vw",
      }}
    >
      {/* Panel Header - Now positioned absolutely */}
      <PanelHeader
        onClose={() => {
          if (window?.electronAPI?.closeWindow) {
            window.electronAPI.closeWindow();
          } else if (onClose) {
            onClose();
          }
        }}
        onMinimize={() => {
          if (window?.electronAPI?.minimizeWindow) {
            window.electronAPI.minimizeWindow();
          }
        }}
        onShowSettings={() => setShowSettingsModal(true)}
        onShowLogsModal={() => setShowLogsModal(true)}
        onShowImportModal={() => setShowImportModal(true)}
        onToggleDiagnostics={() =>
          setShowDetailedDiagnostics(!showDetailedDiagnostics)
        }
        connectionState={connectionState}
        microphoneState={microphoneState}
        hasActiveConnection={hasActiveConnection}
        onDisconnect={disconnectFromDeepgram}
        onReconnect={connectToDeepgram}
      />

      {/* Main Chat Dashboard Layout */}
      <div
        className={`orchos-quantum-dashboard with-sidebar ${
          !enableMatrix ? "single-column" : ""
        }`}
        style={{
          flex: "1 1 auto",
        }}
      >
        {/* Chat History Sidebar */}
        <div
          className={`chat-history-sidebar ${
            mobileSidebarOpen ? "mobile-open" : ""
          }`}
        >
          <ChatHistorySidebar
            conversations={chatHistory.conversations}
            currentConversationId={chatHistory.currentConversationId}
            onSelectConversation={(id: string) => {
              // Clear any pending AI responses when switching conversations
              clearAiResponse();
              clearTranscription();
              chatHistory.selectConversation(id);
            }}
            onCreateNewConversation={() => {
              // Clear any pending AI responses when creating new conversation
              clearAiResponse();
              clearTranscription();
              return chatHistory.createNewConversation();
            }}
            onDeleteConversation={chatHistory.deleteConversation}
            onSearchConversations={chatHistory.searchConversations}
            isProcessing={isProcessing}
          />
        </div>

        {/* Quantum Visualization Zone - Left Panel with Golden Ratio */}
        {/* Lógica corrigida: enableMatrix = true mostra, false esconde */}
        {enableMatrix && (
          <div
            key="quantum-visualization-zone"
            className="quantum-visualization-zone"
          >
            <QuantumVisualizationContainer
              cognitionEvents={cognitionEvents}
              height="100%"
              width="100%"
              lowPerformanceMode={false}
              showLegend={true}
            />
          </div>
        )}

        {/* Conversational Chat Zone - Main Panel */}
        <div
          key="neural-chat-zone"
          className="neural-chat-zone"
          style={{
            height: "100%",
            width: "100%",
            padding: "1.2rem",
            overflow: "hidden" /* Força o chat a usar scroll interno */,
            display: "flex",
            flexDirection: "column",
          }}
        >
          <ConversationalChat
            key={chatKey}
            transcriptionText={texts.transcription}
            onTranscriptionChange={handleTranscriptionChange}
            onClearTranscription={clearTranscription}
            aiResponseText={texts.aiResponse}
            onAiResponseChange={handleAiResponseChange}
            onClearAiResponse={clearAiResponse}
            temporaryContext={temporaryContext}
            onTemporaryContextChange={handleTemporaryContextChange}
            microphoneState={microphoneState}
            onToggleRecording={toggleRecording}
            onSendPrompt={handleSendPrompt}
            // Audio settings props
            language={language}
            setLanguage={setLanguage}
            isMicrophoneOn={isMicrophoneOn}
            setIsMicrophoneOn={setIsMicrophoneOn}
            isSystemAudioOn={isSystemAudioOn}
            setIsSystemAudioOn={setIsSystemAudioOn}
            audioDevices={audioDevices}
            selectedDevices={selectedDevices}
            handleDeviceChange={handleDeviceChange}
            // Chat History props
            currentConversation={chatHistory.currentConversation}
            onAddMessageToConversation={chatHistory.addMessageToConversation}
            onProcessingChange={setIsProcessing}
          />
        </div>
      </div>

      {/* Import Modal - Always on top */}
      {showImportModal && (
        <ImportModal
          show={showImportModal}
          onClose={handleCloseImportModal}
          importFile={importFile}
          setImportFile={setImportFile}
          importUserName={importUserName}
          setImportUserName={setImportUserName}
          importMode={importMode}
          setImportMode={setImportMode}
          importProgress={importProgress}
          importStage={importStage}
          importSummary={importSummary}
          isImporting={isImporting}
          handleFileChange={handleFileChange}
          handleStartImport={handleStartImport}
          handleCloseImportModal={handleCloseImportModal}
        />
      )}

      {/* Modal de Logs de Cognição */}
      {showLogsModal && (
        <div className="fixed inset-0 z-50 flex items-center justify-center bg-black/60">
          <div className="orchos-cognition-logs-modal">
            <div className="orchos-cognition-logs-header">
              <h2 className="text-xl font-bold text-[#7c4dff] flex items-center gap-2">
                <svg width="22" height="22" viewBox="0 0 20 20" fill="none">
                  <ellipse
                    cx="10"
                    cy="10"
                    rx="8"
                    ry="6"
                    stroke="#7c4dff"
                    strokeWidth="2"
                  />
                  <circle cx="10" cy="10" r="3" fill="#7c4dff" />
                </svg>
                Cognition Logs
              </h2>
              <button
                className="ml-2 px-2 py-1 rounded-full hover:bg-[#7c4dff22] transition"
                onClick={() => setShowLogsModal(false)}
                aria-label="Close logs modal"
              >
                <svg width="20" height="20" viewBox="0 0 20 20" fill="none">
                  <path
                    d="M6 6l8 8M14 6l-8 8"
                    stroke="#7c4dff"
                    strokeWidth="2"
                    strokeLinecap="round"
                  />
                </svg>
              </button>
            </div>
            <CognitionLogSection
              cognitionEvents={cognitionEvents}
              exporters={exporters}
              exportEvents={exportEvents}
              clearEvents={clearEvents}
            />
          </div>
        </div>
      )}

      {/* Settings Modal */}
      {showSettingsModal && (
        <SettingsModal
          show={showSettingsModal}
          onClose={() => setShowSettingsModal(false)}
        />
      )}

      {/* Mobile Sidebar Toggle Button */}
      <button
        className="mobile-sidebar-toggle"
        onClick={() => setMobileSidebarOpen(!mobileSidebarOpen)}
        title="Histórico de conversas"
      >
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
          <path
            d="M3 12h18m-18-6h18m-18 12h18"
            stroke="currentColor"
            strokeWidth="2"
            strokeLinecap="round"
          />
        </svg>
      </button>
    </div>
  );
};

export default TranscriptionPanel;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import * as React from "react"
import * as ToastPrimitive from "@radix-ui/react-toast"
import { cn } from "../../lib/utils"
import { AlertCircle, CheckCircle2, Info, X } from "lucide-react"

const ToastProvider = ToastPrimitive.Provider

export type ToastMessage = {
  title: string
  description: string
  variant: ToastVariant
}

const ToastViewport = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Viewport>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitive.Viewport>
>(({ className, ...props }, ref) => (
  <ToastPrimitive.Viewport
    ref={ref}
    className={cn(
      "fixed top-0 right-0 z-[100] flex max-h-screen w-full flex-col-reverse gap-1 p-2 sm:top-0 sm:right-0 sm:flex-col md:max-w-[320px]",
      className
    )}
    {...props}
  />
))
ToastViewport.displayName = ToastPrimitive.Viewport.displayName

type ToastVariant = "neutral" | "success" | "error"

interface ToastProps
  extends React.ComponentPropsWithoutRef<typeof ToastPrimitive.Root> {
  variant?: ToastVariant
  swipeDirection?: "right" | "left" | "up" | "down"
}

const toastVariants: Record<
  ToastVariant,
  { icon: React.ReactNode; bgColor: string }
> = {
  neutral: {
    icon: <Info className="h-3 w-3 text-amber-700" />,
    bgColor: "bg-amber-100"
  },
  success: {
    icon: <CheckCircle2 className="h-3 w-3 text-emerald-700" />,
    bgColor: "bg-emerald-100"
  },
  error: {
    icon: <AlertCircle className="h-3 w-3 text-red-700" />,
    bgColor: "bg-red-100"
  }
}

const Toast = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Root>,
  ToastProps
>(({ className, variant = "neutral", ...props }, ref) => (
  <ToastPrimitive.Root
    ref={ref}
    duration={4000}
    className={cn(
      "group pointer-events-auto relative flex w-full items-center space-x-2 overflow-hidden rounded-md p-2",
      toastVariants[variant].bgColor,
      className
    )}
    {...props}
  >
    {toastVariants[variant].icon}
    <div className="flex-1">{props.children}</div>
    <ToastPrimitive.Close className="absolute right-1 top-1 rounded-md p-0.5 text-zinc-500 opacity-0 transition-opacity hover:text-zinc-700 group-hover:opacity-100">
      <X className="h-2 w-2" />
    </ToastPrimitive.Close>
  </ToastPrimitive.Root>
))
Toast.displayName = ToastPrimitive.Root.displayName

const ToastAction = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Action>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitive.Action>
>(({ className, ...props }, ref) => (
  <ToastPrimitive.Action
    ref={ref}
    className={cn(
      "text-[0.65rem] font-medium text-zinc-600 hover:text-zinc-900",
      className
    )}
    {...props}
  />
))
ToastAction.displayName = ToastPrimitive.Action.displayName

const ToastTitle = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitive.Title>
>(({ className, ...props }, ref) => (
  <ToastPrimitive.Title
    ref={ref}
    className={cn("text-[0.7rem] font-medium text-zinc-900", className)}
    {...props}
  />
))
ToastTitle.displayName = ToastPrimitive.Title.displayName

const ToastDescription = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitive.Description>
>(({ className, ...props }, ref) => (
  <ToastPrimitive.Description
    ref={ref}
    className={cn("text-[0.65rem] text-zinc-600", className)}
    {...props}
  />
))
ToastDescription.displayName = ToastPrimitive.Description.displayName

export type { ToastProps, ToastVariant }
export {
  ToastProvider,
  ToastViewport,
  Toast,
  ToastAction,
  ToastTitle,
  ToastDescription
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia



// Get the platform safely
const getPlatform = () => {
    try {
      return window.electronAPI?.getPlatform() || 'win32' // Default to win32 if API is not available
    } catch {
      return 'win32' // Default to win32 if there's an error
    }
  }
  
  // Platform-specific command key symbol
  export const COMMAND_KEY = getPlatform() === 'darwin' ? '⌘' : 'Ctrl'
  
  // Helper to check if we're on Windows
  export const isWindows = getPlatform() === 'win32'
  
  // Helper to check if we're on macOS
  export const isMacOS = getPlatform() === 'darwin' // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import * as ort from 'onnxruntime-web';

/**
 * onnxruntimeConfig.ts
 * Global ONNX Runtime Web configuration for Orch-OS
 * Applies optimal settings for performance and suppresses benign warnings.
 */
export class OnnxRuntimeConfig {
  /**
   * Apply global ONNX Runtime settings. Call once at startup before any session creation.
   */
  static initialize(): void {
    // WASM flags
    ort.env.wasm.simd = true;
    ort.env.wasm.numThreads = Math.min(4, navigator.hardwareConcurrency || 4);
    ort.env.wasm.proxy = false;

    // WebGPU flags
    if (ort.env.webgpu) {
      ort.env.webgpu.validateInputContent = false;
    }

    // Logging: show warnings and errors only
    ort.env.logSeverityLevel = 2; // 0=VERBOSE,1=INFO,2=WARNING,3=ERROR,4=FATAL
    ort.env.logVerbosityLevel = 0;

    // Suppress known harmless warnings
    this.suppressHarmlessWarnings();
  }

  /**
   * Build optimized SessionOptions for a given provider.
   */
  static getSessionOptions(
    provider: 'webgpu' | 'wasm'
  ): ort.InferenceSession.SessionOptions {
    const executionProviders = provider === 'webgpu' ? ['webgpu', 'wasm'] : ['wasm'];

    return {
      executionProviders,
      graphOptimizationLevel: 'all',
      enableMemPattern: true,
      enableCpuMemArena: true,
      interOpNumThreads: provider === 'wasm' ? 1 : undefined,
      intraOpNumThreads: provider === 'wasm' ? 1 : undefined,
      logSeverityLevel: 2,
      logVerbosityLevel: 0,
    };
  }

  /**
   * Silence known-onboarding warnings from ONNX Runtime Web.
   */
  private static suppressHarmlessWarnings() {
    const origWarn = console.warn;
    const origErr  = console.error;

    const harmlessPatterns = [
      /VerifyEachNodeIsAssignedToAnEp/, // expected fallback info
    ];

    console.warn = (...args: any[]) => {
      const msg = args.join(' ');
      if (harmlessPatterns.some(p => p.test(msg))) return;
      origWarn.apply(console, args);
    };

    console.error = (...args: any[]) => {
      const msg = args.join(' ');
      if (harmlessPatterns.some(p => p.test(msg))) return;
      origErr.apply(console, args);
    };
  }
}

// Auto-initialize if imported in browser
if (typeof window !== 'undefined') {
  OnnxRuntimeConfig.initialize();
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Central user config for the application
import { getUserName } from '../services/StorageService';

// Central user config for the application
export function getPrimaryUser(): string {
  // Symbolic: retrieves the primary user's name from storage cortex
  return getUserName();
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Orch-OS Neural-Symbolic Interface Specification
 * 
 * SimpleModule - Interface para módulos de UI colapsáveis/expansíveis.
 * Intenção simbólica: Representa um córtex neuralmente adaptável que pode
 * expandir ou colapsar para otimizar a densidade cognitiva da interface.
 */

import { ReactNode } from 'react';

export interface SimpleModuleProps {
  /** Título do módulo cortical */
  title: string;
  
  /** Define se o módulo inicia em estado expandido */
  defaultOpen?: boolean;
  
  /** Conteúdo neural do módulo */
  children: ReactNode;
  
  /** Usado apenas para debugging visual - não para produção */
  debugBorder?: boolean;
}

export interface SimpleModuleState {
  /** Estado atual (expandido/colapsado) */
  isExpanded: boolean;
  
  /** Função para alternar entre estados */
  toggle: () => void;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// INeuralSignalService.ts
// Symbolic: Contract for neural signal extraction cortex
import { NeuralSignalResponse } from '../../../components/context/deepgram/interfaces/neural/NeuralSignalTypes';

export interface INeuralSignalService {
  generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ISemanticEnricher.ts
// Symbolic: Contract for semantic enrichment cortex

export interface ISemanticEnricher {
  enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ModeService, OrchOSModeEnum } from "../../../services/ModeService";
import {
  ChatGPTSession,
  ImportChatGPTParams,
  ImportResult,
} from "../interfaces/types";
import { DeduplicationService } from "../services/deduplication/DeduplicationService";
import { EmbeddingService } from "../services/embedding/EmbeddingService";
import { ChatGPTParser } from "../services/parser/ChatGPTParser";
import { VectorStorageService } from "../services/storage/VectorStorageService";
import { TextChunker } from "../services/text-processor/TextChunker";
import { Logger } from "../utils/logging";
import { ProgressReporter } from "../utils/progressReporter";

/**
 * Handler for importing ChatGPT history
 * Orchestrates different services following SOLID principles
 */
export async function importChatGPTHistoryHandler(
  params: ImportChatGPTParams
): Promise<ImportResult> {
  const {
    fileBuffer,
    mode,
    applicationMode,
    openAIService,
    pineconeHelper,
    onProgress,
  } = params;

  // Initialize services
  const logger = new Logger("[ImportChatGPT]");
  const progressReporter = new ProgressReporter(onProgress, logger);

  // Determine mode for service configuration
  let currentMode: OrchOSModeEnum;
  if (applicationMode) {
    // Convert string to enum
    if (applicationMode.toLowerCase() === "basic") {
      currentMode = OrchOSModeEnum.BASIC;
    } else if (applicationMode.toLowerCase() === "advanced") {
      currentMode = OrchOSModeEnum.ADVANCED;
    } else {
      logger.warn(
        `🟡 Unknown applicationMode: "${applicationMode}", falling back to ModeService`
      );
      currentMode = ModeService.getMode();
    }
    logger.info(
      `🔧 Using applicationMode from IPC: "${applicationMode}" -> ${currentMode}`
    );
  } else {
    currentMode = ModeService.getMode();
    logger.info(
      `🔧 No applicationMode provided, using ModeService: ${currentMode}`
    );
  }

  const isBasicMode = currentMode === OrchOSModeEnum.BASIC;
  const storageType = "DuckDB"; // Always use DuckDB for both Basic and Advanced modes
  logger.info(
    `🗄️ Storage mode: ${storageType} (${
      isBasicMode ? "Basic" : "Advanced"
    } mode)`
  );

  // Use the helper passed from IPC (already correct for the mode)
  const vectorHelper = pineconeHelper;

  const parser = new ChatGPTParser(progressReporter, logger);
  const deduplicationService = new DeduplicationService(
    vectorHelper,
    progressReporter,
    logger
  );
  const textChunker = new TextChunker();
  const embeddingService = new EmbeddingService(
    openAIService,
    logger,
    progressReporter,
    applicationMode
  );
  const storageService = new VectorStorageService(
    vectorHelper,
    progressReporter,
    logger,
    true // Always use DuckDB mode (true = isBasicMode)
  );

  try {
    // Log of start
    logger.info(
      `Starting ChatGPT import in ${mode} mode with applicationMode: ${
        applicationMode || "auto-detect"
      } using ${storageType}`
    );

    // 1. Parse the file
    const rawSessions = parser.parseBuffer(fileBuffer);

    // 2. Extract messages
    let allMessages = parser.extractMessages(rawSessions);
    logger.info(`Extracted ${allMessages.length} messages from file`);

    // 3. Ensure all messages have valid IDs
    logger.info("PASSO 3: Ensuring all messages have valid IDs...");
    allMessages = parser.ensureMessageIds(allMessages);
    logger.info("PASSO 3: IDs ensured successfully");

    // 4. Deduplication
    logger.info("PASSO 4: Starting deduplication...");
    let uniqueMessages;
    try {
      uniqueMessages = await deduplicationService.filterDuplicates(
        allMessages,
        mode
      );
      logger.info(
        `PASSO 4: After deduplication: ${uniqueMessages.length} unique messages`
      );
      logger.info("PASSO 4 COMPLETED: Moving to step 5...");

      // Additional verification right after deduplication
      if (!uniqueMessages || !Array.isArray(uniqueMessages)) {
        throw new Error(`Invalid deduplication result: ${uniqueMessages}`);
      }

      logger.info("CRITICAL CHECKPOINT: Deduplication completed successfully");
    } catch (error) {
      logger.error("FATAL ERROR during deduplication:", error);
      if (error instanceof Error) {
        logger.error(`Error details: ${error.message}`);
        logger.error(`Stack trace: ${error.stack || "Not available"}`);
      }
      throw new Error(
        `Deduplication failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    try {
      // 5. Data cleanup (overwrite mode)
      logger.info(`PASSO 5: Starting with mode=${mode}`);
      if (mode === "overwrite") {
        logger.info(
          `PASSO 5: Deleting existing data (overwrite mode) from ${storageType}...`
        );
        await storageService.deleteExistingData();
        logger.info(
          `PASSO 5: Existing data deleted successfully from ${storageType}`
        );
      } else {
        logger.info("PASSO 5: Increment mode - skipping data cleanup");
      }
      logger.info("PASSO 5 COMPLETED: Moving to step 6...");
    } catch (error) {
      logger.error("FATAL ERROR in step 5:", error);
      throw new Error(
        `Step 5 failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // Initialize embedding service based on mode (both use DuckDB storage)
    if (isBasicMode) {
      logger.info(
        "Initializing HuggingFace service for embeddings (Basic mode with DuckDB)..."
      );
    } else if (openAIService) {
      logger.info(
        "Initializing Ollama service for embeddings (Advanced mode with DuckDB)..."
      );
      const initialized = await embeddingService.ensureOpenAIInitialized();
      logger.info(
        `Ollama service initialization status: ${initialized ? "OK" : "FAILED"}`
      );
      if (!initialized) {
        logger.error(
          "Ollama service could not be initialized. Check Ollama configuration."
        );
        logger.info("Ensure Ollama is running and properly configured");
      }
    }

    // Verification of TextChunker before processing
    logger.info("VERIFICATION: Verifying TextChunker before processing...");
    try {
      logger.info(`TextChunker available: ${textChunker ? "YES" : "NO"}`);
      logger.info(
        `Number of messages to be processed: ${uniqueMessages.length}`
      );
      if (uniqueMessages.length > 0) {
        logger.info(
          `Example message for processing: ${JSON.stringify(
            uniqueMessages[0]
          ).substring(0, 150)}...`
        );
      }
    } catch (err) {
      logger.error("Verification failed: Error verifying TextChunker:", err);
    }

    // 6. Process messages into chunks
    logger.info("PASSO 6: Starting message chunk processing...");
    let messageChunks;
    try {
      // Verify input for chunk processor
      logger.info(
        `PASSO 6: Input for processor: ${
          uniqueMessages.length
        } messages, first item: ${JSON.stringify(uniqueMessages[0]).substring(
          0,
          100
        )}...`
      );

      messageChunks = textChunker.processMessagesIntoChunks(uniqueMessages);
      if (!messageChunks) {
        throw new Error("TextChunker returned undefined or null");
      }
      logger.info(
        `PASSO 6: Created ${messageChunks.length} text chunks from ${uniqueMessages.length} messages`
      );
      if (messageChunks.length > 0) {
        logger.info(
          `PASSO 6: Example chunk: ${JSON.stringify(messageChunks[0]).substring(
            0,
            100
          )}...`
        );
      }

      logger.info("PASSO 6 COMPLETED: Moving to step 7...");
    } catch (error) {
      logger.error("FATAL ERROR in chunk processing:", error);
      throw new Error(
        `Chunk processing failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // 7. Create batches for efficient processing
    logger.info("PASSO 7: Creating batches for efficient processing...");
    let batches;
    try {
      if (!messageChunks || messageChunks.length === 0) {
        throw new Error("No chunks available for batch processing");
      }
      batches = textChunker.createProcessingBatches(messageChunks);
      if (!batches) {
        throw new Error(
          "TextChunker returned undefined or null when creating batches"
        );
      }
      logger.info(
        `PASSO 7: Processing ${messageChunks.length} chunks in ${batches.length} batches`
      );
      logger.info(`PASSO 7: First batch has ${batches[0]?.length || 0} chunks`);
      logger.info("PASSO 7 COMPLETED: Moving to step 8...");
    } catch (error) {
      logger.error("FATAL ERROR in batch creation:", error);
      throw new Error(
        `Batch creation failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // Verification of transition between steps
    logger.info("TRANSITION: Moving from step 7 to step 8...");
    logger.info("=========== FIM DO DIAGNÓSTICO ===========");
    logger.info("===============================================");

    // 8. Generate embeddings and create vectors
    logger.info(
      `STARTING EMBEDDINGS GENERATION using ${
        isBasicMode ? "HuggingFace" : "Ollama"
      } with DuckDB storage...`
    );
    let vectors;
    try {
      vectors = await embeddingService.generateEmbeddingsForChunks(
        batches,
        messageChunks
      );
      logger.info(`Generated ${vectors.length} vectors with embeddings`);
    } catch (error) {
      logger.error("FATAL ERROR in embedding generation:", error);
      throw new Error(
        `Embedding generation failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // 9. Save vectors to appropriate storage
    logger.info(
      `[DIAGNOSTIC] Vectors to be saved to ${storageType}: ${vectors.length}`
    );
    if (vectors.length > 0) {
      logger.info(
        `[DIAGNOSTIC] Example vector: ${JSON.stringify(vectors[0]).substring(
          0,
          200
        )}`
      );
    } else {
      logger.warn("[DIAGNOSTIC] No vectors generated for saving!");
    }
    const saveResult = await storageService.saveVectors(vectors);
    logger.info(
      `[DIAGNOSTIC] Save result to ${storageType}: success=${
        saveResult.success
      }, error=${saveResult.error || "none"}`
    );
    if (!saveResult.success) {
      logger.error(
        `[DIAGNOSTIC] Error saving vectors to ${storageType}: ${saveResult.error}`
      );
    }

    // 10. Calculate final statistics
    const totalMessagesInFile = rawSessions.reduce(
      (acc: number, session: ChatGPTSession) => {
        return acc + Object.values(session.mapping || {}).length;
      },
      0
    );

    const skipped = totalMessagesInFile - vectors.length;

    // 11. Log of completion
    logger.info(`=============================================`);
    logger.info(`🎉 IMPORTATION COMPLETED SUCCESSFULLY`);
    logger.info(`📊 Statistics:`);
    logger.info(
      `- Mode: ${mode === "overwrite" ? "OVERWRITE" : "INCREMENTAL"}`
    );
    logger.info(
      `- Storage: ${storageType} (${isBasicMode ? "Basic" : "Advanced"} mode)`
    );
    logger.info(`- Embeddings: ${isBasicMode ? "HuggingFace" : "Ollama"}`);
    logger.info(`- Total messages in file: ${totalMessagesInFile}`);
    logger.info(
      `- Duplicated messages ignored: ${skipped} (${Math.round(
        (skipped / totalMessagesInFile) * 100
      )}%)`
    );
    logger.info(
      `- Vectors saved to ${storageType}: ${vectors.length} (${Math.round(
        (vectors.length / totalMessagesInFile) * 100
      )}%)`
    );
    logger.info(`=============================================`);

    return {
      success: true,
      imported: vectors.length,
      skipped,
      totalMessagesInFile,
      mode,
    };
  } catch (error) {
    logger.error(`Importation failed:`, error);
    return {
      success: false,
      imported: 0,
      skipped: 0,
      totalMessagesInFile: 0,
      mode,
      error: error instanceof Error ? error.message : "Unknown error",
    };
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IOpenAIService } from "../../../components/context/deepgram/interfaces/openai/IOpenAIService";

// ChatGPT data interfaces for artificial brain memory import
export interface ChatGPTMessageContent {
  content_type: string;
  parts: string[];
}

export interface ChatGPTMessageAuthor {
  role: "user" | "assistant" | "developer" | string;
  name?: string;
}

export interface ChatGPTMessage {
  id: string;
  author: ChatGPTMessageAuthor;
  create_time: number;
  content: ChatGPTMessageContent;
  parent?: string;
  children?: string[];
}

export interface ChatGPTMessageItem {
  message: ChatGPTMessage;
  parent?: string;
}

export interface ChatGPTSession {
  title: string;
  create_time: number;
  update_time: number;
  mapping: Record<string, ChatGPTMessageItem>;
}

// Interface for processed messages used in cognitive memory orchestration
export interface ProcessedMessage {
  role: string;
  content: string;
  timestamp: number | null;
  id: string | null;
  parent: string | null;
  session_title: string | null;
  session_create_time: number | null;
  session_update_time: number | null;
}

// Type for message chunks used in memory segmentation
export interface MessageChunk {
  original: ProcessedMessage;
  content: string;
  part?: number;
  totalParts?: number;
}

// Interface para vetores Pinecone
export interface PineconeVector {
  id: string;
  values: number[];
  metadata: Record<string, string | number | boolean | string[]>;
}

// Interface for progress information
export interface ProgressInfo {
  processed: number;
  total: number;
  percentage: number;
  stage: "parsing" | "deduplicating" | "generating_embeddings" | "saving";
}

// Interface for import parameters (using DuckDB helper with legacy name for compatibility)
export interface ImportChatGPTParams {
  fileBuffer: Buffer;
  mode: "increment" | "overwrite";
  applicationMode?: "basic" | "advanced"; // Mode passed from renderer process
  openAIService?: IOpenAIService | null;
  pineconeHelper: any; // DuckDB helper with legacy interface name for compatibility
  onProgress?: (info: ProgressInfo) => void;
}

// Interface for import result
export interface ImportResult {
  success: boolean;
  imported: number;
  skipped: number;
  totalMessagesInFile: number;
  mode: "increment" | "overwrite";
  error?: string;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ProcessedMessage } from '../../interfaces/types';
import { Logger } from '../../utils/logging';
import { ProgressReporter } from '../../utils/progressReporter';

/**
 * Interface for checking existing vectors (abstraction for Pinecone/DuckDB)
 */
interface IVectorChecker {
  checkExistingIds(ids: string[], progressCallback?: (processed: number, total: number) => void): Promise<string[]>;
}

/**
 * Service to check and eliminate duplicate messages
 */
export class DeduplicationService {
  private vectorChecker: IVectorChecker;
  private progressReporter: ProgressReporter;
  private logger: Logger;

  constructor(vectorChecker: IVectorChecker, progressReporter: ProgressReporter, logger?: Logger) {
    this.vectorChecker = vectorChecker;
    this.progressReporter = progressReporter;
    this.logger = logger || new Logger('[DeduplicationService]');
    this.logger.info('DeduplicationService successfully instantiated');
  }

  /**
   * Filters duplicate messages based on existing IDs
   */
  public async filterDuplicates(
    messages: ProcessedMessage[], 
    mode: 'increment' | 'overwrite'
  ): Promise<ProcessedMessage[]> {
    // Log detalhado das primeiras mensagens recebidas
    this.logger.debug('First 3 messages received for deduplication:', JSON.stringify(messages.slice(0, 3), null, 2));

    // Ensure all messages have a valid id
    const messagesWithIds = messages.map((msg, idx) => ({
      ...msg,
      id: msg.id && msg.id !== '' ? msg.id : `fallback_id_${idx}_${Date.now()}`
    }));
    
    // If mode is overwrite, deduplication is not needed
    if (mode === 'overwrite') {
      this.logger.info('OVERWRITE mode selected, skipping duplicate check');
      return messagesWithIds;
    }
    
    this.logger.info('INCREMENT mode selected, verifying duplicates...');
    
    // Start progress
    const originalLength = messagesWithIds.length;
    this.progressReporter.startStage('deduplicating', originalLength);
    
    // Extract all message IDs to check for duplicates
    const messageIdsToCheck = messagesWithIds
      .filter((m: ProcessedMessage) => m.id !== null && m.id !== undefined && m.id !== '')
      .map((m: ProcessedMessage) => m.id as string);

    const limitedMessageIdsToCheck = messageIdsToCheck;
      
    this.logger.debug(`Total of original messages: ${originalLength}`);
    this.logger.debug(`Total messages with valid ID: ${messageIdsToCheck.length}`);
    this.logger.debug(`Messages without valid ID: ${originalLength - messageIdsToCheck.length}`);
    
    if (messageIdsToCheck.length === 0) {
      this.logger.warn('No valid message ID found in file.');
      this.progressReporter.completeStage('deduplicating', originalLength);
      return messages;
    }
    
    // Verify existence of IDs in storage with timeout
    this.logger.info(`Verifying ${limitedMessageIdsToCheck.length} IDs in vector storage`);
    this.logger.debug('First 10 IDs to be verified:', JSON.stringify(limitedMessageIdsToCheck.slice(0, 10)));
    const existingMessageIds = new Set<string>();
    const timeoutPromise = new Promise<string[]>((_, reject) => {
      const timeoutId = setTimeout(() => {
        clearTimeout(timeoutId);
        reject(new Error('Timeout checking IDs in vector storage - operation took more than 5 minutes'));
      }, 5 * 60 * 1000); // 5 minutes timeout
    });
    let existingIds: string[] = [];
    try {
      this.logger.info('Starting complete ID verification with security timeout...');
      this.logger.debug('Calling vectorChecker.checkExistingIds...');
      const checkIdsPromise = this.vectorChecker.checkExistingIds(
        limitedMessageIdsToCheck,
        (processed, total) => {
          this.logger.debug(`[Dedup] Progress: ${processed}/${total}`);
          this.progressReporter.updateProgress('deduplicating', processed, total);
          if (processed % 500 === 0 || processed === total) {
            this.logger.info(`Progress of ID verification: ${processed}/${total}`);
          }
        }
      );
      existingIds = await Promise.race([checkIdsPromise, timeoutPromise]) as string[];
      this.logger.info('checkExistingIds completed successfully');
      this.logger.debug('First 10 IDs returned:', JSON.stringify(existingIds.slice(0, 10)));
      this.logger.info(`Total of existing IDs found: ${existingIds.length}`);

      // Verify result
      if (!existingIds || !Array.isArray(existingIds)) {
        throw new Error(`Invalid result of checkExistingIds: ${existingIds}`);
      }

      existingIds.forEach(id => existingMessageIds.add(id));
      this.logger.info(`Found ${existingIds.length} existing IDs`);
      this.logger.info('ID verification process completed successfully');
    } catch (error) {
      this.logger.error('Error checking existing IDs:', error);
      if (error instanceof Error) {
        this.logger.error(`Error details: ${error.message}`);
        this.logger.error(`Stack trace: ${error.stack || 'Not available'}`);
      }
      this.progressReporter.completeStage('deduplicating', originalLength);
      // In case of error, proceed as if there are no duplicates
      this.logger.warn('Continuing without complete duplicate check due to error');
      return messagesWithIds;
    }
    
    // Filter duplicate messages
    this.logger.info('Starting duplicate message filtering...');
    try {
      const duplicateMessages = messages.filter((m: ProcessedMessage) => 
        m.id && existingMessageIds.has(m.id)
      );
      
      const uniqueMessages = messages.filter((m: ProcessedMessage) => 
        !m.id || !existingMessageIds.has(m.id)
      );
      
      // Deduplication analysis logs
      this.logger.info('Deduplication analysis:');
      this.logger.info(`- Total of messages in file: ${originalLength}`);
      this.logger.info(`- Existing messages (ignored): ${duplicateMessages.length}`);
      this.logger.info(`- New messages to be imported: ${uniqueMessages.length}`);
      
      // Verify data
      if (!uniqueMessages || !Array.isArray(uniqueMessages)) {
        throw new Error(`Invalid result of message filtering: ${uniqueMessages}`);
      }
      
      // Log to indicate process completion
      this.logger.info('====== DEDUPLICATION SUCCESSFULLY COMPLETED ======');
      this.logger.info(`Returning ${uniqueMessages.length} unique messages`);
      
      // Complete progress (ensures this happens even if the return fails)
      this.progressReporter.completeStage('deduplicating', originalLength);
      
      return uniqueMessages;
    } catch (error) {
      // Detailed error log
      this.logger.error('FATAL ERROR filtering messages:', error);
      if (error instanceof Error) {
        this.logger.error(`Error details: ${error.message}`);
        this.logger.error(`Stack trace: ${error.stack || 'Not available'}`);
      }
      
      // Ensure progress is completed even in case of error
      this.progressReporter.completeStage('deduplicating', originalLength);
      
      // Rethrow the error for higher-level handling
      throw new Error(`Failed to process duplicate messages: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IEmbeddingService } from "../../../../components/context/deepgram/interfaces/openai/IEmbeddingService";
import { IOpenAIService } from "../../../../components/context/deepgram/interfaces/openai/IOpenAIService";
import { OllamaEmbeddingService } from "../../../../components/context/deepgram/services/ollama/OllamaEmbeddingService";
import { HuggingFaceEmbeddingService } from "../../../../services/huggingface/HuggingFaceEmbeddingService";
import { ModeService, OrchOSModeEnum } from "../../../../services/ModeService";
import { getOption, STORAGE_KEYS } from "../../../../services/StorageService";
import { getModelDimensions } from "../../../../utils/EmbeddingUtils";
import { MessageChunk, PineconeVector } from "../../interfaces/types";
import { Logger } from "../../utils/logging";
import { ProgressReporter } from "../../utils/progressReporter";

/**
 * Service for generating embeddings
 * Switches between Ollama (Advanced mode) and HuggingFace (Basic mode)
 * Both modes use DuckDB for storage
 */
export class EmbeddingService {
  private openAIService: IOpenAIService | null | undefined;
  private embeddingService: IEmbeddingService;
  private logger: Logger;
  private progressReporter?: ProgressReporter;
  private embeddingDimension: number = 1536; // Default dimension, will be updated dynamically
  private isBasicMode: boolean;

  constructor(
    openAIService: IOpenAIService | null | undefined,
    logger?: Logger,
    progressReporter?: ProgressReporter,
    applicationMode?: "basic" | "advanced"
  ) {
    this.openAIService = openAIService;
    this.logger = logger || new Logger("[EmbeddingService]");
    this.progressReporter = progressReporter;

    // Use provided applicationMode or detect from ModeService
    let currentMode: OrchOSModeEnum;
    if (applicationMode) {
      // Convert string to enum
      if (applicationMode.toLowerCase() === "basic") {
        currentMode = OrchOSModeEnum.BASIC;
      } else if (applicationMode.toLowerCase() === "advanced") {
        currentMode = OrchOSModeEnum.ADVANCED;
      } else {
        this.logger.warn(
          `🟡 [EmbeddingService] Unknown applicationMode: "${applicationMode}", falling back to ModeService`
        );
        currentMode = ModeService.getMode();
      }
      this.logger.info(
        `🔧 [EmbeddingService] Using applicationMode from IPC: "${applicationMode}" -> ${currentMode}`
      );
    } else {
      currentMode = ModeService.getMode();
      this.logger.info(
        `🔧 [EmbeddingService] No applicationMode provided, using ModeService: ${currentMode}`
      );
    }

    this.isBasicMode = currentMode === OrchOSModeEnum.BASIC;

    this.logger.info(`🔍 [EmbeddingService] === MODE DETECTION DEBUG ===`);
    this.logger.info(
      `🔍 [EmbeddingService] Input applicationMode: "${applicationMode}"`
    );
    this.logger.info(
      `🔍 [EmbeddingService] Resolved OrchOSModeEnum: "${currentMode}"`
    );
    this.logger.info(
      `🔍 [EmbeddingService] OrchOSModeEnum.BASIC: "${OrchOSModeEnum.BASIC}"`
    );
    this.logger.info(
      `🔍 [EmbeddingService] OrchOSModeEnum.ADVANCED: "${OrchOSModeEnum.ADVANCED}"`
    );
    this.logger.info(
      `🔍 [EmbeddingService] Final isBasicMode: ${this.isBasicMode}`
    );
    this.logger.info(
      `🔍 [EmbeddingService] Final selected mode: ${
        this.isBasicMode ? "BASIC (HuggingFace)" : "ADVANCED (Ollama)"
      }`
    );

    // Check storage for debugging
    const storageMode =
      typeof window !== "undefined"
        ? window.localStorage?.getItem("APPLICATION_MODE") || "undefined"
        : "not-available-in-main-process";
    this.logger.info(
      `🔍 [EmbeddingService] Storage APPLICATION_MODE: "${storageMode}"`
    );
    this.logger.info(`�� [EmbeddingService] === END MODE DEBUG ===`);

    this.embeddingService = this.createEmbeddingService();

    // Only subscribe to mode changes if no applicationMode was provided
    // (to avoid conflicts between IPC and ModeService)
    if (!applicationMode) {
      // Subscribe to mode changes to update embedding service when needed
      ModeService.onModeChange((newMode: OrchOSModeEnum) => {
        this.logger.info(
          `🔄 [EmbeddingService] Mode change detected: ${newMode}`
        );
        this.updateModeAndService(newMode);
      });
    } else {
      this.logger.info(
        `🔧 [EmbeddingService] Skipping ModeService listener (using IPC mode: ${applicationMode})`
      );
    }
  }

  /**
   * Updates the embedding dimension based on the embedding service
   * @param service The embedding service to get dimension from
   */
  private updateEmbeddingDimension(service: IEmbeddingService): void {
    // Check if the service has a method to get embedding dimension
    if (typeof (service as any).getEmbeddingDimension === "function") {
      try {
        this.embeddingDimension = (service as any).getEmbeddingDimension();
        this.logger.info(
          `Using dynamic embedding dimension from service: ${this.embeddingDimension}`
        );
        return;
      } catch (error) {
        this.logger.warn(
          `Failed to get embedding dimension dynamically: ${error}`
        );
      }
    }

    // If service doesn't provide the dimension or there was an error, use the utility function
    if (this.isBasicMode) {
      const defaultHFModelName =
        getOption(STORAGE_KEYS.HF_EMBEDDING_MODEL) || "";
      this.embeddingDimension = getModelDimensions(defaultHFModelName);
    } else {
      const defaultOllamaModel =
        getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL) || "";
      this.embeddingDimension = getModelDimensions(defaultOllamaModel);
    }

    this.logger.info(
      `Using embedding dimension from utility for mode ${
        this.isBasicMode ? "BASIC" : "ADVANCED"
      }: ${this.embeddingDimension}`
    );
  }

  /**
   * Creates the appropriate embedding service based on mode
   */
  private createEmbeddingService(): IEmbeddingService {
    if (this.isBasicMode) {
      this.logger.info(
        `[EmbeddingService] Creating HuggingFaceEmbeddingService for Basic mode`
      );
      const service = new HuggingFaceEmbeddingService();
      // Use the service's method to get the embedding dimension
      this.updateEmbeddingDimension(service);
      return service;
    } else {
      // In advanced mode, use Ollama with the selected model
      const ollamaModel = getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL);
      this.logger.info(
        `[EmbeddingService] Creating OllamaEmbeddingService with model: ${
          ollamaModel || "default"
        } for Advanced mode`
      );

      if (!this.openAIService) {
        this.logger.error(
          "[EmbeddingService] OpenAI service not available for OllamaEmbeddingService"
        );
        // Fallback to HuggingFace if Ollama service is not available
        const service = new HuggingFaceEmbeddingService();
        this.updateEmbeddingDimension(service);
        return service;
      }

      const service = new OllamaEmbeddingService(this.openAIService, {
        model: ollamaModel,
      });
      // Use the service's method to get the embedding dimension
      this.updateEmbeddingDimension(service);
      return service;
    }
  }

  /**
   * Initializes the embedding service (Ollama for Advanced mode, HuggingFace for Basic mode)
   */
  public async ensureOpenAIInitialized(): Promise<boolean> {
    if (this.isBasicMode) {
      this.logger.info(
        "✅ Basic mode detected - using HuggingFace embeddings (no Ollama required)"
      );
      return true; // In basic mode, we don't need Ollama
    }

    this.logger.info("Verifying Ollama service initialization...");

    if (!this.openAIService) {
      this.logger.error(
        "FATAL ERROR: Ollama service not provided - verify if the service is correctly injected"
      );
      return false;
    }

    // Verify if the service is already initialized
    const isInitialized = this.openAIService.isInitialized();
    this.logger.info(
      `Status of Ollama initialization: ${
        isInitialized ? "Already initialized" : "Not initialized"
      }`
    );

    if (isInitialized) {
      return true;
    }

    this.logger.warn(
      "Ollama client not initialized. Attempting to initialize via loadApiKey()..."
    );

    try {
      // Use the loadApiKey method from the service itself
      this.logger.info("Calling ollamaService.loadApiKey()...");
      await this.openAIService.loadApiKey();

      // Verify if it was initialized
      if (this.openAIService.isInitialized()) {
        this.logger.success(
          "Ollama client initialized successfully via loadApiKey()!"
        );
        return true;
      }

      // If it was not initialized, check if there is an ensureOpenAIClient method
      if (this.openAIService.ensureOpenAIClient) {
        this.logger.info(
          "Attempting to initialize via ensureOllamaClient()..."
        );
        const initialized = await this.openAIService.ensureOpenAIClient();
        if (initialized) {
          this.logger.success(
            "Ollama client initialized successfully via ensureOllamaClient()!"
          );
          return true;
        }
      }

      this.logger.error(
        "Failed to initialize Ollama client. Initialization attempts failed."
      );
      return false;
    } catch (error) {
      this.logger.error("Error initializing Ollama client:", error);
      return false;
    }
  }

  /**
   * Generates embeddings for text chunks using the appropriate service (Ollama or HuggingFace)
   */
  public async generateEmbeddingsForChunks(
    batches: MessageChunk[][],
    allMessageChunks: MessageChunk[]
  ): Promise<PineconeVector[]> {
    // Ensure we have the correct embedding dimension from the service
    this.updateEmbeddingDimension(this.embeddingService);

    // Start the progress of generating embeddings
    if (this.progressReporter) {
      this.progressReporter.startStage(
        "generating_embeddings",
        allMessageChunks.length
      );
    }

    // Verify if we have the embedding service available
    const embeddingInitialized = await this.ensureOpenAIInitialized();
    this.logger.info(
      `Status of embedding service for generation: ${
        embeddingInitialized ? "Initialized" : "Not initialized"
      }`
    );

    if (!embeddingInitialized) {
      if (this.isBasicMode) {
        throw new Error(
          "HuggingFace embedding service not initialized in Basic mode."
        );
      } else {
        throw new Error(
          "Ollama service not initialized. Ensure Ollama is running and properly configured."
        );
      }
    }

    const vectors: PineconeVector[] = [];
    let embeddingsProcessed = 0;

    let processedTotal = 0;
    for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
      const batch = batches[batchIndex];
      this.logger.info(
        `Processing batch ${batchIndex + 1}/${batches.length} with ${
          batch.length
        } messages`
      );

      // Prepare the texts for the current batch
      const batchTexts = batch.map((chunk) => chunk.content);

      // Generate embeddings for the entire batch using the appropriate service
      let batchEmbeddings: number[][] = [];

      try {
        if (this.isBasicMode) {
          // Use HuggingFace service for batch embeddings
          this.logger.info(
            `[BASIC MODE] Generating embeddings with HuggingFace for batch ${
              batchIndex + 1
            }`
          );
          batchEmbeddings = await this.embeddingService.createEmbeddings(
            batchTexts
          );
          this.logger.success(
            `✅ HuggingFace embeddings generated successfully for batch ${
              batchIndex + 1
            }/${batches.length}`
          );
        } else {
          // Use Ollama service for batch embeddings
          this.logger.info(
            `[ADVANCED MODE] Generating embeddings with Ollama for batch ${
              batchIndex + 1
            }`
          );
          if (this.openAIService && this.openAIService.createEmbeddings) {
            // If the API supports batch embeddings
            batchEmbeddings = await this.openAIService.createEmbeddings(
              batchTexts
            );
            this.logger.success(
              `✅ Ollama embeddings generated successfully for batch ${
                batchIndex + 1
              }/${batches.length}`
            );
          } else {
            // Fallback: generate embeddings one by one
            this.logger.warn(
              "API does not support batch embeddings, processing sequentially..."
            );
            batchEmbeddings = await Promise.all(
              batchTexts.map(async (text) => {
                try {
                  return await this.openAIService!.createEmbedding(text);
                } catch (err) {
                  this.logger.error(
                    `Error generating embedding for text: ${text.substring(
                      0,
                      50
                    )}...`,
                    err
                  );
                  throw new Error(
                    `Failed to generate embedding: ${
                      err instanceof Error ? err.message : String(err)
                    }`
                  );
                }
              })
            );
          }
        }
      } catch (batchError) {
        this.logger.error(
          `Error processing batch ${batchIndex + 1}:`,
          batchError
        );
        throw new Error(
          `Failed to process embeddings batch: ${
            batchError instanceof Error
              ? batchError.message
              : String(batchError)
          }`
        );
      }

      // Create vectors from generated embeddings
      for (let i = 0; i < batch.length; i++) {
        const chunk = batch[i];
        const embedding = batchEmbeddings[i];
        const msg = chunk.original;

        embeddingsProcessed++;

        try {
          // Create a unique ID for the vector - if it's part of a split message, add the part number
          const vectorId = chunk.part
            ? `${msg.id || `msg_${Date.now()}`}_part${chunk.part}`
            : msg.id || `msg_${Date.now()}_${embeddingsProcessed}`;

          // Add the vector to the array
          vectors.push({
            id: vectorId,
            values: embedding,
            metadata: {
              // Original ChatGPT fields
              role: msg.role,
              content: chunk.content,
              timestamp: msg.timestamp || 0,
              session_title: msg.session_title || "",
              session_create_time: msg.session_create_time || 0,
              session_update_time: msg.session_update_time || 0,
              imported_from: "chatgpt",
              imported_at: Date.now(),
              messageId: msg.id || vectorId, // Ensure it always has a valid messageId

              // source field for compatibility with the transcription system
              source: msg.role,
              // Chunking metadata for split messages
              ...(chunk.part
                ? {
                    chunking_part: chunk.part,
                    chunking_total_parts: chunk.totalParts || 1,
                    chunking_is_partial: true,
                  }
                : {}),
            },
          });
        } catch (error) {
          this.logger.error(
            `Error processing chunk ${embeddingsProcessed}/${allMessageChunks.length}:`,
            error
          );
        }
      }

      // Update progress after each batch
      processedTotal += batch.length;
      if (this.progressReporter) {
        this.progressReporter.updateProgress(
          "generating_embeddings",
          processedTotal,
          allMessageChunks.length
        );
      }

      // Small pause to avoid API throttling
      if (batchIndex < batches.length - 1) {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }

    // Finalize the progress of generating embeddings
    if (this.progressReporter) {
      this.progressReporter.completeStage(
        "generating_embeddings",
        allMessageChunks.length
      );
    }

    this.logger.info(`Generated ${vectors.length} vectors with embeddings`);
    return vectors;
  }

  private updateModeAndService(newMode: OrchOSModeEnum): void {
    const oldMode = this.isBasicMode;
    this.isBasicMode = newMode === OrchOSModeEnum.BASIC;

    this.logger.info(`🔄 [EmbeddingService] === MODE UPDATE DEBUG ===`);
    this.logger.info(`🔄 [EmbeddingService] Previous isBasicMode: ${oldMode}`);
    this.logger.info(`🔄 [EmbeddingService] New mode from event: "${newMode}"`);
    this.logger.info(
      `🔄 [EmbeddingService] New isBasicMode: ${this.isBasicMode}`
    );
    this.logger.info(
      `🔄 [EmbeddingService] Mode actually changed: ${
        oldMode !== this.isBasicMode
      }`
    );
    this.logger.info(
      `🔄 [EmbeddingService] Final selected mode: ${
        this.isBasicMode ? "BASIC (HuggingFace)" : "ADVANCED (Ollama)"
      }`
    );

    if (oldMode !== this.isBasicMode) {
      this.logger.info(
        `🔄 [EmbeddingService] Creating new embedding service...`
      );
      this.embeddingService = this.createEmbeddingService();
      this.logger.info(
        `🔄 [EmbeddingService] New embedding service created successfully`
      );
    } else {
      this.logger.info(
        `🔄 [EmbeddingService] Mode unchanged, keeping existing service`
      );
    }

    this.logger.info(`🔄 [EmbeddingService] === END MODE UPDATE DEBUG ===`);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ProcessedMessage, ChatGPTSession, ChatGPTMessageItem } from '../../interfaces/types';
import { ProgressReporter } from '../../utils/progressReporter';
import { Logger } from '../../utils/logging';

/**
 * Service responsible for parsing and converting ChatGPT files
 */
export class ChatGPTParser {
  private progressReporter: ProgressReporter;
  private logger: Logger;

  constructor(progressReporter: ProgressReporter, logger?: Logger) {
    this.progressReporter = progressReporter;
    this.logger = logger || new Logger('[ChatGPTParser]');
  }

  /**
   * Converts the raw buffer of the ChatGPT file to JSON
   */
  public parseBuffer(buffer: Buffer | ArrayBuffer | ArrayBufferView): ChatGPTSession[] {
    if (!buffer) throw new Error('No file provided');
    
    // LOG: Type of buffer received
    this.logger.debug(`Type of fileBuffer: ${buffer.constructor.name}`);
    
    // Ensure buffer is a Buffer
    let processedBuffer: Buffer;
    if (buffer instanceof Buffer) {
      processedBuffer = buffer;
    } else if (buffer instanceof ArrayBuffer) {
      processedBuffer = Buffer.from(new Uint8Array(buffer));
    } else if (ArrayBuffer.isView(buffer)) {
      processedBuffer = Buffer.from(new Uint8Array(buffer.buffer));
    } else {
      throw new Error('File type not supported for import');
    }
    
    // LOG: First bytes of the buffer for debug
    this.logger.debug(`First bytes of the buffer: ${processedBuffer.toString().substring(0, 20)}`);
    
    try {
      // Parse JSON
      const jsonString = processedBuffer.toString('utf-8');
      this.logger.debug(`JSON string length: ${jsonString.length} characters`);
      if (jsonString.length === 0) {
        throw new Error('Empty or invalid file');
      }
      return JSON.parse(jsonString);
    } catch (error) {
      this.logger.error('Error converting buffer to JSON', error);
      throw new Error('Invalid or corrupted file: ' + (error instanceof Error ? error.message : String(error)));
    }
  }

  /**
   * Extracts messages from ChatGPT data
   */
  public extractMessages(sessions: ChatGPTSession[]): ProcessedMessage[] {
    const allMessages: ProcessedMessage[] = [];
    
    // Calculate total messages for progress report
    const totalMessages = sessions.reduce((acc: number, session: ChatGPTSession) => {
      return acc + Object.values(session.mapping || {}).length;
    }, 0);
    
    // Start progress report
    this.progressReporter.startStage('parsing', totalMessages);
    this.logger.info(`Starting extraction of ${totalMessages} messages...`);
    
    let messagesProcessed = 0;
    
    // Process each session and extract messages
    for (const session of sessions) {
      if (session.mapping) {
        for (const item of Object.values(session.mapping) as ChatGPTMessageItem[]) {
          messagesProcessed++;
          
          // Update progress every 10 messages
          if (messagesProcessed % 10 === 0) {
            this.progressReporter.updateProgress('parsing', messagesProcessed, totalMessages);
          }
          
          const msg = item.message;
          if (!msg) continue;
          
          const role = msg.author?.role || 'unknown';
          const content = Array.isArray(msg.content?.parts) ? msg.content.parts[0] : msg.content?.parts;
          
          if (!content || typeof content !== 'string' || !content.trim()) continue;
          
          allMessages.push({
            role,
            content: content.trim(),
            timestamp: msg.create_time || null,
            id: msg.id || null,
            parent: msg.parent || null,
            session_title: session.title || null,
            session_create_time: session.create_time || null,
            session_update_time: session.update_time || null
          });
        }
      }
    }
    
    // Finish progress report
    this.progressReporter.completeStage('parsing', totalMessages);
    this.logger.info(`Extracted ${allMessages.length} valid messages from ${totalMessages} items`);
    
    return allMessages;
  }

  /**
   * Generates IDs for messages that do not have them
   */
  public ensureMessageIds(messages: ProcessedMessage[]): ProcessedMessage[] {
    const messagesWithoutIds = messages.filter((m: ProcessedMessage) => !m.id).length;
    
    if (messagesWithoutIds > 0) {
      this.logger.warn(`${messagesWithoutIds} messages do not have IDs. Generating IDs for them...`);
      
      return messages.map((msg: ProcessedMessage) => {
        if (!msg.id) {
          // Generate an ID based on content and timestamp
          const timestamp = Date.now();
          const contentSlice = msg.content.substring(0, 10).replace(/\s+/g, '_');
          const newId = `gen_msg_${timestamp}_${contentSlice}`;
          this.logger.debug(`Generated ID for message: ${newId}`);
          return { ...msg, id: newId };
        }
        return msg;
      });
    }
    
    return messages;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { PineconeVector } from "../../interfaces/types";
import { Logger } from "../../utils/logging";
import { ProgressReporter } from "../../utils/progressReporter";

/**
 * Interface for vector storage (DuckDB only)
 */
interface IVectorHelper {
  deleteAllUserVectors(): Promise<void>;
  saveToDuckDB?(
    vectors: PineconeVector[]
  ): Promise<{ success: boolean; error?: string }>;
}

/**
 * Service for storing vectors using DuckDB for both Basic and Advanced modes
 */
export class VectorStorageService {
  private vectorHelper: IVectorHelper;
  private progressReporter: ProgressReporter;
  private logger: Logger;
  private isBasicMode: boolean;

  constructor(
    vectorHelper: IVectorHelper,
    progressReporter: ProgressReporter,
    logger?: Logger,
    isBasicMode?: boolean
  ) {
    this.vectorHelper = vectorHelper;
    this.progressReporter = progressReporter;
    this.logger = logger || new Logger("[VectorStorageService]");
    this.isBasicMode = isBasicMode || false;
  }

  /**
   * Clears all existing data in overwrite mode
   */
  public async deleteExistingData(): Promise<void> {
    this.logger.info(
      "OVERWRITE mode selected, clearing ALL existing primary user data..."
    );

    if (this.vectorHelper.deleteAllUserVectors) {
      await this.vectorHelper.deleteAllUserVectors();
      this.logger.success(
        "All existing primary user data cleared successfully in OVERWRITE mode"
      );
    } else {
      this.logger.warn(
        "deleteAllUserVectors method not available in vectorHelper"
      );
    }
  }

  /**
   * Saves vectors using DuckDB storage (for both Basic and Advanced modes)
   */
  public async saveVectors(
    vectors: PineconeVector[]
  ): Promise<{ success: boolean; error?: string }> {
    if (!vectors || vectors.length === 0) {
      this.logger.warn("No vectors to save");
      return { success: true };
    }

    const storageType = "DuckDB"; // Always use DuckDB
    this.logger.info(`Saving ${vectors.length} vectors to ${storageType}`);

    try {
      // Divide vectors into batches for better visual feedback and performance
      const BATCH_SIZE = 500; // Ideal batch size for both services
      const batches: PineconeVector[][] = [];

      // Divide into batches
      for (let i = 0; i < vectors.length; i += BATCH_SIZE) {
        batches.push(vectors.slice(i, i + BATCH_SIZE));
      }

      this.logger.info(
        `Saving ${vectors.length} vectors in ${batches.length} batches of up to ${BATCH_SIZE} vectors to ${storageType}`
      );

      let success = true;
      let error = "";
      let totalProcessed = 0;

      // Save by batches for better progress feedback
      for (let i = 0; i < batches.length; i++) {
        const batch = batches[i];
        this.logger.info(
          `Processing batch ${i + 1}/${batches.length} with ${
            batch.length
          } vectors`
        );

        try {
          let batchResult: { success: boolean; error?: string };

          // Always use DuckDB for both Basic and Advanced modes
          if (this.vectorHelper.saveToDuckDB) {
            batchResult = await this.vectorHelper.saveToDuckDB(batch);
          } else {
            throw new Error("DuckDB save method not available");
          }

          if (!batchResult.success) {
            success = false;
            error = batchResult.error || "Unknown error in batch " + (i + 1);
            this.logger.error(`Error in batch ${i + 1}: ${error}`);
          }

          // Update progress
          totalProcessed += batch.length;
          this.progressReporter.updateProgress(
            "saving",
            totalProcessed,
            vectors.length
          );
        } catch (batchError) {
          success = false;
          error =
            batchError instanceof Error ? batchError.message : "Unknown error";
          this.logger.error(`Error in batch ${i + 1}:`, batchError);
        }

        // Small pause to avoid overloading storage service
        if (i < batches.length - 1) {
          await new Promise((resolve) => setTimeout(resolve, 100));
        }
      }

      // Result final
      if (success) {
        this.logger.success(
          `Save operation completed successfully using ${storageType}!`
        );
      } else {
        this.logger.error(`Error in saving to ${storageType}: ${error}`);
      }

      return { success, error: error || undefined };
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : "Unknown error";
      this.logger.error(`Error saving vectors to ${storageType}:`, error);
      return { success: false, error: errorMsg };
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { countTokens } from '../../../../components/context/deepgram/services/memory/utils/tokenUtils';
import { MessageChunk, ProcessedMessage } from '../../interfaces/types';
import { Logger } from '../../utils/logging';

/**
 * Service responsible for dividing texts into smaller chunks for efficient processing
 */
export class TextChunker {
  private readonly maxTokensPerChunk: number;
  private readonly maxTokensPerBatch: number;
  private readonly logger: Logger;

  constructor(maxTokensPerChunk: number = 1000, maxTokensPerBatch: number = 8000) {
    this.maxTokensPerChunk = maxTokensPerChunk;
    this.maxTokensPerBatch = maxTokensPerBatch;
    this.logger = new Logger('[TextChunker]');
  }

  /**
   * Process messages into chunks for better processing
   */
  public processMessagesIntoChunks(messages: ProcessedMessage[]): MessageChunk[] {
    this.logger.info(`Analyzing and processing ${messages.length} messages for optimization...`);
    
    const allMessageChunks: MessageChunk[] = [];
    
    for (const message of messages) {
      // For small content, we don't need to divide
      if (countTokens(message.content) <= this.maxTokensPerChunk) {
        allMessageChunks.push({
          original: message,
          content: message.content
        });
        continue;
      }
      
      // For large content, we divide into semantically coherent chunks
      const chunks = this.splitIntoChunks(message.content);
      if (chunks.length === 1) {
        // If it didn't divide (rare case), we add it as is
        allMessageChunks.push({
          original: message,
          content: chunks[0]
        });
      } else {
        // If it was divided, we add each part with metadata
        chunks.forEach((chunkContent, index) => {
          allMessageChunks.push({
            original: message,
            content: chunkContent,
            part: index + 1,
            totalParts: chunks.length
          });
        });
      }
    }
    
    this.logger.info(`After processing: ${allMessageChunks.length} chunks created from ${messages.length} original messages`);
    return allMessageChunks;
  }

  /**
   * Splits text into semantically coherent chunks
   */
  public splitIntoChunks(text: string, maxTokens: number = this.maxTokensPerChunk): string[] {
    if (!text) return [];
    
    // If the text is already small enough, we don't need to divide
    const totalTokens = countTokens(text);
    if (totalTokens <= maxTokens) return [text];
    
    // Identify possible break points (paragraphs, sentences, etc.)
    const paragraphs = text.split(/\n\s*\n/).filter(p => p.trim().length > 0);
    
    // If we have paragraphs, we try to use them as units of division
    if (paragraphs.length > 1) {
      return this.splitByParagraphs(paragraphs, maxTokens);
    }
    
    // If it's a single large paragraph, try to divide by sentences
    return this.splitBySentences(text, maxTokens);
  }

  private splitByParagraphs(paragraphs: string[], maxTokens: number): string[] {
    const chunks: string[] = [];
    let currentChunk = "";
    let currentChunkTokens = 0;
    
    for (const paragraph of paragraphs) {
      const paragraphTokens = countTokens(paragraph);
      
      // If a single paragraph is too large, we need to divide it by sentences
      if (paragraphTokens > maxTokens) {
        // First add the current chunk if not empty
        if (currentChunkTokens > 0) {
          chunks.push(currentChunk);
          currentChunk = "";
          currentChunkTokens = 0;
        }
        
        // Divide the large paragraph into sentences and create new chunks
        const sentenceChunks = this.splitBySentences(paragraph, maxTokens);
        chunks.push(...sentenceChunks);
      } 
      // If adding this paragraph would exceed the limit, start a new chunk
      else if (currentChunkTokens + paragraphTokens > maxTokens) {
        chunks.push(currentChunk);
        currentChunk = paragraph;
        currentChunkTokens = paragraphTokens;
      } 
      // Add the paragraph to the current chunk
      else {
        if (currentChunkTokens > 0) {
          currentChunk += "\n\n" + paragraph;
        } else {
          currentChunk = paragraph;
        }
        currentChunkTokens += paragraphTokens;
      }
    }
    
    // Add the last chunk if not empty
    if (currentChunkTokens > 0) {
      chunks.push(currentChunk);
    }
    
    return chunks;
  }

  private splitBySentences(text: string, maxTokens: number): string[] {
    return this.splitByDelimiter(text, '. ', maxTokens);
  }

  private splitByDelimiter(text: string, delimiter: string, maxTokens: number): string[] {
    if (countTokens(text) <= maxTokens) return [text];
    
    const parts = text.split(delimiter);
    const chunks: string[] = [];
    let currentChunk = "";
    let currentChunkTokens = 0;
    
    for (const part of parts) {
      const partWithDelimiter = part + (delimiter || "");
      const partTokens = countTokens(partWithDelimiter);
      
      // If a single part is larger than the maximum, we need to divide it further
      if (partTokens > maxTokens) {
        // First add the current chunk if not empty
        if (currentChunk) {
          chunks.push(currentChunk);
          currentChunk = "";
          currentChunkTokens = 0;
        }
        
        // Divide using the last resort method
        const subChunks = this.forceSplitBySize(partWithDelimiter, maxTokens);
        chunks.push(...subChunks);
        continue;
      }
      
      // If adding this part would exceed the token limit, start a new chunk
      if (currentChunkTokens + partTokens > maxTokens) {
        // Save the current chunk and start a new one
        chunks.push(currentChunk);
        currentChunk = partWithDelimiter;
        currentChunkTokens = partTokens;
      } else {
        // Add the part to the current chunk
        currentChunk += partWithDelimiter;
        currentChunkTokens += partTokens;
      }
    }
    
    // Add the last chunk if not empty
    if (currentChunk) {
      chunks.push(currentChunk);
    }
    
    return chunks;
  }

  private forceSplitBySize(text: string, maxTokens: number): string[] {
    const tokens = countTokens(text);
    if (tokens <= maxTokens) return [text];
    
    // Estimate characters per token (approximately)
    const charsPerToken = text.length / tokens;
    const charsPerChunk = Math.floor(maxTokens * charsPerToken) * 0.9; // 10% margin
    
    const chunks: string[] = [];
    let startChar = 0;
    
    while (startChar < text.length) {
      let endChar = Math.min(startChar + charsPerChunk, text.length);
      
      // Try to find a space to make the break cleaner
      if (endChar < text.length) {
        const nextSpace = text.indexOf(' ', endChar - 20);
        if (nextSpace > 0 && nextSpace < endChar + 20) {
          endChar = nextSpace;
        }
      }
      
      const chunk = text.substring(startChar, endChar).trim();
      if (chunk.length > 0) {
        chunks.push(chunk);
      }
      
      startChar = endChar;
    }
    
    return chunks;
  }

  /**
   * Creates batches of chunks for efficient processing
   */
  public createProcessingBatches(messageChunks: MessageChunk[]): MessageChunk[][] {
    const batches: MessageChunk[][] = [];
    let currentBatch: MessageChunk[] = [];
    let currentBatchTokens = 0;
    
    for (const chunk of messageChunks) {
      const chunkTokens = countTokens(chunk.content);
      
      // If adding this chunk would exceed the token limit, start a new batch
      if (currentBatchTokens + chunkTokens > this.maxTokensPerBatch) {
        batches.push([...currentBatch]);
        currentBatch = [];
        currentBatchTokens = 0;
      }
      
      // Add the chunk to the current batch
      currentBatch.push(chunk);
      currentBatchTokens += chunkTokens;
    }
    
    // Add the last batch if not empty
    if (currentBatch.length > 0) {
      batches.push(currentBatch);
    }
    
    this.logger.info(`Processing ${messageChunks.length} chunks in ${batches.length} batches (max ${this.maxTokensPerBatch} tokens per batch)`);
    return batches;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Utility class for standardizing log messages
 */
export class Logger {
  private prefix: string;

  constructor(prefix: string) {
    this.prefix = prefix;
  }

  /**
   * Log information
   */
  public info(message: string): void {
    console.log(`${this.prefix} ${message}`);
  }

  /**
   * Log warning
   */
  public warn(message: string): void {
    console.warn(`${this.prefix} ⚠️ ${message}`);
  }

  /**
   * Log error
   */
  public error(message: string, error?: unknown): void {
    if (error) {
      console.error(`${this.prefix} ❌ ${message}`, error);
    } else {
      console.error(`${this.prefix} ❌ ${message}`);
    }
  }

  /**
   * Log debug (only in development environment)
   */
  public debug(message: string, data?: unknown): void {
    if (process.env.NODE_ENV !== 'production') {
      if (data) {
        console.log(`${this.prefix} 🔍 DEBUG - ${message}`, data);
      } else {
        console.log(`${this.prefix} 🔍 DEBUG - ${message}`);
      }
    }
  }

  /**
   * Log success
   */
  public success(message: string): void {
    console.log(`${this.prefix} ✅ ${message}`);
  }

  /**
   * Log stage
   */
  public stage(stage: string, details: string): void {
    console.log(`${this.prefix} [${stage}] ${details}`);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ProgressInfo } from '../interfaces/types';
import { Logger } from './logging';

/**
 * Class responsible for reporting the progress of operations
 */
export class ProgressReporter {
  private onProgress?: (info: ProgressInfo) => void;
  private logger: Logger;
  private lastUpdate: number = 0;
  private throttleInterval: number = 100; // Minimum ms between updates
  private lastPercentage: number = -1; // Track last percentage to avoid duplicate updates
  private queuedUpdate: NodeJS.Timeout | null = null;

  constructor(onProgress?: (info: ProgressInfo) => void, logger?: Logger) {
    this.onProgress = onProgress;
    this.logger = logger || new Logger('[ProgressReporter]');
  }

  /**
   * Starts a new processing stage
   */
  public startStage(stage: 'parsing' | 'deduplicating' | 'generating_embeddings' | 'saving', total: number): void {
    this.lastPercentage = -1; // Reset percentage tracker on new stage
    this.updateProgress(stage, 0, total);
  }

  /**
   * Updates the progress of a stage with throttling and error handling
   */
  public updateProgress(
    stage: 'parsing' | 'deduplicating' | 'generating_embeddings' | 'saving', 
    processed: number, 
    total: number
  ): void {
    if (!this.onProgress) return;
    
    const now = Date.now();
    const percentage = Math.round((processed / Math.max(total, 1)) * 100);
    
    // Skip if same percentage and not the final update (100%)
    if (percentage === this.lastPercentage && percentage !== 100 && processed !== total) {
      return;
    }
    
    // If we recently updated and this isn't a completion update, throttle it
    if (now - this.lastUpdate < this.throttleInterval && percentage !== 100 && processed !== total) {
      // Clear any existing queued update
      if (this.queuedUpdate) {
        clearTimeout(this.queuedUpdate);
      }
      
      // Queue this update to run after the throttle interval
      this.queuedUpdate = setTimeout(() => {
        this.lastUpdate = Date.now();
        this.lastPercentage = percentage;
        this.safelyCallProgressCallback({
          processed,
          total,
          percentage,
          stage
        });
        this.queuedUpdate = null;
      }, this.throttleInterval - (now - this.lastUpdate));
      return;
    }
    
    // Otherwise, update immediately
    this.lastUpdate = now;
    this.lastPercentage = percentage;
    this.safelyCallProgressCallback({
      processed,
      total,
      percentage,
      stage
    });
  }

  /**
   * Safely calls the progress callback with error handling
   */
  private safelyCallProgressCallback(info: ProgressInfo): void {
    if (!this.onProgress) return;
    
    try {
      this.onProgress(info);
    } catch (error) {
      // Log the error but don't let it crash the process
      this.logger.warn(`Error sending progress update: ${error}`);
      // If we've had one error, don't keep trying to call the callback
      if (error instanceof Error && error.message.includes('Object has been destroyed')) {
        this.logger.warn('WebContents has been destroyed, disabling progress updates');
        this.onProgress = undefined; // Stop trying to call the callback
      }
    }
  }

  /**
   * Completes a processing stage
   */
  public completeStage(stage: 'parsing' | 'deduplicating' | 'generating_embeddings' | 'saving', total: number): void {
    // Clear any queued updates
    if (this.queuedUpdate) {
      clearTimeout(this.queuedUpdate);
      this.queuedUpdate = null;
    }
    
    // Force completion update
    this.lastPercentage = 100;
    this.updateProgress(stage, total, total);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Export the main module for importing ChatGPT history
 * SOLID implementation with separated services and well-defined responsibilities
 */

// Export the main handler
export { importChatGPTHistoryHandler } from './handlers/importChatGPTHandler';

// Export public types and interfaces
export type { 
  ImportChatGPTParams, 
  ImportResult,
  ProgressInfo 
} from './interfaces/types';
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Adapted to simulate the real import handler

jest.mock('../config/UserConfig', () => ({
  getPrimaryUser: () => 'testUser',
}));

// Interfaces for type checking in the test
interface MessageItem {
  message?: {
    id?: string;
    author?: { role?: string };
    content?: { parts?: string[] };
    create_time?: number;
    parent?: string;
  };
}

interface ImportParams {
  fileBuffer: Buffer;
  mode: 'increment' | 'overwrite';
  user: string;
}

interface PineconeHelperDeps {
  pineconeHelper: {
    saveToPinecone: jest.Mock;
    checkExistingIds?: jest.Mock;
    deleteAllUserVectors?: jest.Mock;
  };
}

describe('Integration IPC import-chatgpt-history', () => {
  let pineconeHelper: {
    saveToPinecone: jest.Mock;
    checkExistingIds?: jest.Mock;
    deleteAllUserVectors?: jest.Mock;
  };

  beforeEach(() => {
    // Mock PineconeHelper for deduplication by id
    pineconeHelper = {
      saveToPinecone: jest.fn().mockResolvedValue({ success: true }),
      checkExistingIds: jest.fn().mockImplementation((_user, ids) => {
        console.log(`Verifying ${ids.length} IDs in the test`);
        return Promise.resolve(['id1', 'id2']); 
      }),
      deleteAllUserVectors: jest.fn().mockResolvedValue({ success: true })
    };
  });

  it('imports only new messages in incremental mode', async () => {
    // Simulate file buffer in the real ChatGPT export format
    const mapping = {
      node1: { message: { id: 'id1', author: { role: 'user' }, content: { parts: ['msg1'] }, create_time: 1 } },
      node2: { message: { id: 'id2', author: { role: 'assistant' }, content: { parts: ['msg2'] }, create_time: 2 } },
      node3: { message: { id: 'id3', author: { role: 'user' }, content: { parts: ['msg3'] }, create_time: 3 } },
      node4: { message: { id: 'id4', author: { role: 'assistant' }, content: { parts: ['msg4'] }, create_time: 4 } }
    };
    const fileBuffer = Buffer.from(JSON.stringify([
      { mapping, title: 'sessão', create_time: 0, update_time: 10 }
    ]));

    // Simulate the real handler (adapted from ipcHandlers.ts)
    async function importChatGPTTestHandler(
      { fileBuffer, mode, user }: ImportParams, 
      _event: unknown, 
      deps: PineconeHelperDeps
    ) {
      const raw = JSON.parse(fileBuffer.toString('utf-8'));
      let allMessages = [];
      for (const session of raw) {
        const mapping = session.mapping || {};
        for (const item of Object.values(mapping) as MessageItem[]) {
          const msg = item.message;
          if (!msg) continue;
          const role = msg.author?.role || 'unknown';
          const content = Array.isArray(msg.content?.parts) ? msg.content.parts[0] : msg.content?.parts;
          if (!content || typeof content !== 'string' || !content.trim()) continue;
          allMessages.push({
            role,
            content: content.trim(),
            timestamp: msg.create_time || null,
            id: msg.id || null,
            parent: msg.parent || null,
            session_title: session.title || null,
            session_create_time: session.create_time || null,
            session_update_time: session.update_time || null
          });
        }
      }
      let existingMessageIds = new Set();
      if (mode === 'increment') {
        // Extract message IDs for verification
        const messageIdsToCheck = allMessages
          .filter(msg => msg.id) 
          .map(msg => msg.id as string);
          
        if (deps.pineconeHelper.checkExistingIds && messageIdsToCheck.length > 0) {
          const existingIds = await deps.pineconeHelper.checkExistingIds(user, messageIdsToCheck);
          existingMessageIds = new Set(existingIds);
        }
        allMessages = allMessages.filter(m => m.id && !existingMessageIds.has(m.id));
      }
      if (mode === 'overwrite') {
        if (deps.pineconeHelper.deleteAllUserVectors) {
          await deps.pineconeHelper.deleteAllUserVectors(user);
        }
      }
      // Mock embedding e save
      const vectors = allMessages.map(msg => ({
        id: msg.id,
        values: [0],
        metadata: { ...msg, user, imported_from: 'chatgpt', imported_at: Date.now(), messageId: msg.id || null }
      }));
      if (vectors.length > 0) {
        await deps.pineconeHelper.saveToPinecone(vectors);
      }
      return { success: true, imported: vectors.length, skipped: 4 - vectors.length };
    }

    const result = await importChatGPTTestHandler({
      fileBuffer,
      mode: 'increment',
      user: 'testUser'
    }, undefined, { pineconeHelper: pineconeHelper });
    expect(result.imported).toBe(2); 
    expect(result.skipped).toBe(2);  
  });

  it('imports all messages in overwrite mode', async () => {
    const mapping = {
      node1: { message: { id: 'id1', author: { role: 'user' }, content: { parts: ['msg1'] }, create_time: 1 } },
      node2: { message: { id: 'id2', author: { role: 'assistant' }, content: { parts: ['msg2'] }, create_time: 2 } },
      node3: { message: { id: 'id3', author: { role: 'user' }, content: { parts: ['msg3'] }, create_time: 3 } },
      node4: { message: { id: 'id4', author: { role: 'assistant' }, content: { parts: ['msg4'] }, create_time: 4 } }
    };
    const fileBuffer = Buffer.from(JSON.stringify([
      { mapping, title: 'sessão', create_time: 0, update_time: 10 }
    ]));
    // Reuses the same test function
    async function importChatGPTTestHandler(
      { fileBuffer, mode, user }: ImportParams, 
      _event: unknown, 
      deps: PineconeHelperDeps
    ) {
      const raw = JSON.parse(fileBuffer.toString('utf-8'));
      let allMessages = [];
      for (const session of raw) {
        const mapping = session.mapping || {};
        for (const item of Object.values(mapping) as MessageItem[]) {
          const msg = item.message;
          if (!msg) continue;
          const role = msg.author?.role || 'unknown';
          const content = Array.isArray(msg.content?.parts) ? msg.content.parts[0] : msg.content?.parts;
          if (!content || typeof content !== 'string' || !content.trim()) continue;
          allMessages.push({
            role,
            content: content.trim(),
            timestamp: msg.create_time || null,
            id: msg.id || null,
            parent: msg.parent || null,
            session_title: session.title || null,
            session_create_time: session.create_time || null,
            session_update_time: session.update_time || null
          });
        }
      }
      let existingMessageIds = new Set();
      if (mode === 'increment') {
        // Extract message IDs for verification
        const messageIdsToCheck = allMessages
          .filter(msg => msg.id) 
          .map(msg => msg.id as string);
          
        if (deps.pineconeHelper.checkExistingIds && messageIdsToCheck.length > 0) {
          const existingIds = await deps.pineconeHelper.checkExistingIds(user, messageIdsToCheck);
          existingMessageIds = new Set(existingIds);
        }
        allMessages = allMessages.filter(m => m.id && !existingMessageIds.has(m.id));
      }
      if (mode === 'overwrite') {
        if (deps.pineconeHelper.deleteAllUserVectors) {
          await deps.pineconeHelper.deleteAllUserVectors(user);
        }
      }
      // Mock embedding e save
      const vectors = allMessages.map(msg => ({
        id: msg.id,
        values: [0],
        metadata: { ...msg, user, imported_from: 'chatgpt', imported_at: Date.now(), messageId: msg.id || null }
      }));
      if (vectors.length > 0) {
        await deps.pineconeHelper.saveToPinecone(vectors);
      }
      return { success: true, imported: vectors.length, skipped: 4 - vectors.length };
    }
    const result = await importChatGPTTestHandler({
      fileBuffer,
      mode: 'overwrite',
      user: 'testUser'
    }, undefined, { pineconeHelper: pineconeHelper });
    expect(result.imported).toBe(4); // All
    expect(result.skipped).toBe(0);  // None skipped
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useEffect, useRef } from "react";
import {
  MicrophoneState,
  useDeepgram,
  useMicrophone,
} from "../../components/context";
import { ConnectionState } from "../../components/context/deepgram/interfaces/deepgram/IDeepgramService";
import TranscriptionPanel from "../../components/shared/TranscriptionPanel/TranscriptionPanel";

/**
 * Main transcription module that encapsulates all transcription-related functionality
 * Following Single Responsibility Principle by handling only transcription logic
 */
export const TranscriptionModule: React.FC = () => {
  // Get microphone hooks and state
  const {
    microphoneState,
    startMicrophone,
    stopMicrophone,
    isMicrophoneOn,
    isSystemAudioOn,
    setIsSystemAudioOn,
  } = useMicrophone();

  // Get deepgram services
  const {
    connectToDeepgram,
    disconnectFromDeepgram,
    connectionState,
    flushTranscriptionsToUI,
  } = useDeepgram();

  // Refs to maintain latest values in event handlers
  const microphoneStateRef = useRef(microphoneState);
  const isMicrophoneOnRef = useRef(isMicrophoneOn);
  const isSystemAudioOnRef = useRef(isSystemAudioOn);

  // Keep refs up to date
  useEffect(() => {
    microphoneStateRef.current = microphoneState;
  }, [microphoneState]);

  useEffect(() => {
    isMicrophoneOnRef.current = isMicrophoneOn;
  }, [isMicrophoneOn]);

  useEffect(() => {
    isSystemAudioOnRef.current = isSystemAudioOn;
  }, [isSystemAudioOn]);

  // Connect/disconnect Deepgram based on microphone state
  useEffect(() => {
    if (microphoneState === MicrophoneState.Open) {
      // Start transcription if microphone is open and not already connected
      if (
        connectionState !== ConnectionState.OPEN &&
        connectionState !== ConnectionState.CONNECTING
      ) {
        console.log(
          "🎤 Starting Deepgram connection due to microphone state change"
        );
        connectToDeepgram();
      }
    } else if (microphoneState === MicrophoneState.Stopped) {
      // Stop transcription when recording stops
      if (connectionState === ConnectionState.OPEN) {
        console.log(
          "🛑 Stopping Deepgram connection due to microphone state change"
        );

        // Note: Transcriptions are already displayed in real-time
        // No need to flush on stop as they're already visible
        console.log("🛑 Recording stopped, transcriptions already visible");

        disconnectFromDeepgram();
      }
    }
  }, [
    microphoneState,
    connectToDeepgram,
    disconnectFromDeepgram,
    connectionState,
    flushTranscriptionsToUI,
  ]);

  // Setup keyboard shortcut for recording toggle
  useEffect(() => {
    const unsubscribeToggleRecording = window.electronAPI.toogleNeuralRecording(
      () => {
        console.log("🔊 Shortcut pressed! Toggling recording...");
        if (microphoneStateRef.current === MicrophoneState.Open) {
          console.log("🛑 Stopping recording via shortcut...");
          stopMicrophone();
        } else {
          console.log("🎤 Starting recording via shortcut...");
          // If no audio source is active, activate system audio by default before recording
          if (!isMicrophoneOnRef.current && !isSystemAudioOnRef.current) {
            setIsSystemAudioOn(true);
            setTimeout(() => startMicrophone(), 100);
          } else {
            startMicrophone();
          }
        }
      }
    );

    return () => {
      unsubscribeToggleRecording();
    };
  }, []);

  return (
    <div style={{ height: "100%", width: "100%", overflow: "hidden" }}>
      <TranscriptionPanel
        onClose={() => {}} // No close functionality needed
        width="100%"
      />
    </div>
  );
};

export default TranscriptionModule;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// HuggingFaceNeuralSignalService.ts
// Symbolic: Neural signal extraction service using HuggingFace (cortex: huggingface)
import { NeuralSignalResponse } from "../../../components/context/deepgram/interfaces/neural/NeuralSignalTypes";
import { HuggingFaceServiceFacade } from "../../../components/context/deepgram/services/huggingface/HuggingFaceServiceFacade";
import { INeuralSignalService } from "../../../domain/core/neural/INeuralSignalService";
import { ISemanticEnricher } from "../../../domain/core/neural/ISemanticEnricher";
import { getOption, STORAGE_KEYS } from "../../../services/StorageService";
import {
  buildSystemPrompt,
  buildUserPrompt,
} from "../../../shared/utils/neuralPromptBuilder";
import {
  extractNeuralSignalJsons,
  parseNeuralSignal,
} from "../../../shared/utils/neuralSignalParser";
import { FunctionSchemaRegistry } from "../../../components/context/deepgram/services/function-calling/FunctionSchemaRegistry";

/**
 * Symbolic: HuggingFace implementation of neural signal service
 * This service extracts symbolic neural signals and performs semantic enrichment
 * using local HuggingFace models.
 */
export class HuggingFaceNeuralSignalService
  implements INeuralSignalService, ISemanticEnricher
{
  /**
   * Constructor with dependency injection for HuggingFaceServiceFacade
   */
  constructor(private huggingFaceClient: HuggingFaceServiceFacade) {}

  /**
   * Symbolic: Extracts neural signals using HuggingFace
   */
  async generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse> {
    try {
      const systemPromptContent = buildSystemPrompt();
      let userPromptContent = buildUserPrompt(
        prompt,
        temporaryContext,
        language
      );

      // For HuggingFace models without native function-calling, force JSON output
      userPromptContent += `\n\nIMPORTANT OUTPUT FORMAT:\nReturn ONLY a JSON array with objects following exactly this schema (no markdown, no extra text):\n[{\n  \"core\": \"area\",\n  \"query\": \"symbolic query\",\n  \"intensity\": 0.5,\n  \"keywords\": [\"k1\", \"k2\"],\n  \"topK\": 5,\n  \"filters\": { },\n  \"expand\": false,\n  \"symbolicInsights\": \"...\"\n}]`;

      const activateBrainAreaSchema =
        FunctionSchemaRegistry.getInstance().get("activateBrainArea");
      const tools = activateBrainAreaSchema
        ? [{ type: "function", function: activateBrainAreaSchema }]
        : [];

      const messages = [
        { role: "system" as const, content: systemPromptContent },
        { role: "user" as const, content: userPromptContent },
      ];
      const response = await this.huggingFaceClient.callOpenAIWithFunctions({
        model: getOption(STORAGE_KEYS.HF_MODEL) || "Xenova/llama2.c-stories15M",
        messages,
        tools,
        tool_choice: {
          type: "function",
          function: { name: "activateBrainArea" },
        },
        temperature: 0.2,
      });

      const toolCalls = response.choices?.[0]?.message?.tool_calls;
      let signals: NeuralSignalResponse["signals"] = [];
      if (toolCalls && Array.isArray(toolCalls)) {
        signals = toolCalls
          .filter((call: any) => call.function?.name === "activateBrainArea")
          .map((call: any): any => {
            try {
              const args = call.function?.arguments
                ? JSON.parse(call.function.arguments)
                : {};
              const baseSignal: Partial<any> = {
                core: args.core,
                intensity: Math.max(0, Math.min(1, args.intensity ?? 0.5)),
                symbolic_query: { query: args.query ?? "" },
              };
              if (Array.isArray(args.keywords))
                baseSignal.keywords = args.keywords;
              if (args.filters) baseSignal.filters = args.filters;
              if (typeof args.expand === "boolean")
                baseSignal.expand = args.expand;
              if (args.symbolicInsights)
                baseSignal.symbolicInsights = args.symbolicInsights;
              if (typeof args.topK !== "undefined") baseSignal.topK = args.topK;
              if (typeof baseSignal.core !== "undefined") return baseSignal;
              return undefined;
            } catch {
              return undefined;
            }
          })
          .filter(
            (signal: any): signal is any =>
              !!signal && typeof signal.core !== "undefined"
          );
      }

      // If unable to extract function calls, try to extract text-based signals
      if (signals.length === 0 && response.choices?.[0]?.message?.content) {
        // Parse neural signals using the utility parsers
        const extractedSignals = extractNeuralSignalJsons(
          response.choices?.[0]?.message?.content
        )
          .map(parseNeuralSignal)
          .filter(
            (signal): signal is NeuralSignalResponse["signals"][0] =>
              signal !== null
          );
        signals = extractedSignals;
      }

      return { signals };
    } catch (error) {
      // Log error and return empty signals for graceful degradation
      console.error("Neural signal extraction error:", error);
      return { signals: [] };
    }
  }

  /**
   * Symbolic: Semantic enrichment using HuggingFace
   */
  async enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }> {
    try {
      const enrichSchema = FunctionSchemaRegistry.getInstance().get(
        "enrichSemanticQuery"
      );
      const enrichmentTools = enrichSchema
        ? [{ type: "function", function: enrichSchema }]
        : [];

      const systemPrompt = `You are a semantic enrichment system. Expand queries with related terms while preserving intent. Generate 3-8 relevant keywords. Respond using enrichSemanticQuery function.`;

      let userPrompt = `CORE: ${core}\nINTENSITY: ${intensity}\nORIGINAL QUERY: ${query}`;
      if (context) userPrompt += `\nCONTEXT: ${context}`;
      if (language) userPrompt += `\nLANGUAGE: ${language}`;

      const messages = [
        { role: "system" as const, content: systemPrompt },
        { role: "user" as const, content: userPrompt },
      ];

      const response = await this.huggingFaceClient.callOpenAIWithFunctions({
        model: getOption(STORAGE_KEYS.HF_MODEL) || "sshleifer/tiny-gpt2",
        messages: messages,
        tools: enrichmentTools,
        tool_choice: {
          type: "function",
          function: { name: "enrichSemanticQuery" },
        },
        temperature: 0.2,
      });

      // Symbolic: Use central neural signal parser for all JSON arguments (tool_calls)
      const toolCalls = response.choices?.[0]?.message?.tool_calls;
      if (
        toolCalls &&
        Array.isArray(toolCalls) &&
        toolCalls[0]?.function?.arguments
      ) {
        const signal = parseNeuralSignal(toolCalls[0].function.arguments as string);
        if (signal && signal.symbolic_query?.query) {
          return {
            enrichedQuery: signal.symbolic_query.query,
            keywords: signal.keywords || [],
          };
        } else {
          return { enrichedQuery: query, keywords: [] };
        }
      }
      return { enrichedQuery: query, keywords: [] };
    } catch (error) {
      // Handle general service errors with original query fallback
      console.error("Semantic enrichment error:", error);
      return { enrichedQuery: query, keywords: [] };
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// src/lib/utils.ts

import { type ClassValue, clsx } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
// src/polyfills/sharpStub.js
// Minimal stub for the "sharp" native module so that bundlers/electron can resolve it
// without trying to include the native binary. Any runtime call will explicitly throw
// an error, signalling that image manipulation is not available in this environment.

function unsupported() {
  throw new Error(
    "The 'sharp' module is not available in this environment. If you need image processing, " +
      "run the code in a Node.js environment with the native sharp binary installed."
  );
}

const sharpProxy = new Proxy(unsupported, {
  get() {
    return unsupported;
  },
  apply() {
    return unsupported();
  },
});

export default sharpProxy;
module.exports = sharpProxy;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IEmbeddingService } from "../../components/context/deepgram/interfaces/openai/IEmbeddingService";
import { getOption, STORAGE_KEYS } from "../StorageService";

const ALLOWED_EMBEDDERS = [
  // Prefer lighter MiniLM model (384-d) for faster embeddings
  "Xenova/all-MiniLM-L6-v2"
] as const;
type AllowedEmbedderId = (typeof ALLOWED_EMBEDDERS)[number];

export class HuggingFaceEmbeddingService implements IEmbeddingService {
  private embedder: any = null;
  private modelId: AllowedEmbedderId | null = null;
  private initialized = false;

  constructor() {
    // Initialize environment using centralized configuration
    this.initializeEnvironment().catch((error) => {
      console.error("[HFE] Environment initialization failed:", error);
    });
  }

  /**
   * Initialize transformers.js environment using centralized configuration
   */
  private async initializeEnvironment() {
    if (this.initialized) return;

    try {
      this.initialized = true;
      console.log(
        "✅ [HFE] Environment initialized using centralized configuration"
      );
    } catch (error) {
      console.error(
        "❌ [HFE] Failed to initialize transformers environment:",
        error
      );
      throw error;
    }
  }

  isInitialized(): boolean {
    return this.embedder !== null;
  }

  async initialize(config?: Record<string, any>): Promise<boolean> {
    await this.loadModel(
      getOption(STORAGE_KEYS.HF_EMBEDDING_MODEL) as AllowedEmbedderId
    );
    return true;
  }

  /** Carrega UM dos dois modelos de embedding permitidos */
  async loadModel(
    modelId: AllowedEmbedderId,
    device: "wasm" = "wasm"
  ) {
    // Ensure environment is initialized before loading models
    if (!this.initialized) {
      await this.initializeEnvironment();
    }

    if (!ALLOWED_EMBEDDERS.includes(modelId)) {
      throw new Error(`Embedder não suportado: ${modelId}`);
    }
    if (this.modelId === modelId && this.embedder) return;

    // Use centralized model loading configuration
    const { loadModelWithOptimalConfig } = await import(
      "../../utils/transformersEnvironment"
    );
    this.embedder = await loadModelWithOptimalConfig(
      modelId,
      "feature-extraction",
      {
        device,
        dtype: "fp32", // Use fp32 for better compatibility with all models
      }
    );
    this.modelId = modelId;
  }

  /** Gera embedding de um texto */
  async createEmbedding(text: string): Promise<number[]> {
    const modelId = this.modelId ?? ALLOWED_EMBEDDERS[0];
    await this.loadModel(modelId);
    const output = await this.embedder(text, { pooling: "mean" });
    return Array.isArray(output) ? (output[0] as number[]) : [];
  }

  /** Gera embeddings em batch */
  async createEmbeddings(texts: string[]): Promise<number[][]> {
    const modelId = this.modelId ?? ALLOWED_EMBEDDERS[0];
    await this.loadModel(modelId);
    const outputs = await Promise.all(
      texts.map((t) => this.embedder(t, { pooling: "mean" }))
    );
    return outputs.map((o) => (Array.isArray(o) ? (o[0] as number[]) : []));
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { AutoTokenizer } from "@huggingface/transformers";
import { cleanThinkTags } from "../../components/context/deepgram/utils/ThinkTagCleaner";

// Flag to enable legacy chat_template fallback for models missing template
const ENABLE_FALLBACK_CHAT_TEMPLATE = false;

// modelos suportados no navegador (geração de texto)
export const SUPPORTED_HF_BROWSER_MODELS = [
  "Xenova/distilgpt2", // ~353MB, DistilGPT-2 otimizado - FUNCIONA
  "Xenova/gpt2", // ~548MB, GPT-2 base estável - FUNCIONA
  "Xenova/llama2.c-stories15M", // ~15MB, modelo muito pequeno - FUNCIONA
  "Xenova/TinyLlama-1.1B-Chat-v1.0", // ~1.1B, modelo de chat pequeno - FUNCIONA
] as const;

export type HuggingFaceLocalOptions = {
  model?: (typeof SUPPORTED_HF_BROWSER_MODELS)[number];
  maxTokens?: number;
  temperature?: number;
  dtype?: "q4" | "q8" | "fp32" | "fp16"; // Ordered by performance vs. quality
  device?: "webgpu" | "wasm" | "auto";
  forceReload?: boolean; // Force reload model even if already loaded
};

export class HuggingFaceLocalService {
  private generator: any = null;
  private tokenizer: any = null;
  private currentModel: string | null = null;
  private initialized = false;
  private isLoading = false;

  constructor() {
    // Initialize environment asynchronously
    this.initializeEnvironment().catch((error) => {
      console.error("[HFS] Environment initialization failed:", error);
    });
  }

  /**
   * Initialize transformers.js environment using centralized configuration
   */
  private async initializeEnvironment() {
    if (this.initialized) return;

    try {
      console.log("[HFS] Initializing transformers.js environment...");

      this.initialized = true;
      console.log("✅ [HFS] Environment initialized successfully");
    } catch (error) {
      console.error(
        "❌ [HFS] Failed to initialize transformers environment:",
        error
      );
      throw new Error(
        `Environment initialization failed: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    }
  }

  /**
   * Load model using centralized configuration with enhanced error handling
   */
  async loadModel(opts: {
    modelId: string;
    device: HuggingFaceLocalOptions["device"];
    dtype: HuggingFaceLocalOptions["dtype"];
    forceReload?: boolean;
  }) {
    const { modelId, forceReload } = opts;

    console.log(`[HFS] 🔍 DEBUG: loadModel called with original opts:`, opts);

    // FORCE correct configurations - ignore any incorrect device/dtype from opts
    const device = "wasm"; // Always use wasm for browser compatibility
    const dtype = "fp32"; // Always use fp32 - model-specific configs will override if needed

    console.log(`[HFS] 🔍 DEBUG: Forced device: ${device}, dtype: ${dtype}`);

    // Prevent concurrent loading
    if (this.isLoading) {
      console.log("[HFS] Model loading already in progress, waiting...");
      return;
    }

    // Check if model is already loaded
    if (this.currentModel === modelId && this.generator && !forceReload) {
      console.log(`[HFS] Model ${modelId} already loaded`);
      return;
    }

    // Validate model is supported
    if (!SUPPORTED_HF_BROWSER_MODELS.includes(modelId as any)) {
      throw new Error(
        `Unsupported model: ${modelId}. Supported models: ${SUPPORTED_HF_BROWSER_MODELS.join(
          ", "
        )}`
      );
    }

    // Ensure environment is initialized
    if (!this.initialized) {
      await this.initializeEnvironment();
    }

    this.isLoading = true;
    console.log(
      `[HFS] Loading model: ${modelId} with device: ${device}, dtype: ${dtype}`
    );

    try {
      // Clean up previous model if exists
      if (this.generator && forceReload) {
        await this.dispose();
      }

      // Use loadModelWithOptimalConfig from centralized configuration
      const { loadModelWithOptimalConfig } = await import(
        "../../utils/transformersEnvironment"
      );

      const additionalOptions = {
        // Enhanced progress callback for better user feedback
        progress_callback: (data: any) => {
          if (data.status === "downloading") {
            console.log(
              `[HFS] Downloading: ${data.name || data.file} - ${Math.round(
                data.progress || 0
              )}%`
            );
          } else if (data.status === "loading") {
            console.log(`[HFS] Loading: ${data.name || data.file}`);
          } else if (data.status === "ready") {
            console.log(`[HFS] Ready: ${data.name || data.file}`);
          }
        },

        // Enhanced session options for better performance
        session_options: {
          logSeverityLevel: 3, // Reduce logging noise
          graphOptimizationLevel: "all",
          enableMemPattern: true,
          enableCpuMemArena: true,
          // Always use wasm for browser compatibility
          executionProviders: ["wasm"],
        },

        // Cache configuration - allow downloads but prefer cache
        cache_dir: undefined, // Use environment default
        local_files_only: false, // Allow downloads if not in cache
        use_auth_token: false,

        // Retry configuration for network issues
        max_retries: 3,
        retry_delay: 1000, // 1 second delay between retries

        // IMPORTANT: Don't override device/dtype here - let model-specific configs take precedence
        // Only pass device/dtype if they are explicitly different from defaults
        // This allows model-specific configurations to take precedence
      };

      console.log(
        `[HFS] 🔍 DEBUG: additionalOptions being passed:`,
        additionalOptions
      );

      console.log(`[HFS] Loading with configuration:`, {
        modelId,
        device,
        dtype,
        cache_dir: "using environment default",
        local_files_only: additionalOptions.local_files_only,
      });

      this.generator = await loadModelWithOptimalConfig(
        modelId,
        "text-generation",
        additionalOptions
      );

      // Load tokenizer for chat template
      this.tokenizer = await AutoTokenizer.from_pretrained(modelId);

      // Add default chat template for models that don't have one
      // Fallback chat_template injection disabled for vLLM-only setup
      if (ENABLE_FALLBACK_CHAT_TEMPLATE && !this.tokenizer.chat_template) {
        console.log(`[HFS] Adding default chat template for ${modelId}`);

        // Define a simple but effective chat template
        this.tokenizer.chat_template = `{% for message in messages %}{% if message['role'] == 'system' %}System: {{ message['content'] }}
{% elif message['role'] == 'user' %}User: {{ message['content'] }}
{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}
{% endif %}{% endfor %}{% if add_generation_prompt %}Assistant: {% endif %}`;

        // Also ensure apply_chat_template method exists
        if (!this.tokenizer.apply_chat_template) {
          this.tokenizer.apply_chat_template = function (
            messages: any[],
            options: any = {}
          ) {
            let result = "";

            // Process messages
            for (const message of messages) {
              if (message.role === "system") {
                result += `System: ${message.content}\n`;
              } else if (message.role === "user") {
                result += `User: ${message.content}\n`;
              } else if (message.role === "assistant") {
                result += `Assistant: ${message.content}\n`;
              }
            }

            // Add generation prompt if requested
            if (options.add_generation_prompt) {
              result += "Assistant: ";
            }

            return result;
          };
        }
      }

      this.currentModel = modelId;
      console.log(
        `[HFS] ✅ Model loaded successfully: ${modelId} (${device}/${dtype})`
      );
    } catch (error) {
      console.error(`[HFS] ❌ Failed to load model ${modelId}:`, error);

      // Enhanced error messages with specific guidance
      if (error instanceof Error) {
        if (error.message.includes("<!DOCTYPE")) {
          throw new Error(
            `Model loading failed: Server returned HTML instead of model files. ` +
              `This usually means:\n` +
              `1. The model "${modelId}" doesn't exist or isn't available\n` +
              `2. Network connectivity issues\n` +
              `3. HuggingFace server issues\n` +
              `Try a different model or check your internet connection.`
          );
        } else if (error.message.includes("CORS")) {
          throw new Error(
            `CORS error loading model ${modelId}. This may be due to:\n` +
              `1. Network configuration issues\n` +
              `2. Proxy server problems\n` +
              `3. Browser security settings\n` +
              `Try refreshing the application or check network settings.`
          );
        } else if (
          error.message.includes("fetch") ||
          error.message.includes("NetworkError")
        ) {
          throw new Error(
            `Network error loading model ${modelId}. Please:\n` +
              `1. Check your internet connection\n` +
              `2. Verify the model exists on HuggingFace\n` +
              `3. Try again in a few moments\n` +
              `4. Consider using a smaller model for testing`
          );
        } else if (
          error.message.includes("quota") ||
          error.message.includes("storage")
        ) {
          throw new Error(
            `Storage error loading model ${modelId}. This may be due to:\n` +
              `1. Insufficient disk space\n` +
              `2. Cache directory permissions\n` +
              `3. Storage quota exceeded\n` +
              `Try clearing cache or freeing up disk space.`
          );
        }
      }

      throw new Error(
        `Failed to load model "${modelId}": ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    } finally {
      this.isLoading = false;
    }
  }

  /**
   * Generate text using the loaded model.
   */
  async generate(
    messages: Array<{ role: "system" | "user" | "assistant"; content: string }>,
    opts: HuggingFaceLocalOptions = {}
  ): Promise<string> {
    if (!this.initialized || !this.generator) {
      throw new Error("Model not initialized. Call loadModel() first.");
    }

    try {
      // Build conversation prompt
      const prompt =
        messages
          .map(
            (m) =>
              `${m.role === "assistant" ? "Assistant" : "User"}: ${m.content}`
          )
          .join("\n\n") + "\n\nAssistant:";

      const generationOptions = {
        max_new_tokens: opts.maxTokens || 512,
        temperature: opts.temperature ?? 0.7,
        do_sample: (opts.temperature ?? 0.7) > 0,
      } as any;

      const result: any[] = await this.generator(prompt, generationOptions);

      let first = result[0];
      if (Array.isArray(first)) first = first[0];

      // Extract content depending on result structure
      let generatedTextRaw = first?.generated_text ?? first?.text ?? undefined;

      if (typeof generatedTextRaw === "undefined") {
        throw new Error("Invalid generation result format");
      }

      const content = Array.isArray(generatedTextRaw)
        ? generatedTextRaw.at(-1)?.content ?? JSON.stringify(generatedTextRaw)
        : String(generatedTextRaw);

      // Remove the original prompt from the response if it's included
      let responseText = content;
      if (responseText.includes(prompt)) {
        responseText = responseText.replace(prompt, "").trim();
      }

      // Clean think tags from the response
      const cleanedResponse = cleanThinkTags(responseText);

      return cleanedResponse;
    } catch (error) {
      console.error("[HFS] ❌ Generation error:", error);
      throw error;
    }
  }

  /**
   * Generate text with function calling support.
   */
  async generateWithFunctions(
    messages: Array<{ role: "system" | "user" | "assistant"; content: string }>,
    tools: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }> = [],
    opts: {
      temperature?: number;
      maxTokens?: number;
    } = {}
  ): Promise<{ content?: string; tool_calls?: Array<any> }> {
    if (!this.initialized || !this.generator) {
      throw new Error("Model not initialized. Call loadModel() first.");
    }

    try {
      // Function to extract tool calls from generated text
      const extractToolCalls = (text: string) => {
        const toolCalls: any[] = [];

        // Look for JSON-like function calls in the text
        const functionCallRegex =
          /\{"function":\s*\{"name":\s*"([^"]+)",\s*"arguments":\s*(\{[^}]*\})\}\}/g;
        let match;

        while ((match = functionCallRegex.exec(text)) !== null) {
          try {
            const functionName = match[1];
            const argumentsStr = match[2];
            const args = JSON.parse(argumentsStr);

            toolCalls.push({
              function: {
                name: functionName,
                arguments: args,
              },
            });
          } catch (e) {
            // Skip invalid JSON
            continue;
          }
        }

        // Alternative format: look for function calls in a more flexible way
        if (toolCalls.length === 0) {
          const altRegex = /(\w+)\s*\(\s*([^)]*)\s*\)/g;
          while ((match = altRegex.exec(text)) !== null) {
            const functionName = match[1];
            const argsStr = match[2];

            // Check if this function name matches any of our tools
            const matchingTool = tools.find(
              (t) => t.function.name === functionName
            );
            if (matchingTool) {
              try {
                // Try to parse arguments as JSON or create simple object
                let args = {};
                if (argsStr.trim()) {
                  if (argsStr.includes(":")) {
                    // Try to parse as JSON-like
                    args = JSON.parse(`{${argsStr}}`);
                  } else {
                    // Simple string argument
                    args = { value: argsStr.trim() };
                  }
                }

                toolCalls.push({
                  function: {
                    name: functionName,
                    arguments: args,
                  },
                });
              } catch (e) {
                // Skip invalid arguments
                continue;
              }
            }
          }
        }

        return toolCalls;
      };

      // Build chat prompt passing the tools array per HF docs, with fallback
      let prompt: string | undefined;
      let usedChatTemplate = false;

      // NOTA: Muitos modelos pequenos/comunitários não têm chat_template (ex: Xenova/llama2.c-stories15M)
      // Isso é normal e usaremos um fallback manual nesses casos

      // Verificação inicial se temos um tokenizer com chat_template
      const rawTemplate = (this.tokenizer as any)?.chat_template;
      const hasChatTemplate =
        this.tokenizer &&
        typeof this.tokenizer.apply_chat_template === "function" &&
        typeof rawTemplate === "string" &&
        rawTemplate.trim().length > 0;

      if (!hasChatTemplate) {
        // Model doesn't have chat template - this is expected for many models
        // We'll use manual formatting without logging warnings
      } else {
        try {
          // Formato da chamada seguindo exatamente a documentação
          const templateOpts: any = {
            add_generation_prompt: true,
            tokenize: false, // Force string output
          };

          // Adicionar ferramentas apenas se existirem
          if (Array.isArray(tools) && tools.length > 0) {
            templateOpts.tools = tools;
          }

          let tmpPrompt2: any = this.tokenizer.apply_chat_template(
            messages as any,
            templateOpts
          );
          if (Array.isArray(tmpPrompt2)) {
            tmpPrompt2 = tmpPrompt2.join(" ");
          }
          prompt = String(tmpPrompt2);

          // Verificação extra de qualidade
          if (prompt && prompt.trim().length > 0) {
            usedChatTemplate = true;
            console.log("[HFS] Successfully used chat template");
          } else {
            // Chat template produced empty prompt - use fallback without warning
            prompt = undefined;
          }
        } catch (err) {
          // apply_chat_template failed - use fallback without warning
          // This is expected behavior for some models
          prompt = undefined;
        }
      }

      if (
        !prompt ||
        (typeof prompt === "string" && prompt.trim().length === 0)
      ) {
        // Use simple prompt with tool injection for models without chat template
        // This is the expected behavior, not an error

        // Construir uma representação simples das ferramentas disponíveis para injetar no prompt
        let toolsPrompt = "";
        if (Array.isArray(tools) && tools.length > 0) {
          toolsPrompt = "\n\nAvailable tools:\n";
          tools.forEach((tool) => {
            if (tool.function) {
              toolsPrompt += `- ${tool.function.name}: ${
                tool.function.description || ""
              }\n`;

              // Adicionar informações sobre parâmetros se disponíveis
              if (
                tool.function.parameters &&
                typeof tool.function.parameters === "object"
              ) {
                if (tool.function.parameters.properties) {
                  toolsPrompt += "  Parameters:\n";
                  Object.entries(tool.function.parameters.properties).forEach(
                    ([paramName, paramDef]) => {
                      const paramInfo = paramDef as any;
                      toolsPrompt += `  - ${paramName}: ${
                        paramInfo.description || paramInfo.type || ""
                      }\n`;
                    }
                  );
                }
              }
            }
          });

          toolsPrompt +=
            '\nWhen you need to use a tool, use the format: {"function": {"name": "tool_name", "arguments": {"param1": "value1"}}}\n\n';
        }

        // Construir o prompt completo
        prompt =
          toolsPrompt +
          messages
            .map(
              (m) =>
                `${m.role === "assistant" ? "Assistant" : "User"}: ${m.content}`
            )
            .join("\n\n") +
          "\n\nAssistant:";
      }

      const generationOptions = {
        max_new_tokens: opts.maxTokens || 512,
        temperature: opts.temperature ?? 0.7,
        // For better function calling compliance we disable sampling penalties
        do_sample: (opts.temperature ?? 0.7) > 0,
        stop: opts.maxTokens ? undefined : undefined,
      } as any;

      const result: any[] = await this.generator(prompt, generationOptions);

      let first = result[0];
      if (Array.isArray(first)) first = first[0];

      // Extract content depending on result structure
      let generatedTextRaw = first?.generated_text ?? first?.text ?? undefined;

      if (typeof generatedTextRaw === "undefined") {
        throw new Error("Invalid generation result format");
      }

      const content = Array.isArray(generatedTextRaw)
        ? generatedTextRaw.at(-1)?.content ?? JSON.stringify(generatedTextRaw)
        : String(generatedTextRaw);

      // Clean think tags from the content before processing tool calls
      const cleanedContent = cleanThinkTags(content);

      let tool_calls = extractToolCalls(cleanedContent);

      return {
        content: cleanedContent || undefined,
        tool_calls: tool_calls.length > 0 ? tool_calls : undefined,
      };
    } catch (error) {
      console.error("[HFS] ❌ generateWithFunctions error:", error);
      throw error;
    }
  }

  /**
   * Simple wrapper to generate a plain response string (without tool parsing).
   */
  async generateResponse(
    messages: Array<{ role: "system" | "user" | "assistant"; content: string }>,
    opts: { temperature?: number; maxTokens?: number } = {}
  ): Promise<{ response: string }> {
    const responseText = await this.generate(messages, {
      temperature: opts.temperature,
      maxTokens: opts.maxTokens,
    });
    return { response: responseText };
  }

  /**
   * Get current model status
   */
  getStatus(): {
    initialized: boolean;
    currentModel: string | null;
    isLoading: boolean;
  } {
    return {
      initialized: this.initialized,
      currentModel: this.currentModel,
      isLoading: this.isLoading,
    };
  }

  /**
   * Dispose model and free resources.
   */
  async dispose() {
    console.log("[HFS] Disposing model resources...");
    this.generator = null;
    this.tokenizer = null;
    this.currentModel = null;
    this.initialized = false;
  }

  /**
   * Force reload of the current model.
   */
  async forceReload(): Promise<void> {
    if (this.currentModel) {
      const modelId = this.currentModel;
      await this.dispose();
      console.log(`[HFS] Prepared force reload for model: ${modelId}`);
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ModeService.ts — Orch-OS Mode Cortex
// Symbolic Intent: Central neuron for controlling Orch-OS operational mode (basic/advanced)
// Responsibilities: Exposes current mode, persists mode, notifies listeners, enforces symbolic clarity
// IMPORTANT: Use ModeService.getMode() to determine which AI backend to use in all services:
// - 'basic': Use HuggingFace (local models, no Deepgram, DuckDB storage)
// - 'advanced': Use Ollama, Deepgram, DuckDB storage, full features

import { getOption, setOption } from "./StorageService"; // Use symbolic storage neurons

// Symbolic enum for Orch-OS operational modes
export enum OrchOSModeEnum {
  BASIC = "basic",
  ADVANCED = "advanced",
}
export type OrchOSMode = OrchOSModeEnum.BASIC | OrchOSModeEnum.ADVANCED;

import { STORAGE_KEYS } from "./StorageService";
const MODE_STORAGE_KEY = STORAGE_KEYS.APPLICATION_MODE;
const DEFAULT_MODE: OrchOSMode = OrchOSModeEnum.ADVANCED;

export class ModeService {
  // Internal state
  private static mode: OrchOSMode = DEFAULT_MODE;
  private static listeners: Array<(mode: OrchOSMode) => void> = [];
  private static initialized = false;

  // Initialize mode from storage or fallback
  static initialize() {
    if (this.initialized) return;
    let storedMode: OrchOSMode | null = null;
    try {
      // Attempt to retrieve mode from storage
      storedMode = getOption(MODE_STORAGE_KEY) as OrchOSMode | null;
    } catch (e) {
      // StorageService may not be ready; fallback to localStorage
      if (typeof window !== "undefined" && window.localStorage) {
        // Fallback to localStorage if storage service is not available
        storedMode = window.localStorage.getItem(
          MODE_STORAGE_KEY
        ) as OrchOSMode | null;
      }
    }
    // Validate stored mode and update internal state
    if (
      storedMode === OrchOSModeEnum.BASIC ||
      storedMode === OrchOSModeEnum.ADVANCED
    ) {
      this.mode = storedMode;
    }
    this.initialized = true;
  }

  // Get current mode
  static getMode(): OrchOSMode {
    this.initialize();
    return this.mode;
  }

  // Set mode and notify listeners
  static setMode(mode: OrchOSMode) {
    if (this.mode !== mode) {
      this.mode = mode;
      try {
        setOption(MODE_STORAGE_KEY, mode);
      } catch (e) {
        if (typeof window !== "undefined" && window.localStorage) {
          window.localStorage.setItem(MODE_STORAGE_KEY, mode);
        }
      }
      this.listeners.forEach((listener) => listener(mode));
    }
  }

  // Subscribe to mode changes
  static onModeChange(listener: (mode: OrchOSMode) => void) {
    this.listeners.push(listener);
    // Return unsubscribe function
    return () => {
      this.listeners = this.listeners.filter((l) => l !== listener);
    };
  }
}

// Symbolic: Ensure mode is initialized at startup
ModeService.initialize();
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// StorageService.ts
// Symbolic storage cortex for Orch-OS user settings (localStorage-based)
// Provides pure functions for saving and retrieving user config.

/**
 * STORAGE_KEYS: Mapeamento neural-simbólico das chaves de configuração
 *
 * Este objeto representa o "mapa sináptico" das configurações no sistema,
 * funcionando como uma única fonte de verdade para todas as chaves de armazenamento.
 *
 * Seguindo os princípios neural-simbólicos do Orch-OS, cada chave representa um
 * "sinal" específico que flui através do sistema e deve manter consistência
 * semântica em todos os contextos.
 */
/**
 * STORAGE_KEYS: Mapeamento neural-simbólico das chaves de configuração
 *
 * Organizado por categorias semânticas para seguir o princípio neural-simbólico
 * do Orch-OS de manter uma estrutura cognitiva clara e consistente.
 *
 * IMPORTANTE: Ao adicionar novas chaves, utilize a mesma string que é usada
 * nos componentes para evitar problemas de sincronização.
 */
export const STORAGE_KEYS = {
  // Chave principal para armazenamento de todas as configurações
  SETTINGS_ROOT: "orchos.user.settings",

  // ===== Modelos Hugging Face =====
  HF_MODEL: "huggingfaceModel",
  HF_EMBEDDING_MODEL: "huggingfaceEmbeddingModel",

  // ===== Configurações gerais do sistema =====
  USER_NAME: "userName",
  APPLICATION_MODE: "applicationMode", // 'basic' ou 'advanced'

  // ===== Configurações visuais/interface =====
  // Matriz quântica e efeitos
  ENABLE_MATRIX: "enableMatrix",
  MATRIX_DENSITY: "matrixDensity",
  ENABLE_EFFECTS: "enableEffects",
  ENABLE_ANIMATIONS: "enableAnimations",

  // Estilo e tema
  DARK_MODE: "darkMode",
  THEME: "theme",
  UI_DENSITY: "uiDensity",
  ENABLE_NEUMORPHISM: "enableNeumorphism",
  ENABLE_GLASSMORPHISM: "enableGlassmorphism",
  PANEL_TRANSPARENCY: "panelTransparency",
  COLOR_THEME: "colorTheme",
  SHOW_ADVANCED_SETTINGS: "showAdvancedSettings",

  // ===== Configurações de áudio e transcrição =====
  // Processamento de áudio
  AUDIO_QUALITY: "audioQuality",
  AUTO_GAIN_CONTROL: "autoGainControl",
  NOISE_SUPPRESSION: "noiseSuppression",
  ECHO_CANCELLATION: "echoCancellation",

  // Transcrição
  TRANSCRIPTION_ENABLED: "transcriptionEnabled",
  ENHANCED_PUNCTUATION: "enhancedPunctuation",
  SPEAKER_DIARIZATION: "speakerDiarization",
  SPEAKER_IDENTIFICATION: "speakerIdentificationEnabled",

  // ===== Configurações de APIs e serviços externos =====
  // OpenAI/ChatGPT
  OPENAI_API_KEY: "openaiApiKey", // Mesmo que chatgptApiKey
  CHATGPT_MODEL: "chatgptModel",
  CHATGPT_TEMPERATURE: "chatgptTemperature",
  CHATGPT_MAX_TOKENS: "chatgptMaxTokens",
  OPENAI_EMBEDDING_MODEL: "openaiEmbeddingModel",

  // Ollama Settings
  OLLAMA_MODEL: "ollamaModel",
  OLLAMA_EMBEDDING_MODEL: "ollamaEmbeddingModel",
  OLLAMA_ENABLED: "ollamaEnabled",

  // Deepgram
  DEEPGRAM_API_KEY: "deepgramApiKey",
  DEEPGRAM_MODEL: "deepgramModel",
  DEEPGRAM_LANGUAGE: "deepgramLanguage",
  DEEPGRAM_TIER: "deepgramTier",

  // Pinecone
  PINECONE_API_KEY: "pineconeApiKey",
  PINECONE_ENVIRONMENT: "pineconeEnvironment",
  PINECONE_INDEX: "pineconeIndex",

  // ===== Configurações de banco de dados local =====
  DUCKDB_PATH: "duckdbPath", // Caminho personalizado para o banco DuckDB

  // ===== Configurações de ferramentas (tools) =====
  TOOLS_ENABLED: "toolsEnabled", // Habilita/desabilita ferramentas no modo básico

  // ===== Configurações de debugging =====
  DEBUG_MODE: "debugMode",
  LOG_LEVEL: "logLevel",
};

// Symbolic: All user options are stored as a map/object in the selected storage cortex.
export type UserSettings = Record<string, any>;

function isRenderer(): boolean {
  // Electron renderer or browser
  return (
    typeof window !== "undefined" && typeof window.localStorage !== "undefined"
  );
}

/**
 * Cria e retorna um novo backend para armazenamento
 * IMPORTANTE: Não mais usa cache estático para evitar problemas com estados desatualizados
 */
function getBackend() {
  // Identificar ambiente de execução
  const isInRenderer = isRenderer();

  // Configurar backend apropriado para o ambiente
  if (isInRenderer) {
    return {
      get: () => {
        try {
          const raw = window.localStorage.getItem(STORAGE_KEYS.SETTINGS_ROOT);

          if (!raw) {
            return {};
          }

          const parsedData = JSON.parse(raw) as UserSettings;

          // Extrair informações importantes para logs
          const userName =
            parsedData[STORAGE_KEYS.USER_NAME] || parsedData["name"] || "N/A";
          const lang = parsedData[STORAGE_KEYS.DEEPGRAM_LANGUAGE] || "N/A";
          const model = parsedData[STORAGE_KEYS.DEEPGRAM_MODEL] || "N/A";

          return parsedData;
        } catch (error) {
          console.error("❌ [STORAGE] Erro ao ler localStorage:", error);
          return {};
        }
      },
      set: (data: UserSettings) => {
        try {
          const jsonString = JSON.stringify(data);
          window.localStorage.setItem(STORAGE_KEYS.SETTINGS_ROOT, jsonString);

          // Verificar se os dados foram realmente salvos
          const verification = window.localStorage.getItem(
            STORAGE_KEYS.SETTINGS_ROOT
          );
          if (!verification) {
            console.error(
              "❌ [STORAGE] Falha na verificação: dados não foram salvos no localStorage"
            );
          }
        } catch (error) {
          console.error("❌ [STORAGE] Erro ao salvar no localStorage:", error);
        }
      },
    };
  } else {
    // Node.js/Electron main: use electron-store
    try {
      // eslint-disable-next-line @typescript-eslint/no-var-requires
      const Store = require("electron-store").default;
      const store = new Store();

      return {
        get: () => {
          try {
            const data = store.get(STORAGE_KEYS.SETTINGS_ROOT, {});
            return data;
          } catch (error) {
            console.error("❌ [STORAGE] Erro ao ler do electron-store:", error);
            return {};
          }
        },
        set: (data: UserSettings) => {
          try {
            store.set(STORAGE_KEYS.SETTINGS_ROOT, data);
          } catch (error) {
            console.error(
              "❌ [STORAGE] Erro ao salvar no electron-store:",
              error
            );
          }
        },
      };
    } catch (error) {
      console.error("❌ [STORAGE] Erro ao inicializar electron-store:", error);
      // Fallback para um backend em memória caso o electron-store falhe
      let memoryStore = {};
      return {
        get: () => memoryStore,
        set: (data: UserSettings) => {
          memoryStore = data;
        },
      };
    }
  }
}

/**
 * Saves all user settings (the entire map) to storage cortex.
 */
export function setAllOptions(options: UserSettings): void {
  getBackend().set(options);
}

/**
 * Retrieves all user settings (the entire map) from storage cortex.
 */
export function getAllOptions(): UserSettings {
  return getBackend().get();
}

// Sistema de eventos para notificar sobre mudanças no storage
type StorageChangeListener = (key: string, value: any) => void;
const storageChangeListeners: StorageChangeListener[] = [];

/**
 * Registra um listener para ser notificado quando uma opção específica mudar
 * @param listener Função que será chamada quando a opção mudar
 */
export function subscribeToStorageChanges(
  listener: StorageChangeListener
): () => void {
  storageChangeListeners.push(listener);

  // Retorna uma função para cancelar a inscrição
  return () => {
    const index = storageChangeListeners.indexOf(listener);
    if (index > -1) {
      storageChangeListeners.splice(index, 1);
    }
  };
}

/**
 * Sets a single option by key.
 */
export function setOption<T = any>(key: string, value: T): void {
  const options = getAllOptions();
  options[key] = value;
  setAllOptions(options);

  // Notifica os listeners
  storageChangeListeners.forEach((listener) => {
    try {
      listener(key, value);
    } catch (err) {
      console.error(`Error in storage change listener for ${key}:`, err);
    }
  });
}

/**
 * Gets a single option by key, or undefined if not set.
 */
export function getOption<T = any>(key: string): T | undefined {
  const options = getAllOptions();
  return options[key];
}

// Symbolic helpers for common options

export function getUserName(): string {
  // Função direta: apenas busca o valor usando a chave padrão
  return getOption<string>(STORAGE_KEYS.USER_NAME) || "User";
}

export function setUserName(name: string): void {
  // Usa a chave padronizada para salvar o nome do usuário
  setOption(STORAGE_KEYS.USER_NAME, name);
}

/**
 * Função utilitária para resetar completamente o storage do Orch-OS
 * Útil para testes ou quando há inconsistências nas configurações
 */
export function resetStorage(): void {
  const backend = getBackend();
  backend.set({});

  // Recarregar automaticamente a página se em ambiente de navegador
  if (typeof window !== "undefined") {
    window.location.reload();
  }
}

/**
 * Limpa chaves antigas e desatualizadas do armazenamento
 * Mantém apenas as chaves definidas no objeto STORAGE_KEYS
 */
export function cleanupStorage(): void {
  // Obter todas as opções atuais
  const options = getAllOptions();
  const currentSettings = { ...options };

  // Criar um conjunto com todos os valores (não chaves) de STORAGE_KEYS
  const validValues = new Set<string>();
  Object.values(STORAGE_KEYS).forEach((value) => {
    validValues.add(value as string);
  });

  // Identificar e remover chaves desatualizadas
  let removedCount = 0;
  for (const key in currentSettings) {
    // Pular a chave principal de configurações
    if (key === STORAGE_KEYS.SETTINGS_ROOT) continue;

    // Se a chave não estiver nos valores válidos, removê-la
    if (!validValues.has(key)) {
      delete currentSettings[key];
      removedCount++;
    }
  }

  // Salvar configurações limpas
  setAllOptions(currentSettings);
  console.log(
    `🧠 [STORAGE] Limpeza neural-simbólica concluída: ${removedCount} chaves antigas removidas`
  );
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Dynamic Model Registry - API-based model discovery
// No hardcoded models - everything comes from Ollama API with default versions

export enum ModelSource {
  LOCAL = "local",
  WEB = "web",
}

export interface LocalModelMeta {
  /** API model name (e.g., "llama3.2", "mistral") */
  id: string;
  /** UI display name */
  label: string;
  /** Ollama repo name - always using default/latest tag */
  repo: string;
  /** Estimated size in GB (from API) */
  sizeGB: number;
  /** Model family/type */
  family?: string;
  /** Whether model is currently installed */
  isInstalled?: boolean;
  /** Last modified timestamp from API */
  modified?: string;
}

/**
 * Normalizes model names to use default versions only
 * Always strips version tags to use latest/default
 */
export function normalizeModelName(modelName: string): string {
  // Remove any existing tags to always use default
  const baseName = modelName.split(":")[0];

  // Convert underscores to dots for proper model names
  const cleanName = baseName.replace(/_/g, ".");

  // Return clean model name without any version tags
  // Ollama will automatically use the default/latest version
  return cleanName;
}

/**
 * Converts model name to display-friendly format
 */
export function getDisplayName(modelName: string): string {
  const baseName = normalizeModelName(modelName);

  // Convert to title case and add proper spacing
  return baseName
    .split(".")
    .map((part) => part.charAt(0).toUpperCase() + part.slice(1))
    .join(" ");
}

/**
 * Estimates model size category for UI purposes
 */
export function getModelSizeCategory(
  sizeGB: number
): "small" | "medium" | "large" {
  if (sizeGB < 3) return "small";
  if (sizeGB < 8) return "medium";
  return "large";
}

/**
 * Gets minimum RAM requirement based on model size
 */
export function getMinRamRequirement(sizeGB: number): number {
  // Rule of thumb: model size * 2 for comfortable inference
  return Math.max(4, Math.ceil(sizeGB * 2));
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// neuralSignalParser.ts
// Symbolic: Pure functions for parsing neural signals from model output
import { NeuralSignal } from '../../components/context/deepgram/interfaces/neural/NeuralSignalTypes';

/**
 * Attempts to parse a string as a NeuralSignal JSON object.
 * Returns undefined if parsing fails or required fields are missing.
 */
export function parseNeuralSignal(json: string): NeuralSignal | undefined {
  try {
    const args = JSON.parse(json);
    if (args.core || args.query || args.intensity) {
      const baseSignal: Partial<NeuralSignal> = {
        core: args.core || null,
        intensity: Math.max(0, Math.min(1, args.intensity ?? 0.5)),
        symbolic_query: { query: args.query ?? '' }
      };
      if (Array.isArray(args.keywords)) baseSignal.keywords = args.keywords;
      if (args.filters) baseSignal.filters = args.filters;
      if (typeof args.expand === 'boolean') baseSignal.expand = args.expand;
      if (args.symbolicInsights) baseSignal.symbolicInsights = args.symbolicInsights;
      if (typeof args.topK !== 'undefined') baseSignal.topK = args.topK;
      if (typeof baseSignal.core !== 'undefined') return baseSignal as NeuralSignal;
    }
    return undefined;
  } catch {
    return undefined;
  }
}

/**
 * Extracts all JSON-like objects (or JSON code blocks) from a string.
 * Accepts objects that appear anywhere in the text, including inside ```json``` blocks.
 */
export function extractNeuralSignalJsons(text: string): string[] {
  const matches: string[] = [];

  if (!text || typeof text !== 'string') return matches;

  // 1. Capture fenced ```json``` blocks
  const codeBlockRegex = /```(?:json)?[\s\n]*([\s\S]*?)```/gi;
  let codeMatch: RegExpExecArray | null;
  while ((codeMatch = codeBlockRegex.exec(text)) !== null) {
    if (codeMatch[1]) {
      matches.push(codeMatch[1].trim());
    }
  }

  // 2. Capture standalone JSON objects { ... } that may appear outside blocks
  //    This regex is intentionally simple; deeper validation is performed during JSON.parse.
  const objectRegex = /\{[^\{\}]*\}/g;
  const objectMatches = text.match(objectRegex);
  if (objectMatches) {
    matches.push(...objectMatches.map((m) => m.trim()));
  }

  return matches;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { render, act, renderHook } from '@testing-library/react';
import { CognitionLogProvider, useCognitionLog } from '../../components/context/CognitionLogContext';
import symbolicCognitionTimelineLogger from '../../components/context/deepgram/services/utils/SymbolicCognitionTimelineLoggerSingleton';
import cognitionLogExporterFactory from '../../components/context/deepgram/services/utils/CognitionLogExporterFactory';
import { CognitionEvent } from '../../components/context/deepgram/types/CognitionEvent';

jest.mock('../../components/context/deepgram/services/utils/SymbolicCognitionTimelineLoggerSingleton', () => ({
  __esModule: true,
  default: {
    getTimeline: jest.fn(),
    clear: jest.fn()
  }
}));

jest.mock('../../components/context/deepgram/services/utils/CognitionLogExporterFactory', () => {
  const mockExporter = {
    label: 'Mock Exporter',
    export: jest.fn()
  };
  
  return {
    __esModule: true,
    default: {
      getExporters: jest.fn().mockReturnValue([mockExporter])
    }
  };
});

describe('CognitionLogContext', () => {
  const mockEvents: CognitionEvent[] = [
    { type: 'raw_prompt', timestamp: '2025-04-29T18:30:00.000Z', content: 'Test prompt' },
    { type: 'temporary_context', timestamp: '2025-04-29T18:30:05.000Z', context: 'Test context' }
  ];

  beforeEach(() => {
    jest.clearAllMocks();
    (symbolicCognitionTimelineLogger.getTimeline as jest.Mock).mockReturnValue(mockEvents);
  });

  test('Provides access to events through the useCognitionLog hook', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );
    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    expect(result.current.events).toEqual(mockEvents);
  });

  test('Provides access to exporters through the useCognitionLog hook', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );
    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    expect(result.current.exporters).toHaveLength(1);
    expect(result.current.exporters[0].label).toBe('Mock Exporter');
  });

  test('Calls the logger clear method when clearEvents is invoked', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );

    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    act(() => {
      result.current.clearEvents();
    });
    expect(symbolicCognitionTimelineLogger.clear).toHaveBeenCalledTimes(1);
  });

  test('Calls the exporter export method with the correct exporter when exportEvents is invoked', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );

    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    act(() => {
      result.current.exportEvents('Mock Exporter');
    });
    const mockExporter = cognitionLogExporterFactory.getExporters()[0];
    expect(mockExporter.export).toHaveBeenCalledTimes(1);
    expect(mockExporter.export).toHaveBeenCalledWith(mockEvents);
  });

  test('Does not call any export method if the exporter is not found', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );

    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    act(() => {
      result.current.exportEvents('Exportador Inexistente');
    });
    const mockExporter = cognitionLogExporterFactory.getExporters()[0];
    expect(mockExporter.export).not.toHaveBeenCalled();
  });

  test('Periodically updates events', () => {
    jest.useFakeTimers();
    render(
      <CognitionLogProvider>
        <div>Teste</div>
      </CognitionLogProvider>
    );
    expect(symbolicCognitionTimelineLogger.getTimeline).toHaveBeenCalled();
    jest.clearAllMocks();
    act(() => {
      jest.advanceTimersByTime(1000);
    });
    expect(symbolicCognitionTimelineLogger.getTimeline).toHaveBeenCalled();
    jest.useRealTimers();
  });

  test('useCognitionLog throws error when used outside of provider', () => {
    const originalError = console.error;
    console.error = jest.fn();
    expect(() => {
      renderHook(() => useCognitionLog());
    }).toThrow('useCognitionLog must be used within a CognitionLogProvider');
    console.error = originalError;
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { CognitionLogExporterFactory } from '../../components/context/deepgram/services/utils/CognitionLogExporterFactory';
import { CognitionLogExporter } from '../../components/context/deepgram/services/utils/CognitionLogExporter';
import { CognitionLogJsonExporter } from '../../components/context/deepgram/services/utils/CognitionLogJsonExporter';
import { CognitionLogTxtExporter } from '../../components/context/deepgram/services/utils/CognitionLogTxtExporter';
import { CognitionEvent } from '../../components/context/deepgram/types/CognitionEvent';

class MockCustomExporter implements CognitionLogExporter {
  label = 'Test Exporter';
  export(log: CognitionEvent[], filename?: string): void {
    // Mock
  }
}

describe('CognitionLogExporterFactory', () => {
  let factory: CognitionLogExporterFactory;
  
  beforeEach(() => {
    CognitionLogExporterFactory.instance = undefined;
    factory = CognitionLogExporterFactory.getInstance();
  });
  
  test('Implements Singleton pattern', () => {
    const instance1 = CognitionLogExporterFactory.getInstance();
    const instance2 = CognitionLogExporterFactory.getInstance();
    
    expect(instance1).toBe(instance2);
  });
  
  test('Initializes with default exporters', () => {
    const exporters = factory.getExporters();
    
    expect(exporters.length).toBeGreaterThanOrEqual(2);
    
    expect(exporters.some(e => e.label === 'Export cognitive log (JSON)')).toBe(true);
    
    expect(exporters.some(e => e.label === 'Export cognitive log (TXT)')).toBe(true);
  });
  
  test('Permits the registration of new exporters', () => {
    const customExporter = new MockCustomExporter();
    factory.registerExporter(customExporter);
    
    const exporters = factory.getExporters();
    expect(exporters.some(e => e.label === 'Test Exporter')).toBe(true);
    expect(exporters.length).toBeGreaterThan(2);
  });
  
  test('Removes existing exporters', () => {
    const customExporter = new MockCustomExporter();
    factory.registerExporter(customExporter);
    
    let exporters = factory.getExporters();
    expect(exporters.some(e => e.label === 'Test Exporter')).toBe(true);
    
    factory.unregisterExporter('Test Exporter');
    
    exporters = factory.getExporters();
    expect(exporters.some(e => e.label === 'Test Exporter')).toBe(false);
  });
  
  test('Returns a copy of the exporters to prevent direct modification', () => {
    const exporters1 = factory.getExporters();
    const initialCount = exporters1.length;
    
    exporters1.push(new MockCustomExporter());
    
    const exporters2 = factory.getExporters();
    
    expect(exporters2.length).toBe(initialCount);
    
    expect(exporters1).not.toBe(exporters2);
  });
  
  test('Preserves the correct types of exporters', () => {
    const exporters = factory.getExporters();
    
    const jsonExporter = exporters.find(e => e.label === 'Export cognitive log (JSON)');
    expect(jsonExporter).toBeInstanceOf(CognitionLogJsonExporter);
    
    const txtExporter = exporters.find(e => e.label === 'Export cognitive log (TXT)');
    expect(txtExporter).toBeInstanceOf(CognitionLogTxtExporter);
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// CognitionLogJsonExporter.test.ts
// Tests for the CognitionLogJsonExporter class

import { CognitionLogJsonExporter } from '../../components/context/deepgram/services/utils/CognitionLogJsonExporter';
import { CognitionEvent } from '../../components/context/deepgram/types/CognitionEvent';

describe('CognitionLogJsonExporter', () => {
  let exporter: CognitionLogJsonExporter;
  
  // Mocks for browser environment
  const mockClickFn = jest.fn();

  // Mocks for DOM and browser APIs
  const mockCreateElement = jest.fn();
  const mockCreateObjectURL = jest.fn();
  const mockRevokeObjectURL = jest.fn();
  
  // Example cognitive events for testing
  const sampleEvents: CognitionEvent[] = [
    { type: 'raw_prompt', timestamp: '2025-04-29T18:30:00.000Z', content: 'Como implementar um sistema neural?' },
    { type: 'temporary_context', timestamp: '2025-04-29T18:30:05.000Z', context: 'Contexto para processamento' },
    { 
      type: 'neural_signal', 
      timestamp: '2025-04-29T18:30:10.000Z', 
      core: 'memory', 
      symbolic_query: { query: 'sistema neural', text: 'sistema neural', embedding: [0.1, 0.2] },
      intensity: 0.8,
      topK: 5,
      params: { type: 'memory' }
    }
  ];
  
  beforeEach(() => {
    exporter = new CognitionLogJsonExporter();
    
    // Reset mocks
    mockClickFn.mockReset();
    mockCreateElement.mockReset();
    mockCreateObjectURL.mockReset();
    mockRevokeObjectURL.mockReset();
    
    // Mock for the <a> element
    mockCreateElement.mockReturnValue({
      href: '',
      download: '',
      click: mockClickFn
    });
    
    // Setup global mocks for testing
    Object.defineProperty(document, 'createElement', {
      value: mockCreateElement,
      writable: true
    });
    
    // Setup global mocks for testing
    Object.defineProperty(global, 'URL', {
      value: {
        createObjectURL: mockCreateObjectURL.mockReturnValue('mock-url'),
        revokeObjectURL: mockRevokeObjectURL
      },
      writable: true
    });
    
    // Setup global mocks for testing
    Object.defineProperty(global, 'Blob', {
      value: jest.fn(),
      writable: true
    });
    
    jest.useFakeTimers();
  });
  
  afterEach(() => {
    jest.clearAllMocks();
    jest.useRealTimers();
  });
  
  test('Has a descriptive label', () => {
    expect(exporter.label).toBe('Export cognitive log (JSON)');
  });
  
  test('Exports cognitive events to JSON format', () => {
    // Execute the export function
    exporter.export(sampleEvents);
    
    // Verify that the Blob was created with the expected content
    expect(global.Blob).toHaveBeenCalled();
    
    // Verify that URL.createObjectURL was called
    expect(mockCreateObjectURL).toHaveBeenCalled();
    
    // Verify that document.createElement was called with 'a'
    expect(mockCreateElement).toHaveBeenCalledWith('a');
    
    // Verify that the link was clicked
    expect(mockClickFn).toHaveBeenCalled();
    
    // Advance the timer to verify if URL.revokeObjectURL is called
    jest.advanceTimersByTime(1000);
    expect(mockRevokeObjectURL).toHaveBeenCalled();
  });
  
  test('Uses custom filename when provided', () => {
    const customFilename = 'custom_session_log.json';
    
    jest.clearAllMocks();
    
    // Prepare a new mock with a different value for download
    mockCreateElement.mockReturnValue({
      href: '',
      download: '',
      click: mockClickFn
    });
    
    // Execute the export function with a custom filename
    exporter.export(sampleEvents, customFilename);
    
    // Verify that the <a> element was created
    expect(mockCreateElement).toHaveBeenCalledWith('a');
    
    // Verify that the filename was set correctly
    const anchorElement = mockCreateElement.mock.results[0].value;
    expect(anchorElement.download).toBe(customFilename);
  });
  
  test('Exports valid and formatted JSON', () => {
    exporter.export(sampleEvents);
    
    // Get the parameters passed to the Blob
    const blobContent = (global.Blob as jest.Mock).mock.calls[0][0][0];
    
    // Try to parse to ensure it's a valid JSON
    expect(() => JSON.parse(blobContent)).not.toThrow();
    
    // Verify that the JSON was formatted with indentation (using null, 2)
    const expectedJson = JSON.stringify(sampleEvents, null, 2);
    expect(blobContent).toBe(expectedJson);
    
    // Verify that the Blob MIME type is correct
    const blobOptions = (global.Blob as jest.Mock).mock.calls[0][1];
    expect(blobOptions.type).toBe('application/json');
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// CognitionLogTxtExporter.test.ts
// Tests for the CognitionLogTxtExporter class

import { CognitionLogTxtExporter } from '../../components/context/deepgram/services/utils/CognitionLogTxtExporter';
import { CognitionEvent } from '../../components/context/deepgram/types/CognitionEvent';

describe('CognitionLogTxtExporter', () => {
  let exporter: CognitionLogTxtExporter;
  
  // Mock for simulating the element click
  const mockClickFn = jest.fn();
  
  // Example cognitive events for testing
  const sampleEvents: CognitionEvent[] = [
    {
      type: 'raw_prompt',
      timestamp: '2025-04-29T18:30:00.000Z',
      content: 'How to implement a neural system?'
    },
    {
      type: 'temporary_context',
      timestamp: '2025-04-29T18:30:05.000Z',
      context: 'Context for processing'
    },
    {
      type: 'neural_signal',
      timestamp: '2025-04-29T18:30:10.000Z',
      core: 'memory',
      symbolic_query: {
        text: 'neural system',
        embedding: [0.1, 0.2],
        query: 'neural system'
      },
      intensity: 0.8,
      topK: 5,
      params: {
        type: 'memory'
      }
    }
  ];
  
  // Mocks for the DOM and browser APIs
  const mockCreateElement = jest.fn();
  const mockCreateObjectURL = jest.fn();
  const mockRevokeObjectURL = jest.fn();
  
  beforeEach(() => {
    exporter = new CognitionLogTxtExporter();
    
    // Reset mocks
    mockClickFn.mockReset();
    mockCreateElement.mockReset();
    mockCreateObjectURL.mockReset();
    mockRevokeObjectURL.mockReset();
    
    // Mock for the <a> element
    mockCreateElement.mockReturnValue({
      href: '',
      download: '',
      click: mockClickFn
    });
    
    // Setup global mocks with safe typecasting
    document.createElement = mockCreateElement;
    
    global.URL = {
      createObjectURL: mockCreateObjectURL.mockReturnValue('mock-url'),
      revokeObjectURL: mockRevokeObjectURL
    };
  
    global.Blob = jest.fn();
    
    jest.useFakeTimers();
  });
  
  afterEach(() => {
    jest.clearAllMocks();
    jest.useRealTimers();
  });
  
  test('Has a descriptive label', () => {
    expect(exporter.label).toBe('Export cognitive log (TXT)');
  });
  
  test('Exports cognitive events to TXT format', () => {
    // Execute the export function
    exporter.export(sampleEvents);
    
    // Verify that the Blob was created with the expected content
    expect(global.Blob).toHaveBeenCalled();
    
    // Verify that URL.createObjectURL was called
    expect(mockCreateObjectURL).toHaveBeenCalled();
    
    // Verify that document.createElement was called with 'a'
    expect(mockCreateElement).toHaveBeenCalledWith('a');
    
    // Verify that the link was clicked
    expect(mockClickFn).toHaveBeenCalled();
    
    // Advance the timer to verify if URL.revokeObjectURL is called
    jest.advanceTimersByTime(1000);
    expect(mockRevokeObjectURL).toHaveBeenCalled();
  });
  
  test('Uses custom filename when provided', () => {
    const customFilename = 'custom_session_log.txt';
    
    // Clear previous calls
    jest.clearAllMocks();
    
    // Execute the export with custom filename
    exporter.export(sampleEvents, customFilename);
    
    // Verify that the <a> element was created
    expect(mockCreateElement).toHaveBeenCalledWith('a');
    
    // Verify that the filename was set correctly
    const anchorElement = mockCreateElement.mock.results[0].value;
    expect(anchorElement.download).toBe(customFilename);
  });
  
  test('Separates events with delimiter in the TXT content', () => {
    exporter.export(sampleEvents);
    
    // Get the parameters passed to the Blob
    const blobContent = (global.Blob as jest.Mock).mock.calls[0][0][0];
    
    // Verify that it contains the delimiter and basic data
    expect(blobContent).toContain('---');
    expect(blobContent).toContain('raw_prompt');
    expect(blobContent).toContain('temporary_context');
    expect(blobContent).toContain('neural_signal');
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// SymbolicCognitionTimelineLogger.test.ts
// Tests for the SymbolicCognitionTimelineLogger class

import SymbolicCognitionTimelineLogger from '../../components/context/deepgram/services/utils/SymbolicCognitionTimelineLogger';

// Mock for Date.toISOString to return a fixed timestamp and facilitate tests
const mockTimestamp = '2025-04-29T18:30:00.000Z';
jest.spyOn(Date.prototype, 'toISOString').mockImplementation(() => mockTimestamp);

describe('SymbolicCognitionTimelineLogger', () => {
  let logger: SymbolicCognitionTimelineLogger;
  
  beforeEach(() => {
    // Create a new instance for each test
    logger = new SymbolicCognitionTimelineLogger();
  });

  test('Should initialize with an empty timeline', () => {
    expect(logger.getTimeline()).toEqual([]);
  });

  test('Should log raw prompt correctly', () => {
    const promptText = 'How can I implement a neural symbolic system?';
    logger.logRawPrompt(promptText);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'raw_prompt',
      timestamp: mockTimestamp,
      content: promptText
    });
  });

  test('Should log temporary context correctly', () => {
    const context = 'Temporary context for neural processing';
    logger.logTemporaryContext(context);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'temporary_context',
      timestamp: mockTimestamp,
      context
    });
  });

  test('Should log neural signal correctly', () => {
    const core = 'memory';
    const symbolic_query = { query: 'neural system implementation', text: 'neural system implementation', embedding: [0.1, 0.2] };
    const intensity = 0.8;
    const topK = 5;
    const params = { searchType: 'semantic' };

    logger.logNeuralSignal(core, symbolic_query, intensity, topK, params);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'neural_signal',
      timestamp: mockTimestamp,
      core,
      symbolic_query,
      intensity,
      topK,
      params
    });
  });

  test('Should log symbolic retrieval correctly', () => {
    const core = 'memory';
    const insights = [  
      { id: '1', text: 'Insight 1', score: 0.9, type: 'memory' },
      { id: '2', text: 'Insight 2', score: 0.7, type: 'memory' }
    ];
    const matchCount = 2;
    const durationMs = 150;

    logger.logSymbolicRetrieval(core, insights, matchCount, durationMs);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'symbolic_retrieval',
      timestamp: mockTimestamp,
      core,
      insights,
      matchCount,
      durationMs
    });
  });

  test('Should handle empty insights in symbolic retrieval', () => {
    const core = 'memory';
    const insights: { id: string; text: string; score: number; type: string }[] = [];
    const matchCount = 0;
    const durationMs = 100;

    logger.logSymbolicRetrieval(core, insights, matchCount, durationMs);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'symbolic_retrieval',
      timestamp: mockTimestamp,
      core,
      insights: [],
      matchCount,
      durationMs
    });
  });

  test('Should log fusion initiation correctly', () => {
    logger.logFusionInitiated();

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'fusion_initiated',
      timestamp: mockTimestamp
    });
  });

  test('Should log symbolic context synthesis correctly', () => {
    const context = {
      memoryInsights: ['Insight 1', 'Insight 2'],
      metacognitiveInsights: ['Meta Insight'],
      language: 'Insight of language',
      summary: 'Summary of the symbolic context',
      associativeInsights: ['Associative 1'],
      valenceInsights: ['Valence 1'],
      planningInsights: ['Planning 1']
    };

    logger.logSymbolicContextSynthesized(context);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'symbolic_context_synthesized',
      timestamp: mockTimestamp,
      context
    });
  });

  test('Should log GPT response as string correctly', () => {
    const response = 'This is a GPT response';
    logger.logGptResponse(response);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0].type).toBe('gpt_response');
    // The response is stored in the response field of the structure
    const event = timeline[0] as { type: string; timestamp: string; response: string };
    expect(event.response).toBe(response);
    expect(timeline[0].timestamp).toBe(mockTimestamp);
  });

  test('Should log GPT response as object correctly', () => {
    const responseData = {
      response: 'This is a GPT response',
      symbolicTopics: ['Topic 1', 'Topic 2'],
      insights: [
        { id: '1', text: 'Insight 1', score: 0.9, type: 'memory' },
        { id: '2', text: 'Insight 2', score: 0.8, type: 'metacognitive' }
      ]
    };

    logger.logGptResponse(responseData);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'gpt_response',
      timestamp: mockTimestamp,
      ...responseData
    });
  });

  test('Should clear timeline correctly', () => {
    // Add some events
    logger.logRawPrompt('Prompt 1');
    logger.logTemporaryContext('Context');
    logger.logRawPrompt('Prompt 2');
    
    // Verify that they were added
    expect(logger.getTimeline()).toHaveLength(3);
    
    // Clear the timeline
    logger.clear();
    
    // Verify that it is empty
    expect(logger.getTimeline()).toHaveLength(0);
  });

  test('Should preserve chronological order of events', () => {
    logger.logRawPrompt('First prompt');
    logger.logTemporaryContext('Temporary context');
    logger.logFusionInitiated();
    
    const timeline = logger.getTimeline();
    
    expect(timeline).toHaveLength(3);
    expect(timeline[0].type).toBe('raw_prompt');
    expect(timeline[1].type).toBe('temporary_context');
    expect(timeline[2].type).toBe('fusion_initiated');
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

let uiTranscriptionList: string[] = [];

function updateUI(newTranscription: string): void {
  console.log(`\n📝 New transcription: "${newTranscription}"`);
  
  if (!uiTranscriptionList) uiTranscriptionList = [];
  
  const incomingLines = newTranscription.split('\n')
    .map(l => l.trim())
    .filter(Boolean);
    
  if (incomingLines.length > 0) {
    const lastHistoryLine = uiTranscriptionList[uiTranscriptionList.length - 1];
    const lastIncomingLine = incomingLines[incomingLines.length - 1];
    
    if (incomingLines.length > 1 || lastIncomingLine !== lastHistoryLine) {
      const newLines = incomingLines.filter((line, i, arr) => 
        i === 0 || line !== arr[i-1]
      );
      
      if (newLines.length > 0 && 
          (uiTranscriptionList.length === 0 || 
          newLines[0] !== uiTranscriptionList[uiTranscriptionList.length - 1])) {
        console.log(`✅ Adding ${newLines.length} new line(s) to list`);
        for (const line of newLines) {
          console.log(`  • "${line}"`);
        }
        uiTranscriptionList.push(...newLines);
      } else {
        console.log(`⚠️ No new lines to add to list`);
      }
    } else {
      console.log(`⚠️ Duplicate text from last entry, ignoring...`);
    }
  } else {
    console.log(`⚠️ No lines detected in received text`);
  }
  
  const fullText = uiTranscriptionList.join('\n');
  console.log(`📄 Current state: ${uiTranscriptionList.length} line(s), content:`);
  if (uiTranscriptionList.length > 0) {
    uiTranscriptionList.forEach((line, i) => {
      console.log(`  ${i+1}. "${line}"`);
    });
  } else {
    console.log(`  (empty)`);
  }
}

function runTest() {
  console.log("🧪 TEST INCREMENTAL PROCESSING");
  console.log("=====================================================");
  
  console.log("\n🔄 TEST 1: Sending simple message");
  updateUI("Ola");
  
  console.log("\n🔄 TEST 2: Sending incremental message");
  updateUI("Ola, Tudo bem ?");
  
  console.log("\n🔄 TEST 3: Sending third incremental message");
  updateUI("Ola, Tudo bem ? Estou otimo !");
  
  console.log("\n=====================================================");
  console.log("🏁 FINAL RESULT:");  
  
  const finalText = uiTranscriptionList.join('\n');
  console.log(`📜 Final text for prompt: "${finalText}"`);
  
  const containsIncremental = finalText.includes("Estou otimo");
  console.log(`📊 The final text contains the complete version? ${containsIncremental ? '✅ YES' : '❌ NO'}`);
  
  const linesWithoutDuplicates = [...new Set(uiTranscriptionList)];
  const hasDuplicates = linesWithoutDuplicates.length < uiTranscriptionList.length;
  console.log(`📊 The text contains duplicates? ${hasDuplicates ? '❌ YES' : '✅ NO'}`);
} 

runTest();
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { TranscriptionStorageService } from "../components/context/deepgram/services/transcription/TranscriptionStorageService";
import { ISpeakerIdentificationService } from "../components/context/deepgram/interfaces/utils/ISpeakerIdentificationService";
import { UIUpdater } from "../components/context/deepgram/interfaces/transcription/TranscriptionTypes";

// Mock do serviço de identificação de falantes
class MockSpeakerService implements ISpeakerIdentificationService {
  splitMixedTranscription(text: string) {
    return [{ speaker: "Guilherme", text }];
  }
  
  getPrimaryUserSpeaker(): string {
    return "Guilherme";
  }
  
  filterTranscriptionsBySpeaker(
    speaker: string,
    transcriptions: Array<{ speaker: string; text: string; timestamp: string }>
  ) {
    return transcriptions.filter(t => t.speaker === speaker);
  }
}

// Function to test transcription storage
function testTranscriptionStorage() {
  console.log("🧪 STORAGE SERVICE TEST");
  console.log("==============================================");
  
  // Object to store the UI state
  let uiState = { transcription: "" };
  
  // Callback to update the UI state
  const setTexts = (updater: UIUpdater) => {
    if (typeof updater === 'function') {
      uiState = updater(uiState);
    } else {
      uiState = { ...uiState, ...updater };
    }
    console.log(`📊 UI atualizada: "${uiState.transcription}"`);
  };
  
  // Instantiate the storage service
  const speakerService = new MockSpeakerService();
  const storageService = new TranscriptionStorageService(speakerService, setTexts);
  
  // First message: "Hello"
  console.log("\n🔄 TEST 1: Sending simple message");
  const message1 = "Hello";
  storageService.updateTranscriptionUI(message1);
  console.log(`✅ After first message: "${uiState.transcription}"`);
  
  // Second incremental message: "Hello, How are you ?"
  console.log("\n🔄 TEST 2: Sending incremental message");
  const message2 = "Hello, How are you ?";
  storageService.updateTranscriptionUI(message2);
  console.log(`✅ After second message: "${uiState.transcription}"`);
  
  // Third incremental message: "Hello, How are you ? I'm good!"
  console.log("\n🔄 TEST 3: Sending third incremental message");
  const message3 = "Hello, How are you ? I'm good!";
  storageService.updateTranscriptionUI(message3);
  console.log(`✅ After third message: "${uiState.transcription}"`);
  
  // Verifying the text available for the prompt
  console.log("\n🔍 VERIFYING TEXT AVAILABLE FOR PROMPT");
  const promptText = storageService.getUITranscriptionText();
  console.log(`📜 Text for prompt: "${promptText}"`);
  
  // Expected vs. actual result
  const expected = message3;
  const isSuccess = promptText === expected || promptText.endsWith(expected);
  
  console.log("\n==============================================");
  console.log(`🏁 TEST RESULT: ${isSuccess ? '✅ SUCCESS' : '❌ FAILURE'}`);
  
  if (!isSuccess) {
    console.log(`❌ Expected: "${expected}"`);
    console.log(`❌ Obtained: "${promptText}"`);
  } else {
    console.log(`✅ Final text correct: "${promptText}"`);
  }
}

// Execute the test
testTranscriptionStorage();

export {};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { DuckDBMatch } from "../../electron/vector-database/interfaces/IVectorDatabase";
import type { VllmStatus } from "../../electron/preload/interfaces/IElectronAPI";

export interface ElectronAPI {
  // Core window methods

  toggleMainWindow: () => Promise<{ success: boolean; error?: string }>;
  getPlatform: () => string;
  minimizeWindow: () => void;
  closeWindow: () => void;

  // 🔥 Functions for neural transcription
  startTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  stopTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  sendNeuralPrompt: (
    temporaryContext?: string
  ) => Promise<{ success: boolean; error?: string }>;
  clearNeuralTranscription: () => Promise<{ success: boolean; error?: string }>;

  // 📝 Events for neural transcription
  onRealtimeTranscription: (callback: (data: string) => void) => () => void;
  onNeuralStarted: (callback: () => void) => () => void;
  onNeuralStopped: (callback: () => void) => () => void;
  onNeuralError: (callback: (error: string) => void) => () => void;
  onPromptSend: (callback: () => void) => () => void;
  onPromptSending: (callback: () => void) => () => void;
  onPromptPartialResponse: (callback: (data: string) => void) => () => void;
  onPromptSuccess: (callback: (data: string) => void) => () => void;
  onPromptError: (callback: (error: string) => void) => () => void;
  onClearTranscription: (callback: () => void) => () => void;
  onSendChunk: (callback: (chunk: ArrayBuffer) => void) => () => void;

  // 📝 Method to get environment variables
  getEnv: (key: string) => Promise<string | null>;
  getPath: (
    name: "userData" | "temp" | "desktop" | "documents"
  ) => Promise<string>;
  requestMicrophonePermission: () => Promise<{
    success: boolean;
    status: string;
    error?: string;
  }>;
  sendAudioChunk: (
    chunk: Uint8Array
  ) => Promise<{ success: boolean; error?: string }>;
  sendAudioTranscription: (text: string) => void;
  toogleNeuralRecording: (callback: () => void) => () => void;

  setDeepgramLanguage: (lang: string) => void;

  // 📝 Method to send prompt updates directly
  sendPromptUpdate: (
    type: "partial" | "complete" | "error",
    content: string
  ) => void;

  // Pinecone IPC methods
  queryPinecone: (
    embedding: number[],
    topK?: number,
    keywords?: string[],
    filters?: Record<string, unknown>
  ) => Promise<{ matches: Array<{ metadata?: Record<string, unknown> }> }>;
  saveToPinecone: (
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<void>;
  // DuckDB IPC methods (simplified)
  queryDuckDB: (
    embedding: number[],
    limit?: number,
    keywords?: string[],
    filters?: Record<string, unknown>,
    threshold?: number
  ) => Promise<{ matches: DuckDBMatch[] }>;
  saveToDuckDB: (
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<{ success: boolean; error?: string }>;

  // Directory selection for DuckDB path
  selectDirectory: () => Promise<{
    success: boolean;
    path?: string;
    canceled?: boolean;
    error?: string;
  }>;

  // Reinitialize DuckDB with new path
  reinitializeDuckDB: (newPath: string) => Promise<{
    success: boolean;
    error?: string;
  }>;
  importChatHistory: (params: {
    fileBuffer: Buffer | ArrayBuffer | Uint8Array;
    mode: string;
    user: string;
    applicationMode?: string;
    onProgress?: (data: {
      processed: number;
      total: number;
      percentage?: number;
      stage?: string;
    }) => void;
  }) => Promise<{
    success: boolean;
    error?: string;
    imported?: number;
    skipped?: number;
  }>;

  // VLLM APIs
  vllm?: {
    modelStatus: () => Promise<{
      success: boolean;
      status?: VllmStatus;
      error?: string;
    }>;
    startModel: (
      modelId: string
    ) => Promise<{ success: boolean; error?: string }>;
    generate: (
      payload: any
    ) => Promise<{ success: boolean; data?: any; error?: string }>;
    stopModel: () => Promise<{ success: boolean; error?: string }>;
    downloadModelOnly: (
      modelId: string
    ) => Promise<{ success: boolean; error?: string }>;
  };

  // Ollama APIs
  ollama?: {
    listModels: () => Promise<
      Array<{ name: string; id: string; size?: string }>
    >;
    pullModel: (
      modelId: string
    ) => Promise<{ success: boolean; error?: string }>;
    deleteModel: (
      modelId: string
    ) => Promise<{ success: boolean; error?: string }>;
    generate: (options: {
      model: string;
      prompt: string;
      stream?: boolean;
      temperature?: number;
      max_tokens?: number;
    }) => Promise<{ success: boolean; response?: string; error?: string }>;
    chat: (options: {
      model: string;
      messages: Array<{ role: string; content: string }>;
      stream?: boolean;
      temperature?: number;
      max_tokens?: number;
    }) => Promise<{ success: boolean; response?: string; error?: string }>;
    embeddings: (options: {
      model: string;
      prompt: string;
    }) => Promise<{ success: boolean; embedding?: number[]; error?: string }>;
    isRunning: () => Promise<{
      success: boolean;
      running?: boolean;
      error?: string;
    }>;
  };

  // Legacy VLLM methods (deprecated, use vllm.* instead)
  vllmModelStatus: () => Promise<{
    success: boolean;
    status?: VllmStatus;
    error?: string;
  }>;
  vllmStartModel: (
    modelId: string
  ) => Promise<{ success: boolean; error?: string }>;
  vllmGenerate: (
    payload: any
  ) => Promise<{ success: boolean; data?: any; error?: string }>;
  vllmStopModel: () => Promise<{ success: boolean; error?: string }>;
  listModels(): Promise<OllamaModel[]>;
  getAvailableModels(): Promise<OllamaModel[]>;
  downloadModel(
    modelId: string,
    onProgress?: (progress: number, speed: string, eta: string) => void
  ): Promise<boolean>;
  cancelDownload(modelId: string): Promise<void>;
  removeModel(modelId: string): Promise<void>;
  testConnection(): Promise<{
    success: boolean;
    message?: string;
    error?: string;
  }>;
}

declare global {
  interface Window {
    electronAPI: ElectronAPI;
    electron: {
      ipcRenderer: {
        on: (channel: string, func: (...args: unknown[]) => void) => void;
        removeListener: (
          channel: string,
          func: (...args: unknown[]) => void
        ) => void;
      };
    };
    __LANGUAGE__: string;
    signalMonitoringInterval: NodeJS.Timeout;
    audioSignalDetected: boolean;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import type { ElectronAPI } from './electron';

declare global {
  interface Window {
    electronAPI: ElectronAPI;
    electron: {
      ipcRenderer: {
        on: (channel: string, func: (...args: unknown[]) => void) => void;
        removeListener: (channel: string, func: (...args: unknown[]) => void) => void;
      };
    };
    ipcRenderer?: {
      on: (channel: string, func: (...args: unknown[]) => void) => void;
      removeListener: (channel: string, func: (...args: unknown[]) => void) => void;
    };
    on?: (channel: string, func: (...args: unknown[]) => void) => void;
    removeListener?: (channel: string, func: (...args: unknown[]) => void) => void;
    __LANGUAGE__: string;
    signalMonitoringInterval: NodeJS.Timeout;
    audioSignalDetected: boolean;
  }
}

export {};// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

export interface BatchItem {
  id: string;
  text: string;
  role: string;
  order: number;
  part?: string;
  hash: string;
  metadata: Record<string, string | number | boolean | string[]>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Utilitários para trabalhar com modelos de embedding
 * Implementa princípio DRY (Don't Repeat Yourself)
 */

/**
 * Retorna a dimensionalidade de um modelo de embedding com base em seu nome
 * Suporta apenas os modelos oficialmente disponíveis na UI do Orch-OS
 * @param model Nome do modelo (Hugging Face ou OpenAI)
 * @returns Número de dimensões do embedding
 */
export const getModelDimensions = (model: string): number => {
  // ONNX Community models - novos modelos ONNX otimizados disponíveis na UI
  if (model.includes("all-MiniLM-L6-v2")) return 384;

  // Default para modelos desconhecidos - dimensão intermediária mais comum
  return 768;
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Utility helpers for HuggingFace tools conversion / parsing
// Centralizes logic so every service uses the same code path.

export interface GenericToolDefinition {
  type?: string;
  function: {
    name: string;
    description?: string;
    parameters?: Record<string, unknown>;
  };
}

export interface HuggingFaceToolDefinition {
  type: string;
  function: {
    name: string;
    description: string;
    parameters: Record<string, unknown>;
  };
}

/**
 * Convert generic tool definitions (possibly from OpenAI style) into the
 * structure expected by HuggingFace function-calling prompt templates.
 * Guarantees the presence of the "type" field (defaults to "function").
 */
export function toHuggingFaceTools(
  tools: GenericToolDefinition[] | undefined | null
): HuggingFaceToolDefinition[] {
  if (!Array.isArray(tools) || tools.length === 0) return [];
  return tools.map((tool) => ({
    type: tool.type || "function",
    function: {
      name: tool.function.name,
      description: tool.function.description ?? "",
      parameters: tool.function.parameters ?? {},
    },
  }));
}

// ------------ Parsing Helpers ---------------

export interface ParsedToolCall {
  function: {
    name: string;
    arguments: string;
  };
}

function tryParseJSON(text: string): any | null {
  try {
    return JSON.parse(text);
  } catch {
    return null;
  }
}

/**
 * Extracts tool calls from model output text or JSON. It looks for JSON code
 * blocks or inline JSON objects and returns them as ParsedToolCall[]
 */
export function extractToolCalls(output: string): ParsedToolCall[] {
  const toolCalls: ParsedToolCall[] = [];

  if (!output) return toolCalls;

  const jsonBlockRegex = /```json([\s\S]*?)```/gi;
  const jsonSnippetRegex = /\{[\s\S]*?\}/g; // naive but works for simple outputs

  const candidates: string[] = [];

  // Collect fenced JSON blocks
  let match: RegExpExecArray | null;
  while ((match = jsonBlockRegex.exec(output)) !== null) {
    candidates.push(match[1]);
  }

  // Collect inline snippets
  const inline = output.match(jsonSnippetRegex);
  if (inline) candidates.push(...inline);

  for (const snippet of candidates) {
    const parsed = tryParseJSON(snippet);
    if (!parsed) continue;

    // Support both direct function format and wrapper with .function
    if (parsed && (parsed.name || parsed.function?.name)) {
      const name = parsed.function?.name || parsed.name;
      const args = parsed.function?.arguments || parsed.arguments || {};
      toolCalls.push({
        function: {
          name: String(name),
          arguments: typeof args === "string" ? args : JSON.stringify(args),
        },
      });
    }
  }

  return toolCalls;
}
import { pipeline } from "@huggingface/transformers";

// Função modificada para sempre escolher wasm como dispositivo e dtype automático
export async function getOptimalDeviceConfig() {
  return { device: "wasm", dtype: "fp32" }; // Usar fp32 por padrão para compatibilidade
}

// Configurações específicas por modelo
const MODEL_SPECIFIC_CONFIGS: Record<string, Record<string, any>> = {
  "Xenova/distilgpt2": {
    use_external_data_format: false, // Modelo pequeno, não precisa
    dtype: "fp32",
    device: "wasm", // Forçar wasm para compatibilidade
  },
  "Xenova/gpt2": {
    use_external_data_format: true, // Modelo maior, precisa de external data
    dtype: "fp32",
    device: "wasm", // Forçar wasm para compatibilidade
  },
  "Xenova/llama2.c-stories15M": {
    use_external_data_format: false, // Modelo muito pequeno
    dtype: "fp32",
    device: "wasm", // Forçar wasm para compatibilidade
  },
  "Xenova/TinyLlama-1.1B-Chat-v1.0": {
    use_external_data_format: true, // Modelo grande, precisa de external data
    dtype: "fp32",
    device: "wasm", // Forçar wasm para compatibilidade
  },
};

// Main loading method — mantido!
export async function loadModelWithOptimalConfig(
  modelId: string,
  task: string,
  additionalOptions: Record<string, any> = {}
) {
  console.log(`[TransformersEnv] 🔍 DEBUG: Input parameters:`, {
    modelId,
    task,
    additionalOptions,
  });

  const deviceConfig = await getOptimalDeviceConfig();
  console.log(`[TransformersEnv] 🔍 DEBUG: Device config:`, deviceConfig);

  // Obter configurações específicas do modelo
  const modelSpecificConfig = MODEL_SPECIFIC_CONFIGS[modelId] || {};
  console.log(
    `[TransformersEnv] 🔍 DEBUG: Model-specific config for ${modelId}:`,
    modelSpecificConfig
  );

  // Filter out conflicting options from additionalOptions
  const filteredAdditionalOptions = Object.fromEntries(
    Object.entries(additionalOptions).filter(
      ([key]) => !["device", "dtype", "use_external_data_format"].includes(key)
    )
  );
  console.log(
    `[TransformersEnv] 🔍 DEBUG: Filtered additional options:`,
    filteredAdditionalOptions
  );

  // Configurações base para modelos browser-compatible
  const baseOptions: Record<string, any> = {
    // 1. FORCE correct configurations - ignore any incorrect additionalOptions
    device: "wasm", // Always use wasm for browser compatibility
    dtype: "fp32", // Always use fp32 - model-specific configs will override if needed
    local_files_only: false, // Permitir download se não estiver em cache

    // 2. Configurações de sessão para melhor performance
    session_options: {
      logSeverityLevel: 3, // Reduzir logs
      graphOptimizationLevel: "all" as const,
      enableMemPattern: true,
      enableCpuMemArena: true,
      // Execution providers em ordem de preferência
      executionProviders: ["wasm"],
    },

    // 3. Callback de progresso para debugging
    progress_callback: (data: any) => {
      if (data.status === "downloading") {
        console.log(
          `[TransformersEnv] Downloading: ${
            data.name || data.file
          } - ${Math.round(data.progress || 0)}%`
        );
      } else if (data.status === "loading") {
        console.log(`[TransformersEnv] Loading: ${data.name || data.file}`);
      } else if (data.status === "ready") {
        console.log(`[TransformersEnv] Ready: ${data.name || data.file}`);
      }
    },

    // 4. Aplicar opções adicionais (EXCLUINDO device/dtype/use_external_data_format que podem conflitar)
    ...filteredAdditionalOptions,

    // 5. FINAL: Configurações específicas do modelo têm PRECEDÊNCIA MÁXIMA
    ...modelSpecificConfig,
  };

  console.log(
    `[TransformersEnv] 🔍 DEBUG: Base options before model-specific override:`,
    {
      device: baseOptions.device,
      dtype: baseOptions.dtype,
      use_external_data_format: baseOptions.use_external_data_format,
    }
  );

  // CRITICAL: Force correct values one more time to ensure no overrides
  if (modelSpecificConfig.device) {
    baseOptions.device = modelSpecificConfig.device;
  } else {
    baseOptions.device = "wasm";
  }

  if (modelSpecificConfig.dtype) {
    baseOptions.dtype = modelSpecificConfig.dtype;
  } else {
    baseOptions.dtype = "fp32";
  }

  if (modelSpecificConfig.use_external_data_format !== undefined) {
    baseOptions.use_external_data_format =
      modelSpecificConfig.use_external_data_format;
  }

  console.log(`[TransformersEnv] 🔍 DEBUG: Final forced values:`, {
    device: baseOptions.device,
    dtype: baseOptions.dtype,
    use_external_data_format: baseOptions.use_external_data_format,
  });

  console.log(`[TransformersEnv] Loading model ${modelId} with options:`, {
    device: baseOptions.device,
    dtype: baseOptions.dtype,
    use_external_data_format: baseOptions.use_external_data_format,
    local_files_only: baseOptions.local_files_only,
    modelSpecific: modelSpecificConfig,
  });

  console.log(
    `[TransformersEnv] 🔍 DEBUG: Final baseOptions object:`,
    baseOptions
  );

  // FINAL VALIDATION: Ensure critical values are correct
  if (baseOptions.device !== "wasm") {
    console.error(
      `[TransformersEnv] ❌ CRITICAL ERROR: device is ${baseOptions.device}, forcing to wasm`
    );
    baseOptions.device = "wasm";
  }

  if (baseOptions.dtype !== "fp32") {
    console.error(
      `[TransformersEnv] ❌ CRITICAL ERROR: dtype is ${baseOptions.dtype}, forcing to fp32`
    );
    baseOptions.dtype = "fp32";
  }

  if (
    modelId === "Xenova/llama2.c-stories15M" &&
    baseOptions.use_external_data_format !== false
  ) {
    console.error(
      `[TransformersEnv] ❌ CRITICAL ERROR: use_external_data_format is ${baseOptions.use_external_data_format} for ${modelId}, forcing to false`
    );
    baseOptions.use_external_data_format = false;
  }

  console.log(`[TransformersEnv] 🔍 DEBUG: FINAL VALIDATED OPTIONS:`, {
    device: baseOptions.device,
    dtype: baseOptions.dtype,
    use_external_data_format: baseOptions.use_external_data_format,
  });

  return pipeline(task as any, modelId, baseOptions as any);
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { QueryClient, QueryClientProvider } from "@tanstack/react-query";
import { createContext, useContext, useEffect, useRef, useState } from "react";
import {
  DeepgramProvider,
  MicrophoneProvider,
  TranscriptionProvider,
} from "./components/context";
import { CognitionLogProvider } from "./components/context/CognitionLogContext";
import { LanguageProvider } from "./components/context/LanguageContext";
import { initializeQuantumPerformanceOptimizations } from "./components/shared/QuantumVisualization/utils/performance";
import {
  useGlobalPassiveEventOptimization,
  useHeavyTaskManager,
} from "./components/shared/QuantumVisualization/utils/performanceOptimizations";
import {
  Toast,
  ToastDescription,
  ToastMessage,
  ToastProvider,
  ToastTitle,
  ToastVariant,
  ToastViewport,
} from "./components/ui/toast";
import TranscriptionModule from "./features/transcription/TranscriptionModule";
import "./styles/orchos-theme.css";

const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 0,
      gcTime: Infinity,
      retry: 1,
      refetchOnWindowFocus: false,
    },
    mutations: {
      retry: 1,
    },
  },
});

interface ToastContextType {
  showToast: (
    title: string,
    description: string,
    variant: ToastVariant
  ) => void;
}

export const ToastContext = createContext<ToastContextType | undefined>(
  undefined
);

export function useToast() {
  const context = useContext(ToastContext);
  if (!context) {
    throw new Error("useToast must be used within a ToastProvider");
  }
  return context;
}

// Helper function to safely update language
export function updateLanguage(newLanguage: string) {
  async () => {
    window.__LANGUAGE__ = newLanguage;
  };
}

// This component has been moved to TranscriptionModule

export default function App() {
  const containerRef = useRef<HTMLDivElement>(null);
  const [toastOpen, setToastOpen] = useState(false);
  const [toastMessage, setToastMessage] = useState<ToastMessage>({
    title: "",
    description: "",
    variant: "neutral",
  });

  // Initialize performance optimizations and ONNX Runtime config once at application startup
  useEffect(() => {
    const cleanup = initializeQuantumPerformanceOptimizations();

    return cleanup;
  }, []);

  // Apply global passive event optimization to resolve OrbitControls violations
  useGlobalPassiveEventOptimization();

  // Apply heavy task management to coordinate HuggingFace + 3D rendering
  const { queueHeavyTask } = useHeavyTaskManager();

  const showToast = (
    title: string,
    description: string,
    variant: ToastVariant
  ) => {
    setToastMessage({ title, description, variant });
    setToastOpen(true);
  };

  // Provide heavy task manager to child components via context if needed
  const contextValue = {
    showToast,
    queueHeavyTask, // Make available for HuggingFace service coordination
  };

  return (
    <div ref={containerRef} className="h-screen w-full overflow-hidden">
      <QueryClientProvider client={queryClient}>
        <LanguageProvider>
          <TranscriptionProvider>
            <MicrophoneProvider>
              <DeepgramProvider>
                <CognitionLogProvider>
                  <ToastProvider>
                    <ToastContext.Provider value={contextValue}>
                      <TranscriptionModule />
                    </ToastContext.Provider>
                    <Toast
                      open={toastOpen}
                      onOpenChange={setToastOpen}
                      variant={toastMessage.variant}
                      duration={3000}
                    >
                      <ToastTitle>{toastMessage.title}</ToastTitle>
                      <ToastDescription>
                        {toastMessage.description}
                      </ToastDescription>
                    </Toast>
                    <ToastViewport />
                  </ToastProvider>
                </CognitionLogProvider>
              </DeepgramProvider>
            </MicrophoneProvider>
          </TranscriptionProvider>
        </LanguageProvider>
      </QueryClientProvider>
    </div>
  );
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { DuckDBMatch } from "../electron/vector-database/interfaces/IVectorDatabase";

// <reference types="vite/client" />

type VllmStatus =
  import("../electron/preload/interfaces/IElectronAPI").VllmStatus;

interface ElectronAPI {
  openExternal: (url: string) => void;
  toggleMainWindow: () => Promise<{ success: boolean; error?: string }>;
  getPlatform: () => string;
  openTranscriptionTooltip: (callback: () => void) => () => void;
  startTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  stopTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  sendNeuralPrompt: (
    temporaryContext?: string
  ) => Promise<{ success: boolean; error?: string }>;
  clearNeuralTranscription: () => Promise<{ success: boolean; error?: string }>;
  onRealtimeTranscription: (callback: (data: string) => void) => () => void;
  onNeuralStarted: (callback: () => void) => () => void;
  onNeuralStopped: (callback: () => void) => () => void;
  onNeuralError: (callback: (error: string) => void) => () => void;
  onPromptSend: (callback: () => void) => () => void;
  onPromptSending: (callback: () => void) => () => void;
  onPromptPartialResponse: (callback: (data: string) => void) => () => void;
  onPromptSuccess: (callback: (data: string) => void) => () => void;
  onPromptError: (callback: (error: string) => void) => () => void;
  onClearTranscription: (callback: () => void) => () => void;
  onSendChunk: (callback: (chunk: ArrayBuffer) => void) => () => void;
  getEnv: (key: string) => Promise<string | null>;
  sendAudioChunk: (
    chunk: Uint8Array
  ) => Promise<{ success: boolean; error?: string }>;
  sendAudioTranscription: (text: string) => void;
  toogleNeuralRecording: (callback: () => void) => () => void;
  onForceStyle: (callback: (style: string) => void) => () => void;
  onForceImprovisation: (callback: () => void) => () => void;
  onRepeatResponse: (callback: () => void) => () => void;
  onStopTTS: (callback: () => void) => () => void;
  setDeepgramLanguage: (lang: string) => void;
  sendPromptUpdate: (
    type: "partial" | "complete" | "error",
    content: string
  ) => void;
  // Pinecone methods removed - using DuckDB only
  queryDuckDB: (
    embedding: number[],
    topK?: number,
    keywords?: string[],
    filters?: Record<string, unknown>
  ) => Promise<{ matches: DuckDBMatch[] }>;
  saveToDuckDB: (
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<{ success: boolean; error?: string }>;
  selectDirectory: () => Promise<{
    success: boolean;
    path?: string;
    canceled?: boolean;
    error?: string;
  }>;

  // Reinitialize DuckDB with new path
  reinitializeDuckDB: (newPath: string) => Promise<{
    success: boolean;
    error?: string;
  }>;
  importChatHistory: (params: {
    fileBuffer: Buffer | ArrayBuffer | Uint8Array;
    mode: string;
    user: string;
    applicationMode?: string;
    onProgress?: (data: {
      processed: number;
      total: number;
      percentage?: number;
      stage?: string;
    }) => void;
  }) => Promise<{
    success: boolean;
    error?: string;
    imported?: number;
    skipped?: number;
  }>;
  vllmModelStatus: () => Promise<{
    success: boolean;
    status?: VllmStatus;
    error?: string;
  }>;
  vllmStartModel: (
    modelId: string
  ) => Promise<{ success: boolean; error?: string }>;
  vllmGenerate: (
    payload: any
  ) => Promise<{ success: boolean; data?: any; error?: string }>;
  vllmStopModel: () => Promise<{ success: boolean; error?: string }>;
  listModels(): Promise<OllamaModel[]>;
  getAvailableModels(): Promise<OllamaModel[]>;
  downloadModel(
    modelId: string,
    onProgress?: (progress: number, speed: string, eta: string) => void
  ): Promise<boolean>;
  cancelDownload(modelId: string): Promise<void>;
  removeModel(modelId: string): Promise<void>;
  testConnection(): Promise<{
    success: boolean;
    message?: string;
    error?: string;
  }>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { DuckDBMatch } from "../electron/vector-database/interfaces/IVectorDatabase";

// <reference types="vite/client" />

type VllmStatus =
  import("../electron/preload/interfaces/IElectronAPI").VllmStatus;

interface ElectronAPI {
  openExternal: (url: string) => void;
  toggleMainWindow: () => Promise<{ success: boolean; error?: string }>;
  getPlatform: () => string;
  openTranscriptionTooltip: (callback: () => void) => () => void;
  startTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  stopTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  sendNeuralPrompt: (
    temporaryContext?: string
  ) => Promise<{ success: boolean; error?: string }>;
  clearNeuralTranscription: () => Promise<{ success: boolean; error?: string }>;
  onRealtimeTranscription: (callback: (data: string) => void) => () => void;
  onNeuralStarted: (callback: () => void) => () => void;
  onNeuralStopped: (callback: () => void) => () => void;
  onNeuralError: (callback: (error: string) => void) => () => void;
  onPromptSend: (callback: () => void) => () => void;
  onPromptSending: (callback: () => void) => () => void;
  onPromptPartialResponse: (callback: (data: string) => void) => () => void;
  onPromptSuccess: (callback: (data: string) => void) => () => void;
  onPromptError: (callback: (error: string) => void) => () => void;
  onClearTranscription: (callback: () => void) => () => void;
  onSendChunk: (callback: (chunk: ArrayBuffer) => void) => () => void;
  getEnv: (key: string) => Promise<string | null>;
  sendAudioChunk: (
    chunk: Uint8Array
  ) => Promise<{ success: boolean; error?: string }>;
  sendAudioTranscription: (text: string) => void;
  toogleNeuralRecording: (callback: () => void) => () => void;
  onForceStyle: (callback: (style: string) => void) => () => void;
  onForceImprovisation: (callback: () => void) => () => void;
  onRepeatResponse: (callback: () => void) => () => void;
  onStopTTS: (callback: () => void) => () => void;
  setDeepgramLanguage: (lang: string) => void;
  sendPromptUpdate: (
    type: "partial" | "complete" | "error",
    content: string
  ) => void;
  // Pinecone methods removed - using DuckDB only
  queryDuckDB: (
    embedding: number[],
    topK?: number,
    keywords?: string[],
    filters?: Record<string, unknown>
  ) => Promise<{ matches: DuckDBMatch[] }>;
  saveToDuckDB: (
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<{ success: boolean; error?: string }>;
  selectDirectory: () => Promise<{
    success: boolean;
    path?: string;
    canceled?: boolean;
    error?: string;
  }>;

  // Reinitialize DuckDB with new path
  reinitializeDuckDB: (newPath: string) => Promise<{
    success: boolean;
    error?: string;
  }>;
  importChatHistory: (params: {
    fileBuffer: Buffer | ArrayBuffer | Uint8Array;
    mode: string;
    user: string;
    applicationMode?: string;
    onProgress?: (data: {
      processed: number;
      total: number;
      percentage?: number;
      stage?: string;
    }) => void;
  }) => Promise<{
    success: boolean;
    error?: string;
    imported?: number;
    skipped?: number;
  }>;
  vllmModelStatus: () => Promise<{
    success: boolean;
    status?: VllmStatus;
    error?: string;
  }>;
  vllmStartModel: (
    modelId: string
  ) => Promise<{ success: boolean; error?: string }>;
  vllmGenerate: (
    payload: any
  ) => Promise<{ success: boolean; data?: any; error?: string }>;
  vllmStopModel: () => Promise<{ success: boolean; error?: string }>;
  listModels(): Promise<OllamaModel[]>;
  getAvailableModels(): Promise<OllamaModel[]>;
  downloadModel(
    modelId: string,
    onProgress?: (progress: number, speed: string, eta: string) => void
  ): Promise<boolean>;
  cancelDownload(modelId: string): Promise<void>;
  removeModel(modelId: string): Promise<void>;
  testConnection(): Promise<{
    success: boolean;
    message?: string;
    error?: string;
  }>;
}
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

@import "./styles/orchos-theme.css";
@tailwind base;
@tailwind components;
@tailwind utilities;// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react"
import ReactDOM from "react-dom/client"
import App from "./App"
import "./index.css"

ReactDOM.createRoot(document.getElementById("root")!).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
)
