import PanelHeader from "./components/PanelHeader";
import SettingsModal from "./components/SettingsModal";
import { useChatGptImport } from "./hooks/useChatGptImport";
import { useTranscriptionManager } from "./hooks/useTranscriptionManager";
import { TranscriptionPanelProps } from "./types/interfaces";
// Import para controlar a visibilidade da visualiza√ß√£o qu√¢ntica
import { useGeneralSettings } from "./components/settings/hooks/useGeneralSettings";
// M√≥dulo cortical para cards simples
// Importa√ß√£o dos arquivos CSS modulares - estrutura neural-simb√≥lica
import "./styles/TranscriptionPanel.animations.css"; // Anima√ß√µes e keyframes
import "./styles/TranscriptionPanel.buttons.css"; // Bot√µes e controles interativos
import "./styles/TranscriptionPanel.chathistory.css"; // Hist√≥rico de chats
import "./styles/TranscriptionPanel.layout.css"; // Layout, grid e estrutura espacial
import "./styles/TranscriptionPanel.overrides.css"; // Overrides para single-column mode
import "./styles/TranscriptionPanel.settings.css"; // Componentes de configura√ß√£o
import "./styles/TranscriptionPanel.tooltip.css"; // Tooltips e ajudas contextuais
import "./styles/TranscriptionPanel.variables.css"; // Vari√°veis globais e propriedades customizadas
import "./styles/TranscriptionPanel.visual.css"; // Efeitos visuais e glassmorfismo
// Quantum consciousness visualization import
import { QuantumVisualizationContainer } from "../QuantumVisualization/QuantumVisualizationContainer";
// Conversational Chat import
import { ConversationalChat } from "./components/ConversationalChat";
// Chat History imports
import { ChatHistorySidebar } from "./components/ConversationalChat/components/ChatHistorySidebar";
import { useChatHistory } from "./components/ConversationalChat/hooks/useChatHistory";
// Brain visualization is now handled in a separate module

const TranscriptionPanel: React.FC<TranscriptionPanelProps> = ({
  onClose,
  width,
}) => {
  const transcriptionManager = useTranscriptionManager();
  const { showToast } = useToast();

  // Hook para acessar as configura√ß√µes gerais, incluindo enableMatrix
  const { enableMatrix } = useGeneralSettings();

  // Chat History Hook
  const chatHistory = useChatHistory();

  // Track processing state
  const [isProcessing, setIsProcessing] = useState(false);

  if (!transcriptionManager) return null;

  // Move stable callbacks here when setTexts is already defined

  const {
    language,
    setLanguage,
    microphoneState,
    connectionState,
    toggleRecording,
    handleSendPrompt,
    clearTranscription,
    clearAiResponse,
    toggleExpand,
    isExpanded,
    temporaryContext,
    setTemporaryContext,
    texts,
    setTexts,
    audioDevices,
    selectedDevices,
    handleDeviceChange,
    isMicrophoneOn,
    isSystemAudioOn,
    setIsMicrophoneOn,
    setIsSystemAudioOn,
    showDetailedDiagnostics,
    setShowDetailedDiagnostics,
    connectionDetails,
    setConnectionDetails,
    transcriptionRef,
    getConnectionStatus,
    disconnectFromDeepgram,
    connectToDeepgram,
    waitForConnectionState,
    hasActiveConnection,
    ConnectionState,
  } = transcriptionManager;

  const {
    events: cognitionEvents,
    exporters,
    clearEvents,
    exportEvents,
  } = useCognitionLog();

  const {
    importFile,
    setImportFile,
    importUserName,
    setImportUserName,
    importMode,
    setImportMode,
    importProgress,
    importStage,
    importSummary,
    isImporting,
    showImportModal,
    setShowImportModal,
    handleFileChange,
    handleStartImport,
    handleCloseImportModal,
  } = useChatGptImport(showToast);

  // Stable callbacks defined AFTER setTexts is available to avoid TS errors
  const handleTranscriptionChange = useCallback(
    (value: string) => {
      console.log("üîÑ [PANEL] Transcription change:", value.substring(0, 50));
      setTexts((prev) => ({ ...prev, transcription: value }));
    },
    [setTexts]
  );

  const handleAiResponseChange = useCallback(
    (value: string) => {
      console.log("üîÑ [PANEL] AI response change:", value.substring(0, 50));
      setTexts((prev) => ({ ...prev, aiResponse: value }));
    },
    [setTexts]
  );

  const handleTemporaryContextChange = useCallback(
    (value: string) => {
      console.log(
        "üîÑ [PANEL] Temporary context change:",
        value.substring(0, 50)
      );
      setTemporaryContext(value);
    },
    [setTemporaryContext]
  );

  // Brain state and logic has been moved to BrainVisualization module

  // Brain visualization components have been moved to BrainVisualization module

  // --- Configura√ß√µes simples para Audio/Language Controls ---
  const [showSettings, setShowSettings] = useState(false);
  const settingsContainerRef = useRef<HTMLDivElement>(null);
  const settingsBtnRef = useRef<HTMLButtonElement>(null);
  const popupRef = useRef<HTMLDivElement>(null);
  const [settingsPopupPosition, setSettingsPopupPosition] = useState({
    top: 0,
    left: 0,
  });

  // Fun√ß√£o para alternar a visibilidade das configura√ß√µes
  const toggleSettings = () => {
    setShowSettings(!showSettings);
  };

  // Fechar configura√ß√µes ao clicar fora
  useEffect(() => {
    function handleClickOutside(event: MouseEvent) {
      if (
        settingsBtnRef.current &&
        !settingsBtnRef.current.contains(event.target as Node) &&
        popupRef.current &&
        !popupRef.current.contains(event.target as Node)
      ) {
        setShowSettings(false);
      }
    }
    if (showSettings) {
      document.addEventListener("mousedown", handleClickOutside);
    }
    return () => {
      document.removeEventListener("mousedown", handleClickOutside);
    };
  }, [showSettings]);

  useEffect(() => {
    if (showSettings && settingsBtnRef.current) {
      const rect = settingsBtnRef.current.getBoundingClientRect();
      const POPUP_WIDTH = 350;
      let left = rect.right + window.scrollX - POPUP_WIDTH;
      // Garante que n√£o vaze para fora da viewport
      left = Math.max(8, Math.min(left, window.innerWidth - POPUP_WIDTH - 8));
      setSettingsPopupPosition({
        top: rect.bottom + window.scrollY + 8,
        left,
      });
    }
  }, [showSettings]);

  // Prevent body scroll when dashboard is active
  useEffect(() => {
    document.body.classList.add("orchos-active");
    return () => {
      document.body.classList.remove("orchos-active");
    };
  }, []);

  // Estados para modais
  const [showLogsModal, setShowLogsModal] = useState(false);
  const [showSettingsModal, setShowSettingsModal] = useState(false);
  const [mobileSidebarOpen, setMobileSidebarOpen] = useState(false);

  // Settings content for the chat component
  const settingsContent = (
    <div className="neural-settings-popup">
      <div className="mb-2 pb-2">
        <h3 className="orchos-title flex items-center gap-2">
          <svg
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="#00faff"
            strokeWidth="2"
          >
            <circle cx="12" cy="12" r="3"></circle>
            <path d="M19.4 15a1.65 1.65 0 0 0 .33 1.82l.06.06a2 2 0 0 1 0 2.83 2 2 0 0 1-2.83 0l-.06-.06a1.65 1.65 0 0 0-1.82-.33 1.65 1.65 0 0 0-1 1.51V21a2 2 0 0 1-2 2 2 2 0 0 1-2-2v-.09A1.65 1.65 0 0 0 9 19.4a1.65 1.65 0 0 0-1.82.33l-.06.06a2 2 0 0 1-2.83 0 2 2 0 0 1 0-2.83l.06-.06a1.65 1.65 0 0 0 .33-1.82 1.65 1.65 0 0 0-1.51-1H3a2 2 0 0 1-2-2 2 2 0 0 1 2-2h.09A1.65 1.65 0 0 0 4.6 9a1.65 1.65 0 0 0-.33-1.82l-.06-.06a2 2 0 0 1 0-2.83 2 2 0 0 1 2.83 0l.06.06a1.65 1.65 0 0 0 1.82.33H9a1.65 1.65 0 0 0 1-1.51V3a2 2 0 0 1 2-2 2 2 0 0 1 2 2v.09a1.65 1.65 0 0 0 1 1.51 1.65 1.65 0 0 0 1.82-.33l.06-.06a2 2 0 0 1 2.83 0 2 2 0 0 1 0 2.83l-.06.06a1.65 1.65 0 0 0-.33 1.82V9a1.65 1.65 0 0 0 1.51 1H21a2 2 0 0 1 2 2 2 2 0 0 1-2 2h-.09a1.65 1.65 0 0 0-1.51 1z"></path>
          </svg>
          Neural Settings
        </h3>
      </div>
      <div className="flex flex-col gap-4 px-1">
        <AudioControls
          isMicrophoneOn={isMicrophoneOn}
          setIsMicrophoneOn={setIsMicrophoneOn}
          isSystemAudioOn={isSystemAudioOn}
          setIsSystemAudioOn={setIsSystemAudioOn}
          audioDevices={audioDevices}
          selectedDevices={selectedDevices}
          handleDeviceChange={handleDeviceChange}
        />
      </div>
    </div>
  );

  // Generate a key based on current conversation ID to force remount when conversation changes
  const chatKey = React.useMemo(
    () => chatHistory.currentConversationId || "default-chat",
    [chatHistory.currentConversationId]
  );

  // --- Render ---
  return (
    <div
      style={{
        height: "100vh",
        width: "100vw",
        display: "flex",
        flexDirection: "column",
        overflow: "hidden",
        maxHeight: "100vh",
        maxWidth: "100vw",
      }}
    >
      {/* Panel Header - Now positioned absolutely */}
      <PanelHeader
        onClose={() => {
          if (window?.electronAPI?.closeWindow) {
            window.electronAPI.closeWindow();
          } else if (onClose) {
            onClose();
          }
        }}
        onMinimize={() => {
          if (window?.electronAPI?.minimizeWindow) {
            window.electronAPI.minimizeWindow();
          }
        }}
        onShowSettings={() => setShowSettingsModal(true)}
        onShowLogsModal={() => setShowLogsModal(true)}
        onShowImportModal={() => setShowImportModal(true)}
        onToggleDiagnostics={() =>
          setShowDetailedDiagnostics(!showDetailedDiagnostics)
        }
        connectionState={connectionState}
        microphoneState={microphoneState}
        hasActiveConnection={hasActiveConnection}
        onDisconnect={disconnectFromDeepgram}
        onReconnect={connectToDeepgram}
      />

      {/* Main Chat Dashboard Layout */}
      <div
        className={`orchos-quantum-dashboard with-sidebar ${
          !enableMatrix ? "single-column" : ""
        }`}
        style={{
          flex: "1 1 auto",
        }}
      >
        {/* Chat History Sidebar */}
        <div
          className={`chat-history-sidebar ${
            mobileSidebarOpen ? "mobile-open" : ""
          }`}
        >
          <ChatHistorySidebar
            conversations={chatHistory.conversations}
            currentConversationId={chatHistory.currentConversationId}
            onSelectConversation={(id: string) => {
              // Clear any pending AI responses when switching conversations
              clearAiResponse();
              clearTranscription();
              chatHistory.selectConversation(id);
            }}
            onCreateNewConversation={() => {
              // Clear any pending AI responses when creating new conversation
              clearAiResponse();
              clearTranscription();
              return chatHistory.createNewConversation();
            }}
            onDeleteConversation={chatHistory.deleteConversation}
            onSearchConversations={chatHistory.searchConversations}
            isProcessing={isProcessing}
          />
        </div>

        {/* Quantum Visualization Zone - Left Panel with Golden Ratio */}
        {/* L√≥gica corrigida: enableMatrix = true mostra, false esconde */}
        {enableMatrix && (
          <div
            key="quantum-visualization-zone"
            className="quantum-visualization-zone"
          >
            <QuantumVisualizationContainer
              cognitionEvents={cognitionEvents}
              height="100%"
              width="100%"
              lowPerformanceMode={false}
              showLegend={true}
            />
          </div>
        )}

        {/* Conversational Chat Zone - Main Panel */}
        <div
          key="neural-chat-zone"
          className="neural-chat-zone"
          style={{
            height: "100%",
            width: "100%",
            padding: "1.2rem",
            overflow: "hidden" /* For√ßa o chat a usar scroll interno */,
            display: "flex",
            flexDirection: "column",
          }}
        >
          <ConversationalChat
            key={chatKey}
            transcriptionText={texts.transcription}
            onTranscriptionChange={handleTranscriptionChange}
            onClearTranscription={clearTranscription}
            aiResponseText={texts.aiResponse}
            onAiResponseChange={handleAiResponseChange}
            onClearAiResponse={clearAiResponse}
            temporaryContext={temporaryContext}
            onTemporaryContextChange={handleTemporaryContextChange}
            microphoneState={microphoneState}
            onToggleRecording={toggleRecording}
            onSendPrompt={handleSendPrompt}
            // Audio settings props
            language={language}
            setLanguage={setLanguage}
            isMicrophoneOn={isMicrophoneOn}
            setIsMicrophoneOn={setIsMicrophoneOn}
            isSystemAudioOn={isSystemAudioOn}
            setIsSystemAudioOn={setIsSystemAudioOn}
            audioDevices={audioDevices}
            selectedDevices={selectedDevices}
            handleDeviceChange={handleDeviceChange}
            // Chat History props
            currentConversation={chatHistory.currentConversation}
            onAddMessageToConversation={chatHistory.addMessageToConversation}
            onProcessingChange={setIsProcessing}
          />
        </div>
      </div>

      {/* Import Modal - Always on top */}
      {showImportModal && (
        <ImportModal
          show={showImportModal}
          onClose={handleCloseImportModal}
          importFile={importFile}
          setImportFile={setImportFile}
          importUserName={importUserName}
          setImportUserName={setImportUserName}
          importMode={importMode}
          setImportMode={setImportMode}
          importProgress={importProgress}
          importStage={importStage}
          importSummary={importSummary}
          isImporting={isImporting}
          handleFileChange={handleFileChange}
          handleStartImport={handleStartImport}
          handleCloseImportModal={handleCloseImportModal}
        />
      )}

      {/* Modal de Logs de Cogni√ß√£o */}
      {showLogsModal && (
        <div className="fixed inset-0 z-50 flex items-center justify-center bg-black/60">
          <div className="orchos-cognition-logs-modal">
            <div className="orchos-cognition-logs-header">
              <h2 className="text-xl font-bold text-[#7c4dff] flex items-center gap-2">
                <svg width="22" height="22" viewBox="0 0 20 20" fill="none">
                  <ellipse
                    cx="10"
                    cy="10"
                    rx="8"
                    ry="6"
                    stroke="#7c4dff"
                    strokeWidth="2"
                  />
                  <circle cx="10" cy="10" r="3" fill="#7c4dff" />
                </svg>
                Cognition Logs
              </h2>
              <button
                className="ml-2 px-2 py-1 rounded-full hover:bg-[#7c4dff22] transition"
                onClick={() => setShowLogsModal(false)}
                aria-label="Close logs modal"
              >
                <svg width="20" height="20" viewBox="0 0 20 20" fill="none">
                  <path
                    d="M6 6l8 8M14 6l-8 8"
                    stroke="#7c4dff"
                    strokeWidth="2"
                    strokeLinecap="round"
                  />
                </svg>
              </button>
            </div>
            <CognitionLogSection
              cognitionEvents={cognitionEvents}
              exporters={exporters}
              exportEvents={exportEvents}
              clearEvents={clearEvents}
            />
          </div>
        </div>
      )}

      {/* Settings Modal */}
      {showSettingsModal && (
        <SettingsModal
          show={showSettingsModal}
          onClose={() => setShowSettingsModal(false)}
        />
      )}

      {/* Mobile Sidebar Toggle Button */}
      <button
        className="mobile-sidebar-toggle"
        onClick={() => setMobileSidebarOpen(!mobileSidebarOpen)}
        title="Hist√≥rico de conversas"
      >
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none">
          <path
            d="M3 12h18m-18-6h18m-18 12h18"
            stroke="currentColor"
            strokeWidth="2"
            strokeLinecap="round"
          />
        </svg>
      </button>
    </div>
  );
};

export default TranscriptionPanel;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import * as React from "react"
import * as ToastPrimitive from "@radix-ui/react-toast"
import { cn } from "../../lib/utils"
import { AlertCircle, CheckCircle2, Info, X } from "lucide-react"

const ToastProvider = ToastPrimitive.Provider

export type ToastMessage = {
  title: string
  description: string
  variant: ToastVariant
}

const ToastViewport = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Viewport>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitive.Viewport>
>(({ className, ...props }, ref) => (
  <ToastPrimitive.Viewport
    ref={ref}
    className={cn(
      "fixed top-0 right-0 z-[100] flex max-h-screen w-full flex-col-reverse gap-1 p-2 sm:top-0 sm:right-0 sm:flex-col md:max-w-[320px]",
      className
    )}
    {...props}
  />
))
ToastViewport.displayName = ToastPrimitive.Viewport.displayName

type ToastVariant = "neutral" | "success" | "error"

interface ToastProps
  extends React.ComponentPropsWithoutRef<typeof ToastPrimitive.Root> {
  variant?: ToastVariant
  swipeDirection?: "right" | "left" | "up" | "down"
}

const toastVariants: Record<
  ToastVariant,
  { icon: React.ReactNode; bgColor: string }
> = {
  neutral: {
    icon: <Info className="h-3 w-3 text-amber-700" />,
    bgColor: "bg-amber-100"
  },
  success: {
    icon: <CheckCircle2 className="h-3 w-3 text-emerald-700" />,
    bgColor: "bg-emerald-100"
  },
  error: {
    icon: <AlertCircle className="h-3 w-3 text-red-700" />,
    bgColor: "bg-red-100"
  }
}

const Toast = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Root>,
  ToastProps
>(({ className, variant = "neutral", ...props }, ref) => (
  <ToastPrimitive.Root
    ref={ref}
    duration={4000}
    className={cn(
      "group pointer-events-auto relative flex w-full items-center space-x-2 overflow-hidden rounded-md p-2",
      toastVariants[variant].bgColor,
      className
    )}
    {...props}
  >
    {toastVariants[variant].icon}
    <div className="flex-1">{props.children}</div>
    <ToastPrimitive.Close className="absolute right-1 top-1 rounded-md p-0.5 text-zinc-500 opacity-0 transition-opacity hover:text-zinc-700 group-hover:opacity-100">
      <X className="h-2 w-2" />
    </ToastPrimitive.Close>
  </ToastPrimitive.Root>
))
Toast.displayName = ToastPrimitive.Root.displayName

const ToastAction = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Action>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitive.Action>
>(({ className, ...props }, ref) => (
  <ToastPrimitive.Action
    ref={ref}
    className={cn(
      "text-[0.65rem] font-medium text-zinc-600 hover:text-zinc-900",
      className
    )}
    {...props}
  />
))
ToastAction.displayName = ToastPrimitive.Action.displayName

const ToastTitle = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitive.Title>
>(({ className, ...props }, ref) => (
  <ToastPrimitive.Title
    ref={ref}
    className={cn("text-[0.7rem] font-medium text-zinc-900", className)}
    {...props}
  />
))
ToastTitle.displayName = ToastPrimitive.Title.displayName

const ToastDescription = React.forwardRef<
  React.ElementRef<typeof ToastPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitive.Description>
>(({ className, ...props }, ref) => (
  <ToastPrimitive.Description
    ref={ref}
    className={cn("text-[0.65rem] text-zinc-600", className)}
    {...props}
  />
))
ToastDescription.displayName = ToastPrimitive.Description.displayName

export type { ToastProps, ToastVariant }
export {
  ToastProvider,
  ToastViewport,
  Toast,
  ToastAction,
  ToastTitle,
  ToastDescription
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia



// Get the platform safely
const getPlatform = () => {
    try {
      return window.electronAPI?.getPlatform() || 'win32' // Default to win32 if API is not available
    } catch {
      return 'win32' // Default to win32 if there's an error
    }
  }
  
  // Platform-specific command key symbol
  export const COMMAND_KEY = getPlatform() === 'darwin' ? '‚åò' : 'Ctrl'
  
  // Helper to check if we're on Windows
  export const isWindows = getPlatform() === 'win32'
  
  // Helper to check if we're on macOS
  export const isMacOS = getPlatform() === 'darwin' // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import * as ort from 'onnxruntime-web';

/**
 * onnxruntimeConfig.ts
 * Global ONNX Runtime Web configuration for Orch-OS
 * Applies optimal settings for performance and suppresses benign warnings.
 */
export class OnnxRuntimeConfig {
  /**
   * Apply global ONNX Runtime settings. Call once at startup before any session creation.
   */
  static initialize(): void {
    // WASM flags
    ort.env.wasm.simd = true;
    ort.env.wasm.numThreads = Math.min(4, navigator.hardwareConcurrency || 4);
    ort.env.wasm.proxy = false;

    // WebGPU flags
    if (ort.env.webgpu) {
      ort.env.webgpu.validateInputContent = false;
    }

    // Logging: show warnings and errors only
    ort.env.logSeverityLevel = 2; // 0=VERBOSE,1=INFO,2=WARNING,3=ERROR,4=FATAL
    ort.env.logVerbosityLevel = 0;

    // Suppress known harmless warnings
    this.suppressHarmlessWarnings();
  }

  /**
   * Build optimized SessionOptions for a given provider.
   */
  static getSessionOptions(
    provider: 'webgpu' | 'wasm'
  ): ort.InferenceSession.SessionOptions {
    const executionProviders = provider === 'webgpu' ? ['webgpu', 'wasm'] : ['wasm'];

    return {
      executionProviders,
      graphOptimizationLevel: 'all',
      enableMemPattern: true,
      enableCpuMemArena: true,
      interOpNumThreads: provider === 'wasm' ? 1 : undefined,
      intraOpNumThreads: provider === 'wasm' ? 1 : undefined,
      logSeverityLevel: 2,
      logVerbosityLevel: 0,
    };
  }

  /**
   * Silence known-onboarding warnings from ONNX Runtime Web.
   */
  private static suppressHarmlessWarnings() {
    const origWarn = console.warn;
    const origErr  = console.error;

    const harmlessPatterns = [
      /VerifyEachNodeIsAssignedToAnEp/, // expected fallback info
    ];

    console.warn = (...args: any[]) => {
      const msg = args.join(' ');
      if (harmlessPatterns.some(p => p.test(msg))) return;
      origWarn.apply(console, args);
    };

    console.error = (...args: any[]) => {
      const msg = args.join(' ');
      if (harmlessPatterns.some(p => p.test(msg))) return;
      origErr.apply(console, args);
    };
  }
}

// Auto-initialize if imported in browser
if (typeof window !== 'undefined') {
  OnnxRuntimeConfig.initialize();
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Central user config for the application
import { getUserName } from '../services/StorageService';

// Central user config for the application
export function getPrimaryUser(): string {
  // Symbolic: retrieves the primary user's name from storage cortex
  return getUserName();
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Orch-OS Neural-Symbolic Interface Specification
 * 
 * SimpleModule - Interface para m√≥dulos de UI colaps√°veis/expans√≠veis.
 * Inten√ß√£o simb√≥lica: Representa um c√≥rtex neuralmente adapt√°vel que pode
 * expandir ou colapsar para otimizar a densidade cognitiva da interface.
 */

import { ReactNode } from 'react';

export interface SimpleModuleProps {
  /** T√≠tulo do m√≥dulo cortical */
  title: string;
  
  /** Define se o m√≥dulo inicia em estado expandido */
  defaultOpen?: boolean;
  
  /** Conte√∫do neural do m√≥dulo */
  children: ReactNode;
  
  /** Usado apenas para debugging visual - n√£o para produ√ß√£o */
  debugBorder?: boolean;
}

export interface SimpleModuleState {
  /** Estado atual (expandido/colapsado) */
  isExpanded: boolean;
  
  /** Fun√ß√£o para alternar entre estados */
  toggle: () => void;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// INeuralSignalService.ts
// Symbolic: Contract for neural signal extraction cortex
import { NeuralSignalResponse } from '../../../components/context/deepgram/interfaces/neural/NeuralSignalTypes';

export interface INeuralSignalService {
  generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ISemanticEnricher.ts
// Symbolic: Contract for semantic enrichment cortex

export interface ISemanticEnricher {
  enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ModeService, OrchOSModeEnum } from "../../../services/ModeService";
import {
  ChatGPTSession,
  ImportChatGPTParams,
  ImportResult,
} from "../interfaces/types";
import { DeduplicationService } from "../services/deduplication/DeduplicationService";
import { EmbeddingService } from "../services/embedding/EmbeddingService";
import { ChatGPTParser } from "../services/parser/ChatGPTParser";
import { VectorStorageService } from "../services/storage/VectorStorageService";
import { TextChunker } from "../services/text-processor/TextChunker";
import { Logger } from "../utils/logging";
import { ProgressReporter } from "../utils/progressReporter";

/**
 * Handler for importing ChatGPT history
 * Orchestrates different services following SOLID principles
 */
export async function importChatGPTHistoryHandler(
  params: ImportChatGPTParams
): Promise<ImportResult> {
  const {
    fileBuffer,
    mode,
    applicationMode,
    openAIService,
    pineconeHelper,
    onProgress,
  } = params;

  // Initialize services
  const logger = new Logger("[ImportChatGPT]");
  const progressReporter = new ProgressReporter(onProgress, logger);

  // Determine mode for service configuration
  let currentMode: OrchOSModeEnum;
  if (applicationMode) {
    // Convert string to enum
    if (applicationMode.toLowerCase() === "basic") {
      currentMode = OrchOSModeEnum.BASIC;
    } else if (applicationMode.toLowerCase() === "advanced") {
      currentMode = OrchOSModeEnum.ADVANCED;
    } else {
      logger.warn(
        `üü° Unknown applicationMode: "${applicationMode}", falling back to ModeService`
      );
      currentMode = ModeService.getMode();
    }
    logger.info(
      `üîß Using applicationMode from IPC: "${applicationMode}" -> ${currentMode}`
    );
  } else {
    currentMode = ModeService.getMode();
    logger.info(
      `üîß No applicationMode provided, using ModeService: ${currentMode}`
    );
  }

  const isBasicMode = currentMode === OrchOSModeEnum.BASIC;
  const storageType = "DuckDB"; // Always use DuckDB for both Basic and Advanced modes
  logger.info(
    `üóÑÔ∏è Storage mode: ${storageType} (${
      isBasicMode ? "Basic" : "Advanced"
    } mode)`
  );

  // Use the helper passed from IPC (already correct for the mode)
  const vectorHelper = pineconeHelper;

  const parser = new ChatGPTParser(progressReporter, logger);
  const deduplicationService = new DeduplicationService(
    vectorHelper,
    progressReporter,
    logger
  );
  const textChunker = new TextChunker();
  const embeddingService = new EmbeddingService(
    openAIService,
    logger,
    progressReporter,
    applicationMode
  );
  const storageService = new VectorStorageService(
    vectorHelper,
    progressReporter,
    logger,
    true // Always use DuckDB mode (true = isBasicMode)
  );

  try {
    // Log of start
    logger.info(
      `Starting ChatGPT import in ${mode} mode with applicationMode: ${
        applicationMode || "auto-detect"
      } using ${storageType}`
    );

    // 1. Parse the file
    const rawSessions = parser.parseBuffer(fileBuffer);

    // 2. Extract messages
    let allMessages = parser.extractMessages(rawSessions);
    logger.info(`Extracted ${allMessages.length} messages from file`);

    // 3. Ensure all messages have valid IDs
    logger.info("PASSO 3: Ensuring all messages have valid IDs...");
    allMessages = parser.ensureMessageIds(allMessages);
    logger.info("PASSO 3: IDs ensured successfully");

    // 4. Deduplication
    logger.info("PASSO 4: Starting deduplication...");
    let uniqueMessages;
    try {
      uniqueMessages = await deduplicationService.filterDuplicates(
        allMessages,
        mode
      );
      logger.info(
        `PASSO 4: After deduplication: ${uniqueMessages.length} unique messages`
      );
      logger.info("PASSO 4 COMPLETED: Moving to step 5...");

      // Additional verification right after deduplication
      if (!uniqueMessages || !Array.isArray(uniqueMessages)) {
        throw new Error(`Invalid deduplication result: ${uniqueMessages}`);
      }

      logger.info("CRITICAL CHECKPOINT: Deduplication completed successfully");
    } catch (error) {
      logger.error("FATAL ERROR during deduplication:", error);
      if (error instanceof Error) {
        logger.error(`Error details: ${error.message}`);
        logger.error(`Stack trace: ${error.stack || "Not available"}`);
      }
      throw new Error(
        `Deduplication failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    try {
      // 5. Data cleanup (overwrite mode)
      logger.info(`PASSO 5: Starting with mode=${mode}`);
      if (mode === "overwrite") {
        logger.info(
          `PASSO 5: Deleting existing data (overwrite mode) from ${storageType}...`
        );
        await storageService.deleteExistingData();
        logger.info(
          `PASSO 5: Existing data deleted successfully from ${storageType}`
        );
      } else {
        logger.info("PASSO 5: Increment mode - skipping data cleanup");
      }
      logger.info("PASSO 5 COMPLETED: Moving to step 6...");
    } catch (error) {
      logger.error("FATAL ERROR in step 5:", error);
      throw new Error(
        `Step 5 failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // Initialize embedding service based on mode (both use DuckDB storage)
    if (isBasicMode) {
      logger.info(
        "Initializing HuggingFace service for embeddings (Basic mode with DuckDB)..."
      );
    } else if (openAIService) {
      logger.info(
        "Initializing Ollama service for embeddings (Advanced mode with DuckDB)..."
      );
      const initialized = await embeddingService.ensureOpenAIInitialized();
      logger.info(
        `Ollama service initialization status: ${initialized ? "OK" : "FAILED"}`
      );
      if (!initialized) {
        logger.error(
          "Ollama service could not be initialized. Check Ollama configuration."
        );
        logger.info("Ensure Ollama is running and properly configured");
      }
    }

    // Verification of TextChunker before processing
    logger.info("VERIFICATION: Verifying TextChunker before processing...");
    try {
      logger.info(`TextChunker available: ${textChunker ? "YES" : "NO"}`);
      logger.info(
        `Number of messages to be processed: ${uniqueMessages.length}`
      );
      if (uniqueMessages.length > 0) {
        logger.info(
          `Example message for processing: ${JSON.stringify(
            uniqueMessages[0]
          ).substring(0, 150)}...`
        );
      }
    } catch (err) {
      logger.error("Verification failed: Error verifying TextChunker:", err);
    }

    // 6. Process messages into chunks
    logger.info("PASSO 6: Starting message chunk processing...");
    let messageChunks;
    try {
      // Verify input for chunk processor
      logger.info(
        `PASSO 6: Input for processor: ${
          uniqueMessages.length
        } messages, first item: ${JSON.stringify(uniqueMessages[0]).substring(
          0,
          100
        )}...`
      );

      messageChunks = textChunker.processMessagesIntoChunks(uniqueMessages);
      if (!messageChunks) {
        throw new Error("TextChunker returned undefined or null");
      }
      logger.info(
        `PASSO 6: Created ${messageChunks.length} text chunks from ${uniqueMessages.length} messages`
      );
      if (messageChunks.length > 0) {
        logger.info(
          `PASSO 6: Example chunk: ${JSON.stringify(messageChunks[0]).substring(
            0,
            100
          )}...`
        );
      }

      logger.info("PASSO 6 COMPLETED: Moving to step 7...");
    } catch (error) {
      logger.error("FATAL ERROR in chunk processing:", error);
      throw new Error(
        `Chunk processing failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // 7. Create batches for efficient processing
    logger.info("PASSO 7: Creating batches for efficient processing...");
    let batches;
    try {
      if (!messageChunks || messageChunks.length === 0) {
        throw new Error("No chunks available for batch processing");
      }
      batches = textChunker.createProcessingBatches(messageChunks);
      if (!batches) {
        throw new Error(
          "TextChunker returned undefined or null when creating batches"
        );
      }
      logger.info(
        `PASSO 7: Processing ${messageChunks.length} chunks in ${batches.length} batches`
      );
      logger.info(`PASSO 7: First batch has ${batches[0]?.length || 0} chunks`);
      logger.info("PASSO 7 COMPLETED: Moving to step 8...");
    } catch (error) {
      logger.error("FATAL ERROR in batch creation:", error);
      throw new Error(
        `Batch creation failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // Verification of transition between steps
    logger.info("TRANSITION: Moving from step 7 to step 8...");
    logger.info("=========== FIM DO DIAGN√ìSTICO ===========");
    logger.info("===============================================");

    // 8. Generate embeddings and create vectors
    logger.info(
      `STARTING EMBEDDINGS GENERATION using ${
        isBasicMode ? "HuggingFace" : "Ollama"
      } with DuckDB storage...`
    );
    let vectors;
    try {
      vectors = await embeddingService.generateEmbeddingsForChunks(
        batches,
        messageChunks
      );
      logger.info(`Generated ${vectors.length} vectors with embeddings`);
    } catch (error) {
      logger.error("FATAL ERROR in embedding generation:", error);
      throw new Error(
        `Embedding generation failed: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // 9. Save vectors to appropriate storage
    logger.info(
      `[DIAGNOSTIC] Vectors to be saved to ${storageType}: ${vectors.length}`
    );
    if (vectors.length > 0) {
      logger.info(
        `[DIAGNOSTIC] Example vector: ${JSON.stringify(vectors[0]).substring(
          0,
          200
        )}`
      );
    } else {
      logger.warn("[DIAGNOSTIC] No vectors generated for saving!");
    }
    const saveResult = await storageService.saveVectors(vectors);
    logger.info(
      `[DIAGNOSTIC] Save result to ${storageType}: success=${
        saveResult.success
      }, error=${saveResult.error || "none"}`
    );
    if (!saveResult.success) {
      logger.error(
        `[DIAGNOSTIC] Error saving vectors to ${storageType}: ${saveResult.error}`
      );
    }

    // 10. Calculate final statistics
    const totalMessagesInFile = rawSessions.reduce(
      (acc: number, session: ChatGPTSession) => {
        return acc + Object.values(session.mapping || {}).length;
      },
      0
    );

    const skipped = totalMessagesInFile - vectors.length;

    // 11. Log of completion
    logger.info(`=============================================`);
    logger.info(`üéâ IMPORTATION COMPLETED SUCCESSFULLY`);
    logger.info(`üìä Statistics:`);
    logger.info(
      `- Mode: ${mode === "overwrite" ? "OVERWRITE" : "INCREMENTAL"}`
    );
    logger.info(
      `- Storage: ${storageType} (${isBasicMode ? "Basic" : "Advanced"} mode)`
    );
    logger.info(`- Embeddings: ${isBasicMode ? "HuggingFace" : "Ollama"}`);
    logger.info(`- Total messages in file: ${totalMessagesInFile}`);
    logger.info(
      `- Duplicated messages ignored: ${skipped} (${Math.round(
        (skipped / totalMessagesInFile) * 100
      )}%)`
    );
    logger.info(
      `- Vectors saved to ${storageType}: ${vectors.length} (${Math.round(
        (vectors.length / totalMessagesInFile) * 100
      )}%)`
    );
    logger.info(`=============================================`);

    return {
      success: true,
      imported: vectors.length,
      skipped,
      totalMessagesInFile,
      mode,
    };
  } catch (error) {
    logger.error(`Importation failed:`, error);
    return {
      success: false,
      imported: 0,
      skipped: 0,
      totalMessagesInFile: 0,
      mode,
      error: error instanceof Error ? error.message : "Unknown error",
    };
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IOpenAIService } from "../../../components/context/deepgram/interfaces/openai/IOpenAIService";

// ChatGPT data interfaces for artificial brain memory import
export interface ChatGPTMessageContent {
  content_type: string;
  parts: string[];
}

export interface ChatGPTMessageAuthor {
  role: "user" | "assistant" | "developer" | string;
  name?: string;
}

export interface ChatGPTMessage {
  id: string;
  author: ChatGPTMessageAuthor;
  create_time: number;
  content: ChatGPTMessageContent;
  parent?: string;
  children?: string[];
}

export interface ChatGPTMessageItem {
  message: ChatGPTMessage;
  parent?: string;
}

export interface ChatGPTSession {
  title: string;
  create_time: number;
  update_time: number;
  mapping: Record<string, ChatGPTMessageItem>;
}

// Interface for processed messages used in cognitive memory orchestration
export interface ProcessedMessage {
  role: string;
  content: string;
  timestamp: number | null;
  id: string | null;
  parent: string | null;
  session_title: string | null;
  session_create_time: number | null;
  session_update_time: number | null;
}

// Type for message chunks used in memory segmentation
export interface MessageChunk {
  original: ProcessedMessage;
  content: string;
  part?: number;
  totalParts?: number;
}

// Interface para vetores Pinecone
export interface PineconeVector {
  id: string;
  values: number[];
  metadata: Record<string, string | number | boolean | string[]>;
}

// Interface for progress information
export interface ProgressInfo {
  processed: number;
  total: number;
  percentage: number;
  stage: "parsing" | "deduplicating" | "generating_embeddings" | "saving";
}

// Interface for import parameters (using DuckDB helper with legacy name for compatibility)
export interface ImportChatGPTParams {
  fileBuffer: Buffer;
  mode: "increment" | "overwrite";
  applicationMode?: "basic" | "advanced"; // Mode passed from renderer process
  openAIService?: IOpenAIService | null;
  pineconeHelper: any; // DuckDB helper with legacy interface name for compatibility
  onProgress?: (info: ProgressInfo) => void;
}

// Interface for import result
export interface ImportResult {
  success: boolean;
  imported: number;
  skipped: number;
  totalMessagesInFile: number;
  mode: "increment" | "overwrite";
  error?: string;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ProcessedMessage } from '../../interfaces/types';
import { Logger } from '../../utils/logging';
import { ProgressReporter } from '../../utils/progressReporter';

/**
 * Interface for checking existing vectors (abstraction for Pinecone/DuckDB)
 */
interface IVectorChecker {
  checkExistingIds(ids: string[], progressCallback?: (processed: number, total: number) => void): Promise<string[]>;
}

/**
 * Service to check and eliminate duplicate messages
 */
export class DeduplicationService {
  private vectorChecker: IVectorChecker;
  private progressReporter: ProgressReporter;
  private logger: Logger;

  constructor(vectorChecker: IVectorChecker, progressReporter: ProgressReporter, logger?: Logger) {
    this.vectorChecker = vectorChecker;
    this.progressReporter = progressReporter;
    this.logger = logger || new Logger('[DeduplicationService]');
    this.logger.info('DeduplicationService successfully instantiated');
  }

  /**
   * Filters duplicate messages based on existing IDs
   */
  public async filterDuplicates(
    messages: ProcessedMessage[], 
    mode: 'increment' | 'overwrite'
  ): Promise<ProcessedMessage[]> {
    // Log detalhado das primeiras mensagens recebidas
    this.logger.debug('First 3 messages received for deduplication:', JSON.stringify(messages.slice(0, 3), null, 2));

    // Ensure all messages have a valid id
    const messagesWithIds = messages.map((msg, idx) => ({
      ...msg,
      id: msg.id && msg.id !== '' ? msg.id : `fallback_id_${idx}_${Date.now()}`
    }));
    
    // If mode is overwrite, deduplication is not needed
    if (mode === 'overwrite') {
      this.logger.info('OVERWRITE mode selected, skipping duplicate check');
      return messagesWithIds;
    }
    
    this.logger.info('INCREMENT mode selected, verifying duplicates...');
    
    // Start progress
    const originalLength = messagesWithIds.length;
    this.progressReporter.startStage('deduplicating', originalLength);
    
    // Extract all message IDs to check for duplicates
    const messageIdsToCheck = messagesWithIds
      .filter((m: ProcessedMessage) => m.id !== null && m.id !== undefined && m.id !== '')
      .map((m: ProcessedMessage) => m.id as string);

    const limitedMessageIdsToCheck = messageIdsToCheck;
      
    this.logger.debug(`Total of original messages: ${originalLength}`);
    this.logger.debug(`Total messages with valid ID: ${messageIdsToCheck.length}`);
    this.logger.debug(`Messages without valid ID: ${originalLength - messageIdsToCheck.length}`);
    
    if (messageIdsToCheck.length === 0) {
      this.logger.warn('No valid message ID found in file.');
      this.progressReporter.completeStage('deduplicating', originalLength);
      return messages;
    }
    
    // Verify existence of IDs in storage with timeout
    this.logger.info(`Verifying ${limitedMessageIdsToCheck.length} IDs in vector storage`);
    this.logger.debug('First 10 IDs to be verified:', JSON.stringify(limitedMessageIdsToCheck.slice(0, 10)));
    const existingMessageIds = new Set<string>();
    const timeoutPromise = new Promise<string[]>((_, reject) => {
      const timeoutId = setTimeout(() => {
        clearTimeout(timeoutId);
        reject(new Error('Timeout checking IDs in vector storage - operation took more than 5 minutes'));
      }, 5 * 60 * 1000); // 5 minutes timeout
    });
    let existingIds: string[] = [];
    try {
      this.logger.info('Starting complete ID verification with security timeout...');
      this.logger.debug('Calling vectorChecker.checkExistingIds...');
      const checkIdsPromise = this.vectorChecker.checkExistingIds(
        limitedMessageIdsToCheck,
        (processed, total) => {
          this.logger.debug(`[Dedup] Progress: ${processed}/${total}`);
          this.progressReporter.updateProgress('deduplicating', processed, total);
          if (processed % 500 === 0 || processed === total) {
            this.logger.info(`Progress of ID verification: ${processed}/${total}`);
          }
        }
      );
      existingIds = await Promise.race([checkIdsPromise, timeoutPromise]) as string[];
      this.logger.info('checkExistingIds completed successfully');
      this.logger.debug('First 10 IDs returned:', JSON.stringify(existingIds.slice(0, 10)));
      this.logger.info(`Total of existing IDs found: ${existingIds.length}`);

      // Verify result
      if (!existingIds || !Array.isArray(existingIds)) {
        throw new Error(`Invalid result of checkExistingIds: ${existingIds}`);
      }

      existingIds.forEach(id => existingMessageIds.add(id));
      this.logger.info(`Found ${existingIds.length} existing IDs`);
      this.logger.info('ID verification process completed successfully');
    } catch (error) {
      this.logger.error('Error checking existing IDs:', error);
      if (error instanceof Error) {
        this.logger.error(`Error details: ${error.message}`);
        this.logger.error(`Stack trace: ${error.stack || 'Not available'}`);
      }
      this.progressReporter.completeStage('deduplicating', originalLength);
      // In case of error, proceed as if there are no duplicates
      this.logger.warn('Continuing without complete duplicate check due to error');
      return messagesWithIds;
    }
    
    // Filter duplicate messages
    this.logger.info('Starting duplicate message filtering...');
    try {
      const duplicateMessages = messages.filter((m: ProcessedMessage) => 
        m.id && existingMessageIds.has(m.id)
      );
      
      const uniqueMessages = messages.filter((m: ProcessedMessage) => 
        !m.id || !existingMessageIds.has(m.id)
      );
      
      // Deduplication analysis logs
      this.logger.info('Deduplication analysis:');
      this.logger.info(`- Total of messages in file: ${originalLength}`);
      this.logger.info(`- Existing messages (ignored): ${duplicateMessages.length}`);
      this.logger.info(`- New messages to be imported: ${uniqueMessages.length}`);
      
      // Verify data
      if (!uniqueMessages || !Array.isArray(uniqueMessages)) {
        throw new Error(`Invalid result of message filtering: ${uniqueMessages}`);
      }
      
      // Log to indicate process completion
      this.logger.info('====== DEDUPLICATION SUCCESSFULLY COMPLETED ======');
      this.logger.info(`Returning ${uniqueMessages.length} unique messages`);
      
      // Complete progress (ensures this happens even if the return fails)
      this.progressReporter.completeStage('deduplicating', originalLength);
      
      return uniqueMessages;
    } catch (error) {
      // Detailed error log
      this.logger.error('FATAL ERROR filtering messages:', error);
      if (error instanceof Error) {
        this.logger.error(`Error details: ${error.message}`);
        this.logger.error(`Stack trace: ${error.stack || 'Not available'}`);
      }
      
      // Ensure progress is completed even in case of error
      this.progressReporter.completeStage('deduplicating', originalLength);
      
      // Rethrow the error for higher-level handling
      throw new Error(`Failed to process duplicate messages: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IEmbeddingService } from "../../../../components/context/deepgram/interfaces/openai/IEmbeddingService";
import { IOpenAIService } from "../../../../components/context/deepgram/interfaces/openai/IOpenAIService";
import { OllamaEmbeddingService } from "../../../../components/context/deepgram/services/ollama/OllamaEmbeddingService";
import { HuggingFaceEmbeddingService } from "../../../../services/huggingface/HuggingFaceEmbeddingService";
import { ModeService, OrchOSModeEnum } from "../../../../services/ModeService";
import { getOption, STORAGE_KEYS } from "../../../../services/StorageService";
import { getModelDimensions } from "../../../../utils/EmbeddingUtils";
import { MessageChunk, PineconeVector } from "../../interfaces/types";
import { Logger } from "../../utils/logging";
import { ProgressReporter } from "../../utils/progressReporter";

/**
 * Service for generating embeddings
 * Switches between Ollama (Advanced mode) and HuggingFace (Basic mode)
 * Both modes use DuckDB for storage
 */
export class EmbeddingService {
  private openAIService: IOpenAIService | null | undefined;
  private embeddingService: IEmbeddingService;
  private logger: Logger;
  private progressReporter?: ProgressReporter;
  private embeddingDimension: number = 1536; // Default dimension, will be updated dynamically
  private isBasicMode: boolean;

  constructor(
    openAIService: IOpenAIService | null | undefined,
    logger?: Logger,
    progressReporter?: ProgressReporter,
    applicationMode?: "basic" | "advanced"
  ) {
    this.openAIService = openAIService;
    this.logger = logger || new Logger("[EmbeddingService]");
    this.progressReporter = progressReporter;

    // Use provided applicationMode or detect from ModeService
    let currentMode: OrchOSModeEnum;
    if (applicationMode) {
      // Convert string to enum
      if (applicationMode.toLowerCase() === "basic") {
        currentMode = OrchOSModeEnum.BASIC;
      } else if (applicationMode.toLowerCase() === "advanced") {
        currentMode = OrchOSModeEnum.ADVANCED;
      } else {
        this.logger.warn(
          `üü° [EmbeddingService] Unknown applicationMode: "${applicationMode}", falling back to ModeService`
        );
        currentMode = ModeService.getMode();
      }
      this.logger.info(
        `üîß [EmbeddingService] Using applicationMode from IPC: "${applicationMode}" -> ${currentMode}`
      );
    } else {
      currentMode = ModeService.getMode();
      this.logger.info(
        `üîß [EmbeddingService] No applicationMode provided, using ModeService: ${currentMode}`
      );
    }

    this.isBasicMode = currentMode === OrchOSModeEnum.BASIC;

    this.logger.info(`üîç [EmbeddingService] === MODE DETECTION DEBUG ===`);
    this.logger.info(
      `üîç [EmbeddingService] Input applicationMode: "${applicationMode}"`
    );
    this.logger.info(
      `üîç [EmbeddingService] Resolved OrchOSModeEnum: "${currentMode}"`
    );
    this.logger.info(
      `üîç [EmbeddingService] OrchOSModeEnum.BASIC: "${OrchOSModeEnum.BASIC}"`
    );
    this.logger.info(
      `üîç [EmbeddingService] OrchOSModeEnum.ADVANCED: "${OrchOSModeEnum.ADVANCED}"`
    );
    this.logger.info(
      `üîç [EmbeddingService] Final isBasicMode: ${this.isBasicMode}`
    );
    this.logger.info(
      `üîç [EmbeddingService] Final selected mode: ${
        this.isBasicMode ? "BASIC (HuggingFace)" : "ADVANCED (Ollama)"
      }`
    );

    // Check storage for debugging
    const storageMode =
      typeof window !== "undefined"
        ? window.localStorage?.getItem("APPLICATION_MODE") || "undefined"
        : "not-available-in-main-process";
    this.logger.info(
      `üîç [EmbeddingService] Storage APPLICATION_MODE: "${storageMode}"`
    );
    this.logger.info(`ÔøΩÔøΩ [EmbeddingService] === END MODE DEBUG ===`);

    this.embeddingService = this.createEmbeddingService();

    // Only subscribe to mode changes if no applicationMode was provided
    // (to avoid conflicts between IPC and ModeService)
    if (!applicationMode) {
      // Subscribe to mode changes to update embedding service when needed
      ModeService.onModeChange((newMode: OrchOSModeEnum) => {
        this.logger.info(
          `üîÑ [EmbeddingService] Mode change detected: ${newMode}`
        );
        this.updateModeAndService(newMode);
      });
    } else {
      this.logger.info(
        `üîß [EmbeddingService] Skipping ModeService listener (using IPC mode: ${applicationMode})`
      );
    }
  }

  /**
   * Updates the embedding dimension based on the embedding service
   * @param service The embedding service to get dimension from
   */
  private updateEmbeddingDimension(service: IEmbeddingService): void {
    // Check if the service has a method to get embedding dimension
    if (typeof (service as any).getEmbeddingDimension === "function") {
      try {
        this.embeddingDimension = (service as any).getEmbeddingDimension();
        this.logger.info(
          `Using dynamic embedding dimension from service: ${this.embeddingDimension}`
        );
        return;
      } catch (error) {
        this.logger.warn(
          `Failed to get embedding dimension dynamically: ${error}`
        );
      }
    }

    // If service doesn't provide the dimension or there was an error, use the utility function
    if (this.isBasicMode) {
      const defaultHFModelName =
        getOption(STORAGE_KEYS.HF_EMBEDDING_MODEL) || "";
      this.embeddingDimension = getModelDimensions(defaultHFModelName);
    } else {
      const defaultOllamaModel =
        getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL) || "";
      this.embeddingDimension = getModelDimensions(defaultOllamaModel);
    }

    this.logger.info(
      `Using embedding dimension from utility for mode ${
        this.isBasicMode ? "BASIC" : "ADVANCED"
      }: ${this.embeddingDimension}`
    );
  }

  /**
   * Creates the appropriate embedding service based on mode
   */
  private createEmbeddingService(): IEmbeddingService {
    if (this.isBasicMode) {
      this.logger.info(
        `[EmbeddingService] Creating HuggingFaceEmbeddingService for Basic mode`
      );
      const service = new HuggingFaceEmbeddingService();
      // Use the service's method to get the embedding dimension
      this.updateEmbeddingDimension(service);
      return service;
    } else {
      // In advanced mode, use Ollama with the selected model
      const ollamaModel = getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL);
      this.logger.info(
        `[EmbeddingService] Creating OllamaEmbeddingService with model: ${
          ollamaModel || "default"
        } for Advanced mode`
      );

      if (!this.openAIService) {
        this.logger.error(
          "[EmbeddingService] OpenAI service not available for OllamaEmbeddingService"
        );
        // Fallback to HuggingFace if Ollama service is not available
        const service = new HuggingFaceEmbeddingService();
        this.updateEmbeddingDimension(service);
        return service;
      }

      const service = new OllamaEmbeddingService(this.openAIService, {
        model: ollamaModel,
      });
      // Use the service's method to get the embedding dimension
      this.updateEmbeddingDimension(service);
      return service;
    }
  }

  /**
   * Initializes the embedding service (Ollama for Advanced mode, HuggingFace for Basic mode)
   */
  public async ensureOpenAIInitialized(): Promise<boolean> {
    if (this.isBasicMode) {
      this.logger.info(
        "‚úÖ Basic mode detected - using HuggingFace embeddings (no Ollama required)"
      );
      return true; // In basic mode, we don't need Ollama
    }

    this.logger.info("Verifying Ollama service initialization...");

    if (!this.openAIService) {
      this.logger.error(
        "FATAL ERROR: Ollama service not provided - verify if the service is correctly injected"
      );
      return false;
    }

    // Verify if the service is already initialized
    const isInitialized = this.openAIService.isInitialized();
    this.logger.info(
      `Status of Ollama initialization: ${
        isInitialized ? "Already initialized" : "Not initialized"
      }`
    );

    if (isInitialized) {
      return true;
    }

    this.logger.warn(
      "Ollama client not initialized. Attempting to initialize via loadApiKey()..."
    );

    try {
      // Use the loadApiKey method from the service itself
      this.logger.info("Calling ollamaService.loadApiKey()...");
      await this.openAIService.loadApiKey();

      // Verify if it was initialized
      if (this.openAIService.isInitialized()) {
        this.logger.success(
          "Ollama client initialized successfully via loadApiKey()!"
        );
        return true;
      }

      // If it was not initialized, check if there is an ensureOpenAIClient method
      if (this.openAIService.ensureOpenAIClient) {
        this.logger.info(
          "Attempting to initialize via ensureOllamaClient()..."
        );
        const initialized = await this.openAIService.ensureOpenAIClient();
        if (initialized) {
          this.logger.success(
            "Ollama client initialized successfully via ensureOllamaClient()!"
          );
          return true;
        }
      }

      this.logger.error(
        "Failed to initialize Ollama client. Initialization attempts failed."
      );
      return false;
    } catch (error) {
      this.logger.error("Error initializing Ollama client:", error);
      return false;
    }
  }

  /**
   * Generates embeddings for text chunks using the appropriate service (Ollama or HuggingFace)
   */
  public async generateEmbeddingsForChunks(
    batches: MessageChunk[][],
    allMessageChunks: MessageChunk[]
  ): Promise<PineconeVector[]> {
    // Ensure we have the correct embedding dimension from the service
    this.updateEmbeddingDimension(this.embeddingService);

    // Start the progress of generating embeddings
    if (this.progressReporter) {
      this.progressReporter.startStage(
        "generating_embeddings",
        allMessageChunks.length
      );
    }

    // Verify if we have the embedding service available
    const embeddingInitialized = await this.ensureOpenAIInitialized();
    this.logger.info(
      `Status of embedding service for generation: ${
        embeddingInitialized ? "Initialized" : "Not initialized"
      }`
    );

    if (!embeddingInitialized) {
      if (this.isBasicMode) {
        throw new Error(
          "HuggingFace embedding service not initialized in Basic mode."
        );
      } else {
        throw new Error(
          "Ollama service not initialized. Ensure Ollama is running and properly configured."
        );
      }
    }

    const vectors: PineconeVector[] = [];
    let embeddingsProcessed = 0;

    let processedTotal = 0;
    for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
      const batch = batches[batchIndex];
      this.logger.info(
        `Processing batch ${batchIndex + 1}/${batches.length} with ${
          batch.length
        } messages`
      );

      // Prepare the texts for the current batch
      const batchTexts = batch.map((chunk) => chunk.content);

      // Generate embeddings for the entire batch using the appropriate service
      let batchEmbeddings: number[][] = [];

      try {
        if (this.isBasicMode) {
          // Use HuggingFace service for batch embeddings
          this.logger.info(
            `[BASIC MODE] Generating embeddings with HuggingFace for batch ${
              batchIndex + 1
            }`
          );
          batchEmbeddings = await this.embeddingService.createEmbeddings(
            batchTexts
          );
          this.logger.success(
            `‚úÖ HuggingFace embeddings generated successfully for batch ${
              batchIndex + 1
            }/${batches.length}`
          );
        } else {
          // Use Ollama service for batch embeddings
          this.logger.info(
            `[ADVANCED MODE] Generating embeddings with Ollama for batch ${
              batchIndex + 1
            }`
          );
          if (this.openAIService && this.openAIService.createEmbeddings) {
            // If the API supports batch embeddings
            batchEmbeddings = await this.openAIService.createEmbeddings(
              batchTexts
            );
            this.logger.success(
              `‚úÖ Ollama embeddings generated successfully for batch ${
                batchIndex + 1
              }/${batches.length}`
            );
          } else {
            // Fallback: generate embeddings one by one
            this.logger.warn(
              "API does not support batch embeddings, processing sequentially..."
            );
            batchEmbeddings = await Promise.all(
              batchTexts.map(async (text) => {
                try {
                  return await this.openAIService!.createEmbedding(text);
                } catch (err) {
                  this.logger.error(
                    `Error generating embedding for text: ${text.substring(
                      0,
                      50
                    )}...`,
                    err
                  );
                  throw new Error(
                    `Failed to generate embedding: ${
                      err instanceof Error ? err.message : String(err)
                    }`
                  );
                }
              })
            );
          }
        }
      } catch (batchError) {
        this.logger.error(
          `Error processing batch ${batchIndex + 1}:`,
          batchError
        );
        throw new Error(
          `Failed to process embeddings batch: ${
            batchError instanceof Error
              ? batchError.message
              : String(batchError)
          }`
        );
      }

      // Create vectors from generated embeddings
      for (let i = 0; i < batch.length; i++) {
        const chunk = batch[i];
        const embedding = batchEmbeddings[i];
        const msg = chunk.original;

        embeddingsProcessed++;

        try {
          // Create a unique ID for the vector - if it's part of a split message, add the part number
          const vectorId = chunk.part
            ? `${msg.id || `msg_${Date.now()}`}_part${chunk.part}`
            : msg.id || `msg_${Date.now()}_${embeddingsProcessed}`;

          // Add the vector to the array
          vectors.push({
            id: vectorId,
            values: embedding,
            metadata: {
              // Original ChatGPT fields
              role: msg.role,
              content: chunk.content,
              timestamp: msg.timestamp || 0,
              session_title: msg.session_title || "",
              session_create_time: msg.session_create_time || 0,
              session_update_time: msg.session_update_time || 0,
              imported_from: "chatgpt",
              imported_at: Date.now(),
              messageId: msg.id || vectorId, // Ensure it always has a valid messageId

              // source field for compatibility with the transcription system
              source: msg.role,
              // Chunking metadata for split messages
              ...(chunk.part
                ? {
                    chunking_part: chunk.part,
                    chunking_total_parts: chunk.totalParts || 1,
                    chunking_is_partial: true,
                  }
                : {}),
            },
          });
        } catch (error) {
          this.logger.error(
            `Error processing chunk ${embeddingsProcessed}/${allMessageChunks.length}:`,
            error
          );
        }
      }

      // Update progress after each batch
      processedTotal += batch.length;
      if (this.progressReporter) {
        this.progressReporter.updateProgress(
          "generating_embeddings",
          processedTotal,
          allMessageChunks.length
        );
      }

      // Small pause to avoid API throttling
      if (batchIndex < batches.length - 1) {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }

    // Finalize the progress of generating embeddings
    if (this.progressReporter) {
      this.progressReporter.completeStage(
        "generating_embeddings",
        allMessageChunks.length
      );
    }

    this.logger.info(`Generated ${vectors.length} vectors with embeddings`);
    return vectors;
  }

  private updateModeAndService(newMode: OrchOSModeEnum): void {
    const oldMode = this.isBasicMode;
    this.isBasicMode = newMode === OrchOSModeEnum.BASIC;

    this.logger.info(`üîÑ [EmbeddingService] === MODE UPDATE DEBUG ===`);
    this.logger.info(`üîÑ [EmbeddingService] Previous isBasicMode: ${oldMode}`);
    this.logger.info(`üîÑ [EmbeddingService] New mode from event: "${newMode}"`);
    this.logger.info(
      `üîÑ [EmbeddingService] New isBasicMode: ${this.isBasicMode}`
    );
    this.logger.info(
      `üîÑ [EmbeddingService] Mode actually changed: ${
        oldMode !== this.isBasicMode
      }`
    );
    this.logger.info(
      `üîÑ [EmbeddingService] Final selected mode: ${
        this.isBasicMode ? "BASIC (HuggingFace)" : "ADVANCED (Ollama)"
      }`
    );

    if (oldMode !== this.isBasicMode) {
      this.logger.info(
        `üîÑ [EmbeddingService] Creating new embedding service...`
      );
      this.embeddingService = this.createEmbeddingService();
      this.logger.info(
        `üîÑ [EmbeddingService] New embedding service created successfully`
      );
    } else {
      this.logger.info(
        `üîÑ [EmbeddingService] Mode unchanged, keeping existing service`
      );
    }

    this.logger.info(`üîÑ [EmbeddingService] === END MODE UPDATE DEBUG ===`);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ProcessedMessage, ChatGPTSession, ChatGPTMessageItem } from '../../interfaces/types';
import { ProgressReporter } from '../../utils/progressReporter';
import { Logger } from '../../utils/logging';

/**
 * Service responsible for parsing and converting ChatGPT files
 */
export class ChatGPTParser {
  private progressReporter: ProgressReporter;
  private logger: Logger;

  constructor(progressReporter: ProgressReporter, logger?: Logger) {
    this.progressReporter = progressReporter;
    this.logger = logger || new Logger('[ChatGPTParser]');
  }

  /**
   * Converts the raw buffer of the ChatGPT file to JSON
   */
  public parseBuffer(buffer: Buffer | ArrayBuffer | ArrayBufferView): ChatGPTSession[] {
    if (!buffer) throw new Error('No file provided');
    
    // LOG: Type of buffer received
    this.logger.debug(`Type of fileBuffer: ${buffer.constructor.name}`);
    
    // Ensure buffer is a Buffer
    let processedBuffer: Buffer;
    if (buffer instanceof Buffer) {
      processedBuffer = buffer;
    } else if (buffer instanceof ArrayBuffer) {
      processedBuffer = Buffer.from(new Uint8Array(buffer));
    } else if (ArrayBuffer.isView(buffer)) {
      processedBuffer = Buffer.from(new Uint8Array(buffer.buffer));
    } else {
      throw new Error('File type not supported for import');
    }
    
    // LOG: First bytes of the buffer for debug
    this.logger.debug(`First bytes of the buffer: ${processedBuffer.toString().substring(0, 20)}`);
    
    try {
      // Parse JSON
      const jsonString = processedBuffer.toString('utf-8');
      this.logger.debug(`JSON string length: ${jsonString.length} characters`);
      if (jsonString.length === 0) {
        throw new Error('Empty or invalid file');
      }
      return JSON.parse(jsonString);
    } catch (error) {
      this.logger.error('Error converting buffer to JSON', error);
      throw new Error('Invalid or corrupted file: ' + (error instanceof Error ? error.message : String(error)));
    }
  }

  /**
   * Extracts messages from ChatGPT data
   */
  public extractMessages(sessions: ChatGPTSession[]): ProcessedMessage[] {
    const allMessages: ProcessedMessage[] = [];
    
    // Calculate total messages for progress report
    const totalMessages = sessions.reduce((acc: number, session: ChatGPTSession) => {
      return acc + Object.values(session.mapping || {}).length;
    }, 0);
    
    // Start progress report
    this.progressReporter.startStage('parsing', totalMessages);
    this.logger.info(`Starting extraction of ${totalMessages} messages...`);
    
    let messagesProcessed = 0;
    
    // Process each session and extract messages
    for (const session of sessions) {
      if (session.mapping) {
        for (const item of Object.values(session.mapping) as ChatGPTMessageItem[]) {
          messagesProcessed++;
          
          // Update progress every 10 messages
          if (messagesProcessed % 10 === 0) {
            this.progressReporter.updateProgress('parsing', messagesProcessed, totalMessages);
          }
          
          const msg = item.message;
          if (!msg) continue;
          
          const role = msg.author?.role || 'unknown';
          const content = Array.isArray(msg.content?.parts) ? msg.content.parts[0] : msg.content?.parts;
          
          if (!content || typeof content !== 'string' || !content.trim()) continue;
          
          allMessages.push({
            role,
            content: content.trim(),
            timestamp: msg.create_time || null,
            id: msg.id || null,
            parent: msg.parent || null,
            session_title: session.title || null,
            session_create_time: session.create_time || null,
            session_update_time: session.update_time || null
          });
        }
      }
    }
    
    // Finish progress report
    this.progressReporter.completeStage('parsing', totalMessages);
    this.logger.info(`Extracted ${allMessages.length} valid messages from ${totalMessages} items`);
    
    return allMessages;
  }

  /**
   * Generates IDs for messages that do not have them
   */
  public ensureMessageIds(messages: ProcessedMessage[]): ProcessedMessage[] {
    const messagesWithoutIds = messages.filter((m: ProcessedMessage) => !m.id).length;
    
    if (messagesWithoutIds > 0) {
      this.logger.warn(`${messagesWithoutIds} messages do not have IDs. Generating IDs for them...`);
      
      return messages.map((msg: ProcessedMessage) => {
        if (!msg.id) {
          // Generate an ID based on content and timestamp
          const timestamp = Date.now();
          const contentSlice = msg.content.substring(0, 10).replace(/\s+/g, '_');
          const newId = `gen_msg_${timestamp}_${contentSlice}`;
          this.logger.debug(`Generated ID for message: ${newId}`);
          return { ...msg, id: newId };
        }
        return msg;
      });
    }
    
    return messages;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { PineconeVector } from "../../interfaces/types";
import { Logger } from "../../utils/logging";
import { ProgressReporter } from "../../utils/progressReporter";

/**
 * Interface for vector storage (DuckDB only)
 */
interface IVectorHelper {
  deleteAllUserVectors(): Promise<void>;
  saveToDuckDB?(
    vectors: PineconeVector[]
  ): Promise<{ success: boolean; error?: string }>;
}

/**
 * Service for storing vectors using DuckDB for both Basic and Advanced modes
 */
export class VectorStorageService {
  private vectorHelper: IVectorHelper;
  private progressReporter: ProgressReporter;
  private logger: Logger;
  private isBasicMode: boolean;

  constructor(
    vectorHelper: IVectorHelper,
    progressReporter: ProgressReporter,
    logger?: Logger,
    isBasicMode?: boolean
  ) {
    this.vectorHelper = vectorHelper;
    this.progressReporter = progressReporter;
    this.logger = logger || new Logger("[VectorStorageService]");
    this.isBasicMode = isBasicMode || false;
  }

  /**
   * Clears all existing data in overwrite mode
   */
  public async deleteExistingData(): Promise<void> {
    this.logger.info(
      "OVERWRITE mode selected, clearing ALL existing primary user data..."
    );

    if (this.vectorHelper.deleteAllUserVectors) {
      await this.vectorHelper.deleteAllUserVectors();
      this.logger.success(
        "All existing primary user data cleared successfully in OVERWRITE mode"
      );
    } else {
      this.logger.warn(
        "deleteAllUserVectors method not available in vectorHelper"
      );
    }
  }

  /**
   * Saves vectors using DuckDB storage (for both Basic and Advanced modes)
   */
  public async saveVectors(
    vectors: PineconeVector[]
  ): Promise<{ success: boolean; error?: string }> {
    if (!vectors || vectors.length === 0) {
      this.logger.warn("No vectors to save");
      return { success: true };
    }

    const storageType = "DuckDB"; // Always use DuckDB
    this.logger.info(`Saving ${vectors.length} vectors to ${storageType}`);

    try {
      // Divide vectors into batches for better visual feedback and performance
      const BATCH_SIZE = 500; // Ideal batch size for both services
      const batches: PineconeVector[][] = [];

      // Divide into batches
      for (let i = 0; i < vectors.length; i += BATCH_SIZE) {
        batches.push(vectors.slice(i, i + BATCH_SIZE));
      }

      this.logger.info(
        `Saving ${vectors.length} vectors in ${batches.length} batches of up to ${BATCH_SIZE} vectors to ${storageType}`
      );

      let success = true;
      let error = "";
      let totalProcessed = 0;

      // Save by batches for better progress feedback
      for (let i = 0; i < batches.length; i++) {
        const batch = batches[i];
        this.logger.info(
          `Processing batch ${i + 1}/${batches.length} with ${
            batch.length
          } vectors`
        );

        try {
          let batchResult: { success: boolean; error?: string };

          // Always use DuckDB for both Basic and Advanced modes
          if (this.vectorHelper.saveToDuckDB) {
            batchResult = await this.vectorHelper.saveToDuckDB(batch);
          } else {
            throw new Error("DuckDB save method not available");
          }

          if (!batchResult.success) {
            success = false;
            error = batchResult.error || "Unknown error in batch " + (i + 1);
            this.logger.error(`Error in batch ${i + 1}: ${error}`);
          }

          // Update progress
          totalProcessed += batch.length;
          this.progressReporter.updateProgress(
            "saving",
            totalProcessed,
            vectors.length
          );
        } catch (batchError) {
          success = false;
          error =
            batchError instanceof Error ? batchError.message : "Unknown error";
          this.logger.error(`Error in batch ${i + 1}:`, batchError);
        }

        // Small pause to avoid overloading storage service
        if (i < batches.length - 1) {
          await new Promise((resolve) => setTimeout(resolve, 100));
        }
      }

      // Result final
      if (success) {
        this.logger.success(
          `Save operation completed successfully using ${storageType}!`
        );
      } else {
        this.logger.error(`Error in saving to ${storageType}: ${error}`);
      }

      return { success, error: error || undefined };
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : "Unknown error";
      this.logger.error(`Error saving vectors to ${storageType}:`, error);
      return { success: false, error: errorMsg };
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { countTokens } from '../../../../components/context/deepgram/services/memory/utils/tokenUtils';
import { MessageChunk, ProcessedMessage } from '../../interfaces/types';
import { Logger } from '../../utils/logging';

/**
 * Service responsible for dividing texts into smaller chunks for efficient processing
 */
export class TextChunker {
  private readonly maxTokensPerChunk: number;
  private readonly maxTokensPerBatch: number;
  private readonly logger: Logger;

  constructor(maxTokensPerChunk: number = 1000, maxTokensPerBatch: number = 8000) {
    this.maxTokensPerChunk = maxTokensPerChunk;
    this.maxTokensPerBatch = maxTokensPerBatch;
    this.logger = new Logger('[TextChunker]');
  }

  /**
   * Process messages into chunks for better processing
   */
  public processMessagesIntoChunks(messages: ProcessedMessage[]): MessageChunk[] {
    this.logger.info(`Analyzing and processing ${messages.length} messages for optimization...`);
    
    const allMessageChunks: MessageChunk[] = [];
    
    for (const message of messages) {
      // For small content, we don't need to divide
      if (countTokens(message.content) <= this.maxTokensPerChunk) {
        allMessageChunks.push({
          original: message,
          content: message.content
        });
        continue;
      }
      
      // For large content, we divide into semantically coherent chunks
      const chunks = this.splitIntoChunks(message.content);
      if (chunks.length === 1) {
        // If it didn't divide (rare case), we add it as is
        allMessageChunks.push({
          original: message,
          content: chunks[0]
        });
      } else {
        // If it was divided, we add each part with metadata
        chunks.forEach((chunkContent, index) => {
          allMessageChunks.push({
            original: message,
            content: chunkContent,
            part: index + 1,
            totalParts: chunks.length
          });
        });
      }
    }
    
    this.logger.info(`After processing: ${allMessageChunks.length} chunks created from ${messages.length} original messages`);
    return allMessageChunks;
  }

  /**
   * Splits text into semantically coherent chunks
   */
  public splitIntoChunks(text: string, maxTokens: number = this.maxTokensPerChunk): string[] {
    if (!text) return [];
    
    // If the text is already small enough, we don't need to divide
    const totalTokens = countTokens(text);
    if (totalTokens <= maxTokens) return [text];
    
    // Identify possible break points (paragraphs, sentences, etc.)
    const paragraphs = text.split(/\n\s*\n/).filter(p => p.trim().length > 0);
    
    // If we have paragraphs, we try to use them as units of division
    if (paragraphs.length > 1) {
      return this.splitByParagraphs(paragraphs, maxTokens);
    }
    
    // If it's a single large paragraph, try to divide by sentences
    return this.splitBySentences(text, maxTokens);
  }

  private splitByParagraphs(paragraphs: string[], maxTokens: number): string[] {
    const chunks: string[] = [];
    let currentChunk = "";
    let currentChunkTokens = 0;
    
    for (const paragraph of paragraphs) {
      const paragraphTokens = countTokens(paragraph);
      
      // If a single paragraph is too large, we need to divide it by sentences
      if (paragraphTokens > maxTokens) {
        // First add the current chunk if not empty
        if (currentChunkTokens > 0) {
          chunks.push(currentChunk);
          currentChunk = "";
          currentChunkTokens = 0;
        }
        
        // Divide the large paragraph into sentences and create new chunks
        const sentenceChunks = this.splitBySentences(paragraph, maxTokens);
        chunks.push(...sentenceChunks);
      } 
      // If adding this paragraph would exceed the limit, start a new chunk
      else if (currentChunkTokens + paragraphTokens > maxTokens) {
        chunks.push(currentChunk);
        currentChunk = paragraph;
        currentChunkTokens = paragraphTokens;
      } 
      // Add the paragraph to the current chunk
      else {
        if (currentChunkTokens > 0) {
          currentChunk += "\n\n" + paragraph;
        } else {
          currentChunk = paragraph;
        }
        currentChunkTokens += paragraphTokens;
      }
    }
    
    // Add the last chunk if not empty
    if (currentChunkTokens > 0) {
      chunks.push(currentChunk);
    }
    
    return chunks;
  }

  private splitBySentences(text: string, maxTokens: number): string[] {
    return this.splitByDelimiter(text, '. ', maxTokens);
  }

  private splitByDelimiter(text: string, delimiter: string, maxTokens: number): string[] {
    if (countTokens(text) <= maxTokens) return [text];
    
    const parts = text.split(delimiter);
    const chunks: string[] = [];
    let currentChunk = "";
    let currentChunkTokens = 0;
    
    for (const part of parts) {
      const partWithDelimiter = part + (delimiter || "");
      const partTokens = countTokens(partWithDelimiter);
      
      // If a single part is larger than the maximum, we need to divide it further
      if (partTokens > maxTokens) {
        // First add the current chunk if not empty
        if (currentChunk) {
          chunks.push(currentChunk);
          currentChunk = "";
          currentChunkTokens = 0;
        }
        
        // Divide using the last resort method
        const subChunks = this.forceSplitBySize(partWithDelimiter, maxTokens);
        chunks.push(...subChunks);
        continue;
      }
      
      // If adding this part would exceed the token limit, start a new chunk
      if (currentChunkTokens + partTokens > maxTokens) {
        // Save the current chunk and start a new one
        chunks.push(currentChunk);
        currentChunk = partWithDelimiter;
        currentChunkTokens = partTokens;
      } else {
        // Add the part to the current chunk
        currentChunk += partWithDelimiter;
        currentChunkTokens += partTokens;
      }
    }
    
    // Add the last chunk if not empty
    if (currentChunk) {
      chunks.push(currentChunk);
    }
    
    return chunks;
  }

  private forceSplitBySize(text: string, maxTokens: number): string[] {
    const tokens = countTokens(text);
    if (tokens <= maxTokens) return [text];
    
    // Estimate characters per token (approximately)
    const charsPerToken = text.length / tokens;
    const charsPerChunk = Math.floor(maxTokens * charsPerToken) * 0.9; // 10% margin
    
    const chunks: string[] = [];
    let startChar = 0;
    
    while (startChar < text.length) {
      let endChar = Math.min(startChar + charsPerChunk, text.length);
      
      // Try to find a space to make the break cleaner
      if (endChar < text.length) {
        const nextSpace = text.indexOf(' ', endChar - 20);
        if (nextSpace > 0 && nextSpace < endChar + 20) {
          endChar = nextSpace;
        }
      }
      
      const chunk = text.substring(startChar, endChar).trim();
      if (chunk.length > 0) {
        chunks.push(chunk);
      }
      
      startChar = endChar;
    }
    
    return chunks;
  }

  /**
   * Creates batches of chunks for efficient processing
   */
  public createProcessingBatches(messageChunks: MessageChunk[]): MessageChunk[][] {
    const batches: MessageChunk[][] = [];
    let currentBatch: MessageChunk[] = [];
    let currentBatchTokens = 0;
    
    for (const chunk of messageChunks) {
      const chunkTokens = countTokens(chunk.content);
      
      // If adding this chunk would exceed the token limit, start a new batch
      if (currentBatchTokens + chunkTokens > this.maxTokensPerBatch) {
        batches.push([...currentBatch]);
        currentBatch = [];
        currentBatchTokens = 0;
      }
      
      // Add the chunk to the current batch
      currentBatch.push(chunk);
      currentBatchTokens += chunkTokens;
    }
    
    // Add the last batch if not empty
    if (currentBatch.length > 0) {
      batches.push(currentBatch);
    }
    
    this.logger.info(`Processing ${messageChunks.length} chunks in ${batches.length} batches (max ${this.maxTokensPerBatch} tokens per batch)`);
    return batches;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Utility class for standardizing log messages
 */
export class Logger {
  private prefix: string;

  constructor(prefix: string) {
    this.prefix = prefix;
  }

  /**
   * Log information
   */
  public info(message: string): void {
    console.log(`${this.prefix} ${message}`);
  }

  /**
   * Log warning
   */
  public warn(message: string): void {
    console.warn(`${this.prefix} ‚ö†Ô∏è ${message}`);
  }

  /**
   * Log error
   */
  public error(message: string, error?: unknown): void {
    if (error) {
      console.error(`${this.prefix} ‚ùå ${message}`, error);
    } else {
      console.error(`${this.prefix} ‚ùå ${message}`);
    }
  }

  /**
   * Log debug (only in development environment)
   */
  public debug(message: string, data?: unknown): void {
    if (process.env.NODE_ENV !== 'production') {
      if (data) {
        console.log(`${this.prefix} üîç DEBUG - ${message}`, data);
      } else {
        console.log(`${this.prefix} üîç DEBUG - ${message}`);
      }
    }
  }

  /**
   * Log success
   */
  public success(message: string): void {
    console.log(`${this.prefix} ‚úÖ ${message}`);
  }

  /**
   * Log stage
   */
  public stage(stage: string, details: string): void {
    console.log(`${this.prefix} [${stage}] ${details}`);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ProgressInfo } from '../interfaces/types';
import { Logger } from './logging';

/**
 * Class responsible for reporting the progress of operations
 */
export class ProgressReporter {
  private onProgress?: (info: ProgressInfo) => void;
  private logger: Logger;
  private lastUpdate: number = 0;
  private throttleInterval: number = 100; // Minimum ms between updates
  private lastPercentage: number = -1; // Track last percentage to avoid duplicate updates
  private queuedUpdate: NodeJS.Timeout | null = null;

  constructor(onProgress?: (info: ProgressInfo) => void, logger?: Logger) {
    this.onProgress = onProgress;
    this.logger = logger || new Logger('[ProgressReporter]');
  }

  /**
   * Starts a new processing stage
   */
  public startStage(stage: 'parsing' | 'deduplicating' | 'generating_embeddings' | 'saving', total: number): void {
    this.lastPercentage = -1; // Reset percentage tracker on new stage
    this.updateProgress(stage, 0, total);
  }

  /**
   * Updates the progress of a stage with throttling and error handling
   */
  public updateProgress(
    stage: 'parsing' | 'deduplicating' | 'generating_embeddings' | 'saving', 
    processed: number, 
    total: number
  ): void {
    if (!this.onProgress) return;
    
    const now = Date.now();
    const percentage = Math.round((processed / Math.max(total, 1)) * 100);
    
    // Skip if same percentage and not the final update (100%)
    if (percentage === this.lastPercentage && percentage !== 100 && processed !== total) {
      return;
    }
    
    // If we recently updated and this isn't a completion update, throttle it
    if (now - this.lastUpdate < this.throttleInterval && percentage !== 100 && processed !== total) {
      // Clear any existing queued update
      if (this.queuedUpdate) {
        clearTimeout(this.queuedUpdate);
      }
      
      // Queue this update to run after the throttle interval
      this.queuedUpdate = setTimeout(() => {
        this.lastUpdate = Date.now();
        this.lastPercentage = percentage;
        this.safelyCallProgressCallback({
          processed,
          total,
          percentage,
          stage
        });
        this.queuedUpdate = null;
      }, this.throttleInterval - (now - this.lastUpdate));
      return;
    }
    
    // Otherwise, update immediately
    this.lastUpdate = now;
    this.lastPercentage = percentage;
    this.safelyCallProgressCallback({
      processed,
      total,
      percentage,
      stage
    });
  }

  /**
   * Safely calls the progress callback with error handling
   */
  private safelyCallProgressCallback(info: ProgressInfo): void {
    if (!this.onProgress) return;
    
    try {
      this.onProgress(info);
    } catch (error) {
      // Log the error but don't let it crash the process
      this.logger.warn(`Error sending progress update: ${error}`);
      // If we've had one error, don't keep trying to call the callback
      if (error instanceof Error && error.message.includes('Object has been destroyed')) {
        this.logger.warn('WebContents has been destroyed, disabling progress updates');
        this.onProgress = undefined; // Stop trying to call the callback
      }
    }
  }

  /**
   * Completes a processing stage
   */
  public completeStage(stage: 'parsing' | 'deduplicating' | 'generating_embeddings' | 'saving', total: number): void {
    // Clear any queued updates
    if (this.queuedUpdate) {
      clearTimeout(this.queuedUpdate);
      this.queuedUpdate = null;
    }
    
    // Force completion update
    this.lastPercentage = 100;
    this.updateProgress(stage, total, total);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Export the main module for importing ChatGPT history
 * SOLID implementation with separated services and well-defined responsibilities
 */

// Export the main handler
export { importChatGPTHistoryHandler } from './handlers/importChatGPTHandler';

// Export public types and interfaces
export type { 
  ImportChatGPTParams, 
  ImportResult,
  ProgressInfo 
} from './interfaces/types';
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Adapted to simulate the real import handler

jest.mock('../config/UserConfig', () => ({
  getPrimaryUser: () => 'testUser',
}));

// Interfaces for type checking in the test
interface MessageItem {
  message?: {
    id?: string;
    author?: { role?: string };
    content?: { parts?: string[] };
    create_time?: number;
    parent?: string;
  };
}

interface ImportParams {
  fileBuffer: Buffer;
  mode: 'increment' | 'overwrite';
  user: string;
}

interface PineconeHelperDeps {
  pineconeHelper: {
    saveToPinecone: jest.Mock;
    checkExistingIds?: jest.Mock;
    deleteAllUserVectors?: jest.Mock;
  };
}

describe('Integration IPC import-chatgpt-history', () => {
  let pineconeHelper: {
    saveToPinecone: jest.Mock;
    checkExistingIds?: jest.Mock;
    deleteAllUserVectors?: jest.Mock;
  };

  beforeEach(() => {
    // Mock PineconeHelper for deduplication by id
    pineconeHelper = {
      saveToPinecone: jest.fn().mockResolvedValue({ success: true }),
      checkExistingIds: jest.fn().mockImplementation((_user, ids) => {
        console.log(`Verifying ${ids.length} IDs in the test`);
        return Promise.resolve(['id1', 'id2']); 
      }),
      deleteAllUserVectors: jest.fn().mockResolvedValue({ success: true })
    };
  });

  it('imports only new messages in incremental mode', async () => {
    // Simulate file buffer in the real ChatGPT export format
    const mapping = {
      node1: { message: { id: 'id1', author: { role: 'user' }, content: { parts: ['msg1'] }, create_time: 1 } },
      node2: { message: { id: 'id2', author: { role: 'assistant' }, content: { parts: ['msg2'] }, create_time: 2 } },
      node3: { message: { id: 'id3', author: { role: 'user' }, content: { parts: ['msg3'] }, create_time: 3 } },
      node4: { message: { id: 'id4', author: { role: 'assistant' }, content: { parts: ['msg4'] }, create_time: 4 } }
    };
    const fileBuffer = Buffer.from(JSON.stringify([
      { mapping, title: 'sess√£o', create_time: 0, update_time: 10 }
    ]));

    // Simulate the real handler (adapted from ipcHandlers.ts)
    async function importChatGPTTestHandler(
      { fileBuffer, mode, user }: ImportParams, 
      _event: unknown, 
      deps: PineconeHelperDeps
    ) {
      const raw = JSON.parse(fileBuffer.toString('utf-8'));
      let allMessages = [];
      for (const session of raw) {
        const mapping = session.mapping || {};
        for (const item of Object.values(mapping) as MessageItem[]) {
          const msg = item.message;
          if (!msg) continue;
          const role = msg.author?.role || 'unknown';
          const content = Array.isArray(msg.content?.parts) ? msg.content.parts[0] : msg.content?.parts;
          if (!content || typeof content !== 'string' || !content.trim()) continue;
          allMessages.push({
            role,
            content: content.trim(),
            timestamp: msg.create_time || null,
            id: msg.id || null,
            parent: msg.parent || null,
            session_title: session.title || null,
            session_create_time: session.create_time || null,
            session_update_time: session.update_time || null
          });
        }
      }
      let existingMessageIds = new Set();
      if (mode === 'increment') {
        // Extract message IDs for verification
        const messageIdsToCheck = allMessages
          .filter(msg => msg.id) 
          .map(msg => msg.id as string);
          
        if (deps.pineconeHelper.checkExistingIds && messageIdsToCheck.length > 0) {
          const existingIds = await deps.pineconeHelper.checkExistingIds(user, messageIdsToCheck);
          existingMessageIds = new Set(existingIds);
        }
        allMessages = allMessages.filter(m => m.id && !existingMessageIds.has(m.id));
      }
      if (mode === 'overwrite') {
        if (deps.pineconeHelper.deleteAllUserVectors) {
          await deps.pineconeHelper.deleteAllUserVectors(user);
        }
      }
      // Mock embedding e save
      const vectors = allMessages.map(msg => ({
        id: msg.id,
        values: [0],
        metadata: { ...msg, user, imported_from: 'chatgpt', imported_at: Date.now(), messageId: msg.id || null }
      }));
      if (vectors.length > 0) {
        await deps.pineconeHelper.saveToPinecone(vectors);
      }
      return { success: true, imported: vectors.length, skipped: 4 - vectors.length };
    }

    const result = await importChatGPTTestHandler({
      fileBuffer,
      mode: 'increment',
      user: 'testUser'
    }, undefined, { pineconeHelper: pineconeHelper });
    expect(result.imported).toBe(2); 
    expect(result.skipped).toBe(2);  
  });

  it('imports all messages in overwrite mode', async () => {
    const mapping = {
      node1: { message: { id: 'id1', author: { role: 'user' }, content: { parts: ['msg1'] }, create_time: 1 } },
      node2: { message: { id: 'id2', author: { role: 'assistant' }, content: { parts: ['msg2'] }, create_time: 2 } },
      node3: { message: { id: 'id3', author: { role: 'user' }, content: { parts: ['msg3'] }, create_time: 3 } },
      node4: { message: { id: 'id4', author: { role: 'assistant' }, content: { parts: ['msg4'] }, create_time: 4 } }
    };
    const fileBuffer = Buffer.from(JSON.stringify([
      { mapping, title: 'sess√£o', create_time: 0, update_time: 10 }
    ]));
    // Reuses the same test function
    async function importChatGPTTestHandler(
      { fileBuffer, mode, user }: ImportParams, 
      _event: unknown, 
      deps: PineconeHelperDeps
    ) {
      const raw = JSON.parse(fileBuffer.toString('utf-8'));
      let allMessages = [];
      for (const session of raw) {
        const mapping = session.mapping || {};
        for (const item of Object.values(mapping) as MessageItem[]) {
          const msg = item.message;
          if (!msg) continue;
          const role = msg.author?.role || 'unknown';
          const content = Array.isArray(msg.content?.parts) ? msg.content.parts[0] : msg.content?.parts;
          if (!content || typeof content !== 'string' || !content.trim()) continue;
          allMessages.push({
            role,
            content: content.trim(),
            timestamp: msg.create_time || null,
            id: msg.id || null,
            parent: msg.parent || null,
            session_title: session.title || null,
            session_create_time: session.create_time || null,
            session_update_time: session.update_time || null
          });
        }
      }
      let existingMessageIds = new Set();
      if (mode === 'increment') {
        // Extract message IDs for verification
        const messageIdsToCheck = allMessages
          .filter(msg => msg.id) 
          .map(msg => msg.id as string);
          
        if (deps.pineconeHelper.checkExistingIds && messageIdsToCheck.length > 0) {
          const existingIds = await deps.pineconeHelper.checkExistingIds(user, messageIdsToCheck);
          existingMessageIds = new Set(existingIds);
        }
        allMessages = allMessages.filter(m => m.id && !existingMessageIds.has(m.id));
      }
      if (mode === 'overwrite') {
        if (deps.pineconeHelper.deleteAllUserVectors) {
          await deps.pineconeHelper.deleteAllUserVectors(user);
        }
      }
      // Mock embedding e save
      const vectors = allMessages.map(msg => ({
        id: msg.id,
        values: [0],
        metadata: { ...msg, user, imported_from: 'chatgpt', imported_at: Date.now(), messageId: msg.id || null }
      }));
      if (vectors.length > 0) {
        await deps.pineconeHelper.saveToPinecone(vectors);
      }
      return { success: true, imported: vectors.length, skipped: 4 - vectors.length };
    }
    const result = await importChatGPTTestHandler({
      fileBuffer,
      mode: 'overwrite',
      user: 'testUser'
    }, undefined, { pineconeHelper: pineconeHelper });
    expect(result.imported).toBe(4); // All
    expect(result.skipped).toBe(0);  // None skipped
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useEffect, useRef } from "react";
import {
  MicrophoneState,
  useDeepgram,
  useMicrophone,
} from "../../components/context";
import { ConnectionState } from "../../components/context/deepgram/interfaces/deepgram/IDeepgramService";
import TranscriptionPanel from "../../components/shared/TranscriptionPanel/TranscriptionPanel";

/**
 * Main transcription module that encapsulates all transcription-related functionality
 * Following Single Responsibility Principle by handling only transcription logic
 */
export const TranscriptionModule: React.FC = () => {
  // Get microphone hooks and state
  const {
    microphoneState,
    startMicrophone,
    stopMicrophone,
    isMicrophoneOn,
    isSystemAudioOn,
    setIsSystemAudioOn,
  } = useMicrophone();

  // Get deepgram services
  const {
    connectToDeepgram,
    disconnectFromDeepgram,
    connectionState,
    flushTranscriptionsToUI,
  } = useDeepgram();

  // Refs to maintain latest values in event handlers
  const microphoneStateRef = useRef(microphoneState);
  const isMicrophoneOnRef = useRef(isMicrophoneOn);
  const isSystemAudioOnRef = useRef(isSystemAudioOn);

  // Keep refs up to date
  useEffect(() => {
    microphoneStateRef.current = microphoneState;
  }, [microphoneState]);

  useEffect(() => {
    isMicrophoneOnRef.current = isMicrophoneOn;
  }, [isMicrophoneOn]);

  useEffect(() => {
    isSystemAudioOnRef.current = isSystemAudioOn;
  }, [isSystemAudioOn]);

  // Connect/disconnect Deepgram based on microphone state
  useEffect(() => {
    if (microphoneState === MicrophoneState.Open) {
      // Start transcription if microphone is open and not already connected
      if (
        connectionState !== ConnectionState.OPEN &&
        connectionState !== ConnectionState.CONNECTING
      ) {
        console.log(
          "üé§ Starting Deepgram connection due to microphone state change"
        );
        connectToDeepgram();
      }
    } else if (microphoneState === MicrophoneState.Stopped) {
      // Stop transcription when recording stops
      if (connectionState === ConnectionState.OPEN) {
        console.log(
          "üõë Stopping Deepgram connection due to microphone state change"
        );

        // Note: Transcriptions are already displayed in real-time
        // No need to flush on stop as they're already visible
        console.log("üõë Recording stopped, transcriptions already visible");

        disconnectFromDeepgram();
      }
    }
  }, [
    microphoneState,
    connectToDeepgram,
    disconnectFromDeepgram,
    connectionState,
    flushTranscriptionsToUI,
  ]);

  // Setup keyboard shortcut for recording toggle
  useEffect(() => {
    const unsubscribeToggleRecording = window.electronAPI.toogleNeuralRecording(
      () => {
        console.log("üîä Shortcut pressed! Toggling recording...");
        if (microphoneStateRef.current === MicrophoneState.Open) {
          console.log("üõë Stopping recording via shortcut...");
          stopMicrophone();
        } else {
          console.log("üé§ Starting recording via shortcut...");
          // If no audio source is active, activate system audio by default before recording
          if (!isMicrophoneOnRef.current && !isSystemAudioOnRef.current) {
            setIsSystemAudioOn(true);
            setTimeout(() => startMicrophone(), 100);
          } else {
            startMicrophone();
          }
        }
      }
    );

    return () => {
      unsubscribeToggleRecording();
    };
  }, []);

  return (
    <div style={{ height: "100%", width: "100%", overflow: "hidden" }}>
      <TranscriptionPanel
        onClose={() => {}} // No close functionality needed
        width="100%"
      />
    </div>
  );
};

export default TranscriptionModule;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// HuggingFaceNeuralSignalService.ts
// Symbolic: Neural signal extraction service using HuggingFace (cortex: huggingface)
import { NeuralSignalResponse } from "../../../components/context/deepgram/interfaces/neural/NeuralSignalTypes";
import { HuggingFaceServiceFacade } from "../../../components/context/deepgram/services/huggingface/HuggingFaceServiceFacade";
import { INeuralSignalService } from "../../../domain/core/neural/INeuralSignalService";
import { ISemanticEnricher } from "../../../domain/core/neural/ISemanticEnricher";
import { getOption, STORAGE_KEYS } from "../../../services/StorageService";
import {
  buildSystemPrompt,
  buildUserPrompt,
} from "../../../shared/utils/neuralPromptBuilder";
import {
  extractNeuralSignalJsons,
  parseNeuralSignal,
} from "../../../shared/utils/neuralSignalParser";
import { FunctionSchemaRegistry } from "../../../components/context/deepgram/services/function-calling/FunctionSchemaRegistry";

/**
 * Symbolic: HuggingFace implementation of neural signal service
 * This service extracts symbolic neural signals and performs semantic enrichment
 * using local HuggingFace models.
 */
export class HuggingFaceNeuralSignalService
  implements INeuralSignalService, ISemanticEnricher
{
  /**
   * Constructor with dependency injection for HuggingFaceServiceFacade
   */
  constructor(private huggingFaceClient: HuggingFaceServiceFacade) {}

  /**
   * Symbolic: Extracts neural signals using HuggingFace
   */
  async generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse> {
    try {
      const systemPromptContent = buildSystemPrompt();
      let userPromptContent = buildUserPrompt(
        prompt,
        temporaryContext,
        language
      );

      // For HuggingFace models without native function-calling, force JSON output
      userPromptContent += `\n\nIMPORTANT OUTPUT FORMAT:\nReturn ONLY a JSON array with objects following exactly this schema (no markdown, no extra text):\n[{\n  \"core\": \"area\",\n  \"query\": \"symbolic query\",\n  \"intensity\": 0.5,\n  \"keywords\": [\"k1\", \"k2\"],\n  \"topK\": 5,\n  \"filters\": { },\n  \"expand\": false,\n  \"symbolicInsights\": \"...\"\n}]`;

      const activateBrainAreaSchema =
        FunctionSchemaRegistry.getInstance().get("activateBrainArea");
      const tools = activateBrainAreaSchema
        ? [{ type: "function", function: activateBrainAreaSchema }]
        : [];

      const messages = [
        { role: "system" as const, content: systemPromptContent },
        { role: "user" as const, content: userPromptContent },
      ];
      const response = await this.huggingFaceClient.callOpenAIWithFunctions({
        model: getOption(STORAGE_KEYS.HF_MODEL) || "Xenova/llama2.c-stories15M",
        messages,
        tools,
        tool_choice: {
          type: "function",
          function: { name: "activateBrainArea" },
        },
        temperature: 0.2,
      });

      const toolCalls = response.choices?.[0]?.message?.tool_calls;
      let signals: NeuralSignalResponse["signals"] = [];
      if (toolCalls && Array.isArray(toolCalls)) {
        signals = toolCalls
          .filter((call: any) => call.function?.name === "activateBrainArea")
          .map((call: any): any => {
            try {
              const args = call.function?.arguments
                ? JSON.parse(call.function.arguments)
                : {};
              const baseSignal: Partial<any> = {
                core: args.core,
                intensity: Math.max(0, Math.min(1, args.intensity ?? 0.5)),
                symbolic_query: { query: args.query ?? "" },
              };
              if (Array.isArray(args.keywords))
                baseSignal.keywords = args.keywords;
              if (args.filters) baseSignal.filters = args.filters;
              if (typeof args.expand === "boolean")
                baseSignal.expand = args.expand;
              if (args.symbolicInsights)
                baseSignal.symbolicInsights = args.symbolicInsights;
              if (typeof args.topK !== "undefined") baseSignal.topK = args.topK;
              if (typeof baseSignal.core !== "undefined") return baseSignal;
              return undefined;
            } catch {
              return undefined;
            }
          })
          .filter(
            (signal: any): signal is any =>
              !!signal && typeof signal.core !== "undefined"
          );
      }

      // If unable to extract function calls, try to extract text-based signals
      if (signals.length === 0 && response.choices?.[0]?.message?.content) {
        // Parse neural signals using the utility parsers
        const extractedSignals = extractNeuralSignalJsons(
          response.choices?.[0]?.message?.content
        )
          .map(parseNeuralSignal)
          .filter(
            (signal): signal is NeuralSignalResponse["signals"][0] =>
              signal !== null
          );
        signals = extractedSignals;
      }

      return { signals };
    } catch (error) {
      // Log error and return empty signals for graceful degradation
      console.error("Neural signal extraction error:", error);
      return { signals: [] };
    }
  }

  /**
   * Symbolic: Semantic enrichment using HuggingFace
   */
  async enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }> {
    try {
      const enrichSchema = FunctionSchemaRegistry.getInstance().get(
        "enrichSemanticQuery"
      );
      const enrichmentTools = enrichSchema
        ? [{ type: "function", function: enrichSchema }]
        : [];

      const systemPrompt = `You are a semantic enrichment system. Expand queries with related terms while preserving intent. Generate 3-8 relevant keywords. Respond using enrichSemanticQuery function.`;

      let userPrompt = `CORE: ${core}\nINTENSITY: ${intensity}\nORIGINAL QUERY: ${query}`;
      if (context) userPrompt += `\nCONTEXT: ${context}`;
      if (language) userPrompt += `\nLANGUAGE: ${language}`;

      const messages = [
        { role: "system" as const, content: systemPrompt },
        { role: "user" as const, content: userPrompt },
      ];

      const response = await this.huggingFaceClient.callOpenAIWithFunctions({
        model: getOption(STORAGE_KEYS.HF_MODEL) || "sshleifer/tiny-gpt2",
        messages: messages,
        tools: enrichmentTools,
        tool_choice: {
          type: "function",
          function: { name: "enrichSemanticQuery" },
        },
        temperature: 0.2,
      });

      // Symbolic: Use central neural signal parser for all JSON arguments (tool_calls)
      const toolCalls = response.choices?.[0]?.message?.tool_calls;
      if (
        toolCalls &&
        Array.isArray(toolCalls) &&
        toolCalls[0]?.function?.arguments
      ) {
        const signal = parseNeuralSignal(toolCalls[0].function.arguments as string);
        if (signal && signal.symbolic_query?.query) {
          return {
            enrichedQuery: signal.symbolic_query.query,
            keywords: signal.keywords || [],
          };
        } else {
          return { enrichedQuery: query, keywords: [] };
        }
      }
      return { enrichedQuery: query, keywords: [] };
    } catch (error) {
      // Handle general service errors with original query fallback
      console.error("Semantic enrichment error:", error);
      return { enrichedQuery: query, keywords: [] };
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// src/lib/utils.ts

import { type ClassValue, clsx } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
// src/polyfills/sharpStub.js
// Minimal stub for the "sharp" native module so that bundlers/electron can resolve it
// without trying to include the native binary. Any runtime call will explicitly throw
// an error, signalling that image manipulation is not available in this environment.

function unsupported() {
  throw new Error(
    "The 'sharp' module is not available in this environment. If you need image processing, " +
      "run the code in a Node.js environment with the native sharp binary installed."
  );
}

const sharpProxy = new Proxy(unsupported, {
  get() {
    return unsupported;
  },
  apply() {
    return unsupported();
  },
});

export default sharpProxy;
module.exports = sharpProxy;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IEmbeddingService } from "../../components/context/deepgram/interfaces/openai/IEmbeddingService";
import { getOption, STORAGE_KEYS } from "../StorageService";

const ALLOWED_EMBEDDERS = [
  // Prefer lighter MiniLM model (384-d) for faster embeddings
  "Xenova/all-MiniLM-L6-v2"
] as const;
type AllowedEmbedderId = (typeof ALLOWED_EMBEDDERS)[number];

export class HuggingFaceEmbeddingService implements IEmbeddingService {
  private embedder: any = null;
  private modelId: AllowedEmbedderId | null = null;
  private initialized = false;

  constructor() {
    // Initialize environment using centralized configuration
    this.initializeEnvironment().catch((error) => {
      console.error("[HFE] Environment initialization failed:", error);
    });
  }

  /**
   * Initialize transformers.js environment using centralized configuration
   */
  private async initializeEnvironment() {
    if (this.initialized) return;

    try {
      this.initialized = true;
      console.log(
        "‚úÖ [HFE] Environment initialized using centralized configuration"
      );
    } catch (error) {
      console.error(
        "‚ùå [HFE] Failed to initialize transformers environment:",
        error
      );
      throw error;
    }
  }

  isInitialized(): boolean {
    return this.embedder !== null;
  }

  async initialize(config?: Record<string, any>): Promise<boolean> {
    await this.loadModel(
      getOption(STORAGE_KEYS.HF_EMBEDDING_MODEL) as AllowedEmbedderId
    );
    return true;
  }

  /** Carrega UM dos dois modelos de embedding permitidos */
  async loadModel(
    modelId: AllowedEmbedderId,
    device: "wasm" = "wasm"
  ) {
    // Ensure environment is initialized before loading models
    if (!this.initialized) {
      await this.initializeEnvironment();
    }

    if (!ALLOWED_EMBEDDERS.includes(modelId)) {
      throw new Error(`Embedder n√£o suportado: ${modelId}`);
    }
    if (this.modelId === modelId && this.embedder) return;

    // Use centralized model loading configuration
    const { loadModelWithOptimalConfig } = await import(
      "../../utils/transformersEnvironment"
    );
    this.embedder = await loadModelWithOptimalConfig(
      modelId,
      "feature-extraction",
      {
        device,
        dtype: "fp32", // Use fp32 for better compatibility with all models
      }
    );
    this.modelId = modelId;
  }

  /** Gera embedding de um texto */
  async createEmbedding(text: string): Promise<number[]> {
    const modelId = this.modelId ?? ALLOWED_EMBEDDERS[0];
    await this.loadModel(modelId);
    const output = await this.embedder(text, { pooling: "mean" });
    return Array.isArray(output) ? (output[0] as number[]) : [];
  }

  /** Gera embeddings em batch */
  async createEmbeddings(texts: string[]): Promise<number[][]> {
    const modelId = this.modelId ?? ALLOWED_EMBEDDERS[0];
    await this.loadModel(modelId);
    const outputs = await Promise.all(
      texts.map((t) => this.embedder(t, { pooling: "mean" }))
    );
    return outputs.map((o) => (Array.isArray(o) ? (o[0] as number[]) : []));
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { AutoTokenizer } from "@huggingface/transformers";
import { cleanThinkTags } from "../../components/context/deepgram/utils/ThinkTagCleaner";

// Flag to enable legacy chat_template fallback for models missing template
const ENABLE_FALLBACK_CHAT_TEMPLATE = false;

// modelos suportados no navegador (gera√ß√£o de texto)
export const SUPPORTED_HF_BROWSER_MODELS = [
  "Xenova/distilgpt2", // ~353MB, DistilGPT-2 otimizado - FUNCIONA
  "Xenova/gpt2", // ~548MB, GPT-2 base est√°vel - FUNCIONA
  "Xenova/llama2.c-stories15M", // ~15MB, modelo muito pequeno - FUNCIONA
  "Xenova/TinyLlama-1.1B-Chat-v1.0", // ~1.1B, modelo de chat pequeno - FUNCIONA
] as const;

export type HuggingFaceLocalOptions = {
  model?: (typeof SUPPORTED_HF_BROWSER_MODELS)[number];
  maxTokens?: number;
  temperature?: number;
  dtype?: "q4" | "q8" | "fp32" | "fp16"; // Ordered by performance vs. quality
  device?: "webgpu" | "wasm" | "auto";
  forceReload?: boolean; // Force reload model even if already loaded
};

export class HuggingFaceLocalService {
  private generator: any = null;
  private tokenizer: any = null;
  private currentModel: string | null = null;
  private initialized = false;
  private isLoading = false;

  constructor() {
    // Initialize environment asynchronously
    this.initializeEnvironment().catch((error) => {
      console.error("[HFS] Environment initialization failed:", error);
    });
  }

  /**
   * Initialize transformers.js environment using centralized configuration
   */
  private async initializeEnvironment() {
    if (this.initialized) return;

    try {
      console.log("[HFS] Initializing transformers.js environment...");

      this.initialized = true;
      console.log("‚úÖ [HFS] Environment initialized successfully");
    } catch (error) {
      console.error(
        "‚ùå [HFS] Failed to initialize transformers environment:",
        error
      );
      throw new Error(
        `Environment initialization failed: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    }
  }

  /**
   * Load model using centralized configuration with enhanced error handling
   */
  async loadModel(opts: {
    modelId: string;
    device: HuggingFaceLocalOptions["device"];
    dtype: HuggingFaceLocalOptions["dtype"];
    forceReload?: boolean;
  }) {
    const { modelId, forceReload } = opts;

    console.log(`[HFS] üîç DEBUG: loadModel called with original opts:`, opts);

    // FORCE correct configurations - ignore any incorrect device/dtype from opts
    const device = "wasm"; // Always use wasm for browser compatibility
    const dtype = "fp32"; // Always use fp32 - model-specific configs will override if needed

    console.log(`[HFS] üîç DEBUG: Forced device: ${device}, dtype: ${dtype}`);

    // Prevent concurrent loading
    if (this.isLoading) {
      console.log("[HFS] Model loading already in progress, waiting...");
      return;
    }

    // Check if model is already loaded
    if (this.currentModel === modelId && this.generator && !forceReload) {
      console.log(`[HFS] Model ${modelId} already loaded`);
      return;
    }

    // Validate model is supported
    if (!SUPPORTED_HF_BROWSER_MODELS.includes(modelId as any)) {
      throw new Error(
        `Unsupported model: ${modelId}. Supported models: ${SUPPORTED_HF_BROWSER_MODELS.join(
          ", "
        )}`
      );
    }

    // Ensure environment is initialized
    if (!this.initialized) {
      await this.initializeEnvironment();
    }

    this.isLoading = true;
    console.log(
      `[HFS] Loading model: ${modelId} with device: ${device}, dtype: ${dtype}`
    );

    try {
      // Clean up previous model if exists
      if (this.generator && forceReload) {
        await this.dispose();
      }

      // Use loadModelWithOptimalConfig from centralized configuration
      const { loadModelWithOptimalConfig } = await import(
        "../../utils/transformersEnvironment"
      );

      const additionalOptions = {
        // Enhanced progress callback for better user feedback
        progress_callback: (data: any) => {
          if (data.status === "downloading") {
            console.log(
              `[HFS] Downloading: ${data.name || data.file} - ${Math.round(
                data.progress || 0
              )}%`
            );
          } else if (data.status === "loading") {
            console.log(`[HFS] Loading: ${data.name || data.file}`);
          } else if (data.status === "ready") {
            console.log(`[HFS] Ready: ${data.name || data.file}`);
          }
        },

        // Enhanced session options for better performance
        session_options: {
          logSeverityLevel: 3, // Reduce logging noise
          graphOptimizationLevel: "all",
          enableMemPattern: true,
          enableCpuMemArena: true,
          // Always use wasm for browser compatibility
          executionProviders: ["wasm"],
        },

        // Cache configuration - allow downloads but prefer cache
        cache_dir: undefined, // Use environment default
        local_files_only: false, // Allow downloads if not in cache
        use_auth_token: false,

        // Retry configuration for network issues
        max_retries: 3,
        retry_delay: 1000, // 1 second delay between retries

        // IMPORTANT: Don't override device/dtype here - let model-specific configs take precedence
        // Only pass device/dtype if they are explicitly different from defaults
        // This allows model-specific configurations to take precedence
      };

      console.log(
        `[HFS] üîç DEBUG: additionalOptions being passed:`,
        additionalOptions
      );

      console.log(`[HFS] Loading with configuration:`, {
        modelId,
        device,
        dtype,
        cache_dir: "using environment default",
        local_files_only: additionalOptions.local_files_only,
      });

      this.generator = await loadModelWithOptimalConfig(
        modelId,
        "text-generation",
        additionalOptions
      );

      // Load tokenizer for chat template
      this.tokenizer = await AutoTokenizer.from_pretrained(modelId);

      // Add default chat template for models that don't have one
      // Fallback chat_template injection disabled for vLLM-only setup
      if (ENABLE_FALLBACK_CHAT_TEMPLATE && !this.tokenizer.chat_template) {
        console.log(`[HFS] Adding default chat template for ${modelId}`);

        // Define a simple but effective chat template
        this.tokenizer.chat_template = `{% for message in messages %}{% if message['role'] == 'system' %}System: {{ message['content'] }}
{% elif message['role'] == 'user' %}User: {{ message['content'] }}
{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}
{% endif %}{% endfor %}{% if add_generation_prompt %}Assistant: {% endif %}`;

        // Also ensure apply_chat_template method exists
        if (!this.tokenizer.apply_chat_template) {
          this.tokenizer.apply_chat_template = function (
            messages: any[],
            options: any = {}
          ) {
            let result = "";

            // Process messages
            for (const message of messages) {
              if (message.role === "system") {
                result += `System: ${message.content}\n`;
              } else if (message.role === "user") {
                result += `User: ${message.content}\n`;
              } else if (message.role === "assistant") {
                result += `Assistant: ${message.content}\n`;
              }
            }

            // Add generation prompt if requested
            if (options.add_generation_prompt) {
              result += "Assistant: ";
            }

            return result;
          };
        }
      }

      this.currentModel = modelId;
      console.log(
        `[HFS] ‚úÖ Model loaded successfully: ${modelId} (${device}/${dtype})`
      );
    } catch (error) {
      console.error(`[HFS] ‚ùå Failed to load model ${modelId}:`, error);

      // Enhanced error messages with specific guidance
      if (error instanceof Error) {
        if (error.message.includes("<!DOCTYPE")) {
          throw new Error(
            `Model loading failed: Server returned HTML instead of model files. ` +
              `This usually means:\n` +
              `1. The model "${modelId}" doesn't exist or isn't available\n` +
              `2. Network connectivity issues\n` +
              `3. HuggingFace server issues\n` +
              `Try a different model or check your internet connection.`
          );
        } else if (error.message.includes("CORS")) {
          throw new Error(
            `CORS error loading model ${modelId}. This may be due to:\n` +
              `1. Network configuration issues\n` +
              `2. Proxy server problems\n` +
              `3. Browser security settings\n` +
              `Try refreshing the application or check network settings.`
          );
        } else if (
          error.message.includes("fetch") ||
          error.message.includes("NetworkError")
        ) {
          throw new Error(
            `Network error loading model ${modelId}. Please:\n` +
              `1. Check your internet connection\n` +
              `2. Verify the model exists on HuggingFace\n` +
              `3. Try again in a few moments\n` +
              `4. Consider using a smaller model for testing`
          );
        } else if (
          error.message.includes("quota") ||
          error.message.includes("storage")
        ) {
          throw new Error(
            `Storage error loading model ${modelId}. This may be due to:\n` +
              `1. Insufficient disk space\n` +
              `2. Cache directory permissions\n` +
              `3. Storage quota exceeded\n` +
              `Try clearing cache or freeing up disk space.`
          );
        }
      }

      throw new Error(
        `Failed to load model "${modelId}": ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    } finally {
      this.isLoading = false;
    }
  }

  /**
   * Generate text using the loaded model.
   */
  async generate(
    messages: Array<{ role: "system" | "user" | "assistant"; content: string }>,
    opts: HuggingFaceLocalOptions = {}
  ): Promise<string> {
    if (!this.initialized || !this.generator) {
      throw new Error("Model not initialized. Call loadModel() first.");
    }

    try {
      // Build conversation prompt
      const prompt =
        messages
          .map(
            (m) =>
              `${m.role === "assistant" ? "Assistant" : "User"}: ${m.content}`
          )
          .join("\n\n") + "\n\nAssistant:";

      const generationOptions = {
        max_new_tokens: opts.maxTokens || 512,
        temperature: opts.temperature ?? 0.7,
        do_sample: (opts.temperature ?? 0.7) > 0,
      } as any;

      const result: any[] = await this.generator(prompt, generationOptions);

      let first = result[0];
      if (Array.isArray(first)) first = first[0];

      // Extract content depending on result structure
      let generatedTextRaw = first?.generated_text ?? first?.text ?? undefined;

      if (typeof generatedTextRaw === "undefined") {
        throw new Error("Invalid generation result format");
      }

      const content = Array.isArray(generatedTextRaw)
        ? generatedTextRaw.at(-1)?.content ?? JSON.stringify(generatedTextRaw)
        : String(generatedTextRaw);

      // Remove the original prompt from the response if it's included
      let responseText = content;
      if (responseText.includes(prompt)) {
        responseText = responseText.replace(prompt, "").trim();
      }

      // Clean think tags from the response
      const cleanedResponse = cleanThinkTags(responseText);

      return cleanedResponse;
    } catch (error) {
      console.error("[HFS] ‚ùå Generation error:", error);
      throw error;
    }
  }

  /**
   * Generate text with function calling support.
   */
  async generateWithFunctions(
    messages: Array<{ role: "system" | "user" | "assistant"; content: string }>,
    tools: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }> = [],
    opts: {
      temperature?: number;
      maxTokens?: number;
    } = {}
  ): Promise<{ content?: string; tool_calls?: Array<any> }> {
    if (!this.initialized || !this.generator) {
      throw new Error("Model not initialized. Call loadModel() first.");
    }

    try {
      // Function to extract tool calls from generated text
      const extractToolCalls = (text: string) => {
        const toolCalls: any[] = [];

        // Look for JSON-like function calls in the text
        const functionCallRegex =
          /\{"function":\s*\{"name":\s*"([^"]+)",\s*"arguments":\s*(\{[^}]*\})\}\}/g;
        let match;

        while ((match = functionCallRegex.exec(text)) !== null) {
          try {
            const functionName = match[1];
            const argumentsStr = match[2];
            const args = JSON.parse(argumentsStr);

            toolCalls.push({
              function: {
                name: functionName,
                arguments: args,
              },
            });
          } catch (e) {
            // Skip invalid JSON
            continue;
          }
        }

        // Alternative format: look for function calls in a more flexible way
        if (toolCalls.length === 0) {
          const altRegex = /(\w+)\s*\(\s*([^)]*)\s*\)/g;
          while ((match = altRegex.exec(text)) !== null) {
            const functionName = match[1];
            const argsStr = match[2];

            // Check if this function name matches any of our tools
            const matchingTool = tools.find(
              (t) => t.function.name === functionName
            );
            if (matchingTool) {
              try {
                // Try to parse arguments as JSON or create simple object
                let args = {};
                if (argsStr.trim()) {
                  if (argsStr.includes(":")) {
                    // Try to parse as JSON-like
                    args = JSON.parse(`{${argsStr}}`);
                  } else {
                    // Simple string argument
                    args = { value: argsStr.trim() };
                  }
                }

                toolCalls.push({
                  function: {
                    name: functionName,
                    arguments: args,
                  },
                });
              } catch (e) {
                // Skip invalid arguments
                continue;
              }
            }
          }
        }

        return toolCalls;
      };

      // Build chat prompt passing the tools array per HF docs, with fallback
      let prompt: string | undefined;
      let usedChatTemplate = false;

      // NOTA: Muitos modelos pequenos/comunit√°rios n√£o t√™m chat_template (ex: Xenova/llama2.c-stories15M)
      // Isso √© normal e usaremos um fallback manual nesses casos

      // Verifica√ß√£o inicial se temos um tokenizer com chat_template
      const rawTemplate = (this.tokenizer as any)?.chat_template;
      const hasChatTemplate =
        this.tokenizer &&
        typeof this.tokenizer.apply_chat_template === "function" &&
        typeof rawTemplate === "string" &&
        rawTemplate.trim().length > 0;

      if (!hasChatTemplate) {
        // Model doesn't have chat template - this is expected for many models
        // We'll use manual formatting without logging warnings
      } else {
        try {
          // Formato da chamada seguindo exatamente a documenta√ß√£o
          const templateOpts: any = {
            add_generation_prompt: true,
            tokenize: false, // Force string output
          };

          // Adicionar ferramentas apenas se existirem
          if (Array.isArray(tools) && tools.length > 0) {
            templateOpts.tools = tools;
          }

          let tmpPrompt2: any = this.tokenizer.apply_chat_template(
            messages as any,
            templateOpts
          );
          if (Array.isArray(tmpPrompt2)) {
            tmpPrompt2 = tmpPrompt2.join(" ");
          }
          prompt = String(tmpPrompt2);

          // Verifica√ß√£o extra de qualidade
          if (prompt && prompt.trim().length > 0) {
            usedChatTemplate = true;
            console.log("[HFS] Successfully used chat template");
          } else {
            // Chat template produced empty prompt - use fallback without warning
            prompt = undefined;
          }
        } catch (err) {
          // apply_chat_template failed - use fallback without warning
          // This is expected behavior for some models
          prompt = undefined;
        }
      }

      if (
        !prompt ||
        (typeof prompt === "string" && prompt.trim().length === 0)
      ) {
        // Use simple prompt with tool injection for models without chat template
        // This is the expected behavior, not an error

        // Construir uma representa√ß√£o simples das ferramentas dispon√≠veis para injetar no prompt
        let toolsPrompt = "";
        if (Array.isArray(tools) && tools.length > 0) {
          toolsPrompt = "\n\nAvailable tools:\n";
          tools.forEach((tool) => {
            if (tool.function) {
              toolsPrompt += `- ${tool.function.name}: ${
                tool.function.description || ""
              }\n`;

              // Adicionar informa√ß√µes sobre par√¢metros se dispon√≠veis
              if (
                tool.function.parameters &&
                typeof tool.function.parameters === "object"
              ) {
                if (tool.function.parameters.properties) {
                  toolsPrompt += "  Parameters:\n";
                  Object.entries(tool.function.parameters.properties).forEach(
                    ([paramName, paramDef]) => {
                      const paramInfo = paramDef as any;
                      toolsPrompt += `  - ${paramName}: ${
                        paramInfo.description || paramInfo.type || ""
                      }\n`;
                    }
                  );
                }
              }
            }
          });

          toolsPrompt +=
            '\nWhen you need to use a tool, use the format: {"function": {"name": "tool_name", "arguments": {"param1": "value1"}}}\n\n';
        }

        // Construir o prompt completo
        prompt =
          toolsPrompt +
          messages
            .map(
              (m) =>
                `${m.role === "assistant" ? "Assistant" : "User"}: ${m.content}`
            )
            .join("\n\n") +
          "\n\nAssistant:";
      }

      const generationOptions = {
        max_new_tokens: opts.maxTokens || 512,
        temperature: opts.temperature ?? 0.7,
        // For better function calling compliance we disable sampling penalties
        do_sample: (opts.temperature ?? 0.7) > 0,
        stop: opts.maxTokens ? undefined : undefined,
      } as any;

      const result: any[] = await this.generator(prompt, generationOptions);

      let first = result[0];
      if (Array.isArray(first)) first = first[0];

      // Extract content depending on result structure
      let generatedTextRaw = first?.generated_text ?? first?.text ?? undefined;

      if (typeof generatedTextRaw === "undefined") {
        throw new Error("Invalid generation result format");
      }

      const content = Array.isArray(generatedTextRaw)
        ? generatedTextRaw.at(-1)?.content ?? JSON.stringify(generatedTextRaw)
        : String(generatedTextRaw);

      // Clean think tags from the content before processing tool calls
      const cleanedContent = cleanThinkTags(content);

      let tool_calls = extractToolCalls(cleanedContent);

      return {
        content: cleanedContent || undefined,
        tool_calls: tool_calls.length > 0 ? tool_calls : undefined,
      };
    } catch (error) {
      console.error("[HFS] ‚ùå generateWithFunctions error:", error);
      throw error;
    }
  }

  /**
   * Simple wrapper to generate a plain response string (without tool parsing).
   */
  async generateResponse(
    messages: Array<{ role: "system" | "user" | "assistant"; content: string }>,
    opts: { temperature?: number; maxTokens?: number } = {}
  ): Promise<{ response: string }> {
    const responseText = await this.generate(messages, {
      temperature: opts.temperature,
      maxTokens: opts.maxTokens,
    });
    return { response: responseText };
  }

  /**
   * Get current model status
   */
  getStatus(): {
    initialized: boolean;
    currentModel: string | null;
    isLoading: boolean;
  } {
    return {
      initialized: this.initialized,
      currentModel: this.currentModel,
      isLoading: this.isLoading,
    };
  }

  /**
   * Dispose model and free resources.
   */
  async dispose() {
    console.log("[HFS] Disposing model resources...");
    this.generator = null;
    this.tokenizer = null;
    this.currentModel = null;
    this.initialized = false;
  }

  /**
   * Force reload of the current model.
   */
  async forceReload(): Promise<void> {
    if (this.currentModel) {
      const modelId = this.currentModel;
      await this.dispose();
      console.log(`[HFS] Prepared force reload for model: ${modelId}`);
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// ModeService.ts ‚Äî Orch-OS Mode Cortex
// Symbolic Intent: Central neuron for controlling Orch-OS operational mode (basic/advanced)
// Responsibilities: Exposes current mode, persists mode, notifies listeners, enforces symbolic clarity
// IMPORTANT: Use ModeService.getMode() to determine which AI backend to use in all services:
// - 'basic': Use HuggingFace (local models, no Deepgram, DuckDB storage)
// - 'advanced': Use Ollama, Deepgram, DuckDB storage, full features

import { getOption, setOption } from "./StorageService"; // Use symbolic storage neurons

// Symbolic enum for Orch-OS operational modes
export enum OrchOSModeEnum {
  BASIC = "basic",
  ADVANCED = "advanced",
}
export type OrchOSMode = OrchOSModeEnum.BASIC | OrchOSModeEnum.ADVANCED;

import { STORAGE_KEYS } from "./StorageService";
const MODE_STORAGE_KEY = STORAGE_KEYS.APPLICATION_MODE;
const DEFAULT_MODE: OrchOSMode = OrchOSModeEnum.ADVANCED;

export class ModeService {
  // Internal state
  private static mode: OrchOSMode = DEFAULT_MODE;
  private static listeners: Array<(mode: OrchOSMode) => void> = [];
  private static initialized = false;

  // Initialize mode from storage or fallback
  static initialize() {
    if (this.initialized) return;
    let storedMode: OrchOSMode | null = null;
    try {
      // Attempt to retrieve mode from storage
      storedMode = getOption(MODE_STORAGE_KEY) as OrchOSMode | null;
    } catch (e) {
      // StorageService may not be ready; fallback to localStorage
      if (typeof window !== "undefined" && window.localStorage) {
        // Fallback to localStorage if storage service is not available
        storedMode = window.localStorage.getItem(
          MODE_STORAGE_KEY
        ) as OrchOSMode | null;
      }
    }
    // Validate stored mode and update internal state
    if (
      storedMode === OrchOSModeEnum.BASIC ||
      storedMode === OrchOSModeEnum.ADVANCED
    ) {
      this.mode = storedMode;
    }
    this.initialized = true;
  }

  // Get current mode
  static getMode(): OrchOSMode {
    this.initialize();
    return this.mode;
  }

  // Set mode and notify listeners
  static setMode(mode: OrchOSMode) {
    if (this.mode !== mode) {
      this.mode = mode;
      try {
        setOption(MODE_STORAGE_KEY, mode);
      } catch (e) {
        if (typeof window !== "undefined" && window.localStorage) {
          window.localStorage.setItem(MODE_STORAGE_KEY, mode);
        }
      }
      this.listeners.forEach((listener) => listener(mode));
    }
  }

  // Subscribe to mode changes
  static onModeChange(listener: (mode: OrchOSMode) => void) {
    this.listeners.push(listener);
    // Return unsubscribe function
    return () => {
      this.listeners = this.listeners.filter((l) => l !== listener);
    };
  }
}

// Symbolic: Ensure mode is initialized at startup
ModeService.initialize();
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// StorageService.ts
// Symbolic storage cortex for Orch-OS user settings (localStorage-based)
// Provides pure functions for saving and retrieving user config.

/**
 * STORAGE_KEYS: Mapeamento neural-simb√≥lico das chaves de configura√ß√£o
 *
 * Este objeto representa o "mapa sin√°ptico" das configura√ß√µes no sistema,
 * funcionando como uma √∫nica fonte de verdade para todas as chaves de armazenamento.
 *
 * Seguindo os princ√≠pios neural-simb√≥licos do Orch-OS, cada chave representa um
 * "sinal" espec√≠fico que flui atrav√©s do sistema e deve manter consist√™ncia
 * sem√¢ntica em todos os contextos.
 */
/**
 * STORAGE_KEYS: Mapeamento neural-simb√≥lico das chaves de configura√ß√£o
 *
 * Organizado por categorias sem√¢nticas para seguir o princ√≠pio neural-simb√≥lico
 * do Orch-OS de manter uma estrutura cognitiva clara e consistente.
 *
 * IMPORTANTE: Ao adicionar novas chaves, utilize a mesma string que √© usada
 * nos componentes para evitar problemas de sincroniza√ß√£o.
 */
export const STORAGE_KEYS = {
  // Chave principal para armazenamento de todas as configura√ß√µes
  SETTINGS_ROOT: "orchos.user.settings",

  // ===== Modelos Hugging Face =====
  HF_MODEL: "huggingfaceModel",
  HF_EMBEDDING_MODEL: "huggingfaceEmbeddingModel",

  // ===== Configura√ß√µes gerais do sistema =====
  USER_NAME: "userName",
  APPLICATION_MODE: "applicationMode", // 'basic' ou 'advanced'

  // ===== Configura√ß√µes visuais/interface =====
  // Matriz qu√¢ntica e efeitos
  ENABLE_MATRIX: "enableMatrix",
  MATRIX_DENSITY: "matrixDensity",
  ENABLE_EFFECTS: "enableEffects",
  ENABLE_ANIMATIONS: "enableAnimations",

  // Estilo e tema
  DARK_MODE: "darkMode",
  THEME: "theme",
  UI_DENSITY: "uiDensity",
  ENABLE_NEUMORPHISM: "enableNeumorphism",
  ENABLE_GLASSMORPHISM: "enableGlassmorphism",
  PANEL_TRANSPARENCY: "panelTransparency",
  COLOR_THEME: "colorTheme",
  SHOW_ADVANCED_SETTINGS: "showAdvancedSettings",

  // ===== Configura√ß√µes de √°udio e transcri√ß√£o =====
  // Processamento de √°udio
  AUDIO_QUALITY: "audioQuality",
  AUTO_GAIN_CONTROL: "autoGainControl",
  NOISE_SUPPRESSION: "noiseSuppression",
  ECHO_CANCELLATION: "echoCancellation",

  // Transcri√ß√£o
  TRANSCRIPTION_ENABLED: "transcriptionEnabled",
  ENHANCED_PUNCTUATION: "enhancedPunctuation",
  SPEAKER_DIARIZATION: "speakerDiarization",
  SPEAKER_IDENTIFICATION: "speakerIdentificationEnabled",

  // ===== Configura√ß√µes de APIs e servi√ßos externos =====
  // OpenAI/ChatGPT
  OPENAI_API_KEY: "openaiApiKey", // Mesmo que chatgptApiKey
  CHATGPT_MODEL: "chatgptModel",
  CHATGPT_TEMPERATURE: "chatgptTemperature",
  CHATGPT_MAX_TOKENS: "chatgptMaxTokens",
  OPENAI_EMBEDDING_MODEL: "openaiEmbeddingModel",

  // Ollama Settings
  OLLAMA_MODEL: "ollamaModel",
  OLLAMA_EMBEDDING_MODEL: "ollamaEmbeddingModel",
  OLLAMA_ENABLED: "ollamaEnabled",

  // Deepgram
  DEEPGRAM_API_KEY: "deepgramApiKey",
  DEEPGRAM_MODEL: "deepgramModel",
  DEEPGRAM_LANGUAGE: "deepgramLanguage",
  DEEPGRAM_TIER: "deepgramTier",

  // Pinecone
  PINECONE_API_KEY: "pineconeApiKey",
  PINECONE_ENVIRONMENT: "pineconeEnvironment",
  PINECONE_INDEX: "pineconeIndex",

  // ===== Configura√ß√µes de banco de dados local =====
  DUCKDB_PATH: "duckdbPath", // Caminho personalizado para o banco DuckDB

  // ===== Configura√ß√µes de ferramentas (tools) =====
  TOOLS_ENABLED: "toolsEnabled", // Habilita/desabilita ferramentas no modo b√°sico

  // ===== Configura√ß√µes de debugging =====
  DEBUG_MODE: "debugMode",
  LOG_LEVEL: "logLevel",
};

// Symbolic: All user options are stored as a map/object in the selected storage cortex.
export type UserSettings = Record<string, any>;

function isRenderer(): boolean {
  // Electron renderer or browser
  return (
    typeof window !== "undefined" && typeof window.localStorage !== "undefined"
  );
}

/**
 * Cria e retorna um novo backend para armazenamento
 * IMPORTANTE: N√£o mais usa cache est√°tico para evitar problemas com estados desatualizados
 */
function getBackend() {
  // Identificar ambiente de execu√ß√£o
  const isInRenderer = isRenderer();

  // Configurar backend apropriado para o ambiente
  if (isInRenderer) {
    return {
      get: () => {
        try {
          const raw = window.localStorage.getItem(STORAGE_KEYS.SETTINGS_ROOT);

          if (!raw) {
            return {};
          }

          const parsedData = JSON.parse(raw) as UserSettings;

          // Extrair informa√ß√µes importantes para logs
          const userName =
            parsedData[STORAGE_KEYS.USER_NAME] || parsedData["name"] || "N/A";
          const lang = parsedData[STORAGE_KEYS.DEEPGRAM_LANGUAGE] || "N/A";
          const model = parsedData[STORAGE_KEYS.DEEPGRAM_MODEL] || "N/A";

          return parsedData;
        } catch (error) {
          console.error("‚ùå [STORAGE] Erro ao ler localStorage:", error);
          return {};
        }
      },
      set: (data: UserSettings) => {
        try {
          const jsonString = JSON.stringify(data);
          window.localStorage.setItem(STORAGE_KEYS.SETTINGS_ROOT, jsonString);

          // Verificar se os dados foram realmente salvos
          const verification = window.localStorage.getItem(
            STORAGE_KEYS.SETTINGS_ROOT
          );
          if (!verification) {
            console.error(
              "‚ùå [STORAGE] Falha na verifica√ß√£o: dados n√£o foram salvos no localStorage"
            );
          }
        } catch (error) {
          console.error("‚ùå [STORAGE] Erro ao salvar no localStorage:", error);
        }
      },
    };
  } else {
    // Node.js/Electron main: use electron-store
    try {
      // eslint-disable-next-line @typescript-eslint/no-var-requires
      const Store = require("electron-store").default;
      const store = new Store();

      return {
        get: () => {
          try {
            const data = store.get(STORAGE_KEYS.SETTINGS_ROOT, {});
            return data;
          } catch (error) {
            console.error("‚ùå [STORAGE] Erro ao ler do electron-store:", error);
            return {};
          }
        },
        set: (data: UserSettings) => {
          try {
            store.set(STORAGE_KEYS.SETTINGS_ROOT, data);
          } catch (error) {
            console.error(
              "‚ùå [STORAGE] Erro ao salvar no electron-store:",
              error
            );
          }
        },
      };
    } catch (error) {
      console.error("‚ùå [STORAGE] Erro ao inicializar electron-store:", error);
      // Fallback para um backend em mem√≥ria caso o electron-store falhe
      let memoryStore = {};
      return {
        get: () => memoryStore,
        set: (data: UserSettings) => {
          memoryStore = data;
        },
      };
    }
  }
}

/**
 * Saves all user settings (the entire map) to storage cortex.
 */
export function setAllOptions(options: UserSettings): void {
  getBackend().set(options);
}

/**
 * Retrieves all user settings (the entire map) from storage cortex.
 */
export function getAllOptions(): UserSettings {
  return getBackend().get();
}

// Sistema de eventos para notificar sobre mudan√ßas no storage
type StorageChangeListener = (key: string, value: any) => void;
const storageChangeListeners: StorageChangeListener[] = [];

/**
 * Registra um listener para ser notificado quando uma op√ß√£o espec√≠fica mudar
 * @param listener Fun√ß√£o que ser√° chamada quando a op√ß√£o mudar
 */
export function subscribeToStorageChanges(
  listener: StorageChangeListener
): () => void {
  storageChangeListeners.push(listener);

  // Retorna uma fun√ß√£o para cancelar a inscri√ß√£o
  return () => {
    const index = storageChangeListeners.indexOf(listener);
    if (index > -1) {
      storageChangeListeners.splice(index, 1);
    }
  };
}

/**
 * Sets a single option by key.
 */
export function setOption<T = any>(key: string, value: T): void {
  const options = getAllOptions();
  options[key] = value;
  setAllOptions(options);

  // Notifica os listeners
  storageChangeListeners.forEach((listener) => {
    try {
      listener(key, value);
    } catch (err) {
      console.error(`Error in storage change listener for ${key}:`, err);
    }
  });
}

/**
 * Gets a single option by key, or undefined if not set.
 */
export function getOption<T = any>(key: string): T | undefined {
  const options = getAllOptions();
  return options[key];
}

// Symbolic helpers for common options

export function getUserName(): string {
  // Fun√ß√£o direta: apenas busca o valor usando a chave padr√£o
  return getOption<string>(STORAGE_KEYS.USER_NAME) || "User";
}

export function setUserName(name: string): void {
  // Usa a chave padronizada para salvar o nome do usu√°rio
  setOption(STORAGE_KEYS.USER_NAME, name);
}

/**
 * Fun√ß√£o utilit√°ria para resetar completamente o storage do Orch-OS
 * √ötil para testes ou quando h√° inconsist√™ncias nas configura√ß√µes
 */
export function resetStorage(): void {
  const backend = getBackend();
  backend.set({});

  // Recarregar automaticamente a p√°gina se em ambiente de navegador
  if (typeof window !== "undefined") {
    window.location.reload();
  }
}

/**
 * Limpa chaves antigas e desatualizadas do armazenamento
 * Mant√©m apenas as chaves definidas no objeto STORAGE_KEYS
 */
export function cleanupStorage(): void {
  // Obter todas as op√ß√µes atuais
  const options = getAllOptions();
  const currentSettings = { ...options };

  // Criar um conjunto com todos os valores (n√£o chaves) de STORAGE_KEYS
  const validValues = new Set<string>();
  Object.values(STORAGE_KEYS).forEach((value) => {
    validValues.add(value as string);
  });

  // Identificar e remover chaves desatualizadas
  let removedCount = 0;
  for (const key in currentSettings) {
    // Pular a chave principal de configura√ß√µes
    if (key === STORAGE_KEYS.SETTINGS_ROOT) continue;

    // Se a chave n√£o estiver nos valores v√°lidos, remov√™-la
    if (!validValues.has(key)) {
      delete currentSettings[key];
      removedCount++;
    }
  }

  // Salvar configura√ß√µes limpas
  setAllOptions(currentSettings);
  console.log(
    `üß† [STORAGE] Limpeza neural-simb√≥lica conclu√≠da: ${removedCount} chaves antigas removidas`
  );
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Dynamic Model Registry - API-based model discovery
// No hardcoded models - everything comes from Ollama API with default versions

export enum ModelSource {
  LOCAL = "local",
  WEB = "web",
}

export interface LocalModelMeta {
  /** API model name (e.g., "llama3.2", "mistral") */
  id: string;
  /** UI display name */
  label: string;
  /** Ollama repo name - always using default/latest tag */
  repo: string;
  /** Estimated size in GB (from API) */
  sizeGB: number;
  /** Model family/type */
  family?: string;
  /** Whether model is currently installed */
  isInstalled?: boolean;
  /** Last modified timestamp from API */
  modified?: string;
}

/**
 * Normalizes model names to use default versions only
 * Always strips version tags to use latest/default
 */
export function normalizeModelName(modelName: string): string {
  // Remove any existing tags to always use default
  const baseName = modelName.split(":")[0];

  // Convert underscores to dots for proper model names
  const cleanName = baseName.replace(/_/g, ".");

  // Return clean model name without any version tags
  // Ollama will automatically use the default/latest version
  return cleanName;
}

/**
 * Converts model name to display-friendly format
 */
export function getDisplayName(modelName: string): string {
  const baseName = normalizeModelName(modelName);

  // Convert to title case and add proper spacing
  return baseName
    .split(".")
    .map((part) => part.charAt(0).toUpperCase() + part.slice(1))
    .join(" ");
}

/**
 * Estimates model size category for UI purposes
 */
export function getModelSizeCategory(
  sizeGB: number
): "small" | "medium" | "large" {
  if (sizeGB < 3) return "small";
  if (sizeGB < 8) return "medium";
  return "large";
}

/**
 * Gets minimum RAM requirement based on model size
 */
export function getMinRamRequirement(sizeGB: number): number {
  // Rule of thumb: model size * 2 for comfortable inference
  return Math.max(4, Math.ceil(sizeGB * 2));
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// neuralSignalParser.ts
// Symbolic: Pure functions for parsing neural signals from model output
import { NeuralSignal } from '../../components/context/deepgram/interfaces/neural/NeuralSignalTypes';

/**
 * Attempts to parse a string as a NeuralSignal JSON object.
 * Returns undefined if parsing fails or required fields are missing.
 */
export function parseNeuralSignal(json: string): NeuralSignal | undefined {
  try {
    const args = JSON.parse(json);
    if (args.core || args.query || args.intensity) {
      const baseSignal: Partial<NeuralSignal> = {
        core: args.core || null,
        intensity: Math.max(0, Math.min(1, args.intensity ?? 0.5)),
        symbolic_query: { query: args.query ?? '' }
      };
      if (Array.isArray(args.keywords)) baseSignal.keywords = args.keywords;
      if (args.filters) baseSignal.filters = args.filters;
      if (typeof args.expand === 'boolean') baseSignal.expand = args.expand;
      if (args.symbolicInsights) baseSignal.symbolicInsights = args.symbolicInsights;
      if (typeof args.topK !== 'undefined') baseSignal.topK = args.topK;
      if (typeof baseSignal.core !== 'undefined') return baseSignal as NeuralSignal;
    }
    return undefined;
  } catch {
    return undefined;
  }
}

/**
 * Extracts all JSON-like objects (or JSON code blocks) from a string.
 * Accepts objects that appear anywhere in the text, including inside ```json``` blocks.
 */
export function extractNeuralSignalJsons(text: string): string[] {
  const matches: string[] = [];

  if (!text || typeof text !== 'string') return matches;

  // 1. Capture fenced ```json``` blocks
  const codeBlockRegex = /```(?:json)?[\s\n]*([\s\S]*?)```/gi;
  let codeMatch: RegExpExecArray | null;
  while ((codeMatch = codeBlockRegex.exec(text)) !== null) {
    if (codeMatch[1]) {
      matches.push(codeMatch[1].trim());
    }
  }

  // 2. Capture standalone JSON objects { ... } that may appear outside blocks
  //    This regex is intentionally simple; deeper validation is performed during JSON.parse.
  const objectRegex = /\{[^\{\}]*\}/g;
  const objectMatches = text.match(objectRegex);
  if (objectMatches) {
    matches.push(...objectMatches.map((m) => m.trim()));
  }

  return matches;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { render, act, renderHook } from '@testing-library/react';
import { CognitionLogProvider, useCognitionLog } from '../../components/context/CognitionLogContext';
import symbolicCognitionTimelineLogger from '../../components/context/deepgram/services/utils/SymbolicCognitionTimelineLoggerSingleton';
import cognitionLogExporterFactory from '../../components/context/deepgram/services/utils/CognitionLogExporterFactory';
import { CognitionEvent } from '../../components/context/deepgram/types/CognitionEvent';

jest.mock('../../components/context/deepgram/services/utils/SymbolicCognitionTimelineLoggerSingleton', () => ({
  __esModule: true,
  default: {
    getTimeline: jest.fn(),
    clear: jest.fn()
  }
}));

jest.mock('../../components/context/deepgram/services/utils/CognitionLogExporterFactory', () => {
  const mockExporter = {
    label: 'Mock Exporter',
    export: jest.fn()
  };
  
  return {
    __esModule: true,
    default: {
      getExporters: jest.fn().mockReturnValue([mockExporter])
    }
  };
});

describe('CognitionLogContext', () => {
  const mockEvents: CognitionEvent[] = [
    { type: 'raw_prompt', timestamp: '2025-04-29T18:30:00.000Z', content: 'Test prompt' },
    { type: 'temporary_context', timestamp: '2025-04-29T18:30:05.000Z', context: 'Test context' }
  ];

  beforeEach(() => {
    jest.clearAllMocks();
    (symbolicCognitionTimelineLogger.getTimeline as jest.Mock).mockReturnValue(mockEvents);
  });

  test('Provides access to events through the useCognitionLog hook', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );
    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    expect(result.current.events).toEqual(mockEvents);
  });

  test('Provides access to exporters through the useCognitionLog hook', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );
    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    expect(result.current.exporters).toHaveLength(1);
    expect(result.current.exporters[0].label).toBe('Mock Exporter');
  });

  test('Calls the logger clear method when clearEvents is invoked', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );

    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    act(() => {
      result.current.clearEvents();
    });
    expect(symbolicCognitionTimelineLogger.clear).toHaveBeenCalledTimes(1);
  });

  test('Calls the exporter export method with the correct exporter when exportEvents is invoked', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );

    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    act(() => {
      result.current.exportEvents('Mock Exporter');
    });
    const mockExporter = cognitionLogExporterFactory.getExporters()[0];
    expect(mockExporter.export).toHaveBeenCalledTimes(1);
    expect(mockExporter.export).toHaveBeenCalledWith(mockEvents);
  });

  test('Does not call any export method if the exporter is not found', () => {
    const wrapper = ({ children }: { children: React.ReactNode }) => (
      <CognitionLogProvider>{children}</CognitionLogProvider>
    );

    const { result } = renderHook(() => useCognitionLog(), { wrapper });
    act(() => {
      result.current.exportEvents('Exportador Inexistente');
    });
    const mockExporter = cognitionLogExporterFactory.getExporters()[0];
    expect(mockExporter.export).not.toHaveBeenCalled();
  });

  test('Periodically updates events', () => {
    jest.useFakeTimers();
    render(
      <CognitionLogProvider>
        <div>Teste</div>
      </CognitionLogProvider>
    );
    expect(symbolicCognitionTimelineLogger.getTimeline).toHaveBeenCalled();
    jest.clearAllMocks();
    act(() => {
      jest.advanceTimersByTime(1000);
    });
    expect(symbolicCognitionTimelineLogger.getTimeline).toHaveBeenCalled();
    jest.useRealTimers();
  });

  test('useCognitionLog throws error when used outside of provider', () => {
    const originalError = console.error;
    console.error = jest.fn();
    expect(() => {
      renderHook(() => useCognitionLog());
    }).toThrow('useCognitionLog must be used within a CognitionLogProvider');
    console.error = originalError;
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { CognitionLogExporterFactory } from '../../components/context/deepgram/services/utils/CognitionLogExporterFactory';
import { CognitionLogExporter } from '../../components/context/deepgram/services/utils/CognitionLogExporter';
import { CognitionLogJsonExporter } from '../../components/context/deepgram/services/utils/CognitionLogJsonExporter';
import { CognitionLogTxtExporter } from '../../components/context/deepgram/services/utils/CognitionLogTxtExporter';
import { CognitionEvent } from '../../components/context/deepgram/types/CognitionEvent';

class MockCustomExporter implements CognitionLogExporter {
  label = 'Test Exporter';
  export(log: CognitionEvent[], filename?: string): void {
    // Mock
  }
}

describe('CognitionLogExporterFactory', () => {
  let factory: CognitionLogExporterFactory;
  
  beforeEach(() => {
    CognitionLogExporterFactory.instance = undefined;
    factory = CognitionLogExporterFactory.getInstance();
  });
  
  test('Implements Singleton pattern', () => {
    const instance1 = CognitionLogExporterFactory.getInstance();
    const instance2 = CognitionLogExporterFactory.getInstance();
    
    expect(instance1).toBe(instance2);
  });
  
  test('Initializes with default exporters', () => {
    const exporters = factory.getExporters();
    
    expect(exporters.length).toBeGreaterThanOrEqual(2);
    
    expect(exporters.some(e => e.label === 'Export cognitive log (JSON)')).toBe(true);
    
    expect(exporters.some(e => e.label === 'Export cognitive log (TXT)')).toBe(true);
  });
  
  test('Permits the registration of new exporters', () => {
    const customExporter = new MockCustomExporter();
    factory.registerExporter(customExporter);
    
    const exporters = factory.getExporters();
    expect(exporters.some(e => e.label === 'Test Exporter')).toBe(true);
    expect(exporters.length).toBeGreaterThan(2);
  });
  
  test('Removes existing exporters', () => {
    const customExporter = new MockCustomExporter();
    factory.registerExporter(customExporter);
    
    let exporters = factory.getExporters();
    expect(exporters.some(e => e.label === 'Test Exporter')).toBe(true);
    
    factory.unregisterExporter('Test Exporter');
    
    exporters = factory.getExporters();
    expect(exporters.some(e => e.label === 'Test Exporter')).toBe(false);
  });
  
  test('Returns a copy of the exporters to prevent direct modification', () => {
    const exporters1 = factory.getExporters();
    const initialCount = exporters1.length;
    
    exporters1.push(new MockCustomExporter());
    
    const exporters2 = factory.getExporters();
    
    expect(exporters2.length).toBe(initialCount);
    
    expect(exporters1).not.toBe(exporters2);
  });
  
  test('Preserves the correct types of exporters', () => {
    const exporters = factory.getExporters();
    
    const jsonExporter = exporters.find(e => e.label === 'Export cognitive log (JSON)');
    expect(jsonExporter).toBeInstanceOf(CognitionLogJsonExporter);
    
    const txtExporter = exporters.find(e => e.label === 'Export cognitive log (TXT)');
    expect(txtExporter).toBeInstanceOf(CognitionLogTxtExporter);
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// CognitionLogJsonExporter.test.ts
// Tests for the CognitionLogJsonExporter class

import { CognitionLogJsonExporter } from '../../components/context/deepgram/services/utils/CognitionLogJsonExporter';
import { CognitionEvent } from '../../components/context/deepgram/types/CognitionEvent';

describe('CognitionLogJsonExporter', () => {
  let exporter: CognitionLogJsonExporter;
  
  // Mocks for browser environment
  const mockClickFn = jest.fn();

  // Mocks for DOM and browser APIs
  const mockCreateElement = jest.fn();
  const mockCreateObjectURL = jest.fn();
  const mockRevokeObjectURL = jest.fn();
  
  // Example cognitive events for testing
  const sampleEvents: CognitionEvent[] = [
    { type: 'raw_prompt', timestamp: '2025-04-29T18:30:00.000Z', content: 'Como implementar um sistema neural?' },
    { type: 'temporary_context', timestamp: '2025-04-29T18:30:05.000Z', context: 'Contexto para processamento' },
    { 
      type: 'neural_signal', 
      timestamp: '2025-04-29T18:30:10.000Z', 
      core: 'memory', 
      symbolic_query: { query: 'sistema neural', text: 'sistema neural', embedding: [0.1, 0.2] },
      intensity: 0.8,
      topK: 5,
      params: { type: 'memory' }
    }
  ];
  
  beforeEach(() => {
    exporter = new CognitionLogJsonExporter();
    
    // Reset mocks
    mockClickFn.mockReset();
    mockCreateElement.mockReset();
    mockCreateObjectURL.mockReset();
    mockRevokeObjectURL.mockReset();
    
    // Mock for the <a> element
    mockCreateElement.mockReturnValue({
      href: '',
      download: '',
      click: mockClickFn
    });
    
    // Setup global mocks for testing
    Object.defineProperty(document, 'createElement', {
      value: mockCreateElement,
      writable: true
    });
    
    // Setup global mocks for testing
    Object.defineProperty(global, 'URL', {
      value: {
        createObjectURL: mockCreateObjectURL.mockReturnValue('mock-url'),
        revokeObjectURL: mockRevokeObjectURL
      },
      writable: true
    });
    
    // Setup global mocks for testing
    Object.defineProperty(global, 'Blob', {
      value: jest.fn(),
      writable: true
    });
    
    jest.useFakeTimers();
  });
  
  afterEach(() => {
    jest.clearAllMocks();
    jest.useRealTimers();
  });
  
  test('Has a descriptive label', () => {
    expect(exporter.label).toBe('Export cognitive log (JSON)');
  });
  
  test('Exports cognitive events to JSON format', () => {
    // Execute the export function
    exporter.export(sampleEvents);
    
    // Verify that the Blob was created with the expected content
    expect(global.Blob).toHaveBeenCalled();
    
    // Verify that URL.createObjectURL was called
    expect(mockCreateObjectURL).toHaveBeenCalled();
    
    // Verify that document.createElement was called with 'a'
    expect(mockCreateElement).toHaveBeenCalledWith('a');
    
    // Verify that the link was clicked
    expect(mockClickFn).toHaveBeenCalled();
    
    // Advance the timer to verify if URL.revokeObjectURL is called
    jest.advanceTimersByTime(1000);
    expect(mockRevokeObjectURL).toHaveBeenCalled();
  });
  
  test('Uses custom filename when provided', () => {
    const customFilename = 'custom_session_log.json';
    
    jest.clearAllMocks();
    
    // Prepare a new mock with a different value for download
    mockCreateElement.mockReturnValue({
      href: '',
      download: '',
      click: mockClickFn
    });
    
    // Execute the export function with a custom filename
    exporter.export(sampleEvents, customFilename);
    
    // Verify that the <a> element was created
    expect(mockCreateElement).toHaveBeenCalledWith('a');
    
    // Verify that the filename was set correctly
    const anchorElement = mockCreateElement.mock.results[0].value;
    expect(anchorElement.download).toBe(customFilename);
  });
  
  test('Exports valid and formatted JSON', () => {
    exporter.export(sampleEvents);
    
    // Get the parameters passed to the Blob
    const blobContent = (global.Blob as jest.Mock).mock.calls[0][0][0];
    
    // Try to parse to ensure it's a valid JSON
    expect(() => JSON.parse(blobContent)).not.toThrow();
    
    // Verify that the JSON was formatted with indentation (using null, 2)
    const expectedJson = JSON.stringify(sampleEvents, null, 2);
    expect(blobContent).toBe(expectedJson);
    
    // Verify that the Blob MIME type is correct
    const blobOptions = (global.Blob as jest.Mock).mock.calls[0][1];
    expect(blobOptions.type).toBe('application/json');
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// CognitionLogTxtExporter.test.ts
// Tests for the CognitionLogTxtExporter class

import { CognitionLogTxtExporter } from '../../components/context/deepgram/services/utils/CognitionLogTxtExporter';
import { CognitionEvent } from '../../components/context/deepgram/types/CognitionEvent';

describe('CognitionLogTxtExporter', () => {
  let exporter: CognitionLogTxtExporter;
  
  // Mock for simulating the element click
  const mockClickFn = jest.fn();
  
  // Example cognitive events for testing
  const sampleEvents: CognitionEvent[] = [
    {
      type: 'raw_prompt',
      timestamp: '2025-04-29T18:30:00.000Z',
      content: 'How to implement a neural system?'
    },
    {
      type: 'temporary_context',
      timestamp: '2025-04-29T18:30:05.000Z',
      context: 'Context for processing'
    },
    {
      type: 'neural_signal',
      timestamp: '2025-04-29T18:30:10.000Z',
      core: 'memory',
      symbolic_query: {
        text: 'neural system',
        embedding: [0.1, 0.2],
        query: 'neural system'
      },
      intensity: 0.8,
      topK: 5,
      params: {
        type: 'memory'
      }
    }
  ];
  
  // Mocks for the DOM and browser APIs
  const mockCreateElement = jest.fn();
  const mockCreateObjectURL = jest.fn();
  const mockRevokeObjectURL = jest.fn();
  
  beforeEach(() => {
    exporter = new CognitionLogTxtExporter();
    
    // Reset mocks
    mockClickFn.mockReset();
    mockCreateElement.mockReset();
    mockCreateObjectURL.mockReset();
    mockRevokeObjectURL.mockReset();
    
    // Mock for the <a> element
    mockCreateElement.mockReturnValue({
      href: '',
      download: '',
      click: mockClickFn
    });
    
    // Setup global mocks with safe typecasting
    document.createElement = mockCreateElement;
    
    global.URL = {
      createObjectURL: mockCreateObjectURL.mockReturnValue('mock-url'),
      revokeObjectURL: mockRevokeObjectURL
    };
  
    global.Blob = jest.fn();
    
    jest.useFakeTimers();
  });
  
  afterEach(() => {
    jest.clearAllMocks();
    jest.useRealTimers();
  });
  
  test('Has a descriptive label', () => {
    expect(exporter.label).toBe('Export cognitive log (TXT)');
  });
  
  test('Exports cognitive events to TXT format', () => {
    // Execute the export function
    exporter.export(sampleEvents);
    
    // Verify that the Blob was created with the expected content
    expect(global.Blob).toHaveBeenCalled();
    
    // Verify that URL.createObjectURL was called
    expect(mockCreateObjectURL).toHaveBeenCalled();
    
    // Verify that document.createElement was called with 'a'
    expect(mockCreateElement).toHaveBeenCalledWith('a');
    
    // Verify that the link was clicked
    expect(mockClickFn).toHaveBeenCalled();
    
    // Advance the timer to verify if URL.revokeObjectURL is called
    jest.advanceTimersByTime(1000);
    expect(mockRevokeObjectURL).toHaveBeenCalled();
  });
  
  test('Uses custom filename when provided', () => {
    const customFilename = 'custom_session_log.txt';
    
    // Clear previous calls
    jest.clearAllMocks();
    
    // Execute the export with custom filename
    exporter.export(sampleEvents, customFilename);
    
    // Verify that the <a> element was created
    expect(mockCreateElement).toHaveBeenCalledWith('a');
    
    // Verify that the filename was set correctly
    const anchorElement = mockCreateElement.mock.results[0].value;
    expect(anchorElement.download).toBe(customFilename);
  });
  
  test('Separates events with delimiter in the TXT content', () => {
    exporter.export(sampleEvents);
    
    // Get the parameters passed to the Blob
    const blobContent = (global.Blob as jest.Mock).mock.calls[0][0][0];
    
    // Verify that it contains the delimiter and basic data
    expect(blobContent).toContain('---');
    expect(blobContent).toContain('raw_prompt');
    expect(blobContent).toContain('temporary_context');
    expect(blobContent).toContain('neural_signal');
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// SymbolicCognitionTimelineLogger.test.ts
// Tests for the SymbolicCognitionTimelineLogger class

import SymbolicCognitionTimelineLogger from '../../components/context/deepgram/services/utils/SymbolicCognitionTimelineLogger';

// Mock for Date.toISOString to return a fixed timestamp and facilitate tests
const mockTimestamp = '2025-04-29T18:30:00.000Z';
jest.spyOn(Date.prototype, 'toISOString').mockImplementation(() => mockTimestamp);

describe('SymbolicCognitionTimelineLogger', () => {
  let logger: SymbolicCognitionTimelineLogger;
  
  beforeEach(() => {
    // Create a new instance for each test
    logger = new SymbolicCognitionTimelineLogger();
  });

  test('Should initialize with an empty timeline', () => {
    expect(logger.getTimeline()).toEqual([]);
  });

  test('Should log raw prompt correctly', () => {
    const promptText = 'How can I implement a neural symbolic system?';
    logger.logRawPrompt(promptText);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'raw_prompt',
      timestamp: mockTimestamp,
      content: promptText
    });
  });

  test('Should log temporary context correctly', () => {
    const context = 'Temporary context for neural processing';
    logger.logTemporaryContext(context);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'temporary_context',
      timestamp: mockTimestamp,
      context
    });
  });

  test('Should log neural signal correctly', () => {
    const core = 'memory';
    const symbolic_query = { query: 'neural system implementation', text: 'neural system implementation', embedding: [0.1, 0.2] };
    const intensity = 0.8;
    const topK = 5;
    const params = { searchType: 'semantic' };

    logger.logNeuralSignal(core, symbolic_query, intensity, topK, params);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'neural_signal',
      timestamp: mockTimestamp,
      core,
      symbolic_query,
      intensity,
      topK,
      params
    });
  });

  test('Should log symbolic retrieval correctly', () => {
    const core = 'memory';
    const insights = [  
      { id: '1', text: 'Insight 1', score: 0.9, type: 'memory' },
      { id: '2', text: 'Insight 2', score: 0.7, type: 'memory' }
    ];
    const matchCount = 2;
    const durationMs = 150;

    logger.logSymbolicRetrieval(core, insights, matchCount, durationMs);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'symbolic_retrieval',
      timestamp: mockTimestamp,
      core,
      insights,
      matchCount,
      durationMs
    });
  });

  test('Should handle empty insights in symbolic retrieval', () => {
    const core = 'memory';
    const insights: { id: string; text: string; score: number; type: string }[] = [];
    const matchCount = 0;
    const durationMs = 100;

    logger.logSymbolicRetrieval(core, insights, matchCount, durationMs);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'symbolic_retrieval',
      timestamp: mockTimestamp,
      core,
      insights: [],
      matchCount,
      durationMs
    });
  });

  test('Should log fusion initiation correctly', () => {
    logger.logFusionInitiated();

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'fusion_initiated',
      timestamp: mockTimestamp
    });
  });

  test('Should log symbolic context synthesis correctly', () => {
    const context = {
      memoryInsights: ['Insight 1', 'Insight 2'],
      metacognitiveInsights: ['Meta Insight'],
      language: 'Insight of language',
      summary: 'Summary of the symbolic context',
      associativeInsights: ['Associative 1'],
      valenceInsights: ['Valence 1'],
      planningInsights: ['Planning 1']
    };

    logger.logSymbolicContextSynthesized(context);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'symbolic_context_synthesized',
      timestamp: mockTimestamp,
      context
    });
  });

  test('Should log GPT response as string correctly', () => {
    const response = 'This is a GPT response';
    logger.logGptResponse(response);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0].type).toBe('gpt_response');
    // The response is stored in the response field of the structure
    const event = timeline[0] as { type: string; timestamp: string; response: string };
    expect(event.response).toBe(response);
    expect(timeline[0].timestamp).toBe(mockTimestamp);
  });

  test('Should log GPT response as object correctly', () => {
    const responseData = {
      response: 'This is a GPT response',
      symbolicTopics: ['Topic 1', 'Topic 2'],
      insights: [
        { id: '1', text: 'Insight 1', score: 0.9, type: 'memory' },
        { id: '2', text: 'Insight 2', score: 0.8, type: 'metacognitive' }
      ]
    };

    logger.logGptResponse(responseData);

    const timeline = logger.getTimeline();
    expect(timeline).toHaveLength(1);
    expect(timeline[0]).toEqual({
      type: 'gpt_response',
      timestamp: mockTimestamp,
      ...responseData
    });
  });

  test('Should clear timeline correctly', () => {
    // Add some events
    logger.logRawPrompt('Prompt 1');
    logger.logTemporaryContext('Context');
    logger.logRawPrompt('Prompt 2');
    
    // Verify that they were added
    expect(logger.getTimeline()).toHaveLength(3);
    
    // Clear the timeline
    logger.clear();
    
    // Verify that it is empty
    expect(logger.getTimeline()).toHaveLength(0);
  });

  test('Should preserve chronological order of events', () => {
    logger.logRawPrompt('First prompt');
    logger.logTemporaryContext('Temporary context');
    logger.logFusionInitiated();
    
    const timeline = logger.getTimeline();
    
    expect(timeline).toHaveLength(3);
    expect(timeline[0].type).toBe('raw_prompt');
    expect(timeline[1].type).toBe('temporary_context');
    expect(timeline[2].type).toBe('fusion_initiated');
  });
});
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

let uiTranscriptionList: string[] = [];

function updateUI(newTranscription: string): void {
  console.log(`\nüìù New transcription: "${newTranscription}"`);
  
  if (!uiTranscriptionList) uiTranscriptionList = [];
  
  const incomingLines = newTranscription.split('\n')
    .map(l => l.trim())
    .filter(Boolean);
    
  if (incomingLines.length > 0) {
    const lastHistoryLine = uiTranscriptionList[uiTranscriptionList.length - 1];
    const lastIncomingLine = incomingLines[incomingLines.length - 1];
    
    if (incomingLines.length > 1 || lastIncomingLine !== lastHistoryLine) {
      const newLines = incomingLines.filter((line, i, arr) => 
        i === 0 || line !== arr[i-1]
      );
      
      if (newLines.length > 0 && 
          (uiTranscriptionList.length === 0 || 
          newLines[0] !== uiTranscriptionList[uiTranscriptionList.length - 1])) {
        console.log(`‚úÖ Adding ${newLines.length} new line(s) to list`);
        for (const line of newLines) {
          console.log(`  ‚Ä¢ "${line}"`);
        }
        uiTranscriptionList.push(...newLines);
      } else {
        console.log(`‚ö†Ô∏è No new lines to add to list`);
      }
    } else {
      console.log(`‚ö†Ô∏è Duplicate text from last entry, ignoring...`);
    }
  } else {
    console.log(`‚ö†Ô∏è No lines detected in received text`);
  }
  
  const fullText = uiTranscriptionList.join('\n');
  console.log(`üìÑ Current state: ${uiTranscriptionList.length} line(s), content:`);
  if (uiTranscriptionList.length > 0) {
    uiTranscriptionList.forEach((line, i) => {
      console.log(`  ${i+1}. "${line}"`);
    });
  } else {
    console.log(`  (empty)`);
  }
}

function runTest() {
  console.log("üß™ TEST INCREMENTAL PROCESSING");
  console.log("=====================================================");
  
  console.log("\nüîÑ TEST 1: Sending simple message");
  updateUI("Ola");
  
  console.log("\nüîÑ TEST 2: Sending incremental message");
  updateUI("Ola, Tudo bem ?");
  
  console.log("\nüîÑ TEST 3: Sending third incremental message");
  updateUI("Ola, Tudo bem ? Estou otimo !");
  
  console.log("\n=====================================================");
  console.log("üèÅ FINAL RESULT:");  
  
  const finalText = uiTranscriptionList.join('\n');
  console.log(`üìú Final text for prompt: "${finalText}"`);
  
  const containsIncremental = finalText.includes("Estou otimo");
  console.log(`üìä The final text contains the complete version? ${containsIncremental ? '‚úÖ YES' : '‚ùå NO'}`);
  
  const linesWithoutDuplicates = [...new Set(uiTranscriptionList)];
  const hasDuplicates = linesWithoutDuplicates.length < uiTranscriptionList.length;
  console.log(`üìä The text contains duplicates? ${hasDuplicates ? '‚ùå YES' : '‚úÖ NO'}`);
} 

runTest();
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { TranscriptionStorageService } from "../components/context/deepgram/services/transcription/TranscriptionStorageService";
import { ISpeakerIdentificationService } from "../components/context/deepgram/interfaces/utils/ISpeakerIdentificationService";
import { UIUpdater } from "../components/context/deepgram/interfaces/transcription/TranscriptionTypes";

// Mock do servi√ßo de identifica√ß√£o de falantes
class MockSpeakerService implements ISpeakerIdentificationService {
  splitMixedTranscription(text: string) {
    return [{ speaker: "Guilherme", text }];
  }
  
  getPrimaryUserSpeaker(): string {
    return "Guilherme";
  }
  
  filterTranscriptionsBySpeaker(
    speaker: string,
    transcriptions: Array<{ speaker: string; text: string; timestamp: string }>
  ) {
    return transcriptions.filter(t => t.speaker === speaker);
  }
}

// Function to test transcription storage
function testTranscriptionStorage() {
  console.log("üß™ STORAGE SERVICE TEST");
  console.log("==============================================");
  
  // Object to store the UI state
  let uiState = { transcription: "" };
  
  // Callback to update the UI state
  const setTexts = (updater: UIUpdater) => {
    if (typeof updater === 'function') {
      uiState = updater(uiState);
    } else {
      uiState = { ...uiState, ...updater };
    }
    console.log(`üìä UI atualizada: "${uiState.transcription}"`);
  };
  
  // Instantiate the storage service
  const speakerService = new MockSpeakerService();
  const storageService = new TranscriptionStorageService(speakerService, setTexts);
  
  // First message: "Hello"
  console.log("\nüîÑ TEST 1: Sending simple message");
  const message1 = "Hello";
  storageService.updateTranscriptionUI(message1);
  console.log(`‚úÖ After first message: "${uiState.transcription}"`);
  
  // Second incremental message: "Hello, How are you ?"
  console.log("\nüîÑ TEST 2: Sending incremental message");
  const message2 = "Hello, How are you ?";
  storageService.updateTranscriptionUI(message2);
  console.log(`‚úÖ After second message: "${uiState.transcription}"`);
  
  // Third incremental message: "Hello, How are you ? I'm good!"
  console.log("\nüîÑ TEST 3: Sending third incremental message");
  const message3 = "Hello, How are you ? I'm good!";
  storageService.updateTranscriptionUI(message3);
  console.log(`‚úÖ After third message: "${uiState.transcription}"`);
  
  // Verifying the text available for the prompt
  console.log("\nüîç VERIFYING TEXT AVAILABLE FOR PROMPT");
  const promptText = storageService.getUITranscriptionText();
  console.log(`üìú Text for prompt: "${promptText}"`);
  
  // Expected vs. actual result
  const expected = message3;
  const isSuccess = promptText === expected || promptText.endsWith(expected);
  
  console.log("\n==============================================");
  console.log(`üèÅ TEST RESULT: ${isSuccess ? '‚úÖ SUCCESS' : '‚ùå FAILURE'}`);
  
  if (!isSuccess) {
    console.log(`‚ùå Expected: "${expected}"`);
    console.log(`‚ùå Obtained: "${promptText}"`);
  } else {
    console.log(`‚úÖ Final text correct: "${promptText}"`);
  }
}

// Execute the test
testTranscriptionStorage();

export {};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { DuckDBMatch } from "../../electron/vector-database/interfaces/IVectorDatabase";
import type { VllmStatus } from "../../electron/preload/interfaces/IElectronAPI";

export interface ElectronAPI {
  // Core window methods

  toggleMainWindow: () => Promise<{ success: boolean; error?: string }>;
  getPlatform: () => string;
  minimizeWindow: () => void;
  closeWindow: () => void;

  // üî• Functions for neural transcription
  startTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  stopTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  sendNeuralPrompt: (
    temporaryContext?: string
  ) => Promise<{ success: boolean; error?: string }>;
  clearNeuralTranscription: () => Promise<{ success: boolean; error?: string }>;

  // üìù Events for neural transcription
  onRealtimeTranscription: (callback: (data: string) => void) => () => void;
  onNeuralStarted: (callback: () => void) => () => void;
  onNeuralStopped: (callback: () => void) => () => void;
  onNeuralError: (callback: (error: string) => void) => () => void;
  onPromptSend: (callback: () => void) => () => void;
  onPromptSending: (callback: () => void) => () => void;
  onPromptPartialResponse: (callback: (data: string) => void) => () => void;
  onPromptSuccess: (callback: (data: string) => void) => () => void;
  onPromptError: (callback: (error: string) => void) => () => void;
  onClearTranscription: (callback: () => void) => () => void;
  onSendChunk: (callback: (chunk: ArrayBuffer) => void) => () => void;

  // üìù Method to get environment variables
  getEnv: (key: string) => Promise<string | null>;
  getPath: (
    name: "userData" | "temp" | "desktop" | "documents"
  ) => Promise<string>;
  requestMicrophonePermission: () => Promise<{
    success: boolean;
    status: string;
    error?: string;
  }>;
  sendAudioChunk: (
    chunk: Uint8Array
  ) => Promise<{ success: boolean; error?: string }>;
  sendAudioTranscription: (text: string) => void;
  toogleNeuralRecording: (callback: () => void) => () => void;

  setDeepgramLanguage: (lang: string) => void;

  // üìù Method to send prompt updates directly
  sendPromptUpdate: (
    type: "partial" | "complete" | "error",
    content: string
  ) => void;

  // Pinecone IPC methods
  queryPinecone: (
    embedding: number[],
    topK?: number,
    keywords?: string[],
    filters?: Record<string, unknown>
  ) => Promise<{ matches: Array<{ metadata?: Record<string, unknown> }> }>;
  saveToPinecone: (
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<void>;
  // DuckDB IPC methods (simplified)
  queryDuckDB: (
    embedding: number[],
    limit?: number,
    keywords?: string[],
    filters?: Record<string, unknown>,
    threshold?: number
  ) => Promise<{ matches: DuckDBMatch[] }>;
  saveToDuckDB: (
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<{ success: boolean; error?: string }>;

  // Directory selection for DuckDB path
  selectDirectory: () => Promise<{
    success: boolean;
    path?: string;
    canceled?: boolean;
    error?: string;
  }>;

  // Reinitialize DuckDB with new path
  reinitializeDuckDB: (newPath: string) => Promise<{
    success: boolean;
    error?: string;
  }>;
  importChatHistory: (params: {
    fileBuffer: Buffer | ArrayBuffer | Uint8Array;
    mode: string;
    user: string;
    applicationMode?: string;
    onProgress?: (data: {
      processed: number;
      total: number;
      percentage?: number;
      stage?: string;
    }) => void;
  }) => Promise<{
    success: boolean;
    error?: string;
    imported?: number;
    skipped?: number;
  }>;

  // VLLM APIs
  vllm?: {
    modelStatus: () => Promise<{
      success: boolean;
      status?: VllmStatus;
      error?: string;
    }>;
    startModel: (
      modelId: string
    ) => Promise<{ success: boolean; error?: string }>;
    generate: (
      payload: any
    ) => Promise<{ success: boolean; data?: any; error?: string }>;
    stopModel: () => Promise<{ success: boolean; error?: string }>;
    downloadModelOnly: (
      modelId: string
    ) => Promise<{ success: boolean; error?: string }>;
  };

  // Ollama APIs
  ollama?: {
    listModels: () => Promise<
      Array<{ name: string; id: string; size?: string }>
    >;
    pullModel: (
      modelId: string
    ) => Promise<{ success: boolean; error?: string }>;
    deleteModel: (
      modelId: string
    ) => Promise<{ success: boolean; error?: string }>;
    generate: (options: {
      model: string;
      prompt: string;
      stream?: boolean;
      temperature?: number;
      max_tokens?: number;
    }) => Promise<{ success: boolean; response?: string; error?: string }>;
    chat: (options: {
      model: string;
      messages: Array<{ role: string; content: string }>;
      stream?: boolean;
      temperature?: number;
      max_tokens?: number;
    }) => Promise<{ success: boolean; response?: string; error?: string }>;
    embeddings: (options: {
      model: string;
      prompt: string;
    }) => Promise<{ success: boolean; embedding?: number[]; error?: string }>;
    isRunning: () => Promise<{
      success: boolean;
      running?: boolean;
      error?: string;
    }>;
  };

  // Legacy VLLM methods (deprecated, use vllm.* instead)
  vllmModelStatus: () => Promise<{
    success: boolean;
    status?: VllmStatus;
    error?: string;
  }>;
  vllmStartModel: (
    modelId: string
  ) => Promise<{ success: boolean; error?: string }>;
  vllmGenerate: (
    payload: any
  ) => Promise<{ success: boolean; data?: any; error?: string }>;
  vllmStopModel: () => Promise<{ success: boolean; error?: string }>;
  listModels(): Promise<OllamaModel[]>;
  getAvailableModels(): Promise<OllamaModel[]>;
  downloadModel(
    modelId: string,
    onProgress?: (progress: number, speed: string, eta: string) => void
  ): Promise<boolean>;
  cancelDownload(modelId: string): Promise<void>;
  removeModel(modelId: string): Promise<void>;
  testConnection(): Promise<{
    success: boolean;
    message?: string;
    error?: string;
  }>;
}

declare global {
  interface Window {
    electronAPI: ElectronAPI;
    electron: {
      ipcRenderer: {
        on: (channel: string, func: (...args: unknown[]) => void) => void;
        removeListener: (
          channel: string,
          func: (...args: unknown[]) => void
        ) => void;
      };
    };
    __LANGUAGE__: string;
    signalMonitoringInterval: NodeJS.Timeout;
    audioSignalDetected: boolean;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import type { ElectronAPI } from './electron';

declare global {
  interface Window {
    electronAPI: ElectronAPI;
    electron: {
      ipcRenderer: {
        on: (channel: string, func: (...args: unknown[]) => void) => void;
        removeListener: (channel: string, func: (...args: unknown[]) => void) => void;
      };
    };
    ipcRenderer?: {
      on: (channel: string, func: (...args: unknown[]) => void) => void;
      removeListener: (channel: string, func: (...args: unknown[]) => void) => void;
    };
    on?: (channel: string, func: (...args: unknown[]) => void) => void;
    removeListener?: (channel: string, func: (...args: unknown[]) => void) => void;
    __LANGUAGE__: string;
    signalMonitoringInterval: NodeJS.Timeout;
    audioSignalDetected: boolean;
  }
}

export {};// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

export interface BatchItem {
  id: string;
  text: string;
  role: string;
  order: number;
  part?: string;
  hash: string;
  metadata: Record<string, string | number | boolean | string[]>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Utilit√°rios para trabalhar com modelos de embedding
 * Implementa princ√≠pio DRY (Don't Repeat Yourself)
 */

/**
 * Retorna a dimensionalidade de um modelo de embedding com base em seu nome
 * Suporta apenas os modelos oficialmente dispon√≠veis na UI do Orch-OS
 * @param model Nome do modelo (Hugging Face ou OpenAI)
 * @returns N√∫mero de dimens√µes do embedding
 */
export const getModelDimensions = (model: string): number => {
  // ONNX Community models - novos modelos ONNX otimizados dispon√≠veis na UI
  if (model.includes("all-MiniLM-L6-v2")) return 384;

  // Default para modelos desconhecidos - dimens√£o intermedi√°ria mais comum
  return 768;
};
// SPDX-License-Identifier: MIT OR Apache-2.0
// Utility helpers for HuggingFace tools conversion / parsing
// Centralizes logic so every service uses the same code path.

export interface GenericToolDefinition {
  type?: string;
  function: {
    name: string;
    description?: string;
    parameters?: Record<string, unknown>;
  };
}

export interface HuggingFaceToolDefinition {
  type: string;
  function: {
    name: string;
    description: string;
    parameters: Record<string, unknown>;
  };
}

/**
 * Convert generic tool definitions (possibly from OpenAI style) into the
 * structure expected by HuggingFace function-calling prompt templates.
 * Guarantees the presence of the "type" field (defaults to "function").
 */
export function toHuggingFaceTools(
  tools: GenericToolDefinition[] | undefined | null
): HuggingFaceToolDefinition[] {
  if (!Array.isArray(tools) || tools.length === 0) return [];
  return tools.map((tool) => ({
    type: tool.type || "function",
    function: {
      name: tool.function.name,
      description: tool.function.description ?? "",
      parameters: tool.function.parameters ?? {},
    },
  }));
}

// ------------ Parsing Helpers ---------------

export interface ParsedToolCall {
  function: {
    name: string;
    arguments: string;
  };
}

function tryParseJSON(text: string): any | null {
  try {
    return JSON.parse(text);
  } catch {
    return null;
  }
}

/**
 * Extracts tool calls from model output text or JSON. It looks for JSON code
 * blocks or inline JSON objects and returns them as ParsedToolCall[]
 */
export function extractToolCalls(output: string): ParsedToolCall[] {
  const toolCalls: ParsedToolCall[] = [];

  if (!output) return toolCalls;

  const jsonBlockRegex = /```json([\s\S]*?)```/gi;
  const jsonSnippetRegex = /\{[\s\S]*?\}/g; // naive but works for simple outputs

  const candidates: string[] = [];

  // Collect fenced JSON blocks
  let match: RegExpExecArray | null;
  while ((match = jsonBlockRegex.exec(output)) !== null) {
    candidates.push(match[1]);
  }

  // Collect inline snippets
  const inline = output.match(jsonSnippetRegex);
  if (inline) candidates.push(...inline);

  for (const snippet of candidates) {
    const parsed = tryParseJSON(snippet);
    if (!parsed) continue;

    // Support both direct function format and wrapper with .function
    if (parsed && (parsed.name || parsed.function?.name)) {
      const name = parsed.function?.name || parsed.name;
      const args = parsed.function?.arguments || parsed.arguments || {};
      toolCalls.push({
        function: {
          name: String(name),
          arguments: typeof args === "string" ? args : JSON.stringify(args),
        },
      });
    }
  }

  return toolCalls;
}
import { pipeline } from "@huggingface/transformers";

// Fun√ß√£o modificada para sempre escolher wasm como dispositivo e dtype autom√°tico
export async function getOptimalDeviceConfig() {
  return { device: "wasm", dtype: "fp32" }; // Usar fp32 por padr√£o para compatibilidade
}

// Configura√ß√µes espec√≠ficas por modelo
const MODEL_SPECIFIC_CONFIGS: Record<string, Record<string, any>> = {
  "Xenova/distilgpt2": {
    use_external_data_format: false, // Modelo pequeno, n√£o precisa
    dtype: "fp32",
    device: "wasm", // For√ßar wasm para compatibilidade
  },
  "Xenova/gpt2": {
    use_external_data_format: true, // Modelo maior, precisa de external data
    dtype: "fp32",
    device: "wasm", // For√ßar wasm para compatibilidade
  },
  "Xenova/llama2.c-stories15M": {
    use_external_data_format: false, // Modelo muito pequeno
    dtype: "fp32",
    device: "wasm", // For√ßar wasm para compatibilidade
  },
  "Xenova/TinyLlama-1.1B-Chat-v1.0": {
    use_external_data_format: true, // Modelo grande, precisa de external data
    dtype: "fp32",
    device: "wasm", // For√ßar wasm para compatibilidade
  },
};

// Main loading method ‚Äî mantido!
export async function loadModelWithOptimalConfig(
  modelId: string,
  task: string,
  additionalOptions: Record<string, any> = {}
) {
  console.log(`[TransformersEnv] üîç DEBUG: Input parameters:`, {
    modelId,
    task,
    additionalOptions,
  });

  const deviceConfig = await getOptimalDeviceConfig();
  console.log(`[TransformersEnv] üîç DEBUG: Device config:`, deviceConfig);

  // Obter configura√ß√µes espec√≠ficas do modelo
  const modelSpecificConfig = MODEL_SPECIFIC_CONFIGS[modelId] || {};
  console.log(
    `[TransformersEnv] üîç DEBUG: Model-specific config for ${modelId}:`,
    modelSpecificConfig
  );

  // Filter out conflicting options from additionalOptions
  const filteredAdditionalOptions = Object.fromEntries(
    Object.entries(additionalOptions).filter(
      ([key]) => !["device", "dtype", "use_external_data_format"].includes(key)
    )
  );
  console.log(
    `[TransformersEnv] üîç DEBUG: Filtered additional options:`,
    filteredAdditionalOptions
  );

  // Configura√ß√µes base para modelos browser-compatible
  const baseOptions: Record<string, any> = {
    // 1. FORCE correct configurations - ignore any incorrect additionalOptions
    device: "wasm", // Always use wasm for browser compatibility
    dtype: "fp32", // Always use fp32 - model-specific configs will override if needed
    local_files_only: false, // Permitir download se n√£o estiver em cache

    // 2. Configura√ß√µes de sess√£o para melhor performance
    session_options: {
      logSeverityLevel: 3, // Reduzir logs
      graphOptimizationLevel: "all" as const,
      enableMemPattern: true,
      enableCpuMemArena: true,
      // Execution providers em ordem de prefer√™ncia
      executionProviders: ["wasm"],
    },

    // 3. Callback de progresso para debugging
    progress_callback: (data: any) => {
      if (data.status === "downloading") {
        console.log(
          `[TransformersEnv] Downloading: ${
            data.name || data.file
          } - ${Math.round(data.progress || 0)}%`
        );
      } else if (data.status === "loading") {
        console.log(`[TransformersEnv] Loading: ${data.name || data.file}`);
      } else if (data.status === "ready") {
        console.log(`[TransformersEnv] Ready: ${data.name || data.file}`);
      }
    },

    // 4. Aplicar op√ß√µes adicionais (EXCLUINDO device/dtype/use_external_data_format que podem conflitar)
    ...filteredAdditionalOptions,

    // 5. FINAL: Configura√ß√µes espec√≠ficas do modelo t√™m PRECED√äNCIA M√ÅXIMA
    ...modelSpecificConfig,
  };

  console.log(
    `[TransformersEnv] üîç DEBUG: Base options before model-specific override:`,
    {
      device: baseOptions.device,
      dtype: baseOptions.dtype,
      use_external_data_format: baseOptions.use_external_data_format,
    }
  );

  // CRITICAL: Force correct values one more time to ensure no overrides
  if (modelSpecificConfig.device) {
    baseOptions.device = modelSpecificConfig.device;
  } else {
    baseOptions.device = "wasm";
  }

  if (modelSpecificConfig.dtype) {
    baseOptions.dtype = modelSpecificConfig.dtype;
  } else {
    baseOptions.dtype = "fp32";
  }

  if (modelSpecificConfig.use_external_data_format !== undefined) {
    baseOptions.use_external_data_format =
      modelSpecificConfig.use_external_data_format;
  }

  console.log(`[TransformersEnv] üîç DEBUG: Final forced values:`, {
    device: baseOptions.device,
    dtype: baseOptions.dtype,
    use_external_data_format: baseOptions.use_external_data_format,
  });

  console.log(`[TransformersEnv] Loading model ${modelId} with options:`, {
    device: baseOptions.device,
    dtype: baseOptions.dtype,
    use_external_data_format: baseOptions.use_external_data_format,
    local_files_only: baseOptions.local_files_only,
    modelSpecific: modelSpecificConfig,
  });

  console.log(
    `[TransformersEnv] üîç DEBUG: Final baseOptions object:`,
    baseOptions
  );

  // FINAL VALIDATION: Ensure critical values are correct
  if (baseOptions.device !== "wasm") {
    console.error(
      `[TransformersEnv] ‚ùå CRITICAL ERROR: device is ${baseOptions.device}, forcing to wasm`
    );
    baseOptions.device = "wasm";
  }

  if (baseOptions.dtype !== "fp32") {
    console.error(
      `[TransformersEnv] ‚ùå CRITICAL ERROR: dtype is ${baseOptions.dtype}, forcing to fp32`
    );
    baseOptions.dtype = "fp32";
  }

  if (
    modelId === "Xenova/llama2.c-stories15M" &&
    baseOptions.use_external_data_format !== false
  ) {
    console.error(
      `[TransformersEnv] ‚ùå CRITICAL ERROR: use_external_data_format is ${baseOptions.use_external_data_format} for ${modelId}, forcing to false`
    );
    baseOptions.use_external_data_format = false;
  }

  console.log(`[TransformersEnv] üîç DEBUG: FINAL VALIDATED OPTIONS:`, {
    device: baseOptions.device,
    dtype: baseOptions.dtype,
    use_external_data_format: baseOptions.use_external_data_format,
  });

  return pipeline(task as any, modelId, baseOptions as any);
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { QueryClient, QueryClientProvider } from "@tanstack/react-query";
import { createContext, useContext, useEffect, useRef, useState } from "react";
import {
  DeepgramProvider,
  MicrophoneProvider,
  TranscriptionProvider,
} from "./components/context";
import { CognitionLogProvider } from "./components/context/CognitionLogContext";
import { LanguageProvider } from "./components/context/LanguageContext";
import { initializeQuantumPerformanceOptimizations } from "./components/shared/QuantumVisualization/utils/performance";
import {
  useGlobalPassiveEventOptimization,
  useHeavyTaskManager,
} from "./components/shared/QuantumVisualization/utils/performanceOptimizations";
import {
  Toast,
  ToastDescription,
  ToastMessage,
  ToastProvider,
  ToastTitle,
  ToastVariant,
  ToastViewport,
} from "./components/ui/toast";
import TranscriptionModule from "./features/transcription/TranscriptionModule";
import "./styles/orchos-theme.css";

const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 0,
      gcTime: Infinity,
      retry: 1,
      refetchOnWindowFocus: false,
    },
    mutations: {
      retry: 1,
    },
  },
});

interface ToastContextType {
  showToast: (
    title: string,
    description: string,
    variant: ToastVariant
  ) => void;
}

export const ToastContext = createContext<ToastContextType | undefined>(
  undefined
);

export function useToast() {
  const context = useContext(ToastContext);
  if (!context) {
    throw new Error("useToast must be used within a ToastProvider");
  }
  return context;
}

// Helper function to safely update language
export function updateLanguage(newLanguage: string) {
  async () => {
    window.__LANGUAGE__ = newLanguage;
  };
}

// This component has been moved to TranscriptionModule

export default function App() {
  const containerRef = useRef<HTMLDivElement>(null);
  const [toastOpen, setToastOpen] = useState(false);
  const [toastMessage, setToastMessage] = useState<ToastMessage>({
    title: "",
    description: "",
    variant: "neutral",
  });

  // Initialize performance optimizations and ONNX Runtime config once at application startup
  useEffect(() => {
    const cleanup = initializeQuantumPerformanceOptimizations();

    return cleanup;
  }, []);

  // Apply global passive event optimization to resolve OrbitControls violations
  useGlobalPassiveEventOptimization();

  // Apply heavy task management to coordinate HuggingFace + 3D rendering
  const { queueHeavyTask } = useHeavyTaskManager();

  const showToast = (
    title: string,
    description: string,
    variant: ToastVariant
  ) => {
    setToastMessage({ title, description, variant });
    setToastOpen(true);
  };

  // Provide heavy task manager to child components via context if needed
  const contextValue = {
    showToast,
    queueHeavyTask, // Make available for HuggingFace service coordination
  };

  return (
    <div ref={containerRef} className="h-screen w-full overflow-hidden">
      <QueryClientProvider client={queryClient}>
        <LanguageProvider>
          <TranscriptionProvider>
            <MicrophoneProvider>
              <DeepgramProvider>
                <CognitionLogProvider>
                  <ToastProvider>
                    <ToastContext.Provider value={contextValue}>
                      <TranscriptionModule />
                    </ToastContext.Provider>
                    <Toast
                      open={toastOpen}
                      onOpenChange={setToastOpen}
                      variant={toastMessage.variant}
                      duration={3000}
                    >
                      <ToastTitle>{toastMessage.title}</ToastTitle>
                      <ToastDescription>
                        {toastMessage.description}
                      </ToastDescription>
                    </Toast>
                    <ToastViewport />
                  </ToastProvider>
                </CognitionLogProvider>
              </DeepgramProvider>
            </MicrophoneProvider>
          </TranscriptionProvider>
        </LanguageProvider>
      </QueryClientProvider>
    </div>
  );
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { DuckDBMatch } from "../electron/vector-database/interfaces/IVectorDatabase";

// <reference types="vite/client" />

type VllmStatus =
  import("../electron/preload/interfaces/IElectronAPI").VllmStatus;

interface ElectronAPI {
  openExternal: (url: string) => void;
  toggleMainWindow: () => Promise<{ success: boolean; error?: string }>;
  getPlatform: () => string;
  openTranscriptionTooltip: (callback: () => void) => () => void;
  startTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  stopTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  sendNeuralPrompt: (
    temporaryContext?: string
  ) => Promise<{ success: boolean; error?: string }>;
  clearNeuralTranscription: () => Promise<{ success: boolean; error?: string }>;
  onRealtimeTranscription: (callback: (data: string) => void) => () => void;
  onNeuralStarted: (callback: () => void) => () => void;
  onNeuralStopped: (callback: () => void) => () => void;
  onNeuralError: (callback: (error: string) => void) => () => void;
  onPromptSend: (callback: () => void) => () => void;
  onPromptSending: (callback: () => void) => () => void;
  onPromptPartialResponse: (callback: (data: string) => void) => () => void;
  onPromptSuccess: (callback: (data: string) => void) => () => void;
  onPromptError: (callback: (error: string) => void) => () => void;
  onClearTranscription: (callback: () => void) => () => void;
  onSendChunk: (callback: (chunk: ArrayBuffer) => void) => () => void;
  getEnv: (key: string) => Promise<string | null>;
  sendAudioChunk: (
    chunk: Uint8Array
  ) => Promise<{ success: boolean; error?: string }>;
  sendAudioTranscription: (text: string) => void;
  toogleNeuralRecording: (callback: () => void) => () => void;
  onForceStyle: (callback: (style: string) => void) => () => void;
  onForceImprovisation: (callback: () => void) => () => void;
  onRepeatResponse: (callback: () => void) => () => void;
  onStopTTS: (callback: () => void) => () => void;
  setDeepgramLanguage: (lang: string) => void;
  sendPromptUpdate: (
    type: "partial" | "complete" | "error",
    content: string
  ) => void;
  // Pinecone methods removed - using DuckDB only
  queryDuckDB: (
    embedding: number[],
    topK?: number,
    keywords?: string[],
    filters?: Record<string, unknown>
  ) => Promise<{ matches: DuckDBMatch[] }>;
  saveToDuckDB: (
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<{ success: boolean; error?: string }>;
  selectDirectory: () => Promise<{
    success: boolean;
    path?: string;
    canceled?: boolean;
    error?: string;
  }>;

  // Reinitialize DuckDB with new path
  reinitializeDuckDB: (newPath: string) => Promise<{
    success: boolean;
    error?: string;
  }>;
  importChatHistory: (params: {
    fileBuffer: Buffer | ArrayBuffer | Uint8Array;
    mode: string;
    user: string;
    applicationMode?: string;
    onProgress?: (data: {
      processed: number;
      total: number;
      percentage?: number;
      stage?: string;
    }) => void;
  }) => Promise<{
    success: boolean;
    error?: string;
    imported?: number;
    skipped?: number;
  }>;
  vllmModelStatus: () => Promise<{
    success: boolean;
    status?: VllmStatus;
    error?: string;
  }>;
  vllmStartModel: (
    modelId: string
  ) => Promise<{ success: boolean; error?: string }>;
  vllmGenerate: (
    payload: any
  ) => Promise<{ success: boolean; data?: any; error?: string }>;
  vllmStopModel: () => Promise<{ success: boolean; error?: string }>;
  listModels(): Promise<OllamaModel[]>;
  getAvailableModels(): Promise<OllamaModel[]>;
  downloadModel(
    modelId: string,
    onProgress?: (progress: number, speed: string, eta: string) => void
  ): Promise<boolean>;
  cancelDownload(modelId: string): Promise<void>;
  removeModel(modelId: string): Promise<void>;
  testConnection(): Promise<{
    success: boolean;
    message?: string;
    error?: string;
  }>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { DuckDBMatch } from "../electron/vector-database/interfaces/IVectorDatabase";

// <reference types="vite/client" />

type VllmStatus =
  import("../electron/preload/interfaces/IElectronAPI").VllmStatus;

interface ElectronAPI {
  openExternal: (url: string) => void;
  toggleMainWindow: () => Promise<{ success: boolean; error?: string }>;
  getPlatform: () => string;
  openTranscriptionTooltip: (callback: () => void) => () => void;
  startTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  stopTranscriptNeural: () => Promise<{ success: boolean; error?: string }>;
  sendNeuralPrompt: (
    temporaryContext?: string
  ) => Promise<{ success: boolean; error?: string }>;
  clearNeuralTranscription: () => Promise<{ success: boolean; error?: string }>;
  onRealtimeTranscription: (callback: (data: string) => void) => () => void;
  onNeuralStarted: (callback: () => void) => () => void;
  onNeuralStopped: (callback: () => void) => () => void;
  onNeuralError: (callback: (error: string) => void) => () => void;
  onPromptSend: (callback: () => void) => () => void;
  onPromptSending: (callback: () => void) => () => void;
  onPromptPartialResponse: (callback: (data: string) => void) => () => void;
  onPromptSuccess: (callback: (data: string) => void) => () => void;
  onPromptError: (callback: (error: string) => void) => () => void;
  onClearTranscription: (callback: () => void) => () => void;
  onSendChunk: (callback: (chunk: ArrayBuffer) => void) => () => void;
  getEnv: (key: string) => Promise<string | null>;
  sendAudioChunk: (
    chunk: Uint8Array
  ) => Promise<{ success: boolean; error?: string }>;
  sendAudioTranscription: (text: string) => void;
  toogleNeuralRecording: (callback: () => void) => () => void;
  onForceStyle: (callback: (style: string) => void) => () => void;
  onForceImprovisation: (callback: () => void) => () => void;
  onRepeatResponse: (callback: () => void) => () => void;
  onStopTTS: (callback: () => void) => () => void;
  setDeepgramLanguage: (lang: string) => void;
  sendPromptUpdate: (
    type: "partial" | "complete" | "error",
    content: string
  ) => void;
  // Pinecone methods removed - using DuckDB only
  queryDuckDB: (
    embedding: number[],
    topK?: number,
    keywords?: string[],
    filters?: Record<string, unknown>
  ) => Promise<{ matches: DuckDBMatch[] }>;
  saveToDuckDB: (
    vectors: Array<{
      id: string;
      values: number[];
      metadata: Record<string, unknown>;
    }>
  ) => Promise<{ success: boolean; error?: string }>;
  selectDirectory: () => Promise<{
    success: boolean;
    path?: string;
    canceled?: boolean;
    error?: string;
  }>;

  // Reinitialize DuckDB with new path
  reinitializeDuckDB: (newPath: string) => Promise<{
    success: boolean;
    error?: string;
  }>;
  importChatHistory: (params: {
    fileBuffer: Buffer | ArrayBuffer | Uint8Array;
    mode: string;
    user: string;
    applicationMode?: string;
    onProgress?: (data: {
      processed: number;
      total: number;
      percentage?: number;
      stage?: string;
    }) => void;
  }) => Promise<{
    success: boolean;
    error?: string;
    imported?: number;
    skipped?: number;
  }>;
  vllmModelStatus: () => Promise<{
    success: boolean;
    status?: VllmStatus;
    error?: string;
  }>;
  vllmStartModel: (
    modelId: string
  ) => Promise<{ success: boolean; error?: string }>;
  vllmGenerate: (
    payload: any
  ) => Promise<{ success: boolean; data?: any; error?: string }>;
  vllmStopModel: () => Promise<{ success: boolean; error?: string }>;
  listModels(): Promise<OllamaModel[]>;
  getAvailableModels(): Promise<OllamaModel[]>;
  downloadModel(
    modelId: string,
    onProgress?: (progress: number, speed: string, eta: string) => void
  ): Promise<boolean>;
  cancelDownload(modelId: string): Promise<void>;
  removeModel(modelId: string): Promise<void>;
  testConnection(): Promise<{
    success: boolean;
    message?: string;
    error?: string;
  }>;
}
/* SPDX-License-Identifier: MIT OR Apache-2.0
 * Copyright (c) 2025 Guilherme Ferrari Brescia
 */

@import "./styles/orchos-theme.css";
@tailwind base;
@tailwind components;
@tailwind utilities;// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react"
import ReactDOM from "react-dom/client"
import App from "./App"
import "./index.css"

ReactDOM.createRoot(document.getElementById("root")!).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
)
