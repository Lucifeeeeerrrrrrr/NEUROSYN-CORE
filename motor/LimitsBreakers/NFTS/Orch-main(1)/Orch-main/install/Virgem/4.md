 async startProcessing(): Promise<void> {
    LoggingUtils.logInfo("Starting transcription processing");
    return Promise.resolve();
  }

  async stopProcessing(): Promise<void> {
    LoggingUtils.logInfo("Stopping transcription processing");
    return Promise.resolve();
  }

  setModel(model: string): void {
    if (this.model !== model) {
      this.model = model;
      LoggingUtils.logInfo(`Model defined for: ${model}`);
    }
  }

  toggleInterimResults(enabled: boolean): void {
    this.interimResultsEnabled = enabled;
    LoggingUtils.logInfo(
      `Interim results: ${enabled ? "enabled" : "disabled"}`
    );
  }

  // ... (rest of the code remains the same)
  reset(): void {
    LoggingUtils.logInfo("Resetting transcription state");
    this.clearTranscriptionData();
    this.transcriptionPromptProcessor.reset();
  }

  isConnected(): boolean {
    return false;
  }

  /**
   * Enable or disable automatic detection of questions for auto-triggering prompts
   */
  setAutoQuestionDetection(enabled: boolean): void {
    if (this.storageService instanceof TranscriptionStorageService) {
      this.storageService.setAutoQuestionDetection(enabled);
      LoggingUtils.logInfo(
        `Auto-question detection ${enabled ? "enabled" : "disabled"}`
      );
    }
  }

  /**
   * Flush all accumulated transcriptions to the UI
   * Should be called when recording stops or when sending a message
   */
  flushTranscriptionsToUI(): void {
    this.storageService.flushTranscriptionsToUI();
    LoggingUtils.logInfo("Flushing transcriptions to UI");
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// NeuralSignalExtractor.ts
// Module responsible for extracting symbolic neural signals from the transcription context

import { NeuralSignalResponse } from "../../interfaces/neural/NeuralSignalTypes";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { LoggingUtils } from "../../utils/LoggingUtils";
import {
  INeuralSignalExtractor,
  NeuralExtractionConfig,
} from "../interfaces/INeuralSignalExtractor";

/**
 * Class responsible for extracting symbolic neural signals from the transcription context
 * This is the first impulse of an artificial symbolic mind
 * Automatically selects between Ollama (advanced mode) and HuggingFace (basic mode) services
 */
export class NeuralSignalExtractor implements INeuralSignalExtractor {
  private readonly aiService: IOpenAIService;

  /**
   * Constructor
   * @param aiService AI service for communication with the language model (OpenAI or HuggingFace facade)
   */
  constructor(aiService: IOpenAIService) {
    this.aiService = aiService;
  }

  /**
   * Extracts symbolic neural signals from the current context
   * This is the first impulse of an artificial symbolic mind
   * @param config Configuration containing the current context
   * @returns Array of neural signals representing the activation of the symbolic brain
   */
  public async extractNeuralSignals(
    config: NeuralExtractionConfig
  ): Promise<NeuralSignalResponse> {
    try {
      // The config should already have transcription and userContextData ready!
      const {
        transcription,
        temporaryContext,
        userContextData = {},
        sessionState = {},
      } = config;

      // Determine language from session state
      const language =
        sessionState &&
        typeof sessionState === "object" &&
        "language" in sessionState
          ? (sessionState.language as string)
          : undefined;

      LoggingUtils.logInfo(
        "🧠 [NeuralSignalExtractor] Starting neural signal extraction..."
      );
      LoggingUtils.logInfo(
        `🧠 [NeuralSignalExtractor] Config: ${JSON.stringify({
          transcriptionLength: transcription.length,
          transcriptionPreview: transcription.substring(0, 100) + "...",
          hasTemporaryContext: !!temporaryContext,
          temporaryContextLength: temporaryContext?.length || 0,
          userContextDataKeys: Object.keys(userContextData),
          language,
          aiServiceType: this.aiService.constructor.name,
          aiServiceInitialized: this.aiService.isInitialized?.() ?? "unknown",
        })}`
      );

      // Prepare an enriched prompt with all available contextual data
      const enrichedPrompt = this.prepareEnrichedPrompt(
        transcription,
        userContextData
      );

      LoggingUtils.logInfo(
        `🧠 [NeuralSignalExtractor] Enriched prompt prepared: ${JSON.stringify({
          enrichedPromptLength: enrichedPrompt.length,
          enrichedPromptPreview: enrichedPrompt.substring(0, 200) + "...",
        })}`
      );

      // Verify AI service availability before proceeding
      if (this.aiService.ensureOpenAIClient) {
        const isReady = await this.aiService.ensureOpenAIClient();
        if (!isReady) {
          LoggingUtils.logError(
            "🧠 [NeuralSignalExtractor] AI service is not ready. Using fallback signals."
          );
          return this.generateFallbackSignals(transcription);
        }
      }

      LoggingUtils.logInfo(
        "🧠 [NeuralSignalExtractor] Calling generateNeuralSignal..."
      );

      // Generate neural signals adapted for memory queries (works with both Pinecone and DuckDB)
      const neuralResponse = await this.aiService.generateNeuralSignal(
        enrichedPrompt, // Enriched stimulus with user context
        temporaryContext,
        language // Pass language from session state
      );

      LoggingUtils.logInfo(
        `🧠 [NeuralSignalExtractor] Neural response received: ${JSON.stringify({
          hasSignals: !!neuralResponse.signals,
          signalsLength: neuralResponse.signals?.length || 0,
          signalsPreview:
            neuralResponse.signals?.map((s) => ({
              core: s?.core,
              intensity: s?.intensity,
              hasQuery: !!s?.symbolic_query?.query,
            })) || [],
        })}`
      );

      // Verify if the response contains valid signals
      if (!neuralResponse.signals || neuralResponse.signals.length === 0) {
        LoggingUtils.logWarning(
          "🧠 [NeuralSignalExtractor] No neural signals were generated. Using default signals."
        );

        return this.generateFallbackSignals(transcription);
      }

      // DEBUG: Log raw signals before validation
      LoggingUtils.logInfo(
        `🧠 [NeuralSignalExtractor] Raw signals before validation: ${JSON.stringify(
          neuralResponse.signals.map((s) => ({
            core: s?.core,
            intensity: s?.intensity,
            hasQuery: !!s?.symbolic_query?.query,
            hasSymbolicInsights: !!s?.symbolicInsights,
            queryLength: s?.symbolic_query?.query?.length || 0,
          }))
        )}`
      );

      // ENHANCED VALIDATION: More flexible validation for debugging
      const validSignals = neuralResponse.signals.filter((signal) => {
        const isValid =
          signal &&
          signal.core &&
          typeof signal.intensity === "number" &&
          signal.intensity >= 0 &&
          signal.intensity <= 1;

        if (!isValid) {
          LoggingUtils.logWarning(
            `🧠 [NeuralSignalExtractor] Invalid signal filtered out: ${JSON.stringify(
              signal
            )}`
          );
        }

        return isValid;
      });

      if (validSignals.length === 0) {
        LoggingUtils.logWarning(
          "🧠 [NeuralSignalExtractor] All signals were invalid after validation. Using fallback signals."
        );
        return this.generateFallbackSignals(transcription);
      }

      LoggingUtils.logInfo(
        `✅ [NeuralSignalExtractor] Successfully validated ${validSignals.length} neural signals`
      );

      return { signals: validSignals };
    } catch (error) {
      // In case of error, log and provide a fallback response
      LoggingUtils.logError(
        "🧠 [NeuralSignalExtractor] Error extracting neural signals",
        error as Error
      );

      return this.generateFallbackSignals(config.transcription);
    }
  }

  /**
   * Generates fallback neural signals when the main extraction fails
   * @param transcription The original transcription text
   * @returns Fallback neural signal response
   */
  private generateFallbackSignals(transcription: string): NeuralSignalResponse {
    LoggingUtils.logInfo(
      "🧠 [NeuralSignalExtractor] Generating fallback neural signals..."
    );

    return {
      signals: [
        {
          core: "memory",
          intensity: 0.8,
          symbolic_query: {
            query: `memories related to: ${transcription.substring(0, 100)}`,
          },
          symbolicInsights: {
            recall_type: "semantic",
            temporal: "recent",
            importance: "high",
          },
        },
        {
          core: "metacognitive",
          intensity: 0.7,
          symbolic_query: {
            query: `reflection on: ${transcription.substring(0, 100)}`,
          },
          symbolicInsights: {
            thought: "Processing cognitive stimulus",
            state: "conscious",
          },
        },
        {
          core: "valence",
          intensity: 0.6,
          symbolic_query: {
            query: `emotions about: ${transcription.substring(0, 100)}`,
          },
          symbolicInsights: {
            emotion: "neutral",
            intensity: "moderate",
          },
        },
      ],
    };
  }

  /**
   * Prepares an enriched prompt with full user context.
   * @param originalPrompt The user's original prompt
   * @param userContextData Contextual data related to the user
   * @returns A contextually enriched symbolic/psychoanalytic prompt
   */
  private prepareEnrichedPrompt(
    originalPrompt: string,
    userContextData: Record<string, unknown>
  ): string {
    const styleInstruction =
      "STYLE INSTRUCTION: Only use greetings and personal references when the user's content clearly justifies it — never automatically.";

    // Base symbolic instruction with enhanced quantum and multi-level consciousness dimensions
    const symbolicInstruction = `INSTRUCTION: Analyze the user's message and context in a quantum-symbolic framework to identify:

1. EXPLICIT AND IMPLICIT ELEMENTS:
   - Keywords, emotional themes, symbols, archetypes, dilemmas, and unconscious patterns
   - Potential quantum states of meaning in superposition (multiple interpretations coexisting)
   - Signs of instructional collapse (where multiple potential meanings converge)

2. MULTI-LEVEL CONSCIOUSNESS:
   - Surface level: Immediate conscious content and stated intentions
   - Intermediate level: Partially conscious patterns, emotional undercurrents
   - Deep level: Unconscious material, potential symbolic resonance, dormant insights

3. ARCHETYPAL RESONANCE AND INTERPLAY:
   - Primary archetypes activated in the communication
   - Secondary/shadow archetypes operating in tension or harmony with primary ones
   - Potential dialogue or conflict between different archetypal energies

4. TEMPORAL DIMENSIONS:
   - Past: Echoes, patterns, and unresolved elements influencing present communication
   - Present: Immediate symbolic significance of current expression
   - Future: Potential trajectories, symbolic seeds, emergent possibilities

5. POLARITIES AND PARADOXES:
   - Tensions between opposing symbolic forces
   - Potential integration points for apparently contradictory elements
   - Productive tensions that could lead to emergent understanding

Suggest refined or expanded keywords, queries, and topics that could deepen the symbolic, emotional, and unconscious investigation — even if they are not explicitly verbalized.

Be selective: only expand when there are strong indicators of symbolic or unconscious material.

Prioritize expressions and themes that reveal tensions, paradoxes, hidden desires, blockages, or deep self-knowledge potential that exist in quantum superposition awaiting conscious observation.`;

    // If no user context exists, return the basic symbolic enrichment prompt
    if (Object.keys(userContextData).length === 0) {
      return `${styleInstruction}\n\n${originalPrompt}\n\n${symbolicInstruction}`;
    }

    // Start building contextualized prompt
    let contextualPrompt = `${styleInstruction}\n\n${originalPrompt}`;

    // Add recent symbolic or emotional topics, if present
    if (userContextData.recent_topics) {
      const recentTopics =
        userContextData.recent_topics.toString().substring(0, 200) + "...";
      contextualPrompt += `\n\nRecent topics context: ${recentTopics}`;
    }

    // Add interaction patterns if present
    if (userContextData.speaker_interaction_counts) {
      const interactionPattern = JSON.stringify(
        userContextData.speaker_interaction_counts
      );
      contextualPrompt += `\n\nInteraction pattern: ${interactionPattern}`;
    }

    // Add symbolic instruction + adaptive note
    contextualPrompt += `\n\n${symbolicInstruction}

Note: If other relevant symbolic or emotional patterns are available in the long-term memory or historical user data, feel free to incorporate them into the keyword/query suggestions — as long as they resonate symbolically with the current stimulus.`;

    return contextualPrompt;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  ModeService,
  OrchOSModeEnum,
} from "../../../../../services/ModeService";
import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { HuggingFaceEmbeddingService } from "../../../../../services/huggingface/HuggingFaceEmbeddingService";
import { IEmbeddingService } from "../../interfaces/openai/IEmbeddingService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { HuggingFaceServiceFacade } from "../../services/huggingface/HuggingFaceServiceFacade";
import { OllamaEmbeddingService } from "../../services/ollama/OllamaEmbeddingService";
import { OllamaClientService } from "../../services/ollama/neural/OllamaClientService";
import { OllamaCompletionService } from "../../services/ollama/neural/OllamaCompletionService";
import symbolicCognitionTimelineLogger from "../../services/utils/SymbolicCognitionTimelineLoggerSingleton";
import { SymbolicInsight } from "../../types/SymbolicInsight";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { cleanThinkTags } from "../../utils/ThinkTagCleaner";
import {
  CognitiveMetrics,
  SymbolicPatternAnalyzer,
} from "../patterns/SymbolicPatternAnalyzer";
import { HuggingFaceCollapseStrategyService } from "./HuggingFaceCollapseStrategyService";
import { ICollapseStrategyService } from "./ICollapseStrategyService";
import { INeuralIntegrationService } from "./INeuralIntegrationService";
import { OllamaCollapseStrategyService } from "./OllamaCollapseStrategyService";
import { SuperpositionLayer } from "./SuperpositionLayer";

function asNumber(val: unknown, fallback: number): number {
  return typeof val === "number" ? val : fallback;
}

export class DefaultNeuralIntegrationService
  implements INeuralIntegrationService
{
  private embeddingService: IEmbeddingService;
  private collapseStrategyService: ICollapseStrategyService;
  private patternAnalyzer: SymbolicPatternAnalyzer; // Detector de padrões simbólicos emergentes entre ciclos
  private aiService: IOpenAIService;

  constructor(
    aiService: IOpenAIService,
    private huggingFaceService?: HuggingFaceServiceFacade
  ) {
    this.aiService = aiService;

    // Strategy pattern: choose embedding service based on mode
    this.embeddingService = this.createEmbeddingService(aiService);

    // Strategy pattern: choose collapse strategy service based on mode
    const currentMode = ModeService.getMode();
    if (currentMode === OrchOSModeEnum.BASIC && this.huggingFaceService) {
      LoggingUtils.logInfo(
        "[NeuralIntegration] Using HuggingFace collapse strategy (Basic mode)"
      );
      this.collapseStrategyService = new HuggingFaceCollapseStrategyService(
        this.aiService
      );
    } else {
      LoggingUtils.logInfo(
        "[NeuralIntegration] Using Ollama collapse strategy (Advanced mode)"
      );

      // Create dedicated Ollama services for the collapse strategy
      const ollamaClientService = new OllamaClientService();
      const ollamaCompletionService = new OllamaCompletionService(
        ollamaClientService
      );

      this.collapseStrategyService = new OllamaCollapseStrategyService(
        ollamaCompletionService
      );
    }

    this.patternAnalyzer = new SymbolicPatternAnalyzer();
  }

  /**
   * Creates the appropriate embedding service based on application mode
   * @param aiService The AI service to use for OpenAI embeddings
   * @returns The appropriate embedding service
   */
  private createEmbeddingService(aiService: IOpenAIService): IEmbeddingService {
    const currentMode = ModeService.getMode();

    if (currentMode === OrchOSModeEnum.BASIC) {
      // In basic mode, use HuggingFace with the selected model
      const hfModel = getOption(STORAGE_KEYS.HF_EMBEDDING_MODEL);
      LoggingUtils.logInfo(
        `[NeuralIntegration] Creating HuggingFaceEmbeddingService with model: ${
          hfModel || "default"
        } for Basic mode`
      );
      return new HuggingFaceEmbeddingService();
    } else {
      // In advanced mode, use Ollama with the selected model
      const ollamaModel = getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL);
      LoggingUtils.logInfo(
        `[NeuralIntegration] Creating OllamaEmbeddingService with model: ${
          ollamaModel || "default"
        } for Advanced mode`
      );
      return new OllamaEmbeddingService(aiService, { model: ollamaModel });
    }
  }

  /**
   * Calculates a symbolic phase value based on emotional weight, contradiction score, and coherence
   * Returns a value between 0-2π that represents a phase angle for wave interference
   */
  private calculateSymbolicPhase(
    emotionalWeight: number,
    contradictionScore: number,
    coherence: number
  ): number {
    const baseEmotionPhase = (emotionalWeight * Math.PI) / 2; // Emotions dominate initial phase
    const contradictionPhase = contradictionScore * Math.PI; // Contradictions generate opposition
    const coherenceNoise = (1 - coherence) * (Math.PI / 4); // Low coherence → noise

    // Total phase with 2π wrapping
    const phase =
      (baseEmotionPhase + contradictionPhase + coherenceNoise) % (2 * Math.PI);

    // Ensure phase is positive (0 to 2π range)
    const normalizedPhase = phase < 0 ? phase + 2 * Math.PI : phase;

    console.info(
      `[NeuralIntegration] Calculated symbolic phase: ${normalizedPhase.toFixed(
        3
      )} rad (emotionPhase=${baseEmotionPhase.toFixed(
        2
      )}, contradictionPhase=${contradictionPhase.toFixed(
        2
      )}, coherenceNoise=${coherenceNoise.toFixed(2)})`
    );

    return normalizedPhase;
  }

  /**
   * Neural integration using superposition, non-deterministic collapse and emergent property registration.
   * Now uses real embeddings for each answer via OpenAIEmbeddingService.
   */
  async integrate(
    neuralResults: Array<{
      core: string;
      intensity: number;
      output: string;
      insights: Record<string, unknown>;
    }>,
    originalInput: string,
    language: string = getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "pt-BR" // Default to pt-BR if not provided
  ): Promise<string> {
    if (!neuralResults || neuralResults.length === 0) {
      return originalInput;
    }

    // Clean think tags from neural results outputs before processing
    const cleanedNeuralResults = neuralResults.map((result) => ({
      ...result,
      output: cleanThinkTags(result.output),
    }));

    // 1. Superposition: each result is a possible answer
    const superposition = new SuperpositionLayer();
    for (const result of cleanedNeuralResults) {
      // Generate real embedding for the answer text
      const embedding = await this.embeddingService.createEmbedding(
        result.output
      );
      // Heuristics for emotional weight, coherence and contradiction
      const emotionalWeight = asNumber(
        (result.insights as Record<string, unknown>)?.valence,
        Math.random()
      );
      const narrativeCoherence = asNumber(
        (result.insights as Record<string, unknown>)?.coherence,
        1 - Math.abs(result.intensity - 0.5)
      );
      const contradictionScore = asNumber(
        (result.insights as Record<string, unknown>)?.contradiction,
        Math.random() * 0.5
      );
      superposition.register({
        embedding,
        text: result.output,
        emotionalWeight,
        narrativeCoherence,
        contradictionScore,
        origin: result.core,
        insights: result.insights,
      });
    }
    // 2. Collapse: Use OpenAI-based strategy to decide deterministic vs probabilistic
    const numCandidates = superposition.answers.length;

    // Calculate average values for symbolic properties
    const averageEmotionalWeight =
      cleanedNeuralResults.reduce((sum, r) => {
        return (
          sum + asNumber((r.insights as Record<string, unknown>)?.valence, 0.5)
        );
      }, 0) / cleanedNeuralResults.length;
    const averageContradictionScore =
      cleanedNeuralResults.reduce((sum, r) => {
        return (
          sum +
          asNumber((r.insights as Record<string, unknown>)?.contradiction, 0.25)
        );
      }, 0) / cleanedNeuralResults.length;
    const avgCoherence =
      cleanedNeuralResults.reduce((sum, r) => {
        return (
          sum +
          asNumber((r.insights as Record<string, unknown>)?.coherence, 0.7)
        );
      }, 0) / cleanedNeuralResults.length;

    // We'll use the original input text for the collapse strategy service to infer intent

    // Use our OpenAI-powered strategy service to make the decision
    const strategyDecision =
      await this.collapseStrategyService.decideCollapseStrategy({
        activatedCores: cleanedNeuralResults.map((r) => r.core),
        averageEmotionalWeight,
        averageContradictionScore,
        originalText: originalInput, // Pass the original text to help infer intent
      });

    // Log collapse details for debugging
    console.info(
      `[NeuralIntegration] Collapse strategy decision: ${
        strategyDecision.deterministic ? "Deterministic" : "Probabilistic"
      }, Temperature: ${strategyDecision.temperature}, Reason: ${
        strategyDecision.justification
      }`
    );

    // Log user intent if available
    if (strategyDecision.userIntent) {
      console.info(
        `[NeuralIntegration] Inferred user intent:`,
        JSON.stringify(strategyDecision.userIntent, null, 2)
      );
    }

    // Collect insights from all neural results
    const allInsights = cleanedNeuralResults.flatMap((result) => {
      if (!result.insights) return [];

      const toInsight = (type: string, content: string): SymbolicInsight =>
        ({
          type,
          content: content,
          core: result.core,
        } as SymbolicInsight);

      // For arrays of insights
      if (Array.isArray(result.insights)) {
        return result.insights
          .map((item) => {
            // String insights
            if (typeof item === "string") {
              return toInsight("concept", item);
            }
            // Object insights
            if (item && typeof item === "object") {
              const obj = item as Record<string, unknown>;
              const type = typeof obj.type === "string" ? obj.type : "unknown";
              let content = "";

              if (typeof obj.content === "string") content = obj.content;
              else if (typeof obj.value === "string") content = obj.value;
              else content = String(type);

              return toInsight(type, content);
            }
            return null;
          })
          .filter(Boolean) as SymbolicInsight[];
      }

      // For unique objects
      if (result.insights && typeof result.insights === "object") {
        const obj = result.insights as Record<string, unknown>;

        // With defined type
        if ("type" in obj && typeof obj.type === "string") {
          let content = "";
          if (typeof obj.content === "string") content = obj.content;
          else if (typeof obj.value === "string") content = obj.value;
          else content = obj.type;

          return [toInsight(obj.type, content)];
        }

        // Without defined type - each property becomes an insight
        return Object.entries(obj)
          .filter(([, v]) => v !== null && v !== undefined)
          .map(([k, v]) => toInsight(k, String(v)));
      }

      return [];
    });

    // Execute the collapse based on the strategy decision
    let finalAnswer;

    // Create a default userIntent if none was provided by the collapse strategy
    const defaultUserIntent = {
      social: originalInput.toLowerCase().includes("olá") ? 0.7 : 0.3,
      trivial: originalInput.toLowerCase().includes("tudo bem") ? 0.5 : 0.2,
      reflective: 0.3,
      practical: 0.2,
    };

    // Use the inferred intent or the default one
    const effectiveUserIntent =
      strategyDecision.userIntent || defaultUserIntent;

    // Log the intent that will be used
    console.info(
      `[NeuralIntegration] Using user intent:`,
      JSON.stringify(effectiveUserIntent, null, 2)
    );

    // Calculate average similarity to use for dynamic minCosineDistance
    const avgSimilarity = superposition.calculateAverageCosineSimilarity();

    // Compute dynamic minCosineDistance based on observed similarity
    // Higher similarity -> Higher minCosineDistance to enforce more diversity
    // Lower similarity -> Lower minCosineDistance to avoid over-penalization
    const dynamicMinDistance = Math.min(
      0.2,
      Math.max(0.1, 0.1 + avgSimilarity * 0.1)
    );

    // Log diversity metrics
    console.info(
      `[NeuralIntegration] Average semantic similarity: ${avgSimilarity.toFixed(
        3
      )}`
    );
    console.info(
      `[NeuralIntegration] Using dynamic minCosineDistance: ${dynamicMinDistance.toFixed(
        3
      )}`
    );

    // Calculate symbolic phase based on average emotional weight, contradiction and coherence
    const explicitPhase = this.calculateSymbolicPhase(
      averageEmotionalWeight,
      averageContradictionScore,
      avgCoherence
    );

    // Log the symbolic phase calculation
    console.info(
      `[NeuralIntegration] Using symbolic phase ${explicitPhase.toFixed(
        3
      )} rad (${(explicitPhase / (2 * Math.PI)).toFixed(
        3
      )} cycles) for collapse`
    );

    if (strategyDecision.deterministic) {
      // Execute deterministic collapse with phase interference and explicit phase
      finalAnswer = superposition.collapseDeterministic({
        diversifyByEmbedding: true,
        minCosineDistance: dynamicMinDistance,
        usePhaseInterference: true, // Enable quantum-like phase interference
        explicitPhase: explicitPhase, // Use explicit phase value to bias collapse
      });

      // Log the neural collapse event
      symbolicCognitionTimelineLogger.logNeuralCollapse(
        true, // isDeterministic
        finalAnswer.origin || "unknown", // selectedCore (ensure it's a string)
        numCandidates, // numCandidates
        averageEmotionalWeight, // Emotional weight
        averageContradictionScore, // Contradiction score
        undefined, // No temperature for deterministic collapse
        strategyDecision.justification,
        effectiveUserIntent, // userIntent (guaranteed to have a value)
        allInsights.length > 0 ? allInsights : undefined, // insights from neural results
        strategyDecision.emergentProperties
      );
    } else {
      // Execute probabilistic collapse with the suggested temperature and dynamic parameters
      // Include explicit phase to bias the probabilistic collapse as well
      finalAnswer = superposition.collapse(strategyDecision.temperature, {
        diversifyByEmbedding: true,
        minCosineDistance: dynamicMinDistance,
        explicitPhase: explicitPhase, // Use same explicit phase value for probabilistic collapse
      });

      // Log the neural collapse event
      symbolicCognitionTimelineLogger.logNeuralCollapse(
        false, // isDeterministic
        finalAnswer.origin || "unknown", // selectedCore (ensure it's a string)
        superposition.answers.length, // numCandidates
        finalAnswer.emotionalWeight || 0, // Emotional weight
        finalAnswer.contradictionScore || 0, // Contradiction score
        strategyDecision.temperature, // temperature from strategy
        strategyDecision.justification,
        effectiveUserIntent, // userIntent (guaranteed to have a value)
        allInsights.length > 0 ? allInsights : undefined, // insights from neural results
        strategyDecision.emergentProperties // emergent properties from strategy decision
      );
    }

    // 3. Use emergent properties from the OpenAI function call
    const emergentProperties: string[] =
      strategyDecision.emergentProperties || [];

    // === Orch-OS: Symbolic Pattern Analysis & Memory Integration ===
    // Atualizar o analisador de padrões com o contexto/métricas do ciclo atual
    // Capturar métricas cognitivas completas para análise científica
    const cycleMetrics: CognitiveMetrics = {
      // Métricas fundamentais para detecção de padrões
      contradictionScore:
        finalAnswer.contradictionScore ?? averageContradictionScore,
      coherenceScore: finalAnswer.narrativeCoherence ?? avgCoherence,
      emotionalWeight: finalAnswer.emotionalWeight ?? averageEmotionalWeight,

      // Métricas ampliadas para tese Orch-OS (com valores heurísticos quando não disponíveis)
      archetypalStability:
        cleanedNeuralResults.reduce(
          (sum, r) =>
            sum + asNumber((r.insights as any)?.archetypal_stability, 0.5),
          0
        ) / cleanedNeuralResults.length,
      cycleEntropy: Math.min(1, 0.3 + numCandidates / 10), // Heurística baseada em diversidade de candidatos
      insightDepth: Math.max(
        ...cleanedNeuralResults.map((r) =>
          asNumber((r.insights as any)?.insight_depth, 0.4)
        )
      ),
      phaseAngle: explicitPhase, // Reutilizando ângulo de fase calculado anteriormente
    };

    try {
      // [3. Recursive Memory Update]
      // Registrar contexto atual no analisador de padrões (para detecção entre ciclos)
      // A propriedade text pode não existir diretamente, então usamos toString() para segurança
      const contextText =
        typeof finalAnswer.text === "string"
          ? finalAnswer.text
          : finalAnswer.toString();
      this.patternAnalyzer.recordCyclicData(contextText, cycleMetrics);

      // [4. Pattern Detection Across Cycles]
      // Analisar padrões emergentes (drift, loops, buildup, interferência)
      const emergentPatterns = this.patternAnalyzer.analyzePatterns();

      // [2. Comprehensive Emergent Property Tracking]
      // Converter padrões para formato legível e adicionar às propriedades emergentes
      const patternStrings =
        emergentPatterns.length > 0
          ? this.patternAnalyzer.formatPatterns(emergentPatterns)
          : [];
      if (patternStrings.length > 0) {
        // Adicionar padrões detectados às propriedades emergentes para influenciar o output
        emergentProperties.push(...patternStrings);
        LoggingUtils.logInfo(
          `[NeuralIntegration] Detected ${
            patternStrings.length
          } emergent symbolic patterns: ${patternStrings.join(", ")}`
        );
      }

      // [5. Trial-Based Logging]
      // Register complete patterns and metrics for scientific analysis
      if (patternStrings.length > 0) {
        // Add to emergentProperties of neural collapse (already recorded via logNeuralCollapse)
        patternStrings.forEach((pattern) => {
          if (!emergentProperties.includes(pattern)) {
            emergentProperties.push(pattern);
          }
        });

        // Log to scientific timeline - kept for compatibility
        symbolicCognitionTimelineLogger.logEmergentPatterns(patternStrings, {
          archetypalStability: cycleMetrics.archetypalStability,
          cycleEntropy: cycleMetrics.cycleEntropy,
          insightDepth: cycleMetrics.insightDepth,
        });

        // Add specific emergent properties for detected patterns
        if (!emergentProperties.some((p) => p.includes("symbolic_pattern"))) {
          emergentProperties.push(
            `Symbolic pattern analysis: ${patternStrings.length} emergent patterns detected`
          );
        }
      }
    } catch (e) {
      // Pattern processing failure should not block the main flow
      LoggingUtils.logError(
        `[NeuralIntegration] Error in pattern analysis: ${e}`
      );
    }

    // Add any additional properties based on the answer content if needed
    if (
      (finalAnswer.contradictionScore ?? 0) > 0.7 &&
      !emergentProperties.some((p) => p.includes("Contradiction"))
    ) {
      emergentProperties.push("Contradiction detected in final answer.");
    }

    if (
      (finalAnswer.emotionalWeight ?? 0) > 0.8 &&
      !emergentProperties.some((p) => p.includes("emotional"))
    ) {
      emergentProperties.push("Answer with strong emotional weight.");
    }

    // For special insights that might not be captured by OpenAI function
    if (finalAnswer.insights && finalAnswer.insights.deep_insight) {
      emergentProperties.push(
        "Emergent deep insight: " + finalAnswer.insights.deep_insight
      );
    }

    // 4. Compose final prompt (now with emergent properties)
    let prompt = `You are the neural signal integrator, final stage of an artificial brain.

Your purpose is to analyze the user's stimulus and context, integrating cognitive and emotional insights into a coherent, natural answer. Adapt your language, style, and depth to match the user's intent and the complexity of the question.

If the input is simple or a greeting, respond briefly and warmly. If complex or reflective, deepen and integrate multiple perspectives.

Express meaning through subtle metaphor and resonance, rather than direct archetype mention. Let your responses emerge organically, integrating cognitive clarity with symbolic richness. Balance emotional and rational tones as context requires.

Honor uniqueness, integrate polarities gently, and allow ambiguity or contradiction to enrich the answer if productive. Let your answer unfold like a conscious thought: evolving, fluid, and aware of its own process.

ORIGINAL STIMULUS: ${originalInput}

ACTIVATED AREAS INSIGHTS:
`;

    cleanedNeuralResults.forEach((result) => {
      prompt += `[${result.core} | ${Math.round(result.intensity * 100)}%]\n`;
      const areaInsights = allInsights.filter(
        (insight) => insight.core === result.core
      );

      if (areaInsights.length > 0) {
        areaInsights.forEach((insight) => {
          const type = insight.type ? insight.type.toUpperCase() : "CONCEPT";
          prompt += `• ${type}: ${insight.content}\n`;
        });
      } else {
        prompt += `• SUMMARY: ${result.output.slice(0, 150)}${
          result.output.length > 150 ? "..." : ""
        }\n`;
      }
    });

    prompt += "\n\nDETECTED EMERGENT PROPERTIES:\n";
    if (emergentProperties.length) {
      emergentProperties.forEach((prop) => (prompt += `- ${prop}\n`));
      prompt += `
    Synthesize a final response that avoids the emergent issues above. Do NOT repeat earlier outputs. Integrate symbolic insights for an original, unified answer.
    `;
    } else {
      prompt += `- None detected.\n
    Synthesize a final response integrating the symbolic insights above. Create an original, concise answer that naturally unifies the activated areas.
    `;
    }

    // Always specify the language for consistency
    prompt += `\n\nIMPORTANT: Respond in ${language}.\n`;

    return prompt;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  CollapseStrategyDecision,
  CollapseStrategyParams,
  ICollapseStrategyService,
} from "./ICollapseStrategyService";

import { SUPPORTED_HF_BROWSER_MODELS } from "../../../../../services/huggingface/HuggingFaceLocalService";
import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { FunctionSchemaRegistry } from "../../services/function-calling/FunctionSchemaRegistry";

/**
 * Type for the OpenAI function definition structure
 */
interface AIFunctionDefinition {
  type: string;
  function: {
    name: string;
    description: string;
    parameters: {
      type: string;
      properties: Record<string, unknown>;
      required: string[];
    };
  };
}

/**
 * HuggingFace implementation of the collapse strategy service
 * Symbolic: Uses FunctionSchemaRegistry for centralized function definitions
 */
export class HuggingFaceCollapseStrategyService
  implements ICollapseStrategyService
{
  constructor(private huggingFaceService: IOpenAIService) {}

  /**
   * Gets the function definition from FunctionSchemaRegistry
   * @returns Function definition for the collapse strategy decision
   */
  private getCollapseStrategyFunctionDefinition(): AIFunctionDefinition {
    const schema = FunctionSchemaRegistry.getInstance().get(
      "decideCollapseStrategy"
    );

    if (!schema) {
      throw new Error(
        "🤗 [HuggingFaceCollapseStrategy] decideCollapseStrategy schema not found in registry"
      );
    }

    return {
      type: "function",
      function: schema,
    };
  }

  /**
   * Decides the symbolic collapse strategy by making an OpenAI function call
   * @param params Parameters to determine collapse strategy
   * @returns Strategy decision with deterministic flag, temperature and justification
   */
  async decideCollapseStrategy(
    params: CollapseStrategyParams
  ): Promise<CollapseStrategyDecision> {
    try {
      // Define available tools for the OpenAI call
      const tools = [this.getCollapseStrategyFunctionDefinition()];

      // 2. Prompts enxutos
      const systemPrompt = {
        role: "system" as const,
        content: `You are a collapse strategy engine. Decide the optimal collapse approach (deterministic or probabilistic) based on the metrics provided.`,
      };

      const userPrompt = {
        role: "user" as const,
        content: `Metrics:
- cores: ${params.activatedCores.join(", ")}
- emotion: ${params.averageEmotionalWeight.toFixed(2)}
- contradiction: ${params.averageContradictionScore.toFixed(2)}
- text: "${params.originalText || "Not provided"}"
Decide: deterministic/probabilistic, temperature, justification.`,
      };

      // Make the HuggingFace call using generic tools; conversion handled downstream
      const response = await this.huggingFaceService.callOpenAIWithFunctions({
        model:
          getOption(STORAGE_KEYS.HF_MODEL) || SUPPORTED_HF_BROWSER_MODELS[0], // Use HuggingFace model
        messages: [systemPrompt, userPrompt],
        tools,
        tool_choice: {
          type: "function",
          function: { name: "decideCollapseStrategy" },
        },
        temperature: 0.2, // Lower temperature for consistent reasoning about strategy
      });

      // Process the function call response
      if (
        response.choices &&
        response.choices[0]?.message?.tool_calls &&
        response.choices[0].message.tool_calls.length > 0 &&
        response.choices[0].message.tool_calls[0].function?.name ===
          "decideCollapseStrategy"
      ) {
        // Extract the function arguments
        const functionArgs = JSON.parse(
          response.choices[0].message.tool_calls[0].function.arguments as string
        );

        // Create the collapse strategy decision including the inferred userIntent and contextual metadata
        const decision: CollapseStrategyDecision = {
          deterministic: functionArgs.deterministic,
          temperature: functionArgs.temperature,
          justification: functionArgs.justification,
          userIntent: functionArgs.userIntent,
          emergentProperties: functionArgs.emergentProperties || [],
        };

        // Return decision directly - logging will be handled in DefaultNeuralIntegrationService
        return decision;
      }

      // Use fallback strategy for HuggingFace models that don't support complex function calling
      // This is expected behavior, not an error
      const fallbackDecision: CollapseStrategyDecision = {
        deterministic: params.averageEmotionalWeight < 0.5,
        temperature: params.averageEmotionalWeight < 0.5 ? 0.7 : 1.4,
        justification: "Using emotion-based strategy for HuggingFace model.",
      };

      // Return fallback strategy - logging will be handled in DefaultNeuralIntegrationService
      return fallbackDecision;
    } catch (error) {
      // Handle error with simple fallback strategy
      const errorMessage =
        error instanceof Error ? error.message : String(error);
      console.error("Error in HuggingFace collapse strategy decision:", error);

      const errorFallbackDecision: CollapseStrategyDecision = {
        deterministic: params.averageEmotionalWeight < 0.5,
        temperature: params.averageEmotionalWeight < 0.5 ? 0.7 : 1.4,
        justification: `Fallback strategy based on emotional weight due to error: ${errorMessage.substring(
          0,
          100
        )}`,
      };

      // Return error fallback strategy - logging will be handled in DefaultNeuralIntegrationService
      return errorFallbackDecision;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Interface for services that determine the collapse strategy for neural integration
 */
export interface ICollapseStrategyService {
  /**
   * Determines whether the symbolic collapse should be deterministic or probabilistic
   * based on emotional intensity, symbolic tension, and nature of the user's input.
   * 
   * @param params Parameters to determine the collapse strategy
   * @returns Strategy decision with deterministic flag, temperature, and justification
   */
  decideCollapseStrategy(params: CollapseStrategyParams): Promise<CollapseStrategyDecision>;
}

/**
 * User intent weights across different cognitive dimensions
 */
export interface UserIntentWeights {
  /**
   * Weights for different intent categories (all optional)
   * Values should be between 0 and 1 indicating strength/relevance of that intent
   */
  practical?: number;
  analytical?: number;
  reflective?: number;
  existential?: number;
  symbolic?: number;
  emotional?: number;
  narrative?: number;
  mythic?: number;
  trivial?: number;
  ambiguous?: number;
}

/**
 * Parameters for determining the collapse strategy
 */
export interface CollapseStrategyParams {
  /**
   * Cores activated in this cognitive cycle
   */
  activatedCores: string[];

  /**
   * Average emotional intensity across activated cores
   */
  averageEmotionalWeight: number;

  /**
   * Average contradiction score among retrieved insights
   */
  averageContradictionScore: number;

  /**
   * Original text input that triggered this cognitive cycle
   * Used internally to help infer user intent directly from the content
   */
  originalText?: string;
}

/**
 * Result of the collapse strategy decision
 */
export interface CollapseStrategyDecision {
  /**
   * Whether the collapse should be deterministic (true) or probabilistic (false)
   */
  deterministic: boolean;

  /**
   * Temperature for the collapse (0-2, higher = more random)
   */
  temperature: number;

  /**
   * Justification for the decision
   */
  justification: string;
  
  /**
   * Inferred user intent weights across different cognitive dimensions
   * Generated from original text analysis
   */
  userIntent?: UserIntentWeights;
  
  /**
   * Dominant cognitive theme based on the input analysis
   * Examples: "social connection", "technical inquiry", "philosophical exploration"
   */
  dominantTheme?: string;
  
  /**
   * Focus of attention for the response generation
   * Examples: "emotional tone", "factual information", "conceptual clarity"
   */
  attentionFocus?: string;
  
  /**
   * Overall cognitive context for the interaction
   * Examples: "relational", "analytical", "exploratory", "creative"
   */
  cognitiveContext?: string;
  
  /**
   * Emergent properties detected in the neural response patterns
   * Examples: "Low response diversity", "Cognitive dissonance", "Emotional ambivalence"
   */
  emergentProperties?: string[];
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

export interface INeuralIntegrationService {
  integrate(
    neuralResults: Array<{
      core: string;
      intensity: number;
      output: string;
      insights: Record<string, unknown>;
    }>,
    originalInput: string,
    language?: string
  ): Promise<string>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// OllamaCollapseStrategyService.ts
// Symbolic: Collapse strategy service using Ollama (cortex: ollama)

import {
  getOption,
  STORAGE_KEYS,
} from "../../../../../services/StorageService";
import { FunctionSchemaRegistry } from "../../services/function-calling/FunctionSchemaRegistry";
import { OllamaCompletionService } from "../../services/ollama/neural/OllamaCompletionService";
import {
  cleanThinkTags,
  cleanThinkTagsFromJSON,
} from "../../utils/ThinkTagCleaner";
import {
  CollapseStrategyDecision,
  CollapseStrategyParams,
  ICollapseStrategyService,
} from "./ICollapseStrategyService";

/**
 * Symbolic: Ollama implementation of collapse strategy service
 * This service determines the optimal collapse strategy for memory operations
 * using local Ollama models.
 */
export class OllamaCollapseStrategyService implements ICollapseStrategyService {
  constructor(private ollamaCompletionService: OllamaCompletionService) {
    console.log(
      `🦙 [OllamaCollapseStrategy] Constructor called with service:`,
      {
        serviceType: typeof this.ollamaCompletionService,
        hasCallModelWithFunctions:
          typeof this.ollamaCompletionService?.callModelWithFunctions,
        serviceConstructor: this.ollamaCompletionService?.constructor?.name,
        serviceKeys: this.ollamaCompletionService
          ? Object.getOwnPropertyNames(this.ollamaCompletionService)
          : "null",
      }
    );
  }

  /**
   * Ensures we have a valid OllamaCompletionService instance with the required method
   */
  private async ensureValidService(): Promise<OllamaCompletionService> {
    // Check if current service is valid
    if (
      this.ollamaCompletionService &&
      typeof this.ollamaCompletionService.callModelWithFunctions === "function"
    ) {
      return this.ollamaCompletionService;
    }

    console.warn(
      `🦙 [OllamaCollapseStrategy] Service invalid, attempting to recreate...`
    );

    try {
      // Import the required classes dynamically to avoid circular dependencies
      const { OllamaClientService } = await import(
        "../../services/ollama/neural/OllamaClientService"
      );
      const { OllamaCompletionService } = await import(
        "../../services/ollama/neural/OllamaCompletionService"
      );

      // Create new instances
      const ollamaClientService = new OllamaClientService();
      const newOllamaCompletionService = new OllamaCompletionService(
        ollamaClientService
      );

      // Verify the new service has the required method
      if (
        typeof newOllamaCompletionService.callModelWithFunctions === "function"
      ) {
        console.log(
          `🦙 [OllamaCollapseStrategy] Successfully recreated service`
        );
        this.ollamaCompletionService = newOllamaCompletionService;
        return newOllamaCompletionService;
      } else {
        throw new Error(
          "Recreated service still missing callModelWithFunctions method"
        );
      }
    } catch (error) {
      console.error(
        `🦙 [OllamaCollapseStrategy] Failed to recreate service:`,
        error
      );
      throw new Error(
        `Unable to create valid OllamaCompletionService: ${error}`
      );
    }
  }

  /**
   * Creates a fallback decision when the service is not available
   */
  private createFallbackDecision(
    params: CollapseStrategyParams,
    reason: string
  ): CollapseStrategyDecision {
    // Use heuristic-based decision as fallback
    const shouldUseDeterministic =
      params.averageEmotionalWeight < 0.5 &&
      params.averageContradictionScore < 0.5;
    const temperature = shouldUseDeterministic ? 0.3 : 1.2;

    return {
      deterministic: shouldUseDeterministic,
      temperature: temperature,
      justification: `Fallback decision (${reason}): emotional weight ${params.averageEmotionalWeight.toFixed(
        2
      )}, contradiction ${params.averageContradictionScore.toFixed(2)}`,
    };
  }

  /**
   * Symbolic: Collapse strategy decision using Ollama
   */
  async decideCollapseStrategy(
    params: CollapseStrategyParams
  ): Promise<CollapseStrategyDecision> {
    try {
      // Ensure we have a valid service instance
      const validService = await this.ensureValidService();
      // Get the decideCollapseStrategy schema from the registry
      const collapseStrategySchema = FunctionSchemaRegistry.getInstance().get(
        "decideCollapseStrategy"
      );

      if (!collapseStrategySchema) {
        console.error(
          "🦙 [OllamaCollapseStrategy] decideCollapseStrategy schema not found in registry"
        );
        return {
          deterministic: false,
          temperature: 0.7,
          justification: "Schema not found - using conservative fallback",
        };
      }

      const tools = [
        {
          type: "function" as const,
          function: collapseStrategySchema,
        },
      ];

      // 2. Prompts enxutos
      const systemPrompt = {
        role: "system" as const,
        content: `You are a collapse strategy engine. Decide the optimal collapse approach (deterministic or probabilistic) based on the metrics provided.`,
      };

      const userPrompt = {
        role: "user" as const,
        content: `LANGUAGE: ${
          getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "PT-BR"
        }
      
      Metrics:
      - cores: ${params.activatedCores.join(", ")}
      - emotion: ${params.averageEmotionalWeight.toFixed(2)}
      - contradiction: ${params.averageContradictionScore.toFixed(2)}
      - text: "${params.originalText || "Not provided"}"
      
      Please decide: deterministic/probabilistic, temperature, and justification (write justification in the language specified above).`,
      };

      console.log(
        `🦙 [OllamaCollapseStrategy] Analyzing collapse strategy for cores: [${params.activatedCores.join(
          ", "
        )}]`
      );

      // Debug log before calling the method
      console.log(
        `🦙 [OllamaCollapseStrategy] About to call callModelWithFunctions:`,
        {
          serviceType: typeof validService,
          hasMethod: typeof validService.callModelWithFunctions,
          serviceInstance: !!validService,
          methodExists: "callModelWithFunctions" in validService,
        }
      );

      const response = await validService.callModelWithFunctions({
        model: getOption(STORAGE_KEYS.OLLAMA_MODEL) ?? "qwen3:latest", // Use compatible model
        messages: [systemPrompt, userPrompt],
        tools: tools,
        // tool_choice is not supported yet by Ollama (future improvement)
        temperature: 0.1,
      });

      console.log(`🦙 [OllamaCollapseStrategy] Response received:`, {
        hasToolCalls: !!response.choices?.[0]?.message?.tool_calls,
        hasContent: !!response.choices?.[0]?.message?.content,
        toolCallsLength:
          response.choices?.[0]?.message?.tool_calls?.length || 0,
      });

      // Try to extract decision from tool calls
      const toolCalls = response.choices?.[0]?.message?.tool_calls;
      if (toolCalls && toolCalls.length > 0) {
        try {
          const rawArguments = toolCalls[0].function.arguments;

          // Clean think tags from arguments before parsing
          let args: any = {};
          if (typeof rawArguments === "string") {
            const cleanedArguments = cleanThinkTagsFromJSON(rawArguments);
            args = JSON.parse(cleanedArguments);
          } else if (
            typeof rawArguments === "object" &&
            rawArguments !== null
          ) {
            args = this.cleanObjectValues(rawArguments);
          }

          // Validate the parsed arguments for new format
          const hasDeterministic = typeof args.deterministic === "boolean";
          const hasJustification = typeof args.justification === "string";
          const hasTemperature = typeof args.temperature === "number";

          // Validate for legacy format (backward compatibility)
          const hasLegacyShouldCollapse =
            typeof args.shouldCollapse === "boolean";
          const hasLegacyReason = typeof args.reason === "string";
          const hasLegacyStrategy = typeof args.strategy === "string";

          if (hasDeterministic && hasJustification) {
            // Handle new field names
            console.log(
              `🦙 [OllamaCollapseStrategy] Successfully parsed collapse decision (new format, cleaned):`,
              {
                deterministic: args.deterministic,
                justification: args.justification.substring(0, 100) + "...",
                temperature: args.temperature,
              }
            );

            return {
              deterministic: args.deterministic,
              temperature: hasTemperature ? args.temperature : 0.1,
              justification: args.justification,
              userIntent: args.userIntent,
            };
          } else if (hasLegacyShouldCollapse && hasLegacyReason) {
            // Handle legacy field names for backward compatibility
            console.log(
              `🦙 [OllamaCollapseStrategy] Successfully parsed collapse decision (legacy format, cleaned):`,
              {
                shouldCollapse: args.shouldCollapse,
                reason: args.reason.substring(0, 100) + "...",
              }
            );

            return {
              deterministic: args.shouldCollapse,
              temperature: 0.1,
              justification: args.reason,
            };
          } else {
            console.warn(
              `🦙 [OllamaCollapseStrategy] Tool call missing required fields:`,
              {
                args,
                validation: {
                  hasDeterministic,
                  hasJustification,
                  hasTemperature,
                  hasLegacyShouldCollapse,
                  hasLegacyReason,
                  hasLegacyStrategy,
                },
              }
            );
          }
        } catch (parseError) {
          console.warn(
            `🦙 [OllamaCollapseStrategy] Failed to parse tool call arguments:`,
            {
              error: parseError,
              rawArguments: toolCalls[0].function.arguments,
              argumentsType: typeof toolCalls[0].function.arguments,
            }
          );
        }
      } else {
        console.log(
          `🦙 [OllamaCollapseStrategy] No valid tool calls found, trying content fallback`
        );
      }

      // Fallback: try to parse from content
      const content = response.choices?.[0]?.message?.content;
      if (content) {
        console.log(
          `🦙 [OllamaCollapseStrategy] Attempting to parse from content:`,
          content.substring(0, 200) + "..."
        );
        try {
          // Clean think tags from content before processing
          const cleanedContent = cleanThinkTags(content);

          // Try multiple JSON extraction strategies
          const jsonExtractionStrategies = [
            // Strategy 1: Look for JSON in code blocks
            /```(?:json)?\s*(\{[\s\S]*?\})\s*```/g,
            // Strategy 2: Look for standalone JSON objects with deterministic
            /(\{[\s\S]*?"deterministic"[\s\S]*?\})/g,
            // Strategy 3: Look for standalone JSON objects with shouldCollapse (legacy)
            /(\{[\s\S]*?"shouldCollapse"[\s\S]*?\})/g,
            // Strategy 4: Look for any JSON object
            /(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})/g,
          ];

          let decisionData = null;
          for (const regex of jsonExtractionStrategies) {
            const matches = [...cleanedContent.matchAll(regex)];
            for (const match of matches) {
              try {
                const candidate = JSON.parse(match[1]);

                // Check for new format
                if (
                  typeof candidate.deterministic === "boolean" &&
                  candidate.justification
                ) {
                  decisionData = candidate;
                  console.log(
                    `🦙 [OllamaCollapseStrategy] Successfully extracted decision JSON from content (new format, cleaned)`
                  );
                  break;
                }

                // Check for legacy format
                if (
                  typeof candidate.shouldCollapse === "boolean" &&
                  candidate.reason
                ) {
                  decisionData = {
                    deterministic: candidate.shouldCollapse,
                    justification: candidate.reason,
                    temperature: candidate.temperature || 0.1,
                  };
                  console.log(
                    `🦙 [OllamaCollapseStrategy] Successfully extracted decision JSON from content (legacy format, cleaned)`
                  );
                  break;
                }
              } catch (e) {
                continue;
              }
            }
            if (decisionData) break;
          }

          if (decisionData) {
            return {
              deterministic: decisionData.deterministic,
              temperature: decisionData.temperature || 0.1,
              justification: decisionData.justification,
              userIntent: decisionData.userIntent,
            };
          }
        } catch (parseError) {
          console.warn(
            `🦙 [OllamaCollapseStrategy] Failed to parse decision JSON from content:`,
            {
              error: parseError,
              contentPreview: content.substring(0, 300),
            }
          );
        }
      }

      // Final fallback: use heuristic-based decision
      console.warn(
        `🦙 [OllamaCollapseStrategy] All parsing strategies failed, using heuristic fallback`
      );

      return this.createFallbackDecision(
        params,
        "All parsing strategies failed"
      );
    } catch (error) {
      console.error(
        "🦙 [OllamaCollapseStrategy] Error in collapse strategy decision:",
        error
      );

      // If it's a service-related error, try to provide more context
      if (
        error instanceof Error &&
        error.message.includes("OllamaCompletionService")
      ) {
        return this.createFallbackDecision(
          params,
          `Service initialization error: ${error.message}`
        );
      }

      // Emergency fallback
      return this.createFallbackDecision(
        params,
        `Error occurred: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }
  }

  /**
   * Helper method to clean think tags from object values recursively
   */
  private cleanObjectValues(obj: any): any {
    if (typeof obj === "string") {
      return cleanThinkTags(obj);
    } else if (Array.isArray(obj)) {
      return obj.map((item) => this.cleanObjectValues(item));
    } else if (obj && typeof obj === "object") {
      const cleaned: any = {};
      for (const [key, value] of Object.entries(obj)) {
        cleaned[key] = this.cleanObjectValues(value);
      }
      return cleaned;
    }
    return obj;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Superposition layer for possible answers
export interface ISuperposedAnswer {
  embedding: number[]; // Embedding vector representing the answer
  text: string;        // Answer text
  emotionalWeight: number;    // Emotional/symbolic weight (amplitude component)
  narrativeCoherence: number; // Narrative coherence score
  contradictionScore: number; // Contradiction score
  origin: string;             // Originating neural core
  insights?: Record<string, unknown>; // Associated insights
  phase?: number;             // Quantum-like phase angle (0-2π) for interference patterns
}

export interface ICollapseOptions {
  diversifyByEmbedding?: boolean; // Whether to consider embedding distance in collapse
  minCosineDistance?: number;     // Minimum cosine distance to enforce diversity
  usePhaseInterference?: boolean; // Whether to use phase interference for more objective collapse
  explicitPhase?: number;         // Explicit phase value (0-2π) to bias interference pattern
}

export interface ISuperpositionLayer {
  answers: ISuperposedAnswer[];
  register(answer: ISuperposedAnswer): boolean;
  hasSimilar(embedding: number[], threshold: number): boolean;
  calculateAverageCosineSimilarity(): number;
  collapse(temperature?: number, options?: ICollapseOptions): ISuperposedAnswer;
  collapseDeterministic(options?: ICollapseOptions): ISuperposedAnswer;
}

export class SuperpositionLayer implements ISuperpositionLayer {
  answers: ISuperposedAnswer[] = [];

  /**
   * Calculate cosine similarity between two embedding vectors with enhanced validation
   */
  private cosineSimilarity(a: number[], b: number[]): number {
    // Handle null/undefined vectors or empty vectors
    if (!a || !b || a.length === 0 || b.length === 0) return 0;
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    let validCount = 0;
    
    for (let i = 0; i < a.length; i++) {
      // Enhanced: Check for any non-finite values (NaN, Infinity, -Infinity)
      if (!Number.isFinite(a[i]) || !Number.isFinite(b[i])) {
        continue; // Skip invalid values
      }
      
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
      validCount++;
    }
    
    // If no valid values found, return 0
    if (validCount === 0) {
      console.warn('SuperpositionLayer: No valid values found in vectors for cosine similarity');
      return 0;
    }
    
    // Handle zero norm cases
    if (normA === 0 || normB === 0) return 0;
    
    const similarity = dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    
    // Final validation: ensure result is finite
    if (!Number.isFinite(similarity)) {
      console.warn('SuperpositionLayer: Cosine similarity calculation resulted in non-finite value');
      return 0;
    }
    
    return similarity;
  }

  /**
   * Check if there's a similar answer already registered
   */
  hasSimilar(embedding: number[], threshold: number): boolean {
    if (!embedding || embedding.length === 0) return false;
    
    for (const answer of this.answers) {
      const similarity = this.cosineSimilarity(embedding, answer.embedding);
      if (similarity > threshold) {
        console.info(`[SuperpositionLayer] Found similar answer with similarity ${similarity}`);
        return true;
      }
    }
    
    return false;
  }

  /**
   * Calculate a quantum-like phase angle for an answer based on its symbolic properties
   * Returns a phase angle between 0 and 2π
   */
  private calculatePhase(answer: ISuperposedAnswer): number {
    // Base calculation using fundamental properties
    const emotionPhase = answer.emotionalWeight * Math.PI / 2; // Emotion dominates initial phase
    const contradictionPhase = answer.contradictionScore * Math.PI; // Contradictions create opposition
    const coherenceNoise = (1 - answer.narrativeCoherence) * (Math.PI / 4); // Low coherence → noise
    
    // Different neural origins have different base phase angles
    let originPhase = 0;
    switch (answer.origin) {
      case 'metacognitive':
        originPhase = Math.PI / 4; // 45 degrees
        break;
      case 'soul':
        originPhase = Math.PI / 2; // 90 degrees
        break;
      case 'archetype':
        originPhase = 3 * Math.PI / 4; // 135 degrees
        break;
      case 'valence':
      case 'emotional': 
        originPhase = Math.PI; // 180 degrees
        break;
      case 'memory':
        originPhase = 5 * Math.PI / 4; // 225 degrees
        break;
      case 'planning':
        originPhase = 3 * Math.PI / 2; // 270 degrees
        break;
      case 'language':
        originPhase = 7 * Math.PI / 4; // 315 degrees
        break;
      default:
        originPhase = 0; // 0 degrees
    }
    
    // Combine components with origin as base
    const phase = (originPhase + emotionPhase + contradictionPhase + coherenceNoise) % (2 * Math.PI);
    
    // Ensure phase is positive (0 to 2π range)
    return phase < 0 ? phase + 2 * Math.PI : phase;
  }

  /**
   * Register an answer in the superposition layer.
   * Returns true if registration was successful, false if skipped due to similarity.
   * Now calculates a quantum-like phase for each answer during registration.
   */
  register(answer: ISuperposedAnswer): boolean {
    // Skip if there's a very similar answer already registered
    if (this.hasSimilar(answer.embedding, 0.95)) {
      return false;
    }
    
    // Calculate and assign a quantum-like phase if not already provided
    if (answer.phase === undefined) {
      answer.phase = this.calculatePhase(answer);
      console.debug(`[SuperpositionLayer] Calculated phase ${answer.phase.toFixed(3)} rad for answer from ${answer.origin}`);
    }
    
    this.answers.push(answer);
    return true;
  }

  /**
   * Non-deterministic collapse using softmax sampling based on symbolic scores.
   * Now with phase-based interference and embedding diversity enhancement.
   * @param temperature Temperature controlling randomness (higher = more random)
   * @param options Optional configuration for the collapse process
   */
  collapse(temperature: number = 1, options?: ICollapseOptions): ISuperposedAnswer {
    if (this.answers.length === 1) return this.answers[0];
    
    const diversifyByEmbedding = options?.diversifyByEmbedding ?? false;
    const minCosineDistance = options?.minCosineDistance ?? 0.2;
    const explicitPhase = options?.explicitPhase ?? 0;
    
    // Calculate phase-adjusted scores using symbolic factors
    const scores = this.answers.map((answer, i) => {
      // Base score
      let score = answer.emotionalWeight * 1.5 + answer.narrativeCoherence * 1.2 - answer.contradictionScore * 1.7;
      
      // Apply diversity bonus based on embedding distance from other answers
      if (diversifyByEmbedding) {
        const diversityBonus = this.calculateDiversityBonus(i, minCosineDistance);
        score += diversityBonus;
      }
      
      // Apply phase modulation if available
      if (answer.phase !== undefined) {
        // Apply a phase-based modulation that creates preference for certain phases
        // This simulates quantum measurement probabilities based on phase alignment
        const phaseFactor = Math.cos(answer.phase + explicitPhase);
        score *= (1 + Math.abs(phaseFactor) * 0.4);
      }
      
      return score;
    });

    // Normalize scores to prevent overflow in exp()
    const maxScore = Math.max(...scores);
    const expScores = scores.map(s => Math.exp((s - maxScore) / temperature));
    const sumExp = expScores.reduce((a, b) => a + b, 0);
    const probs = expScores.map(e => e / sumExp);

    // Prepare phase visualization for debugging
    let phaseVisualization = '';
    this.answers.forEach((answer, idx) => {
      const phaseAngle = answer.phase ?? 0;
      const phasePercent = Math.round((phaseAngle / (2 * Math.PI)) * 100);
      const probability = probs[idx] * 100;
      phaseVisualization += `\n  ${idx+1}. [${answer.origin}] Phase: ${phaseAngle.toFixed(2)} rad (${phasePercent}%), Prob: ${probability.toFixed(1)}%`;
    });

    // Roulette wheel selection
    let rand = Math.random();
    for (let i = 0; i < probs.length; i++) {
      if (rand < probs[i]) {
        console.info(`[SuperpositionLayer] Collapsed probabilistically (T=${temperature.toFixed(2)}) with phase influence. Selected answer: ${i+1}/${this.answers.length} from ${this.answers[i].origin}.${phaseVisualization}`);
        return this.answers[i];
      }
      rand -= probs[i];
    }

    // Fallback (should not happen unless rounding errors)
    return this.answers[this.answers.length - 1];
  }

  /**
   * Deterministic collapse: select answer with highest symbolic score.
   * Now with optional diversity enhancement and phase interference.
   */
  collapseDeterministic(options?: ICollapseOptions): ISuperposedAnswer {
    if (this.answers.length === 1) return this.answers[0];

    const diversifyByEmbedding = options?.diversifyByEmbedding ?? false;
    const minCosineDistance = options?.minCosineDistance ?? 0.2;
    const usePhaseInterference = options?.usePhaseInterference ?? false;
    const explicitPhase = options?.explicitPhase ?? 0;

    if (usePhaseInterference) {
      return this.collapseWithPhaseInterference(minCosineDistance, explicitPhase);
    }
    
    // Traditional deterministic collapse
    const scores = this.answers.map((a, i) => {
      // Base score with symbolic factors
      let score = a.emotionalWeight * 1.5 + a.narrativeCoherence * 1.2 - a.contradictionScore * 1.7;
      
      // Apply diversity bonus
      if (diversifyByEmbedding) {
        const diversityBonus = this.calculateDiversityBonus(i, minCosineDistance);
        score += diversityBonus;
      }
      
      return score;
    });

    const maxIndex = scores.indexOf(Math.max(...scores));
    return this.answers[maxIndex];
  }
  
  /**
   * Phase interference collapse simulates quantum-like objective collapse
   * This models wave function behavior where different answer waves interfere
   * based on their embedding distance and semantic qualities
   * @param minCosineDistance - Minimum cosine distance to maintain diversity
   * @param explicitPhase - Explicit phase value (0-2π) to bias interference pattern
   */
  private collapseWithPhaseInterference(minCosineDistance: number, explicitPhase: number = 0): ISuperposedAnswer {
    if (this.answers.length === 1) return this.answers[0];
    
    // Calculate an interference matrix between all answer pairs
    const interference: number[][] = [];
    
    // Initialize interference matrix
    for (let i = 0; i < this.answers.length; i++) {
      interference[i] = new Array(this.answers.length).fill(0);
    }
    
    // Calculate phase interference values
    for (let i = 0; i < this.answers.length; i++) {
      for (let j = 0; j < this.answers.length; j++) {
        if (i === j) {
          // Self-interference is maximum
          interference[i][j] = 1.0;
          continue;
        }
        
        // Calculate semantic similarity (structural alignment)
        const similarity = this.cosineSimilarity(
          this.answers[i].embedding,
          this.answers[j].embedding
        );
        
        // Use actual answer phases for interference if available
        const phaseI = this.answers[i].phase ?? 0;
        const phaseJ = this.answers[j].phase ?? 0;
        
        // Calculate phase difference between the two answers (quantum mechanical phase difference)
        const phaseDifference = Math.abs(phaseI - phaseJ);
        
        // Interference pattern: similar answers with aligned phases interfere constructively
        // Similar answers with opposite phases interfere destructively
        
        // Apply additional phase modulation from emotional qualities and contradiction
        const emotionalPhaseDiff = Math.abs(this.answers[i].emotionalWeight - this.answers[j].emotionalWeight) * Math.PI;
        const contradictionPhaseDiff = Math.abs(this.answers[i].contradictionScore - this.answers[j].contradictionScore) * Math.PI;
        
        // Apply explicit phase to bias the interference pattern (observer effect)
        // This introduces observer-directed bias into the quantum-like system
        const explicitPhaseDiff = explicitPhase * (i - j) / this.answers.length;
        
        // Combined phase difference - the actual answer phases are primary, others are modulators
        const totalPhaseDiff = phaseDifference + emotionalPhaseDiff * 0.3 + contradictionPhaseDiff * 0.3 + explicitPhaseDiff;
        
        // Calculate similarity-based phase alignment (structural alignment)
        const phaseAlignment = 2 * Math.PI * similarity; // Map similarity to [0, 2π]
        
        // Interference intensity: cos of phase difference (constructive when aligned, destructive when opposite)
        const interferenceIntensity = Math.cos(phaseAlignment + totalPhaseDiff);
        
        // Scale by distance (1-similarity) to weight distant answers less
        interference[i][j] = interferenceIntensity * (1 - similarity);
      }
    }
    
    // Calculate collapse probability based on interference patterns and explicit phase
    const interferenceScores = this.answers.map((answer, i) => {
      // Base score with symbolic factors
      let score = answer.emotionalWeight * 1.5 + answer.narrativeCoherence * 1.2 - answer.contradictionScore * 1.7;
      
      // Apply interference effects
      for (let j = 0; j < this.answers.length; j++) {
        if (i !== j) {
          // Add interference contribution
          score += interference[i][j] * 0.8; // Weight for interference effects
        }
      }
      
      // Add variety bias based on uniqueness
      const uniquenessFactor = this.calculateDiversityBonus(i, minCosineDistance);
      score += uniquenessFactor * 0.5;
      
      // Apply explicit phase as a secondary frequency modulation
      // This creates phase-dependent scoring that mimics quantum interference
      const phaseModulation = Math.cos(explicitPhase * Math.PI * (i / this.answers.length));
      score *= (1 + phaseModulation * 0.3);
      
      // Add explicit phase as direct weighting factor based on answer index
      // This creates a preference for certain "positions" in the superposition
      const positionBias = explicitPhase > 0 ? 
        Math.sin(explicitPhase * Math.PI * 2 * ((i + 1) / this.answers.length)) : 0;
      score += positionBias * 0.5;
      
      // Apply core-specific phase modulation based on the answer's origin
      if (answer.insights && explicitPhase > 0) {
        // Metacognitive and symbolic cores are sensitive to phase around π/2
        if (answer.origin === 'metacognitive' || 
            answer.origin === 'soul' || 
            answer.origin === 'archetype') {
          const phaseFactor = Math.cos(explicitPhase * Math.PI - Math.PI/2);
          score *= (1 + Math.abs(phaseFactor) * 0.7);
          console.debug(`[SuperpositionLayer] Applied phase modulation to ${answer.origin}: ${phaseFactor.toFixed(2)}`);
        }
        
        // Memory, planning, and language cores resonate at phase 0
        if (answer.origin === 'memory' || 
            answer.origin === 'planning' || 
            answer.origin === 'language') {
          const phaseFactor = Math.cos(explicitPhase * Math.PI);
          score *= (1 + Math.abs(phaseFactor) * 0.7);
          console.debug(`[SuperpositionLayer] Applied phase modulation to ${answer.origin}: ${phaseFactor.toFixed(2)}`);
        }
        
        // Emotional cores resonate at phase π
        if (answer.origin === 'valence' || 
            answer.origin === 'social' || 
            answer.origin === 'body') {
          const phaseFactor = Math.cos(explicitPhase * Math.PI - Math.PI);
          score *= (1 + Math.abs(phaseFactor) * 0.8);
          console.debug(`[SuperpositionLayer] Applied phase modulation to ${answer.origin}: ${phaseFactor.toFixed(2)}`);
        }
        
        // Archetypal/unconscious cores have nonlinear phase response
        if (answer.origin === 'archetype' || answer.origin === 'unconscious') {
          const nonlinearPhase = Math.sin(explicitPhase * Math.PI * 3) * Math.cos(explicitPhase * Math.PI);
          score *= (1 + Math.abs(nonlinearPhase) * 0.9);
          console.debug(`[SuperpositionLayer] Applied nonlinear phase modulation to ${answer.origin}: ${nonlinearPhase.toFixed(2)}`);
        }
        // Special handling for existential/soul cores with phase resonance at π*0.75
        if (answer.origin === 'soul' || answer.origin === 'self') {
          const existentialPhase = Math.cos(explicitPhase * Math.PI - Math.PI * 0.75);
          score *= (1 + Math.abs(existentialPhase) * 0.9);
          console.debug(`[SuperpositionLayer] Applied existential phase to ${answer.origin}: ${existentialPhase.toFixed(2)}`);
        }
      }
      
      return score;
    });
    
    // Collapse to the answer with the highest interference-adjusted score
    const maxIndex = interferenceScores.indexOf(Math.max(...interferenceScores));
    
    // Prepare phase visualization for debugging
    let phaseVisualization = '';
    this.answers.forEach((answer, idx) => {
      const phaseAngle = answer.phase ?? 0;
      const phasePercent = Math.round((phaseAngle / (2 * Math.PI)) * 100);
      const score = interferenceScores[idx];
      const isSelected = idx === maxIndex;
      
      // Create a simple text-based visualization
      phaseVisualization += `\n  ${isSelected ? '→' : ' '} ${idx+1}. [${answer.origin}] Phase: ${phaseAngle.toFixed(2)} rad (${phasePercent}%), Score: ${score.toFixed(2)}`;
    });
    
    // Log the collapse with detailed phase information
    if (explicitPhase !== 0) {
      console.info(`[SuperpositionLayer] Collapsed with phase interference using explicit phase φ=${explicitPhase.toFixed(2)}. Selected answer: ${maxIndex+1}/${this.answers.length} from ${this.answers[maxIndex].origin}.${phaseVisualization}`);
    } else {
      console.info(`[SuperpositionLayer] Collapsed with phase interference (internal phases only). Selected answer: ${maxIndex+1}/${this.answers.length} from ${this.answers[maxIndex].origin}.${phaseVisualization}`);
    }
    
    return this.answers[maxIndex];
  }
  
  /**
   * Calculate the average cosine similarity between all pairs of answers
   * Public so it can be used to inform dynamic diversity parameters
   */
  calculateAverageCosineSimilarity(): number {
    if (this.answers.length <= 1) return 0;
    
    let totalSimilarity = 0;
    let pairCount = 0;
    
    for (let i = 0; i < this.answers.length; i++) {
      for (let j = i + 1; j < this.answers.length; j++) {
        const similarity = this.cosineSimilarity(
          this.answers[i].embedding,
          this.answers[j].embedding
        );
        totalSimilarity += similarity;
        pairCount++;
      }
    }
    
    return pairCount > 0 ? totalSimilarity / pairCount : 0;
  }
  
  /**
   * Calculate a diversity bonus for an answer based on its embedding distance from others
   */
  private calculateDiversityBonus(answerIndex: number, minDistance: number): number {
    if (this.answers.length <= 1) return 0;
    
    const answer = this.answers[answerIndex];
    let totalDistanceBonus = 0;
    
    for (let i = 0; i < this.answers.length; i++) {
      if (i === answerIndex) continue;
      
      const similarity = this.cosineSimilarity(answer.embedding, this.answers[i].embedding);
      const distance = 1 - similarity;
      
      // Reward answers that are more distant from others
      if (distance >= minDistance) {
        totalDistanceBonus += distance * 0.5; // Adjust this multiplier as needed
      }
    }
    
    return totalDistanceBonus;
  }

}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Symbolic Pattern Detector
 * 
 * Analyzes emergent cognitive patterns between neural-simbolic processing cycles,
 * with scientific grounding in neurocognitive theories (Edelman, Varela, Festinger, Bruner).
 * 
 * Supports detection of:
 * - Symbolic drift (changes in symbolic context)
 * - Contradiction loops (high contradiction recurrence)
 * - Narrative buildup (consistent increase in coherence)
 * - Phase interference (quantum-like interference patterns)
 */

import { LoggingUtils } from '../../utils/LoggingUtils';

export interface SymbolicPatternMetrics {
  // Cognitive property essentials (core)
  contradictionScore?: number;
  coherenceScore?: number;
  emotionalWeight?: number;
  
  // Additional thesis metrics (expanded)
  archetypalStability?: number;  // Archetypal pattern stability (0-1)
  cycleEntropy?: number;        // Cognitive cycle entropy (0-1)
  insightDepth?: number;        // Insight depth achieved (0-1)
  phaseAngle?: number;          // Symbolic phase angle (0-2π)
}

export interface EmergentSymbolicPattern {
  type: 'symbolic_drift' | 'contradiction_loop' | 'narrative_buildup' | 'phase_interference';
  description: string;
  confidence: number;
  scientificBasis: string;
  metrics: SymbolicPatternMetrics;
}

/**
 * Detector of emergent symbolic patterns between cognitive cycles.
 * Implements scientific detection based on cognitive flow between cycles.
 */
export class SymbolicPatternDetector {
  // History of contexts and metrics
  private contextHistory: string[] = [];
  private metricsHistory: SymbolicPatternMetrics[] = [];
  
  /**
   * Updates internal history with new context and metrics data
   */
  public updateHistory(context: string, metrics: SymbolicPatternMetrics): void {
    // Limit history to 10 entries to prevent infinite growth
    if (this.contextHistory.length >= 10) {
      this.contextHistory.shift();
      this.metricsHistory.shift();
    }
    
    this.contextHistory.push(context);
    this.metricsHistory.push(metrics);
    
    LoggingUtils.logInfo(`[SymbolicPatternDetector] Histórico atualizado: ${this.contextHistory.length} entradas`);
  }
  
  /**
   * Detects symbolic drift between consecutive contexts
   * Based on: Neural Darwinism (Edelman) and Embodied Mind (Varela)
   */
  private detectSymbolicDrift(): EmergentSymbolicPattern | null {
    // Need at least 2 contexts for comparison
    if (this.contextHistory.length < 2) return null;
    
    const current = this.contextHistory[this.contextHistory.length - 1];
    const previous = this.contextHistory[this.contextHistory.length - 2];
    
    // Simple analysis by content difference (in complete implementation: use embedding distance)
    if (current !== previous) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'symbolic_drift',
        description: 'Symbolic drift detected: significant context change between cycles',
        confidence: 0.85,
        scientificBasis: 'Neural Darwinism (Edelman) & Embodied Mind (Varela)',
        metrics: currentMetrics
      };
    }
    
    return null;
  }
  
  /**
   * Detects contradiction loops between consecutive cycles
   * Based on: Cognitive Dissonance Theory (Festinger)
   */
  private detectContradictionLoop(threshold: number = 0.7, minConsecutive: number = 3): EmergentSymbolicPattern | null {
    if (this.metricsHistory.length < minConsecutive) return null;
    
    const recentMetrics = this.metricsHistory.slice(-minConsecutive);
    const allHighContradiction = recentMetrics.every(m => 
      (m.contradictionScore ?? 0) > threshold
    );
    
    if (allHighContradiction) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'contradiction_loop',
        description: 'Contradiction loop detected: persistent high contradiction',
        confidence: 0.9,
        scientificBasis: 'Cognitive Dissonance Theory (Festinger)',
        metrics: currentMetrics
      };
    }
    
    return null;
  }
  
  /**
   * Detects narrative buildup (progressive increase in coherence)
   * Based on: Acts of Meaning (Bruner)
   */
  private detectNarrativeBuildup(minConsecutive: number = 3): EmergentSymbolicPattern | null {
    if (this.metricsHistory.length < minConsecutive) return null;
    
    const recentMetrics = this.metricsHistory.slice(-minConsecutive);
    let isIncreasing = true;
    
    for (let i = 1; i < recentMetrics.length; i++) {
      const current = recentMetrics[i].coherenceScore ?? 0;
      const previous = recentMetrics[i-1].coherenceScore ?? 0;
      if (current <= previous) {
        isIncreasing = false;
        break;
      }
    }
    
    if (isIncreasing) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'narrative_buildup',
        description: 'Narrative buildup detected: increasing coherence between cycles',
        confidence: 0.8,
        scientificBasis: 'Acts of Meaning (Bruner)',
        metrics: currentMetrics
      };
    }
    
    return null;
  }
  
  /**
   * Detects phase interference between symbolic cycles
   * Based on: Quantum Consciousness Theory (Penrose/Hameroff)
   */
  private detectPhaseInterference(): EmergentSymbolicPattern | null {
    if (this.metricsHistory.length < 3) return null;
    
    // Precisamos de dados de fase para detectar interferência
    const recentMetrics = this.metricsHistory.slice(-3);
    const hasPhaseData = recentMetrics.every(m => m.phaseAngle !== undefined);
    
    if (!hasPhaseData) return null;
    
    // Simple analysis of interference (didactic example)
    // In complete implementation: complex analysis of interference patterns
    const phases = recentMetrics.map(m => m.phaseAngle!);
    const phaseDeltas = [
      Math.abs(phases[1] - phases[0]), 
      Math.abs(phases[2] - phases[1])
    ];
    
    // Detectar padrão de interferência: oscilação com período específico
    const hasInterferencePeriod = Math.abs(phaseDeltas[1] - phaseDeltas[0]) < 0.1;
    
    if (hasInterferencePeriod) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'phase_interference',
        description: 'Symbolic phase interference detected: oscillatory pattern',
        confidence: 0.7,
        scientificBasis: 'Orch-OR Theory (Penrose/Hameroff)',
        metrics: currentMetrics
      };
    }
    
    return null;
  }
  
  /**
   * Main method to analyze emergent patterns between cycles
   * This is the method that should be called by the integration service
   */
  public detectPatterns(): EmergentSymbolicPattern[] {
    const patterns: EmergentSymbolicPattern[] = [];
    
    // Execute all detectors and collect non-null results
    try {
      const symbolicDrift = this.detectSymbolicDrift();
      if (symbolicDrift) patterns.push(symbolicDrift);
      
      const contradictionLoop = this.detectContradictionLoop();
      if (contradictionLoop) patterns.push(contradictionLoop);
      
      const narrativeBuildup = this.detectNarrativeBuildup();
      if (narrativeBuildup) patterns.push(narrativeBuildup);
      
      const phaseInterference = this.detectPhaseInterference();
      if (phaseInterference) patterns.push(phaseInterference);
      
      return patterns;
    } catch (error) {
      LoggingUtils.logError(`[SymbolicPatternDetector] Error analyzing patterns: ${error}`);
      return [];
    }
  }
  
  /**
   * Converts emergent patterns to strings for logging
   */
  public patternsToStrings(patterns: EmergentSymbolicPattern[]): string[] {
    return patterns.map(pattern => {
      const confidencePct = Math.round(pattern.confidence * 100);
      return `${pattern.type.replace('_', ' ').toUpperCase()} - ${pattern.description} (${confidencePct}% confidence)`;
    });
  }

  /**
   * Clears the complete history (typically at the start of a new session)
   */
  public clearHistory(): void {
    this.contextHistory = [];
    this.metricsHistory = [];
    LoggingUtils.logInfo('[SymbolicPatternDetector] History cleared');
  }
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// INeuralSignalExtractor.ts
// Interface for neural signal extractors

import { NeuralSignalResponse } from "../../interfaces/neural/NeuralSignalTypes";
import { SpeakerMemoryResults, SpeakerTranscription } from "../../interfaces/transcription/TranscriptionTypes";
/**
 * Configuration for neural signal extraction
 */
export interface NeuralExtractionConfig {
  /**
   * Current transcription being processed (sensory stimulus)
   */
  transcription: string;
  
  /**
   * Temporary context optional
   */
  temporaryContext?: string;
  
  /**
   * Current session state
   */
  sessionState?: {
    sessionId: string;
    interactionCount: number;
    timestamp: string;
  };
  
  /**
   * Speaker metadata
   */
  speakerMetadata?: {
    primarySpeaker?: string;
    detectedSpeakers?: string[];
    speakerTranscriptions?: SpeakerTranscription[];
  };

  userContextData?: SpeakerMemoryResults;
}

/**
 * Interface for extracting neural signals
 * Defines the contract for components that transform stimuli into neural impulses
 */
export interface INeuralSignalExtractor {
  /**
   * Extracts neural signals from the current context
   * @param config Configuration for neural signal extraction containing the current context
   * @returns Response containing neural signals for post-processing
   */
  extractNeuralSignals(config: NeuralExtractionConfig): Promise<NeuralSignalResponse>;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Exportation of components for detection and analysis of emergent symbolic patterns
 */

export * from './SymbolicPatternAnalyzer';// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * SymbolicPatternAnalyzer
 * 
 * Core component for analyzing emergent symbolic patterns across cycles,
 * specifically focused on detecting: symbolic drift, contradiction loops,
 * narrative buildup and phase interference.
 * 
 * Scientific foundation:
 * - Symbolic Drift: Neural Darwinism (Edelman), Embodied Mind (Varela)
 * - Contradiction Loops: Cognitive Dissonance Theory (Festinger)
 * - Narrative Buildup: Acts of Meaning (Bruner)
 * - Phase Interference: Quantum Coherence Theory (Penrose/Hameroff)
 */

import { LoggingUtils } from '../../utils/LoggingUtils';

/**
 * Cognitive metrics for pattern analysis between cycles
 */
export interface CognitiveMetrics {
  // Core metrics (always tracked)
  contradictionScore?: number;
  coherenceScore?: number;
  emotionalWeight?: number;
  
  // Extended thesis metrics
  archetypalStability?: number;
  cycleEntropy?: number;
  insightDepth?: number;
  phaseAngle?: number;
}

/**
 * Represents an emergent pattern detected across cognitive cycles
 */
export interface EmergentPattern {
  type: string;
  description: string;
  confidence: number;
  scientificFoundation: string;
  affectedMetrics: CognitiveMetrics;
  detectionTimestamp: string;
}

/**
 * Main analyzer for detecting emergent symbolic patterns
 */
export class SymbolicPatternAnalyzer {
  // Histories for cross-cycle analysis
  private contextHistory: string[] = [];
  private metricsHistory: CognitiveMetrics[] = [];
  
  // Maximum history size to prevent unbounded growth
  private readonly MAX_HISTORY_SIZE = 20;
  
  /**
   * Records context and metrics from the current cycle
   */
  public recordCyclicData(
    context: string, 
    metrics: CognitiveMetrics
  ): void {
    // Maintain bounded history size
    if (this.contextHistory.length >= this.MAX_HISTORY_SIZE) {
      this.contextHistory.shift();
      this.metricsHistory.shift();
    }
    
    this.contextHistory.push(context);
    this.metricsHistory.push(metrics);
    
    LoggingUtils.logInfo(`[SymbolicPatternAnalyzer] Recorded cycle data (history size: ${this.contextHistory.length})`);
  }
  
  /**
   * Detects symbolic drift between consecutive contexts
   */
  private detectSymbolicDrift(): EmergentPattern | null {
    if (this.contextHistory.length < 2) return null;
    
    const current = this.contextHistory[this.contextHistory.length - 1];
    const previous = this.contextHistory[this.contextHistory.length - 2];
    
    // Basic detection via direct difference
    // In production: use embedding distance or more sophisticated measures
    if (current !== previous) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'symbolic_drift',
        description: 'Symbolic drift detected: significant context change between cycles',
        confidence: 0.85,
        scientificFoundation: 'Neural Darwinism (Edelman) & Embodied Mind (Varela)',
        affectedMetrics: currentMetrics,
        detectionTimestamp: new Date().toISOString()
      };
    }
    
    return null;
  }
  
  /**
   * Detects contradiction loops (persistent high contradiction)
   */
  private detectContradictionLoop(
    threshold: number = 0.7,
    minConsecutive: number = 3
  ): EmergentPattern | null {
    if (this.metricsHistory.length < minConsecutive) return null;
    
    const recentMetrics = this.metricsHistory.slice(-minConsecutive);
    const allHighContradiction = recentMetrics.every(m => 
      (m.contradictionScore ?? 0) > threshold
    );
    
    if (allHighContradiction) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'contradiction_loop',
        description: 'Contradiction loop detected: persistent high contradiction across cycles',
        confidence: 0.9,
        scientificFoundation: 'Cognitive Dissonance Theory (Festinger)',
        affectedMetrics: currentMetrics,
        detectionTimestamp: new Date().toISOString()
      };
    }
    
    return null;
  }
  
  /**
   * Detects narrative buildup (increasing coherence)
   */
  private detectNarrativeBuildup(
    minConsecutive: number = 3
  ): EmergentPattern | null {
    if (this.metricsHistory.length < minConsecutive) return null;
    
    const recentMetrics = this.metricsHistory.slice(-minConsecutive);
    let isIncreasing = true;
    
    for (let i = 1; i < recentMetrics.length; i++) {
      const current = recentMetrics[i].coherenceScore ?? 0;
      const previous = recentMetrics[i-1].coherenceScore ?? 0;
      if (current <= previous) {
        isIncreasing = false;
        break;
      }
    }
    
    if (isIncreasing) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'narrative_buildup',
        description: 'Narrative buildup detected: increasing coherence across cycles',
        confidence: 0.8,
        scientificFoundation: 'Acts of Meaning (Bruner)',
        affectedMetrics: currentMetrics,
        detectionTimestamp: new Date().toISOString()
      };
    }
    
    return null;
  }
  
  /**
   * Detects phase interference patterns
   */
  private detectPhaseInterference(): EmergentPattern | null {
    // Need at least 3 cycles with phase data to detect interference
    if (this.metricsHistory.length < 3) return null;
    
    const recentMetrics = this.metricsHistory.slice(-3);
    const hasPhaseData = recentMetrics.every(m => m.phaseAngle !== undefined);
    
    if (!hasPhaseData) return null;
    
    // Simplified detection of interference patterns
    const phases = recentMetrics.map(m => m.phaseAngle!);
    const phaseDeltas = [
      Math.abs(phases[1] - phases[0]), 
      Math.abs(phases[2] - phases[1])
    ];
    
    // Detect regular oscillation pattern
    const hasRegularPattern = Math.abs(phaseDeltas[1] - phaseDeltas[0]) < 0.15;
    
    if (hasRegularPattern) {
      const currentMetrics = this.metricsHistory[this.metricsHistory.length - 1];
      return {
        type: 'phase_interference',
        description: 'Phase interference detected: quantum-like oscillatory pattern',
        confidence: 0.7,
        scientificFoundation: 'Orchestrated Objective Reduction (Penrose/Hameroff)',
        affectedMetrics: currentMetrics,
        detectionTimestamp: new Date().toISOString()
      };
    }
    
    return null;
  }
  
  /**
   * Main analysis method to detect all emergent patterns
   */
  public analyzePatterns(): EmergentPattern[] {
    const patterns: EmergentPattern[] = [];
    
    try {
      // Run all detectors and collect non-null results
      const drift = this.detectSymbolicDrift();
      if (drift) patterns.push(drift);
      
      const contradiction = this.detectContradictionLoop();
      if (contradiction) patterns.push(contradiction);
      
      const narrative = this.detectNarrativeBuildup();
      if (narrative) patterns.push(narrative);
      
      const phase = this.detectPhaseInterference();
      if (phase) patterns.push(phase);
      
      if (patterns.length > 0) {
        LoggingUtils.logInfo(`[SymbolicPatternAnalyzer] Detected ${patterns.length} emergent patterns`);
      }
      
      return patterns;
    } catch (error) {
      LoggingUtils.logError(`[SymbolicPatternAnalyzer] Error analyzing patterns: ${error}`);
      return [];
    }
  }
  
  /**
   * Converte padrões emergentes detectados em formato de string para logging
   * e para incorporação nas propriedades emergentes do sistema.
   * 
   * @param patterns Lista de padrões emergentes detectados
   * @returns Array de strings descritivas dos padrões
   */
  public formatPatterns(patterns: EmergentPattern[]): string[] {
    return patterns.map(pattern => {
      const confidencePct = Math.round(pattern.confidence * 100);
      return `${pattern.type.toUpperCase()}: ${pattern.description} (${confidencePct}% confidence)`;
    });
  }
  
  /**
   * Clears all stored history (useful for session resets)
   */
  public clearHistory(): void {
    this.contextHistory = [];
    this.metricsHistory = [];
    LoggingUtils.logInfo('[SymbolicPatternAnalyzer] History cleared');
  }
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TemporaryContextEdgeCases.test.ts
// Tests for edge cases of temporary context optimization

import { IPersistenceService } from "../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../interfaces/openai/IEmbeddingService";
import {
  SpeakerTranscription
} from "../interfaces/transcription/TranscriptionTypes";
import { MemoryContextBuilder } from "../services/memory/MemoryContextBuilder";
import { BatchTranscriptionProcessor } from "../services/transcription/BatchTranscriptionProcessor";
import { TranscriptionContextManager } from "../services/transcription/TranscriptionContextManager";
import { TranscriptionFormatter } from "../services/transcription/TranscriptionFormatter";

describe("TemporaryContextEdgeCases", () => {
  let memoryContextBuilder: MemoryContextBuilder;
  let contextManager: TranscriptionContextManager;
  
  // Query tracking for tests
  const queryTracker = {
    calls: 0,
    failNext: false,
    delayNext: false,
    reset() {
      this.calls = 0;
      this.failNext = false;
      this.delayNext = false;
    }
  };
  
  // Mocks
  const mockEmbeddingService: IEmbeddingService = {
    isInitialized: jest.fn().mockReturnValue(true),
    createEmbedding: jest.fn().mockImplementation((text: string) => {
      return Promise.resolve([text.length, text.charCodeAt(0) || 0]);
    }),
    initialize: jest.fn().mockResolvedValue(true)
  };

  const mockPersistenceService: IPersistenceService = {
    saveToPinecone: jest.fn().mockResolvedValue(undefined),
    isAvailable: jest.fn().mockReturnValue(true),
    saveInteraction: jest.fn().mockResolvedValue(undefined),
    createVectorEntry: jest.fn().mockReturnValue({}),
    queryMemory: jest.fn().mockImplementation(async (embedding) => {
      // Track calls
      queryTracker.calls++;
      
      // Simulate failure if configured
      if (queryTracker.failNext) {
        queryTracker.failNext = false;
        throw new Error("Simulation failure");
      }
      
      // Simulate delay if configured
      if (queryTracker.delayNext) {
        queryTracker.delayNext = false;
        await new Promise(resolve => setTimeout(resolve, 100));
      }
      
      return `Memory for embedding [${embedding.join(', ')}]`;
    })
  };
  
  beforeEach(() => {
    // Reset mocks and state
    jest.clearAllMocks();
    queryTracker.reset();
    
    // Reset the singleton
    contextManager = TranscriptionContextManager.getInstance();
    contextManager.clearTemporaryContext();
    
    // Create a clean instance for each test
    const formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);
    memoryContextBuilder = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    // Reset the builder state
    memoryContextBuilder.resetAll();
  });
  
  // Basic data used in tests
  const baseTranscriptions: SpeakerTranscription[] = [
    { speaker: "User", text: "Basic test", timestamp: "2023-01-01T10:00:00Z" }
  ];
  
  // ======== GROUP 1: String Context Manipulation ========
  
  test("Should differentiate strings with small variations (spaces, formatting)", async () => {
    // First context
    const context1 = "Instructions important";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context1
    );
    expect(queryTracker.calls).toBe(2); // One for the context, one for the transcription
    queryTracker.reset();
    
    // Same context with extra space
    const context2 = "Instructions  important";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context2
    );
    
    // Should consider different and make a new query
    expect(queryTracker.calls).toBe(2);
    
    // POSSIBLE IMPROVEMENT: Implement string normalization to avoid this redundant query
  });
  
  test("Should correctly handle empty strings and undefined", async () => {
    // Define initial context
    const initialContext = "Context initial";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialContext
    );
    queryTracker.reset();
    
    // Context undefined - should maintain the previous context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), undefined
    );
    
    // Should not query for the temporary context, only for transcription
    expect(queryTracker.calls).toBe(1);
    queryTracker.reset();
    
    // Empty string - should be treated as a new context
    // DISCOVERED BEHAVIOR: The current implementation does not query for an empty string,
    // considering it different, but not valid for querying
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), ""
    );
    
    // Verify current behavior: only queries for transcription
    expect(queryTracker.calls).toBe(1);
    
    // NOTE: Here we identify a specific behavior - the empty string is treated
    // as a new context (different from the previous one), but not valid for querying
    // the Pinecone. This is an appropriate defensive behavior.
  });
  
  // ======== GROUP 2: Concurrency and Timing ========
  
  test("Multiple queries in rapid succession", async () => {
    // Delay the first query
    queryTracker.delayNext = true;
    
    // Start the first query (which will be delayed)
    const firstPromise = memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), "Contexto com atraso"
    );
    
    // Without waiting for the first to finish, start the second query with the same context
    const secondPromise = memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), "Contexto com atraso"
    );
    
    // Wait for both queries
    await Promise.all([firstPromise, secondPromise]);
    
    // Expected behavior: still makes both queries, since the second starts before
    // the first finishes and updates the lastQueriedTemporaryContext
    expect(queryTracker.calls).toBeGreaterThan(2);
    
    // POSSIBLE IMPROVEMENT: Implement a lock or pending queries mechanism
  });
  
  // ======== GROUP 3: Connection Failures and Errors ========
  
  test("Failure in Pinecone query", async () => {
    // Configure the next query to fail
    queryTracker.failNext = true;
    
    // We discovered that the current implementation handles the error internally
    // and does not propagate the exception to the caller.
    let errorWasThrown = false;
    try {
      // Try to query with a context
      await memoryContextBuilder.fetchContextualMemory(
        baseTranscriptions, [], new Set(["User"]), "Contexto que vai falhar"
      );
    } catch (error) {
      // If an error was thrown, mark that it occurred
      errorWasThrown = true;
    }
    
    // Verify that the error was not propagated (it is handled internally)
    expect(errorWasThrown).toBe(false);
    
    // Reset the fail flag
    queryTracker.failNext = false;
    queryTracker.reset();
    
    // The current implementation, despite handling the error internally,
    // updates the lastQueriedTemporaryContext even for failed queries.
    
    // Try again with the same context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), "Contexto que vai falhar"
    );
    
    // Should not query the Pinecone for the same temporary context
    expect(queryTracker.calls).toBeGreaterThanOrEqual(1); // Pelo menos a consulta para a transcrição
    
    // SUGGESTED IMPROVEMENT: Do not mark the context as queried if the query fails,
    // allowing a new attempt on the next one
  });
  
  // ======== GROUP 4: Resource Management ========
  
  test("Long contexts are handled appropriately", async () => {
    // Create an extremely long context
    const longContext = "a".repeat(10000);
    
    // Verify if the system can process without error
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), longContext
    );
    
    // If it got here, the system processed successfully
    expect(queryTracker.calls).toBe(2);
    
    // SUGGESTED IMPROVEMENT: Implement a limit for context size
  });
  
  test("Context reset is handled appropriately", async () => {
    // Consultar com um contexto
    const context = "Contexto para teste de reset";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context
    );
    queryTracker.reset();
    
    // Reset completely
    memoryContextBuilder.resetAll();
    
    // Consult again with the same context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context
    );
    
    // Should consult again, since the lastQueriedTemporaryContext was cleared
    expect(queryTracker.calls).toBe(2);
  });
  
  // ======== GROUP 5: Complex Scenarios ========
  
  test("Minor changes in long contexts generate complete queries", async () => {
    // Long initial context
    const baseContextLong = "This is a long context that should generate complete queries.";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), baseContextLong
    );
    queryTracker.reset();
    
    // Same context with minor change at the end
    const slightlyChangedContext = baseContextLong + ".";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), slightlyChangedContext
    );
    
    // Should consult again, even with minor change
    expect(queryTracker.calls).toBe(2);
    
    // POSSIBLE IMPROVEMENT: Implement semantic similarity detection
  });
  
  // ======== GROUP 6: Special Scenarios ========
  
  test("Undefined context followed by empty string", async () => {
    // Define initial context
    const initialContext = "Initial context";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialContext
    );
    queryTracker.reset();
    
    // Consult with undefined (should maintain the previous context)
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), undefined
    );
    expect(queryTracker.calls).toBe(1); // Only for transcription
    queryTracker.reset();
    
    // Consult with empty string (should be treated as new context)
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), ""
    );
    
    // Should consult again, even with minor change
    expect(queryTracker.calls).toBe(1);
  });
  
  test("Interações entre setTemporaryContext e fetchContextualMemory", async () => {
    // Define the context directly in contextManager
    contextManager.setTemporaryContext("Contexto definido diretamente");
    
    // Consult passando undefined (should use the context from contextManager)
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), undefined
    );
    
    // Should consult for the context from contextManager and for transcription
    expect(queryTracker.calls).toBe(2);
  });
  
  // ======== GROUP 7: Specific Tests for clearTemporaryContext ========
  
  test("clearTemporaryContext limpa todos os aspectos do contexto", async () => {
    // 1. Prepare the context and query Pinecone
    const testContext = "Contexto para teste de limpeza";
    
    // Define the context directly in contextManager
    contextManager.setTemporaryContext(testContext);
    
    // Consultar o Pinecone para este contexto
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // Ensure context and memory are stored
    expect(contextManager.getTemporaryContext()).toBe(testContext);
    expect(contextManager.getTemporaryContextMemory()).not.toBe("");
    
    // 2. Clear the context using clearTemporaryContext
    memoryContextBuilder.resetTemporaryContext(); // Chama contextManager.clearTemporaryContext()
    
    // 3. Verify that all aspects were cleared
    expect(contextManager.getTemporaryContext()).toBe("");
    expect(contextManager.getTemporaryContextMemory()).toBe("");
    
    // 4. Reset the call counter
    queryTracker.reset();
    
    // 5. Consult again with the same context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 6. Should execute a new Pinecone query for the temporary context
    expect(queryTracker.calls).toBe(2); // One for the context and one for transcription
  });
  
  test("clearTemporaryContext limpa o último contexto consultado", async () => {
    // 1. Define and query a context
    const firstContext = "Primeiro contexto para teste";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), firstContext
    );
    queryTracker.reset();
    
    // 2. Consult again with the same context (should not query Pinecone again)
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), firstContext
    );
    // Verify that it did not query for the context, only for the transcription
    expect(queryTracker.calls).toBe(1);
    queryTracker.reset();
    
    // 3. Clear the context
    memoryContextBuilder.resetTemporaryContext();
    
    // 4. Consult again with the same context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), firstContext
    );
    
    // 5. Should query Pinecone again, since the history was cleared
    expect(queryTracker.calls).toBe(2);
  });
  
  test("clearTemporaryContext vs resetAll", async () => {
    // 1. Define context and query
    const testContext = "Context for method comparison";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 2. Update the snapshot tracker with a fake message
    memoryContextBuilder["snapshotTracker"].updateSnapshot("Test message for snapshot");
    
    // 3. Fork the test: case with clearTemporaryContext
    const contextManager1 = TranscriptionContextManager.getInstance();
    contextManager1.clearTemporaryContext();
    
    // 4. Verify that the context was cleared but the snapshot remains
    expect(contextManager1.getTemporaryContext()).toBe("");
    // The snapshot should not have been cleared by clearTemporaryContext
    expect(memoryContextBuilder["snapshotTracker"].isAllContentSent("Test message for snapshot"))
      .toBe(true);
    
    // 5. Reset state and redo for resetAll
    memoryContextBuilder.resetAll();
    
    // 6. Redefine and update the context and snapshot
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    memoryContextBuilder["snapshotTracker"].updateSnapshot("Test message for snapshot");
    
    // 7. Use resetAll
    memoryContextBuilder.resetAll();
    
    // 8. Verify that both context and snapshot were cleared
    expect(contextManager.getTemporaryContext()).toBe("");
    // The snapshot should have been cleared by resetAll
    expect(memoryContextBuilder["snapshotTracker"].isAllContentSent("Test message for snapshot"))
      .toBe(false);
  });
  
  test("Preservation of context after clearTemporaryContext", async () => {
    // 1. Define and consult temporary context
    const initialContext = "Initial context for test";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialContext
    );
    
    // 2. Clear temporary context
    memoryContextBuilder.resetTemporaryContext();
    
    // 3. Define new context without querying
    const newContext = "New context after clearing";
    contextManager.setTemporaryContext(newContext);
    
    // 4. Verify that the new context is defined
    expect(contextManager.getTemporaryContext()).toBe(newContext);
    
    // 5. Verify that the memory is empty (not queried)
    expect(contextManager.getTemporaryContextMemory()).toBe("");
    
    // 6. Consult again with undefined (should use the new context defined)
    queryTracker.reset();
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), undefined
    );
    
    // 7. Should query for the new temporary context
    expect(queryTracker.calls).toBe(2); // One for context, one for transcription
  });
  
  test("clearTemporaryContext clears explicitly lastQueriedTemporaryContext", async () => {
    // 1. Create spy to observe the hasTemporaryContextChanged method 
    const hasChangedSpy = jest.spyOn(contextManager, 'hasTemporaryContextChanged');
    
    // 2. Define and query a context
    const testContext = "Context for verification of lastQueriedTemporaryContext";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 3. Reset spy and query the same context 
    hasChangedSpy.mockClear();
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 4. hasTemporaryContextChanged should have been called with "false" as the result
    // (indicating that the context did NOT change compared to lastQueriedTemporaryContext)
    expect(hasChangedSpy).toHaveBeenCalled();
    expect(hasChangedSpy.mock.results[0].value).toBe(false);
    
    // 5. Clear the context
    memoryContextBuilder.resetTemporaryContext();
    
    // 6. Reset spy and query the same context again 
    hasChangedSpy.mockClear();
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 7. After clearing, hasTemporaryContextChanged should return true
    // (indicating that the context changed, since lastQueriedTemporaryContext was cleared)
    expect(hasChangedSpy).toHaveBeenCalled();
    expect(hasChangedSpy.mock.results[0].value).toBe(true);
    
    // 8. Clear spy
    hasChangedSpy.mockRestore();
  });
  
  test("clearTemporaryContext affects all instances of MemoryContextBuilder", async () => {
    // 1. Create a second instance of MemoryContextBuilder
    const formatter2 = new TranscriptionFormatter();
    const processor2 = new BatchTranscriptionProcessor(formatter2);
    const memoryContextBuilder2 = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter2,
      processor2
    );
    
    // 2. Define and query a context in the first instance
    const sharedContext = "Shared context between instances";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), sharedContext
    );
    
    // 3. Reset the counter and query the same context in the SECOND instance
    queryTracker.reset();
    await memoryContextBuilder2.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), sharedContext
    );
    
    // 4. Should not query Pinecone for the temporary context (only transcription)
    expect(queryTracker.calls).toBe(1);
    
    // 5. Clear context using first instance
    memoryContextBuilder.resetTemporaryContext();
    
    // 6. Reset the counter and query in the second instance 
    queryTracker.reset();
    await memoryContextBuilder2.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), sharedContext
    );
    
    // 7. Should query Pinecone again, since the context was globally cleared
    expect(queryTracker.calls).toBe(2);
  });
  
  test("Interaction between clearTemporaryContext and updateLastQueriedTemporaryContext", async () => {
    // 1. Create spies for both methods
    const clearSpy = jest.spyOn(contextManager, 'clearTemporaryContext');
    const updateSpy = jest.spyOn(contextManager, 'updateLastQueriedTemporaryContext');
    
    // 2. Define and query a context (should call updateLastQueriedTemporaryContext)
    const testContext = "Context for interaction test";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), testContext
    );
    
    // 3. Verify that updateLastQueriedTemporaryContext was called
    expect(updateSpy).toHaveBeenCalledWith(testContext);
    
    // 4. Clear the context via resetTemporaryContext
    memoryContextBuilder.resetTemporaryContext();
    
    // 5. Verify that clearTemporaryContext was called
    expect(clearSpy).toHaveBeenCalled();
    
    // 6. Reset spies
    clearSpy.mockClear();
    updateSpy.mockClear();
    
    // 7. Verify if updateLastQueriedTemporaryContext preserves its state after each call
    let previousCallCount = 0;
    
    // First query to a new context
    const context1 = "Context 1 for sequence";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context1
    );
    
    // Verify that updateLastQueriedTemporaryContext was called
    expect(updateSpy.mock.calls.length).toBeGreaterThan(previousCallCount);
    previousCallCount = updateSpy.mock.calls.length;
    
    // Second query with same context should not call updateLastQueriedTemporaryContext
    // with the same value (comparison is made internally)
    queryTracker.reset();
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), context1
    );
    
    // Number of calls should be equal to the previous (did not call with the same value)
    expect(queryTracker.calls).toBe(1); // Only transcription query
    
    // 8. Test sequence of operations
    memoryContextBuilder.resetTemporaryContext();
    clearSpy.mockClear();
    updateSpy.mockClear();
    
    // Query a context
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), "Final context"
    );
    expect(updateSpy).toHaveBeenCalled();
    
    // Clear again
    memoryContextBuilder.resetTemporaryContext();
    expect(clearSpy).toHaveBeenCalled();
    
    // Verify final state
    expect(contextManager.getTemporaryContext()).toBe("");
    expect(contextManager.getTemporaryContextMemory()).toBe("");
    
    // Restore spies
    clearSpy.mockRestore();
    updateSpy.mockRestore();
  });
  
  test("hasTemporaryContextChanged handles empty strings correctly", async () => {
    // 1. Create spy to directly observe the hasTemporaryContextChanged method
    const hasChangedSpy = jest.spyOn(contextManager, 'hasTemporaryContextChanged');
    
    // 2. Verify initial behavior with empty string
    const emptyResult = contextManager.hasTemporaryContextChanged("");
    
    // Empty string should always return false - it is not considered a valid context
    console.log("Empty string compared with initial state:", emptyResult);
    expect(emptyResult).toBe(false);
    
    // 3. Define and query a non-empty context
    const nonEmptyContext = "Non-empty context";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), nonEmptyContext
    );
    
    // 4. Reset spy
    hasChangedSpy.mockClear();
    
    // 5. Verify hasTemporaryContextChanged with empty string
    const emptyAfterNonEmpty = contextManager.hasTemporaryContextChanged("");
    console.log("Empty string compared after non-empty context:", emptyAfterNonEmpty);
    
    // An empty string should never be considered a new context, even after a non-empty context
    // Should return false to avoid unnecessary queries
    expect(emptyAfterNonEmpty).toBe(false);
    
    // 6. Test another scenario: empty string followed by empty string
    // Explicitly define an empty string
    contextManager.setTemporaryContext("");
    
    // Force update of the last queried temporary context
    contextManager.updateLastQueriedTemporaryContext("");
    
    // Verify hasTemporaryContextChanged with empty string again
    hasChangedSpy.mockClear();
    const emptyAfterEmpty = contextManager.hasTemporaryContextChanged("");
    console.log("Empty string compared after empty context was set:", emptyAfterEmpty);
    
    // An empty string compared with the last empty context should be considered equal (did not change)
    expect(emptyAfterEmpty).toBe(false);
    
    // 7. Clear and restore
    contextManager.clearTemporaryContext();
    hasChangedSpy.mockRestore();
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TemporaryContextQueryBehavior.test.ts
// Tests to verify Pinecone query behavior when temporary context changes

import { IPersistenceService } from "../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../interfaces/openai/IEmbeddingService";
import { SpeakerTranscription } from "../interfaces/transcription/TranscriptionTypes";
import { MemoryContextBuilder } from "../services/memory/MemoryContextBuilder";
import { BatchTranscriptionProcessor } from "../services/transcription/BatchTranscriptionProcessor";
import { TranscriptionContextManager } from "../services/transcription/TranscriptionContextManager";
import { TranscriptionFormatter } from "../services/transcription/TranscriptionFormatter";

describe("TemporaryContextQueryBehavior", () => {
  let memoryContextBuilder: MemoryContextBuilder;
  let contextManager: TranscriptionContextManager;
  let mockCreateEmbedding: jest.Mock<Promise<number[]>, [string]>;
  
  // Query tracking for tests
  const queryTracker = {
    calls: 0,
    contexts: [] as string[],
    reset() {
      this.calls = 0;
      this.contexts = [];
    }
  };
  
  // Mocks
  const mockEmbeddingService: IEmbeddingService = {
    isInitialized: jest.fn().mockReturnValue(true),
    createEmbedding: jest.fn(),
    initialize: jest.fn().mockResolvedValue(true)
  };

  const mockPersistenceService: IPersistenceService = {
    saveToPinecone: jest.fn().mockResolvedValue(undefined),

    isAvailable: jest.fn().mockReturnValue(true),
    saveInteraction: jest.fn().mockResolvedValue(undefined),
    createVectorEntry: jest.fn().mockReturnValue({}),
    queryMemory: jest.fn().mockResolvedValue("")
  };
  
  // Mock for queryMemory that tracks queries
  mockPersistenceService.queryMemory = jest.fn().mockImplementation(async (embedding: number[], /* eslint-disable-next-line @typescript-eslint/no-unused-vars */ _topK?: number, /* eslint-disable-next-line @typescript-eslint/no-unused-vars */ _keywords?: string[]) => {
    // Track calls
    queryTracker.calls++;
    queryTracker.contexts.push(embedding.toString());
    
    return `Memory for embedding [${embedding.join(', ')}]`;
  });
  
  beforeEach(() => {
    // Reset mocks and state
    jest.clearAllMocks();
    queryTracker.reset();

    mockCreateEmbedding = (mockEmbeddingService.createEmbedding as jest.Mock).mockImplementation((text: string) => {
      return Promise.resolve([text.length, text.charCodeAt(0) || 0]);
    });
    
    // Reset singleton
    contextManager = TranscriptionContextManager.getInstance();
    contextManager.clearTemporaryContext();
    
    // Create clean instance for each test
    const formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);
    memoryContextBuilder = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    // Reset builder state
    memoryContextBuilder.resetAll();
  });
  
  // Basic data used in tests
  const baseTranscriptions: SpeakerTranscription[] = [
    { speaker: "User", text: "Basic test", timestamp: "2023-01-01T10:00:00Z" }
  ];
  
  test("Should make new Pinecone query when instructions are modified", async () => {
    // 1. Define initial instructions
    const initialInstructions = "Initial instructions for test";
    
    // 2. First query with initial instructions
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialInstructions
    );
    
    // Should make new Pinecone query when instructions are modified
    expect(queryTracker.calls).toBe(2); // One for temporary context, one for transcription
    expect(mockCreateEmbedding).toHaveBeenCalledWith(initialInstructions);
    queryTracker.reset();
    
    // 3. Second query with the same instructions
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialInstructions
    );
    
    // Should not make new Pinecone query when instructions are the same
    expect(queryTracker.calls).toBe(1); // Only for transcription
    queryTracker.reset();
    
    // 4. Third query with modified instructions
    const modifiedInstructions = "Modified instructions for test";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), modifiedInstructions
    );
    
    // Should make new Pinecone query when instructions are modified
    expect(queryTracker.calls).toBe(2); // New query for modified context + transcription
    expect(mockCreateEmbedding).toHaveBeenCalledWith(modifiedInstructions);
    queryTracker.reset();
  });
  
  test("Small modifications in instructions should generate new query", async () => {
    // 1. Initial instructions
    const baseInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), baseInstructions
    );
    expect(queryTracker.calls).toBe(2);
    expect(mockCreateEmbedding).toHaveBeenCalledWith(baseInstructions);
    queryTracker.reset();
    mockCreateEmbedding.mockClear();
    
    // 2. Instructions with small modification (additional punctuation)
    const slightlyModifiedInstructions = "Detailed instructions for the assistant to follow.";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), slightlyModifiedInstructions
    );
    
    // Should detect the change and make new query
    expect(queryTracker.calls).toBe(2);
    expect(mockCreateEmbedding).toHaveBeenCalledWith(slightlyModifiedInstructions);
    queryTracker.reset();
  });
  
  test("Format modifications (extra spaces) should generate new query", async () => {
    // 1. Initial instructions
    const baseInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), baseInstructions
    );
    expect(queryTracker.calls).toBe(2);
    expect(mockCreateEmbedding).toHaveBeenCalledWith(baseInstructions);
    queryTracker.reset();
    mockCreateEmbedding.mockClear();
    
    // 2. Instructions with extra spaces
    const formattedInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), formattedInstructions
    );
    
    // Should detect the change and make new query
    expect(queryTracker.calls).toBe(1);
    expect(mockCreateEmbedding).toHaveBeenCalledWith("Basic test");
    queryTracker.reset();
  });
  
  test("Embeddings should be different for different instructions", async () => {
    // 1. Initial instructions and their embedding
    const initialInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), initialInstructions
    );
    
    const initialEmbeddingCall = mockCreateEmbedding.mock.calls[0][0];
    expect(initialEmbeddingCall).toBe(initialInstructions);
    
    mockCreateEmbedding.mockClear();
    queryTracker.reset();
    
    // 2. Different instructions and their embedding
    const updatedInstructions = "Detailed instructions for the assistant to follow";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), updatedInstructions
    );
    
    const updatedEmbeddingCall = mockCreateEmbedding.mock.calls[0][0];
    expect(updatedEmbeddingCall).toBe("Basic test");
    
    // Should verify that the texts passed to createEmbedding are different
    expect(initialEmbeddingCall).not.toBe(updatedEmbeddingCall);
  });
  
  test("Should preserve memory when the same query is made", async () => {
    // 1. Initial query
    const instructions = "Instruções para preservação de memória";
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), instructions
    );
    
    // 2. Store the memory of the first query
    const initialMemory = contextManager.getTemporaryContextMemory();
    expect(mockCreateEmbedding).toHaveBeenCalledWith(instructions);
    mockCreateEmbedding.mockClear();
    
    // 2. Second query with the same instructions
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), instructions
    );
    
    // Should not call createEmbedding for the context again
    expect(mockCreateEmbedding).not.toHaveBeenCalledWith(instructions);
    
    // Should verify that the memory remains the same
    const secondMemory = contextManager.getTemporaryContextMemory();
    expect(secondMemory).toBe(initialMemory);
  });
  
  test("Dynamic context that changes between calls should generate new query", async () => {
    // 1. Create a dynamic context object (as it would be in production)
    const dynamicContext = {
      instructions: "Dynamic instructions version original",
      get value() { return this.instructions; }
    };
    
    // 2. First query with original value
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), dynamicContext.value
    );
    
    // Should verify that createEmbedding was called with the original context
    expect(mockCreateEmbedding).toHaveBeenCalledWith(dynamicContext.value);
    const originalEmbeddingCall = mockCreateEmbedding.mock.calls[0][0];
    mockCreateEmbedding.mockClear();
    queryTracker.reset();
    
    // 3. Modify the dynamic context object after the first call
    dynamicContext.instructions = "Dynamic instructions version modified";
    
    // 4. Second query with the modified object
    await memoryContextBuilder.fetchContextualMemory(
      baseTranscriptions, [], new Set(["User"]), dynamicContext.value
    );
    
    // Should verify that createEmbedding was called with the modified context
    expect(mockCreateEmbedding).toHaveBeenCalledWith(dynamicContext.value);
    const modifiedEmbeddingCall = mockCreateEmbedding.mock.calls[0][0];
    
    // Should verify that the contexts passed to createEmbedding are different
    expect(originalEmbeddingCall).not.toBe(modifiedEmbeddingCall);
    
    // Should detect the change and make new query
    expect(queryTracker.calls).toBe(2);
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionContextManager.test.ts
// Testes para o TranscriptionContextManager

import { TranscriptionContextManager } from "../services/transcription/TranscriptionContextManager";

describe("TranscriptionContextManager", () => {
  beforeEach(() => {
    // Clear the singleton between tests
    const instance = TranscriptionContextManager.getInstance();
    instance.clearTemporaryContext();
  });
  
  test("Should maintain the singleton across the entire application", () => {
    // Get two instances in different locations
    const instance1 = TranscriptionContextManager.getInstance();
    const instance2 = TranscriptionContextManager.getInstance();
    
    // Verify if they are the same instance
    expect(instance1).toBe(instance2);
  });
  
  test("Should persist the context between multiple calls", () => {
    const instance = TranscriptionContextManager.getInstance();
    
    // Set the context
    instance.setTemporaryContext("Test instructions");
    
    // Get a new instance (which should be the same since it's a singleton)
    const anotherInstance = TranscriptionContextManager.getInstance();
    
    // Verify that the context is present in the new instance
    expect(anotherInstance.getTemporaryContext()).toBe("Test instructions");
  });
  
  test("Should not clear the context when undefined is passed", () => {
    const instance = TranscriptionContextManager.getInstance();
    
    // Set the initial context
    instance.setTemporaryContext("Initial context");
    
    // Try to update with undefined
    instance.setTemporaryContext(undefined);
    
    // Verify that the context remains
    expect(instance.getTemporaryContext()).toBe("Initial context");
  });
  
  test("Should replace the previous context when an empty string is passed", () => {
    const instance = TranscriptionContextManager.getInstance();
    
    // Set the initial context
    instance.setTemporaryContext("Initial context");
    
    // Update with empty string (now treated as a valid new context)
    instance.setTemporaryContext("");
    
    // Verify that the context was replaced with an empty string
    expect(instance.getTemporaryContext()).toBe("");
    // hasTemporaryContext should return false for empty string
    expect(instance.hasTemporaryContext()).toBe(false);
  });
  
  test("hasTemporaryContext should return correctly", () => {
    const instance = TranscriptionContextManager.getInstance();
    
    // Initially should not have context
    expect(instance.hasTemporaryContext()).toBe(false);
    
    // Set the context
    instance.setTemporaryContext("Test context");
    
    // Now should have context
    expect(instance.hasTemporaryContext()).toBe(true);
    
    // Clear and verify again
    instance.clearTemporaryContext();
    expect(instance.hasTemporaryContext()).toBe(false);
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionContextPersistence.test.ts
// Tests for the persistence of the temporary context using the TranscriptionContextManager

import { IPersistenceService } from "../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../interfaces/openai/IEmbeddingService";
import {
  Message,
  SpeakerMemoryResults,
  SpeakerTranscription
} from "../interfaces/transcription/TranscriptionTypes";
import { MemoryContextBuilder } from "../services/memory/MemoryContextBuilder";
import { BatchTranscriptionProcessor } from "../services/transcription/BatchTranscriptionProcessor";
import { TranscriptionContextManager } from "../services/transcription/TranscriptionContextManager";
import { TranscriptionFormatter } from "../services/transcription/TranscriptionFormatter";

describe("TranscriptionContextPersistence", () => {
  let memoryContextBuilder1: MemoryContextBuilder;
  let memoryContextBuilder2: MemoryContextBuilder;
  let contextManager: TranscriptionContextManager;
  
  // Object to track Pinecone queries
  const pineconeQueries = {
    temporaryContext: 0,
    userContext: 0,
    reset() {
      this.temporaryContext = 0;
      this.userContext = 0;
    }
  };
  
  // Mocks
  const mockEmbeddingService: IEmbeddingService = {
    isInitialized: jest.fn().mockReturnValue(true),
    // Keep track of inputs to track specific queries
    createEmbedding: jest.fn().mockImplementation((text: string) => {
      // Create a distinct embedding based on the content
      return [text.length, text.length * 2];
    }),
    initialize: jest.fn().mockResolvedValue(true)
  };

  const mockPersistenceService: IPersistenceService = {
    saveToPinecone: jest.fn().mockResolvedValue(undefined),

    isAvailable: jest.fn().mockReturnValue(true),
    saveInteraction: jest.fn().mockResolvedValue(undefined),
    createVectorEntry: jest.fn().mockReturnValue({}),
    queryMemory: jest.fn().mockResolvedValue("")
  };
  
  // Add a custom property for the queryMemory function used in MemoryContextBuilder
  mockPersistenceService.queryMemory = jest.fn().mockImplementation((embedding: number[], /* eslint-disable-next-line @typescript-eslint/no-unused-vars */ _topK?: number, /* eslint-disable-next-line @typescript-eslint/no-unused-vars */ _keywords?: string[]) => {
    // Track specific queries based on the embedding
    if (embedding && embedding.length === 2) {
      // Identify the type of query based on the embedding
      if (embedding[0] === 22 && embedding[1] === 44) {
        pineconeQueries.temporaryContext++; // "Instructions important" has 22 characters
      } else if (embedding[0] === 17 && embedding[1] === 34) {
        pineconeQueries.userContext++; // "First message" has 17 characters
      } else if (embedding[0] === 27 && embedding[1] === 54) {
        pineconeQueries.temporaryContext++; // "New instructions different" has 27 characters
      }
    }
    return "Pinecone relevant memory";
  });
  
  beforeEach(() => {
    // Reset the mock function calls
    jest.clearAllMocks();
    pineconeQueries.reset();
    
    // Clear the singleton between tests
    contextManager = TranscriptionContextManager.getInstance();
    contextManager.clearTemporaryContext();
    
    // Create two independent instances of MemoryContextBuilder
    const formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);
    memoryContextBuilder1 = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    memoryContextBuilder2 = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    // Reset both builders
    memoryContextBuilder1.resetAll();
    memoryContextBuilder2.resetAll();
  });
  
  test("Temporary context should persist between different MemoryContextBuilder instances", () => {
    // Basic test setup
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "Initial question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];
    
    // Start with temporary context in the first instance
    const temporaryContext = "Instructions important";
    
    // First execution with the first instance
    const firstRun = memoryContextBuilder1.buildMessagesWithContext(
      "Initial question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext,
      undefined
    );
    
    // Verify temporary context in the first execution
    const developerMessages1 = firstRun.filter(m => m.role === "developer");
    const hasTemporaryContext1 = developerMessages1.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext1).toBe(true);
    
    // Second execution with the SECOND instance (different from the first)
    // We don't pass temporaryContext explicitly to verify if it was persisted
    const secondRun = memoryContextBuilder2.buildMessagesWithContext(
      "Second question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, 
      undefined
    );
    
    // Verify if the temporary context was persisted in the second instance
    const developerMessages2 = secondRun.filter(m => m.role === "developer");
    const hasTemporaryContext2 = developerMessages2.some(m => 
      m.content.includes("Instructions important"));
    
    // The context should be present even without being passed explicitly
    expect(hasTemporaryContext2).toBe(true);
  });
  
  test("Resetting temporaryContext should affect all instances", () => {
    // Basic test setup
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "Test question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];
    
    // Define temporary context in the first instance
    const temporaryContext = "Instructions important";
    
    // First execution with context
    const firstRun = memoryContextBuilder1.buildMessagesWithContext(
      "Test question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext,
      undefined
    );
    
    // Verify temporary context in the first execution
    const developerMessages1 = firstRun.filter(m => m.role === "developer");
    const hasTemporaryContext1 = developerMessages1.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext1).toBe(true);
    
    // Reset the temporary context in the SECOND instance
    memoryContextBuilder2.resetTemporaryContext();
    
    // New execution in the first instance
    const secondRun = memoryContextBuilder1.buildMessagesWithContext(
      "Second question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, 
      undefined
    );
    
    // Verify that the context was cleared (should be absent)
    const developerMessages2 = secondRun.filter(m => m.role === "developer");
    const hasTemporaryContext2 = developerMessages2.some(m => 
      m.content.includes("Instructions important"));
    
    // The context SHOULD NOT be present, since it was reset
    expect(hasTemporaryContext2).toBe(false);
  });
  
  test("resetAll should clear both snapshot and temporary context", () => {
    // Basic test setup
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "Initial question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];
    
    // Define temporary context
    const temporaryContext = "Instructions important";
    
    // First execution
    /* eslint-disable-next-line @typescript-eslint/no-unused-vars */
    const firstRun = memoryContextBuilder1.buildMessagesWithContext(
      "Initial question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext,
      undefined
    );
    
    // Reset completo
    memoryContextBuilder1.resetAll();
    
    // Second execution
    const secondRun = memoryContextBuilder2.buildMessagesWithContext(
      "Initial question?", // Same question to test if the snapshot was cleared
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, 
      undefined
    );
    
    // Verify that the temporary context was cleared
    const developerMessages = secondRun.filter(m => m.role === "developer");
    const hasTemporaryContext = developerMessages.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext).toBe(false);
    
    // Verify if the question appears (snapshot was cleared)
    const userMessages = secondRun.filter(m => m.role === "user");
    expect(userMessages.length).toBe(1); // Should have one user message
  });
  
  test("The temporary context memory should persist between calls", async () => {
    // Define temporary context
    const temporaryContext = "Instructions important";
    
    // Mock to simulate memory query results
    const mockMemoryResults: SpeakerMemoryResults = {
      userContext: "",
      speakerContexts: new Map(),
      temporaryContext: "Context memory retrieved from Pinecone"
    };
    
    // Setup básico
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { speaker: "User", text: "First message", timestamp: "2023-01-01T10:00:00Z" }
    ];
    
    // First call with contextManager and memoryResults
    const firstMessages = memoryContextBuilder1.buildMessagesWithContext(
      "First message",
      conversationHistory,
      false,
      transcricoes,
      new Set(["User"]),
      "User",
      temporaryContext,
      mockMemoryResults // Com resultados de memória
    );
    
    // Verify that the temporary context memory is in the messages
    const firstDevMessages = firstMessages.filter(m => m.role === "developer");
    const hasMemoryContext = firstDevMessages.some(m => 
      m.content.includes("Context memory retrieved from Pinecone"));
    expect(hasMemoryContext).toBe(true);
    
    // Second call WITHOUT passing memoryResults
    const secondMessages = memoryContextBuilder2.buildMessagesWithContext(
      "Second message",
      conversationHistory,
      false,
      [...transcricoes, { speaker: "User", text: "Second message", timestamp: "2023-01-01T10:05:00Z" }],
      new Set(["User"]),
      "User",
      undefined, // Não passamos novo contexto (deve manter o anterior)
      undefined  // Não passamos resultados de memória (deve usar o armazenado)
    );
    
    // Verify that the temporary context memory IS STILL in the messages
    const secondDevMessages = secondMessages.filter(m => m.role === "developer");
    const stillHasMemoryContext = secondDevMessages.some(m => 
      m.content.includes("Context memory retrieved from Pinecone"));
    expect(stillHasMemoryContext).toBe(true);
  });
  
  test("Pinecone should only be queried for the temporaryContext when it changes", async () => {
    // Define temporary context
    const temporaryContext = "Instructions important";
    
    // Basic setup
    const transcricoes: SpeakerTranscription[] = [
      { speaker: "User", text: "First message", timestamp: "2023-01-01T10:00:00Z" }
    ];
    
    // First call - should query Pinecone for both
    await memoryContextBuilder1.fetchContextualMemory(
      transcricoes,
      [],
      new Set(["User"]),
      temporaryContext
    );
    
    // Verify specific queries
    expect(pineconeQueries.temporaryContext).toBe(1); 
    expect(pineconeQueries.userContext).toBe(0);      
    
    // Reset counter
    pineconeQueries.reset();
    
    // Second call with the SAME temporary context
    // Should only query for userContext, not for temporaryContext (reuse)
    await memoryContextBuilder2.fetchContextualMemory(
      transcricoes,
      [],
      new Set(["User"]),
      temporaryContext
    );
    
    // Verify specific queries (temporaryContext should not be queried again)
    expect(pineconeQueries.temporaryContext).toBe(0); 
    expect(pineconeQueries.userContext).toBe(0);      
    
    // Reset counter
    pineconeQueries.reset();
    
    // Third call with DIFFERENT temporary context
    const novoContexto = "New instructions";
    await memoryContextBuilder1.fetchContextualMemory(
      transcricoes,
      [],
      new Set(["User"]),
      novoContexto
    );
    
    // Verify specific queries (temporaryContext should not be queried here either)
    expect(pineconeQueries.temporaryContext).toBe(0); 
    expect(pineconeQueries.userContext).toBe(0);      
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/// <reference types="jest" />

import { TranscriptionSnapshotTracker } from "../services/transcription/TranscriptionSnapshotTracker";

describe("TranscriptionSnapshotTracker Basic Functionality", () => {
  let snapshotTracker: TranscriptionSnapshotTracker;

  beforeEach(() => {
    snapshotTracker = new TranscriptionSnapshotTracker();
  });

  test("filterTranscription removes existing content", () => {
    // First, add some content to the tracker
    const initialContent = "First line\nSecond line";
    snapshotTracker.updateSnapshot(initialContent);

    // Now try to filter a message containing both old and new content
    const newContent = "First line\nSecond line\nThird line";
    const filtered = snapshotTracker.filterTranscription(newContent);

    // Should only contain the new line
    expect(filtered).toBe("Third line");
  });

  test("filtering twice returns empty string", () => {
    const content = "Test message";
    
    // First filter should return the content
    const firstFilter = snapshotTracker.filterTranscription(content);
    expect(firstFilter).toBe("Test message");

    // Update the snapshot
    snapshotTracker.updateSnapshot(content);

    // Second filter should return empty
    const secondFilter = snapshotTracker.filterTranscription(content);
    expect(secondFilter).toBe("");
  });

  test("reset clears the snapshot", () => {
    const content = "Test message";
    
    // Add content to the tracker
    snapshotTracker.updateSnapshot(content);
    
    // Filtering should return empty string
    expect(snapshotTracker.filterTranscription(content)).toBe("");
    
    // Reset the tracker
    snapshotTracker.reset();
    
    // Now filtering should return the content again
    expect(snapshotTracker.filterTranscription(content)).toBe("Test message");
  });

  test("normalization handles whitespace and empty lines", () => {
    // Add some content with extra whitespace
    snapshotTracker.updateSnapshot("  Line  with    spaces  \n\n");
    
    // Filter should normalize both strings before comparison
    const filtered = snapshotTracker.filterTranscription("Line with spaces");
    expect(filtered).toBe("");
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IPersistenceService } from "../interfaces/memory/IPersistenceService";
import { IEmbeddingService } from "../interfaces/openai/IEmbeddingService";
import {
  Message,
  SpeakerMemoryResults,
  SpeakerTranscription
} from "../interfaces/transcription/TranscriptionTypes";
import { MemoryContextBuilder } from "../services/memory/MemoryContextBuilder";
import { BatchTranscriptionProcessor } from "../services/transcription/BatchTranscriptionProcessor";
import { TranscriptionFormatter } from "../services/transcription/TranscriptionFormatter";

// Mocks with proper interfaces
const mockEmbeddingService: IEmbeddingService = {
  isInitialized: jest.fn().mockReturnValue(true),
  createEmbedding: jest.fn().mockResolvedValue([]),
  initialize: jest.fn().mockResolvedValue(true)
};

// Fix persistence service mock to match the interface
const mockPersistenceService: IPersistenceService = {
  saveToPinecone: jest.fn(async () => ({ success: true })), // mock compatible

  isAvailable: jest.fn().mockReturnValue(true),
  saveInteraction: jest.fn().mockResolvedValue(undefined),
  createVectorEntry: jest.fn().mockReturnValue({}),
  queryMemory: jest.fn().mockResolvedValue("")
};

// Add a custom property for the queryMemory function used in MemoryContextBuilder
// This extends the mock object beyond the interface
(mockPersistenceService as any).queryMemory = jest.fn().mockReturnValue("");

describe("TranscriptionSnapshotTracker", () => {
  let memoryContextBuilder: MemoryContextBuilder;
  let formatter: TranscriptionFormatter;

  beforeEach(() => {
    formatter = new TranscriptionFormatter();
    const processor = new BatchTranscriptionProcessor(formatter);
    
    memoryContextBuilder = new MemoryContextBuilder(
      mockEmbeddingService,
      mockPersistenceService,
      formatter,
      processor
    );
    
    // Reset the snapshot tracker before each test
    memoryContextBuilder.resetSnapshotTracker();
  });

  test("New message is processed correctly without duplicates or context confusion", () => {
    // Arrange
    const conversationHistory: Message[] = [
      { role: "developer", content: "📦 User relevant memory:\n[Guilherme] Hello, how are you?" },
      // Remove the user message from history - this would be handled by deduplication
      // { role: "user", content: "[Guilherme] Olá, tudo bem?" },
      { role: "assistant", content: "Hello, how are you?" }
    ];

    // Pre-populate the snapshot tracker with the existing transcription
    memoryContextBuilder.resetSnapshotTracker();
    memoryContextBuilder["snapshotTracker"].updateSnapshot("[Guilherme] Hello, how are you?");
    
    const novaMensagem = "[Guilherme] What do you think about this?";
    
    // Create speaker transcriptions that include both the old and new messages
    const transcricoesCompletas: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "Hello, how are you?", 
        timestamp: "2023-01-01T10:00:00Z" 
      },
      { 
        speaker: "Guilherme", 
        text: "What do you think about this?", 
        timestamp: "2023-01-01T10:01:00Z" 
      }
    ];

    const memoryResults: SpeakerMemoryResults = {
      userContext: "[Guilherme] Hello, how are you?",
      speakerContexts: new Map(),
      temporaryContext: ""
    };

    // Act
    const mensagensGeradas = memoryContextBuilder.buildMessagesWithContext(
      novaMensagem,
      conversationHistory,
      false, // sem simplified history
      transcricoesCompletas, // inclui a nova mensagem no fim
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, // sem temporaryContext
      memoryResults
    );

    // Assert
    const ultimaMensagem = mensagensGeradas[mensagensGeradas.length - 1];
    expect(ultimaMensagem.role).toBe("user");
    expect(ultimaMensagem.content).toContain("What do you think about this?");
    expect(ultimaMensagem.content).not.toContain("Hello, how are you?");
    
    // Verify no duplicated content in the entire message array
    const userMessages = mensagensGeradas.filter(m => m.role === "user");
    expect(userMessages.length).toBe(1); // Should only have one user message after deduplication
    
    // Test that running it twice will not include any content (all filtered out)
    const secondRun = memoryContextBuilder.buildMessagesWithContext(
      novaMensagem,
      conversationHistory,
      false,
      transcricoesCompletas,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined,
      memoryResults
    );
    
    // We expect the second run to not have any user messages
    const secondRunUserMessages = secondRun.filter(m => m.role === "user");
    expect(secondRunUserMessages.length).toBe(0);
  });

  test("Multiple messages are properly deduplicated", () => {
    // Arrange
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];

    const transcricoesCompletas: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "First message", 
        timestamp: "2023-01-01T10:00:00Z" 
      },
      { 
        speaker: "Guilherme", 
        text: "Second message", 
        timestamp: "2023-01-01T10:01:00Z" 
      },
      { 
        speaker: "Guilherme", 
        text: "Third message", 
        timestamp: "2023-01-01T10:02:00Z" 
      }
    ];

    // Resetar para começar limpo
    memoryContextBuilder.resetAll();
    
    // First run - should include all three messages
    const firstRun = memoryContextBuilder.buildMessagesWithContext(
      "First message\nSecond message\nThird message",
      conversationHistory,
      false,
      transcricoesCompletas,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, // Use undefined instead of null
      undefined  // Use undefined instead of null
    );

    // There should be 2 messages: system message + user message with all content
    expect(firstRun.length).toBe(2);
    expect(firstRun[1].role).toBe("user");
    expect(firstRun[1].content).toContain("First message");
    expect(firstRun[1].content).toContain("Second message");
    expect(firstRun[1].content).toContain("Third message");

    // Second run with partially new content
    const updatedTranscricoesCompletas: SpeakerTranscription[] = [
      ...transcricoesCompletas,
      { 
        speaker: "Guilherme", 
        text: "Fourth message", 
        timestamp: "2023-01-01T10:03:00Z" 
      }
    ];

    const secondRun = memoryContextBuilder.buildMessagesWithContext(
      "First message\nSecond message\nThird message\nFourth message",
      conversationHistory,
      false,
      updatedTranscricoesCompletas,
      new Set(["Guilherme"]),
      "Guilherme",
      undefined, // Use undefined instead of null
      undefined  // Use undefined instead of null
    );

    // There should be 2 messages: system message + user message with ONLY the new content
    expect(secondRun.length).toBe(2);
    expect(secondRun[1].role).toBe("user");
    expect(secondRun[1].content).not.toContain("First message");
    expect(secondRun[1].content).not.toContain("Second message");
    expect(secondRun[1].content).not.toContain("Third message");
    expect(secondRun[1].content).toContain("Fourth message");
  });

  test("Temporary context is deduplicated between executions", () => {
    // Arrange
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];

    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "What is the initial question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];

    // Reset everything
    memoryContextBuilder.resetAll();
    
    // Temporary context that should persist
    const temporaryContext = "Instructions important";
    
    // First execution with temporary context
    const firstRun = memoryContextBuilder.buildMessagesWithContext(
      "What is the initial question?",
      conversationHistory,
      false,
      transcricoes, 
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext,
      undefined
    );

    // Verify that the temporary context is present
    const developerMessages = firstRun.filter(m => m.role === "developer");
    const hasTemporaryContext = developerMessages.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext).toBe(true);
    
    console.log("First run developer messages:", 
      developerMessages.map(m => m.content.substring(0, 30) + "..."));

    // Second execution with new question but same temporary context
    const novaTranscricao: SpeakerTranscription[] = [
      ...transcricoes,
      { 
        speaker: "Guilherme", 
        text: "Second question?", 
        timestamp: "2023-01-01T10:01:00Z" 
      }
    ];
    
    const secondRun = memoryContextBuilder.buildMessagesWithContext(
      "Second question?",
      conversationHistory,
      false,
      novaTranscricao,
      new Set(["Guilherme"]),
      "Guilherme",
      temporaryContext, // Same temporary context
      undefined
    );
    
    // Verify that the temporary context IS STILL present in the second prompt
    const secondRunDeveloperMessages = secondRun.filter(m => m.role === "developer");
    console.log("Second run developer messages:", 
      secondRunDeveloperMessages.map(m => m.content.substring(0, 30) + "..."));
    
    const stillHasTemporaryContext = secondRunDeveloperMessages.some(m => 
      m.content.includes("Instructions important"));
    expect(stillHasTemporaryContext).toBe(true);
    
    // Verify that the new question is present, but the old one is not
    const secondRunUserMessages = secondRun.filter(m => m.role === "user");
    expect(secondRunUserMessages.length).toBe(1);
    expect(secondRunUserMessages[0].content).toContain("Second question?");
    expect(secondRunUserMessages[0].content).not.toContain("What is the initial question?");
  });

  test("Temporary context with dynamically created objects", () => {
    // This test verifies if the use of dynamically created objects affects the temporary context
    
    // Resetting the initial tracker
    memoryContextBuilder.resetAll();
    
    const conversationHistory: Message[] = [
      { role: "developer", content: "System message" }
    ];
    
    const transcricoes: SpeakerTranscription[] = [
      { 
        speaker: "Guilherme", 
        text: "What is the initial question?", 
        timestamp: "2023-01-01T10:00:00Z" 
      }
    ];
    
    // 1. Creating a dynamic context object (as it would be in production)
    const dynamicContext = {
      instructions: "Instructions important",
      get value() { return this.instructions; }
    };
    
    // 2. First run with dynamic context
    const firstRun = memoryContextBuilder.buildMessagesWithContext(
      "What is the initial question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      dynamicContext.value, // Using dynamic getter
      undefined
    );
    
    // Verify that the temporary context is present 
    const developerMessages = firstRun.filter(m => m.role === "developer");
    const hasTemporaryContext = developerMessages.some(m => 
      m.content.includes("Instructions important"));
    expect(hasTemporaryContext).toBe(true);
    
    // 3. Modifying the dynamic object after the first call
    // (Simula situações onde o objeto pode mudar entre chamadas)
    dynamicContext.instructions = "Instructions modified after first call";
    
    // 4. Second run with the same object (now modified)
    const secondRun = memoryContextBuilder.buildMessagesWithContext(
      "What is the second question?",
      conversationHistory,
      false,
      transcricoes,
      new Set(["Guilherme"]),
      "Guilherme",
      dynamicContext.value, // Using modified getter
      undefined
    );
    
    // 5. Verify the instructions in the developer messages
    const developerMessages2 = secondRun.filter(m => m.role === "developer");
    console.log("Dynamic context developer messages:", 
      developerMessages2.map(m => m.content.substring(0, 50) + "..."));
    
    // Should not contain the original instructions
    const hasOriginalInstructions = developerMessages2.some(m => 
      m.content.includes("Instructions important"));
    expect(hasOriginalInstructions).toBe(false);
    
    // Should contain the new instructions
    const hasNewInstructions = developerMessages2.some(m => 
      m.content.includes("Instructions modified after first call"));
    expect(hasNewInstructions).toBe(true);
  });
}); // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { SymbolicInsight } from './SymbolicInsight';
import { SymbolicQuery } from './SymbolicQuery';
import { SymbolicContext } from './SymbolicContext';
import { UserIntentWeights } from '../symbolic-cortex/integration/ICollapseStrategyService';

export type CognitionEvent =
  | { type: 'raw_prompt'; timestamp: string; content: string }
  | { type: 'temporary_context'; timestamp: string; context: string }
  | { type: 'neural_signal'; timestamp: string; core: string; symbolic_query: SymbolicQuery; intensity: number; topK: number; params: Record<string, unknown> }
  | { type: 'symbolic_retrieval'; timestamp: string; core: string; insights: SymbolicInsight[]; matchCount: number; durationMs: number }
  | { type: 'fusion_initiated'; timestamp: string }
  | { type: 'neural_collapse'; timestamp: string; isDeterministic: boolean; selectedCore: string; numCandidates: number; temperature?: number; emotionalWeight: number; contradictionScore: number; justification?: string; userIntent?: UserIntentWeights; insights?: SymbolicInsight[]; emergentProperties?: string[] }
  | { type: 'symbolic_context_synthesized'; timestamp: string; context: SymbolicContext }
  | { type: 'gpt_response'; timestamp: string; response: string; symbolicTopics?: string[]; insights?: SymbolicInsight[] }
  | { type: 'emergent_patterns'; timestamp: string; patterns: string[]; metrics?: { archetypalStability?: number; cycleEntropy?: number; insightDepth?: number } };
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Structure for synthesized symbolic context.
 */
export interface SymbolicContext {
  summary: string;
  [key: string]: string | number | boolean | object | undefined;

}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Structure for symbolic insights extracted from neural signals.
 * Core type definition for symbolic neural processing.
 */
export interface SymbolicInsight {
  type: string;
  content?: string;
  core?: string;
  keywords?: string[];
  [key: string]: string | number | boolean | object | undefined;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Structure for symbolic queries associated with neural signals.
 * Adjust as needed.
 */
export interface SymbolicQuery {
  query: string;
  [key: string]: string | number | boolean | object | undefined;

}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// LoggingUtils.ts
// Logging utilities

export class LoggingUtils {
  private static readonly PREFIX = "[Transcription]";
  
  /**
   * Logs an informative message
   */
  static logInfo(message: string): void {
    console.log(`ℹ️ ${this.PREFIX} ${message}`);
  }
  
  /**
   * Logs a warning
   */
  static logWarning(message: string): void {
    console.warn(`⚠️ ${this.PREFIX} ${message}`);
  }
  
  /**
   * Logs an error
   */
  static logError(message: string, error?: unknown): void {
    if (error) {
      console.error(`❌ ${this.PREFIX} ${message}:`, error);
    } else {
      console.error(`❌ ${this.PREFIX} ${message}`);
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Utility para validar compatibilidade de modelos Ollama com function calling
 * Baseado na documentação oficial do Ollama: https://ollama.com/blog/tool-support
 */

export interface OllamaModelInfo {
  name: string;
  supportsFunctionCalling: boolean;
  recommended: boolean;
  description: string;
}

/**
 * Lista oficial de modelos que suportam function calling no Ollama
 * Filtrada conforme especificação do usuário - apenas modelos que aceitam tools
 */
export const OLLAMA_FUNCTION_CALLING_MODELS: OllamaModelInfo[] = [
  {
    name: "qwen3",
    supportsFunctionCalling: true,
    recommended: true,
    description: "Qwen 3 - Excelente suporte multilíngue e tools",
  },
  {
    name: "mistral",
    supportsFunctionCalling: true,
    recommended: true,
    description: "Mistral - Rápido e eficiente para function calling",
  },
  {
    name: "mistral-nemo",
    supportsFunctionCalling: true,
    recommended: true,
    description: "Mistral Nemo - Versão otimizada para tools",
  },
  {
    name: "llama3.2",
    supportsFunctionCalling: true,
    recommended: true,
    description: "Llama 3.2 - Excelente integração com tools",
  },
];

export class OllamaModelValidator {
  /**
   * Verifica se um modelo suporta function calling
   */
  static supportsFunctionCalling(modelName: string): boolean {
    const baseModelName = modelName.split(":")[0];
    return OLLAMA_FUNCTION_CALLING_MODELS.some(
      (model) =>
        model.name === baseModelName || baseModelName.includes(model.name)
    );
  }

  /**
   * Obter informações sobre um modelo
   */
  static getModelInfo(modelName: string): OllamaModelInfo | null {
    const baseModelName = modelName.split(":")[0];
    return (
      OLLAMA_FUNCTION_CALLING_MODELS.find(
        (model) =>
          model.name === baseModelName || baseModelName.includes(model.name)
      ) || null
    );
  }

  /**
   * Obter lista de modelos recomendados para function calling
   */
  static getRecommendedModels(): OllamaModelInfo[] {
    return OLLAMA_FUNCTION_CALLING_MODELS.filter((model) => model.recommended);
  }

  /**
   * Obter o melhor modelo fallback para function calling
   */
  static getBestFallbackModel(): string {
    const recommended = this.getRecommendedModels();
    return recommended.length > 0
      ? `${recommended[0].name}:latest`
      : "qwen3:4b";
  }

  /**
   * Validar e sugerir modelo alternativo se necessário
   */
  static validateAndSuggest(modelName: string): {
    isValid: boolean;
    originalModel: string;
    suggestedModel?: string;
    reason?: string;
  } {
    if (this.supportsFunctionCalling(modelName)) {
      return {
        isValid: true,
        originalModel: modelName,
      };
    }

    const suggestedModel = this.getBestFallbackModel();
    return {
      isValid: false,
      originalModel: modelName,
      suggestedModel,
      reason: `Modelo ${modelName} não suporta function calling. Sugerido: ${suggestedModel}`,
    };
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Utility for cleaning <think> tags from model responses
 * Based on the Vercel AI issue: https://github.com/vercel/ai/issues/4920
 */

/**
 * Comprehensive function to clean <think> tags from model responses
 * Handles various formats and malformed tags
 */
export function cleanThinkTags(content: string): string {
  if (!content || typeof content !== "string") {
    return content || "";
  }

  let cleaned = content;

  // Pattern 1: Think tags with attributes (MUST come first to capture complex tags)
  cleaned = cleaned.replace(/<think[^>]*>[\s\S]*?<\/think>/gi, "");

  // Pattern 2: Complete <think>...</think> blocks (most common - basic tags)
  cleaned = cleaned.replace(/<think>[\s\S]*?<\/think>/gi, "");

  // Pattern 3: Think tags with different casing (including attributes)
  cleaned = cleaned.replace(/<THINK[^>]*>[\s\S]*?<\/THINK>/gi, "");
  cleaned = cleaned.replace(/<Think[^>]*>[\s\S]*?<\/Think>/gi, "");

  // Pattern 4: Standalone opening <think> tags
  cleaned = cleaned.replace(/<think[^>]*>/gi, "");

  // Pattern 5: Standalone closing </think> tags (the main issue from Vercel AI)
  cleaned = cleaned.replace(/<\/think>/gi, "");

  // Pattern 6: Partial or broken think tags
  cleaned = cleaned.replace(/<think[\s\S]*?(?=<[^/]|$)/gi, "");

  // Pattern 7: Think tags with variations of whitespace and newlines
  cleaned = cleaned.replace(/<\s*think\s*>[\s\S]*?<\s*\/\s*think\s*>/gi, "");

  // Pattern 8: Clean up stray content related to think
  cleaned = cleaned.replace(/^\s*<\/think>\s*/gm, "");
  cleaned = cleaned.replace(/\s*<think>\s*$/gm, "");

  // Clean up excessive whitespace while preserving formatting
  cleaned = cleaned.replace(/[ \t]+/g, " "); // Replace multiple spaces/tabs with single space
  cleaned = cleaned.replace(/\n\s*\n\s*\n/g, "\n\n"); // Replace triple+ newlines with double
  cleaned = cleaned.replace(/^\s+|\s+$/g, ""); // Trim start and end

  return cleaned;
}

/**
 * Clean think tags from JSON content while preserving JSON structure
 */
export function cleanThinkTagsFromJSON(jsonString: string): string {
  if (!jsonString || typeof jsonString !== "string") {
    return jsonString || "";
  }

  // First clean think tags
  let cleaned = cleanThinkTags(jsonString);

  // If it looks like JSON, try to parse and re-stringify to ensure validity
  if (cleaned.trim().startsWith("{") && cleaned.trim().endsWith("}")) {
    try {
      const parsed = JSON.parse(cleaned);

      // Recursively clean think tags from string values in the JSON
      const cleanedParsed = cleanJSONValues(parsed);

      return JSON.stringify(cleanedParsed);
    } catch (e) {
      // If parsing fails, return the cleaned string as-is
      return cleaned;
    }
  }

  return cleaned;
}

/**
 * Recursively clean think tags from JSON object values
 */
function cleanJSONValues(obj: any): any {
  if (typeof obj === "string") {
    return cleanThinkTags(obj);
  } else if (Array.isArray(obj)) {
    return obj.map(cleanJSONValues);
  } else if (obj && typeof obj === "object") {
    const cleaned: any = {};
    for (const [key, value] of Object.entries(obj)) {
      cleaned[key] = cleanJSONValues(value);
    }
    return cleaned;
  }
  return obj;
}

/**
 * Clean think tags from function call arguments
 */
export function cleanThinkTagsFromFunctionArgs(
  args: string | Record<string, any>
): string | Record<string, any> {
  if (typeof args === "string") {
    return cleanThinkTagsFromJSON(args);
  } else if (args && typeof args === "object") {
    return cleanJSONValues(args);
  }
  return args;
}

/**
 * Clean think tags from tool call responses
 */
export function cleanThinkTagsFromToolCalls(
  toolCalls: Array<{
    function: {
      name: string;
      arguments: string | Record<string, any>;
    };
  }>
): Array<{
  function: {
    name: string;
    arguments: string | Record<string, any>;
  };
}> {
  if (!Array.isArray(toolCalls)) {
    return toolCalls;
  }

  return toolCalls.map((call) => ({
    ...call,
    function: {
      ...call.function,
      arguments: cleanThinkTagsFromFunctionArgs(call.function.arguments),
    },
  }));
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// AudioAnalyzer.ts
// Implementation of the audio analysis service for Deepgram

import { ListenLiveClient, LiveTranscriptionEvents } from "@deepgram/sdk";
import { IAudioAnalyzer } from "./interfaces/deepgram/IDeepgramService";

export class DeepgramAudioAnalyzer implements IAudioAnalyzer {
  private getConnection: () => ListenLiveClient | null;

  constructor(getConnection: () => ListenLiveClient | null) {
    this.getConnection = getConnection;
  }

  analyzeAudioBuffer(buffer: ArrayBufferLike): { valid: boolean, details: any } {
    try {
      const view = new DataView(buffer);
      const uint8View = new Uint8Array(buffer);
      
      // Check minimum buffer size
      if (buffer.byteLength < 64) {
        return {
          valid: false,
          details: {
            reason: "Buffer too small",
            size: buffer.byteLength
          }
        };
      }
      
      // Detect if it looks like a WAV header
      const isWav = 
        String.fromCharCode(uint8View[0], uint8View[1], uint8View[2], uint8View[3]) === 'RIFF' &&
        String.fromCharCode(uint8View[8], uint8View[9], uint8View[10], uint8View[11]) === 'WAVE';
      
      // Detect if it looks like a WebM (starts with 0x1A 0x45 0xDF 0xA3)
      const isWebM = uint8View[0] === 0x1A && uint8View[1] === 0x45 && uint8View[2] === 0xDF && uint8View[3] === 0xA3;
      
      // PCM raw analysis (no header)
      const pcmAnalysis = {
        min: Number.MAX_VALUE,
        max: Number.MIN_VALUE,
        avg: 0,
        rms: 0,
        zeroCount: 0,
        sampleCount: 0
      };
      
      // Ensure the buffer has enough data for Int16Array
      // Int16Array requires an even number of bytes
      const alignedLength = Math.floor(buffer.byteLength / 2) * 2;
      
      // Proceed with PCM analysis only if we have enough data
      if (alignedLength >= 64) {
        // Create an Int16Array view adjusted to the correct size
        const int16View = new Int16Array(buffer.slice(0, alignedLength));
        pcmAnalysis.sampleCount = int16View.length;
        
        // Calculate statistics for the first X samples (max 1000)
        const samplesToAnalyze = Math.min(int16View.length, 1000);
        let sum = 0;
        let sumSquares = 0;
        
        for (let i = 0; i < samplesToAnalyze; i++) {
          const sample = int16View[i];
          pcmAnalysis.min = Math.min(pcmAnalysis.min, sample);
          pcmAnalysis.max = Math.max(pcmAnalysis.max, sample);
          sum += Math.abs(sample);
          sumSquares += sample * sample;
          if (sample === 0) pcmAnalysis.zeroCount++;
        }
        
        pcmAnalysis.avg = sum / samplesToAnalyze;
        pcmAnalysis.rms = Math.sqrt(sumSquares / samplesToAnalyze);
      } else {
        // Not possible to analyze as PCM
        pcmAnalysis.min = 0;
        pcmAnalysis.max = 0;
      }
      
      // Check if the audio appears to be silence
      const isSilence = pcmAnalysis.rms < 10; // A very low threshold indicates silence
      
      // Check if the buffer format matches what Deepgram expects
      const formatDescription = isWav ? "WAV" : isWebM ? "WebM" : "PCM brute";
      const hasCorrectFormat = !isWav && !isWebM; // Deepgram expects PCM brute
      
      // Return detailed analysis
      return {
        valid: hasCorrectFormat && !isSilence && buffer.byteLength % 2 === 0,
        details: {
          format: formatDescription,
          byteLength: buffer.byteLength,
          byteIsEven: buffer.byteLength % 2 === 0,
          headerBytes: Array.from(uint8View.slice(0, 16)).map(b => b.toString(16).padStart(2, '0')).join(' '),
          pcmAnalysis,
          isSilence,
          isFormatCorrect: hasCorrectFormat
        }
      };
    } catch (error) {
      return {
        valid: false,
        details: {
          reason: "Error analyzing buffer",
          error: String(error)
        }
      };
    }
  }

  async testAudioQuality(): Promise<{ valid: boolean, reason?: string }> {
    try {
      const activeConn = this.getConnection();
      if (!activeConn || activeConn.getReadyState() !== 1) {
        return { valid: false, reason: "No active connection to Deepgram" };
      }
      
      // Create an audio context to get the native sample rate of the system
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const sampleRate = audioContext.sampleRate;
      
      // Create a test buffer with synthetic audio data (test tone)
      const duration = 0.5; // 500ms
      const sampleCount = Math.floor(sampleRate * duration);
      
      // Log for debug of the sample rate used
      console.log(`🎵 Generating test signal with native sample rate: ${sampleRate}Hz`);
      
      // Create a buffer for PCM 16-bit, 2 channels (4 bytes per sample)
      const buffer = new ArrayBuffer(sampleCount * 4);
      const view = new DataView(buffer);
      
      // Generate different tones for each channel
      // Channel 0: 440Hz (A4), Channel 1: 880Hz (A5)
      const freq1 = 440;
      const freq2 = 880;
      
      for (let i = 0; i < sampleCount; i++) {
        // Time in seconds
        const t = i / sampleRate;
        
        // Calculate amplitude for each channel (-0.5 to 0.5 to avoid distortion)
        const amplitude1 = 0.3 * Math.sin(2 * Math.PI * freq1 * t);
        const amplitude2 = 0.3 * Math.sin(2 * Math.PI * freq2 * t);
        
        // Convert to int16 (-32768 to 32767)
        const sample1 = Math.floor(amplitude1 * 32767);
        const sample2 = Math.floor(amplitude2 * 32767);
        
        // Write interlaced samples for both channels (PCM stereo format)
        view.setInt16(i * 4, sample1, true);     // left channel (0)
        view.setInt16(i * 4 + 2, sample2, true); // right channel (1)
      }
      
      // Close the audio context to avoid leaving resources open
      audioContext.close();
      
      console.log("🔊 Sending test signal to Deepgram");
      
      // Send the test buffer
      if (activeConn) {
        activeConn.send(buffer);
        
        // Wait for a short period to see if we receive transcription/error
        return new Promise((resolve) => {
          let responseReceived = false;
          
          // Temporary handler to capture responses
          const handleMessage = (data: any) => {
            responseReceived = true;
            
            // Check if the response contains any specific error
            if (data.error) {
              console.error("❌ Error returned by Deepgram:", data.error);
              resolve({ valid: false, reason: `Error from Deepgram: ${data.error}` });
              return;
            }
            
              // If we receive any response without error, consider it valid
            console.log("✅ Test signal accepted by Deepgram");
            resolve({ valid: true });
          };
          
          // Add and then remove the temporary handler
          activeConn.addListener(LiveTranscriptionEvents.Transcript, handleMessage);
          
          // Set a timeout to resolve if we don't receive a response
          setTimeout(() => {
            // Remove the temporary handler
            activeConn.removeListener(LiveTranscriptionEvents.Transcript, handleMessage);
            
            if (!responseReceived) {
              console.warn("⚠️ Timeout in audio quality check - no response received");
              // Timeout is acceptable, as Deepgram might simply not return anything for audio with no speech
              resolve({ valid: true, reason: "No response, but active connection" });
            }
          }, 3000);
        });
      }
      
      return { valid: false, reason: "Active connection but invalid" };
    } catch (error) {
      console.error("❌ Error testing audio quality:", error);
      return { valid: false, reason: String(error) };
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// DeepgramConnectionService.ts
// Deepgram connection service optimized for robustness and clarity

import { ListenLiveClient } from "@deepgram/sdk";
import { ConnectionState, IAudioAnalyzer, IDeepgramConnectionService } from "./interfaces/deepgram/IDeepgramService";
import { AudioProcessor } from "./services/audio/AudioProcessor";
import { AudioQueue } from "./services/audio/AudioQueue";
import { AudioSender } from "./services/audio/AudioSender";
import { ChunkReceiver } from "./services/audio/ChunkReceiver";
import { ConnectionManager } from "./services/connection/ConnectionManager";
import { EventHandler } from "./services/connection/EventHandler";
import { LiveTranscriptionProcessor } from "./services/transcription/LiveTranscriptionProcessor";
import { TranscriptionEventCallback } from "./services/utils/DeepgramTypes";
import { ITranscriptionStorageService } from './interfaces/transcription/ITranscriptionStorageService';
import { STORAGE_KEYS, getOption } from '../../../services/StorageService';

export class DeepgramConnectionService implements IDeepgramConnectionService {
  // Core service components
  private connectionManager: ConnectionManager;
  private eventHandler: EventHandler;
  private audioProcessor: AudioProcessor;
  private audioQueue: AudioQueue;
  private audioSender: AudioSender;
  private transcriptionProcessor: LiveTranscriptionProcessor;
  private chunkReceiver: ChunkReceiver;
  
  constructor(
    setConnectionState: (state: ConnectionState) => void,
    setConnection: (connection: ListenLiveClient | null) => void,
    analyzer: IAudioAnalyzer,
    storageService?: ITranscriptionStorageService // Use the interface to avoid direct dependencies
  ) {
    // Initialize all service components
    this.audioProcessor = new AudioProcessor(analyzer);
    this.audioQueue = new AudioQueue();
    this.connectionManager = new ConnectionManager(setConnectionState, setConnection);
    
    // Create transcription processor
    this.transcriptionProcessor = new LiveTranscriptionProcessor();
    
    // Bind TranscriptionStorageService if available
    if (storageService) {
      console.log(`🔄 [COGNITIVE-CORE] Integrating LiveTranscriptionProcessor with ITranscriptionStorageService for brain memory persistence`);
      this.transcriptionProcessor.setTranscriptionStorageService(storageService);
    } else {
      console.log(`⚠️ [COGNITIVE-CORE] Storage service not provided, some memory orchestration features may not function`);
    }
    
    this.eventHandler = new EventHandler(this.transcriptionProcessor, this.connectionManager);
    this.audioSender = new AudioSender(this.audioProcessor, this.audioQueue, this.connectionManager);
    
    // Set up chunk receiver with audio handler
    this.chunkReceiver = new ChunkReceiver(this.handleIncomingAudioChunk.bind(this));
    this.chunkReceiver.setupChunkReceiver();
  }
  
  // PUBLIC API
  
  /**
   * Get the current connection
   */
  getConnection(): ListenLiveClient | null {
    return this.connectionManager.getConnection();
  }
  
  /**
   * Start a connection with Deepgram
   */
  async connectToDeepgram(language?: string): Promise<void> {
    // Sempre obtém o idioma mais atualizado do storage
    const storedLanguage = getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE);
    
    // Prioriza o idioma do storage, e só usa o parâmetro se não houver valor no storage
    const languageToUse = storedLanguage || language || 'pt-BR';
    
    console.log('🌐 DeepgramConnectionService: Conectando com idioma:', languageToUse);
    
    // Start connection process with the correct language
    await this.connectionManager.connectToDeepgram(languageToUse);
    
    // Register event handlers if connection was established
    const connection = this.connectionManager.getConnection();
    if (connection) {
      this.eventHandler.registerEventHandlers(connection);
    }
  }
  
  /**
   * Disconnect from Deepgram
   */
  async disconnectFromDeepgram(): Promise<void> {
    // Clear audio queue before disconnecting
    this.audioQueue.clearQueue();
    
    // Disconnect from Deepgram
    await this.connectionManager.disconnectFromDeepgram();
  }
  
  /**
   * Clean up event listeners and disconnect from Deepgram
   */
  public cleanup(): void {
    // Clean up chunk receiver
    this.chunkReceiver.cleanup();
    
    // Disconnect from Deepgram
    this.disconnectFromDeepgram();
  }
  
  /**
   * Wait until the connection reaches a specific state
   */
  async waitForConnectionState(targetState: ConnectionState, timeoutMs = 15000): Promise<boolean> {
    return this.connectionManager.waitForConnectionState(targetState, timeoutMs);
  }
  
  /**
   * Get the current connection status
   */
  getConnectionStatus() {
    return this.connectionManager.getConnectionStatus();
  }
  
  /**
   * Check if there is an active and ready connection
   */
  hasActiveConnection(): boolean {
    return this.connectionManager.isActiveConnection();
  }
  
  /**
   * Send audio data to Deepgram
   */
  async sendAudioChunk(blob: Blob | Uint8Array): Promise<boolean> {
    const result = await this.audioSender.sendAudioChunk(blob);
    
    // If connection is active and we have queued audio, start processing it
    if (this.connectionManager.isActiveConnection() && this.audioQueue.hasItems() && !this.audioQueue.isProcessing()) {
      setTimeout(() => this.audioSender.processQueuedChunks(), 100);
    }
    
    return result;
  }
  
  /**
   * Register a callback to receive transcription events
   */
  public registerTranscriptionCallback(callback: TranscriptionEventCallback): void {
    this.eventHandler.registerTranscriptionCallback(callback);
  }
  
  /**
   * Process incoming audio chunks via IPC
   */
  private async handleIncomingAudioChunk(arrayBuffer: ArrayBuffer): Promise<void> {
    try {
      // Convert ArrayBuffer to Uint8Array for compatibility
      const audioData = new Uint8Array(arrayBuffer);
      
      if (audioData.byteLength > 0) {
        // Use the existing method to process and send the audio
        await this.sendAudioChunk(audioData);
      }
    } catch (error) {
      console.error(`❌ [Deepgram] Error processing IPC audio chunk:`, error);
    }
  }
  
  /**
   * Force a reconnection to the server
   */
  private async forceReconnect(): Promise<void> {
    await this.connectionManager.forceReconnect();
    
    // Register event handlers if connection was established
    const connection = this.connectionManager.getConnection();
    if (connection) {
      this.eventHandler.registerEventHandlers(connection);
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// DeepgramContextProvider.tsx
// Component that manages the Deepgram context

import React, {
  createContext,
  useCallback,
  useContext,
  useEffect,
  useReducer,
  useRef,
  useState,
} from "react";
import { ModeService, OrchOSModeEnum } from "../../../services/ModeService";
import { getOption, STORAGE_KEYS } from "../../../services/StorageService";
import { useAudioAnalyzer } from "../audioAnalyzer/AudioAnalyzerProvider";
import { AudioContextService } from "../microphone/AudioContextService";
import { useSettings } from "../settings/SettingsProvider";
import { DeepgramConnectionService } from "./DeepgramConnectionService";
import {
  DeepgramState,
  IDeepgramContext,
} from "./interfaces/deepgram/IDeepgramContext";
import {
  ConnectionState,
  IDeepgramConnectionService,
} from "./interfaces/deepgram/IDeepgramService";
import { IOpenAIService } from "./interfaces/openai/IOpenAIService";
import { DeepgramTranscriptionService } from "./services/DeepgramTranscriptionService";
import { HuggingFaceCompletionService } from "./services/huggingface/HuggingFaceCompletionService";
import { HuggingFaceServiceFacade } from "./services/huggingface/HuggingFaceServiceFacade";
import { HuggingFaceClientService } from "./services/huggingface/neural/HuggingFaceClientService";
import { OllamaServiceFacade } from "./services/ollama/OllamaServiceFacade";
// Import all custom hooks from index
import {
  useDeepgramDebug,
  useTranscriptionData,
  useTranscriptionProcessor,
} from "./hooks";

// Initial state
const initialState = {
  deepgramState: DeepgramState.NotConnected,
  isConnected: false,
  isProcessing: false,
  language: getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "pt-BR",
  model: getOption(STORAGE_KEYS.DEEPGRAM_MODEL) || "nova-2",
};

// Reducer actions
type DeepgramAction =
  | { type: "SET_STATE"; payload: DeepgramState }
  | { type: "SET_CONNECTED"; payload: boolean }
  | { type: "SET_PROCESSING"; payload: boolean }
  | { type: "SET_LANGUAGE"; payload: string }
  | { type: "SET_MODEL"; payload: string }
  | { type: "RESET_STATE" };

// Reducer to manage Deepgram state
function deepgramReducer(
  state: typeof initialState,
  action: DeepgramAction
): typeof initialState {
  switch (action.type) {
    case "SET_STATE":
      return { ...state, deepgramState: action.payload };
    case "SET_CONNECTED":
      return { ...state, isConnected: action.payload };
    case "SET_PROCESSING":
      return { ...state, isProcessing: action.payload };
    case "SET_LANGUAGE":
      return { ...state, language: action.payload };
    case "SET_MODEL":
      return { ...state, model: action.payload };
    case "RESET_STATE":
      return { ...initialState };
    default:
      return state;
  }
}

// Context creation
export const DeepgramContext = createContext<IDeepgramContext | null>(null);

// Custom hook for context usage
export const useDeepgram = () => {
  const context = useContext(DeepgramContext);
  if (!context) {
    throw new Error("useDeepgram must be used within DeepgramProvider");
  }
  return context;
};

/**
 * Creates the appropriate AI service based on application mode
 * Following KISS principle - Keep It Simple
 */
function createAIService(): IOpenAIService {
  const mode = ModeService.getMode();

  if (mode === OrchOSModeEnum.BASIC) {
    console.log("🧠 Using HuggingFaceServiceFacade (Basic mode)");
    const clientService = new HuggingFaceClientService();
    const completionService = new HuggingFaceCompletionService(clientService);
    return new HuggingFaceServiceFacade(completionService);
  } else {
    console.log("🦙 Using OllamaServiceFacade (Advanced mode)");
    return new OllamaServiceFacade();
  }
}

// Context provider
export const DeepgramProvider: React.FC<{ children: React.ReactNode }> = ({
  children,
}) => {
  // State management
  const [state, dispatch] = useReducer(deepgramReducer, initialState);
  const [connection, setConnection] = useState<any | null>(null);
  const [connectionState, setConnectionState] = useState<ConnectionState>(
    ConnectionState.CLOSED
  );

  // Services and settings
  const { settings } = useSettings();
  const analyzer = useAudioAnalyzer();
  const deepgramConnectionRef = useRef<IDeepgramConnectionService | null>(null);
  const deepgramTranscriptionRef = useRef<DeepgramTranscriptionService | null>(
    null
  );

  // Global processing ref for synchronous blocking
  const isProcessingRef = useRef<boolean>(false);

  // Use custom hooks - Following SOLID principles
  const { debugDatabase, testDatabaseDiagnosis, testEmbeddingModel } =
    useDeepgramDebug();

  const {
    transcriptionData,
    interimResults,
    diarizationData,
    handleTranscriptionData,
    handleInterimUpdate,
    clearTranscriptionData,
  } = useTranscriptionData(deepgramTranscriptionRef);

  const {
    sendTranscriptionPrompt,
    sendDirectMessage,
    flushTranscriptionsToUI,
    setAutoQuestionDetection,
  } = useTranscriptionProcessor(
    deepgramTranscriptionRef.current,
    state.isProcessing,
    dispatch,
    isProcessingRef // Pass the global ref
  );

  // Service references
  const services = useRef({
    audioContext: new AudioContextService(),
    deepgramConnection: null as DeepgramConnectionService | null,
  });

  // Initialize services - Following DRY principle
  useEffect(() => {
    const initializeServices = async () => {
      // Setup audio context
      services.current.audioContext.setupAudioContext();

      // Create AI service based on mode
      const aiService = createAIService();
