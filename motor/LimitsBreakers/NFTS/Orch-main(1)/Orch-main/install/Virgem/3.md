 /**
   * Obtém o modelo de embeddings configurado ou o padrão
   */
  private getEmbeddingModel(): string {
    // Prioridade: 1. Configuração via construtor, 2. Storage, 3. Padrão (bge-m3:latest)
    const model =
      this.options.model ||
      getOption(STORAGE_KEYS.OLLAMA_EMBEDDING_MODEL) ||
      "bge-m3:latest";

    // Log para diagnóstico
    console.log(`[OllamaEmbedding] Using embedding model: ${model}`);

    return model;
  }

  /**
   * Creates an embedding for the provided text using Ollama
   */
  async createEmbedding(text: string): Promise<number[]> {
    if (!text?.trim()) {
      return [];
    }

    try {
      // Delegate to the Ollama service with the selected model
      const model = this.getEmbeddingModel();
      return await this.ollamaService.createEmbedding(text.trim(), model);
    } catch (error) {
      LoggingUtils.logError("Error creating embedding", error);
      return [];
    }
  }

  /**
   * Creates embeddings for a batch of texts using Ollama
   * @param texts Array of texts to create embeddings for
   * @returns Array of embeddings (array of number arrays)
   */
  async createEmbeddings(texts: string[]): Promise<number[][]> {
    if (!texts?.length) {
      return [];
    }

    try {
      // Get the selected model
      const model = this.getEmbeddingModel();

      // Check if the Ollama service supports batch embeddings directly
      if (this.ollamaService.createEmbeddings) {
        // Use the batch API if available
        return await this.ollamaService.createEmbeddings(
          texts.map((text) => text.trim()),
          model
        );
      } else {
        // Fallback: process embeddings one by one
        const embeddings = await Promise.all(
          texts.map(async (text) => {
            try {
              return await this.ollamaService.createEmbedding(
                text.trim(),
                model
              );
            } catch (err) {
              LoggingUtils.logError(
                `Error generating embedding for text: ${text.substring(
                  0,
                  50
                )}...`,
                err
              );
              return []; // Return empty array on error
            }
          })
        );

        return embeddings;
      }
    } catch (error) {
      LoggingUtils.logError("Error creating batch embeddings", error);
      return [];
    }
  }

  /**
   * Checks if the embedding service is initialized
   */
  isInitialized(): boolean {
    return this.ollamaService.isInitialized();
  }

  /**
   * Initializes the embedding service
   */
  async initialize(config?: Record<string, any>): Promise<boolean> {
    if (!this.ollamaService) {
      return false;
    }

    if (this.isInitialized()) {
      return true;
    }

    try {
      // If base URL is provided in config, use it
      if (config?.baseUrl) {
        this.ollamaService.initializeOpenAI(config.baseUrl);
        return this.isInitialized();
      }

      // Otherwise try to load from environment
      await this.ollamaService.loadApiKey();
      return this.isInitialized();
    } catch (error) {
      LoggingUtils.logError("Error initializing embedding service", error);
      return false;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// OllamaServiceFacade.ts
// Symbolic: Fachada neural que integra e coordena diferentes serviços neurais especializados

import { OllamaNeuralSignalService } from "../../infrastructure/neural/ollama/OllamaNeuralSignalService";
import { NeuralSignalResponse } from "../../interfaces/neural/NeuralSignalTypes";
import { ModelStreamResponse } from "../../interfaces/openai/ICompletionService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { Message } from "../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { OllamaClientService } from "./neural/OllamaClientService";
import { OllamaCompletionService } from "./neural/OllamaCompletionService";

/**
 * Fachada que implementa IOpenAIService e coordena os serviços especializados
 * Symbolic: Córtex de integração neural que combina neurônios especializados
 */
export class OllamaServiceFacade implements IOpenAIService {
  private clientService: OllamaClientService;
  private completionService: OllamaCompletionService;
  private neuralSignalService: OllamaNeuralSignalService;

  constructor() {
    // Inicializar os serviços especializados
    this.clientService = new OllamaClientService();
    this.completionService = new OllamaCompletionService(this.clientService);
    this.neuralSignalService = new OllamaNeuralSignalService(
      this.completionService
    );

    LoggingUtils.logInfo(
      "Initialized Ollama Service Facade with specialized neural services"
    );
  }

  /**
   * Inicializa o cliente Ollama
   * Symbolic: Estabelecimento de conexão neural com modelo local
   */
  initializeOpenAI(config?: string): void {
    this.clientService.initializeClient(config);
  }

  /**
   * Carrega a configuração do Ollama do armazenamento
   * Symbolic: Recuperação de credencial neural
   */
  async loadApiKey(): Promise<void> {
    await this.clientService.loadApiKey();
  }

  /**
   * Garante que o cliente Ollama está disponível
   * Symbolic: Verificação de integridade do caminho neural
   */
  async ensureOpenAIClient(): Promise<boolean> {
    return this.clientService.ensureClient();
  }

  /**
   * Envia requisição para Ollama e processa o stream de resposta
   * Symbolic: Fluxo neural contínuo de processamento de linguagem
   */
  async streamOpenAIResponse(
    messages: Message[]
  ): Promise<ModelStreamResponse> {
    // Mapear as mensagens para o formato esperado pelo serviço de completion
    const mappedMessages = messages.map((m) => ({
      role: m.role,
      content: m.content,
    }));

    return await this.completionService.streamModelResponse(mappedMessages);
  }

  /**
   * Cria embeddings para o texto fornecido
   * Symbolic: Transformação de texto em representação neural vetorial
   */
  async createEmbedding(text: string): Promise<number[]> {
    return this.clientService.createEmbedding(text);
  }

  /**
   * Cria embeddings para um lote de textos (processamento em batch)
   * Symbolic: Transformação em massa de textos em vetores neurais
   */
  async createEmbeddings(texts: string[]): Promise<number[][]> {
    return this.clientService.createEmbeddings(texts);
  }

  /**
   * Verifica se o cliente Ollama está inicializado
   * Symbolic: Consulta do estado de conexão neural
   */
  isInitialized(): boolean {
    return this.clientService.isInitialized();
  }

  /**
   * Gera sinais neurais simbólicos baseados em um prompt
   * Symbolic: Extração de padrões de ativação neural a partir de estímulo de linguagem
   */
  async generateNeuralSignal(
    prompt: string,
    temporaryContext?: string,
    language?: string
  ): Promise<NeuralSignalResponse> {
    return this.neuralSignalService.generateNeuralSignal(
      prompt,
      temporaryContext,
      language
    );
  }

  /**
   * Expande semanticamente a query de um núcleo cerebral
   * Symbolic: Expansão de campo semântico para ativação cortical específica
   */
  async enrichSemanticQueryForSignal(
    core: string,
    query: string,
    intensity: number,
    context?: string,
    language?: string
  ): Promise<{ enrichedQuery: string; keywords: string[] }> {
    return this.neuralSignalService.enrichSemanticQueryForSignal(
      core,
      query,
      intensity,
      context,
      language
    );
  }

  /**
   * Envia uma requisição ao Ollama com suporte a function calling
   * Symbolic: Processamento neural para geração de texto ou execução de função
   */
  async callOpenAIWithFunctions(options: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    tools?: Array<{
      type: string;
      function: {
        name: string;
        description: string;
        parameters: Record<string, unknown>;
      };
    }>;
    tool_choice?: { type: string; function: { name: string } };
    temperature?: number;
    max_tokens?: number;
  }): Promise<{
    choices: Array<{
      message: {
        content?: string;
        tool_calls?: Array<{
          function: {
            name: string;
            arguments: string | Record<string, any>;
          };
        }>;
      };
    }>;
  }> {
    return this.completionService.callModelWithFunctions(options);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { LoggingUtils } from "../../../utils/LoggingUtils";

/**
 * QuestionDetector - Single Responsibility: Detect questions and manage question cycles
 * Follows YAGNI principle by only implementing what's currently needed
 */
export class QuestionDetector {
  private lastQuestionPrompt: string = "";
  private lastPromptTimestamp: number = 0;
  private autoQuestionDetectionEnabled: boolean = false;
  private questionTimerId: NodeJS.Timeout | null = null;
  private pendingQuestionText: string = "";
  private inQuestionCycle: boolean = false;

  /**
   * Check if auto-detection is enabled
   */
  isAutoDetectionEnabled(): boolean {
    return this.autoQuestionDetectionEnabled;
  }

  /**
   * Enable or disable automatic question detection
   */
  setAutoDetection(enabled: boolean): void {
    this.autoQuestionDetectionEnabled = enabled;
    LoggingUtils.logInfo(
      `Auto-question detection ${enabled ? "enabled" : "disabled"}`
    );
  }

  /**
   * Check if text is a question
   */
  isQuestion(text: string): boolean {
    return text.trim().endsWith("?");
  }

  /**
   * Check if this is a duplicate question
   */
  isDuplicateQuestion(text: string): boolean {
    const trimmedText = text.trim();
    if (trimmedText === this.lastQuestionPrompt) {
      LoggingUtils.logInfo(`Duplicate question detected: "${trimmedText}"`);
      return true;
    }
    return false;
  }

  /**
   * Check if currently in a question cycle
   */
  isInQuestionCycle(): boolean {
    return this.inQuestionCycle;
  }

  /**
   * Start a question cycle
   */
  startQuestionCycle(questionText: string): void {
    if (this.inQuestionCycle) {
      LoggingUtils.logInfo(
        `Already in a question cycle, ignoring new question: "${questionText}"`
      );
      return;
    }

    this.lastQuestionPrompt = questionText;
    this.lastPromptTimestamp = Date.now();
    this.pendingQuestionText = questionText;
    this.inQuestionCycle = true;

    LoggingUtils.logInfo(
      `Question detected: "${questionText}". Starting question cycle...`
    );

    // Note: Auto-send is disabled by default
    LoggingUtils.logInfo(
      `Question detected but NOT sent (auto-send disabled): "${questionText}"`
    );
  }

  /**
   * End question cycle
   */
  endQuestionCycle(): void {
    this.inQuestionCycle = false;
    this.pendingQuestionText = "";
    LoggingUtils.logInfo("Question cycle ended.");
  }

  /**
   * Cancel pending question timer
   */
  cancelPendingTimer(): void {
    if (this.questionTimerId) {
      LoggingUtils.logInfo(
        `Canceling pending question timer for: "${this.pendingQuestionText}"`
      );
      clearTimeout(this.questionTimerId);
      this.questionTimerId = null;
    }
    this.endQuestionCycle();
  }

  /**
   * Handle new transcription during question cycle
   */
  handleNewTranscriptionDuringCycle(text: string): void {
    if (this.inQuestionCycle) {
      LoggingUtils.logInfo(
        `New transcription received, ending question cycle: "${text.trim()}"`
      );
      this.cancelPendingTimer();
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  EXTERNAL_SPEAKER_LABEL,
  SpeakerTranscription,
  SpeakerTranscriptionLog,
} from "../../../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../../../interfaces/utils/ISpeakerIdentificationService";

/**
 * TranscriptionLogger - Single Responsibility: Format transcription logs
 * Follows KISS principle by keeping formatting logic simple and clear
 */
export class TranscriptionLogger {
  private speakerService: ISpeakerIdentificationService;

  constructor(speakerService: ISpeakerIdentificationService) {
    this.speakerService = speakerService;
  }

  /**
   * Generate transcription logs grouped by speaker
   */
  generateLogs(
    speakerTranscriptions: SpeakerTranscription[]
  ): SpeakerTranscriptionLog[] {
    return this.getTranscriptionLogsByUser(speakerTranscriptions);
  }

  /**
   * Returns logs grouped by speaker
   */
  private getTranscriptionLogsByUser(
    speakerTranscriptions: SpeakerTranscription[]
  ): SpeakerTranscriptionLog[] {
    const tempGroups = new Map<
      string,
      {
        isUser: boolean;
        displayName: string;
        transcriptions: { text: string; timestamp: string }[];
      }
    >();

    const processedTexts = new Map<string, Set<string>>();
    const speakerNumbers = new Map<string, string>();

    for (const transcription of speakerTranscriptions) {
      this.processTranscription(
        transcription,
        tempGroups,
        processedTexts,
        speakerNumbers
      );
    }

    return this.formatLogsOutput(tempGroups);
  }

  /**
   * Process a single transcription
   */
  private processTranscription(
    transcription: SpeakerTranscription,
    tempGroups: Map<string, any>,
    processedTexts: Map<string, Set<string>>,
    speakerNumbers: Map<string, string>
  ): void {
    if (transcription.text.includes("[") && transcription.text.includes("]")) {
      const segments = this.speakerService.splitMixedTranscription(
        transcription.text
      );

      for (const segment of segments) {
        this.processSegment(
          segment,
          transcription.timestamp,
          tempGroups,
          processedTexts,
          speakerNumbers
        );
      }
    } else {
      this.processSimpleTranscription(
        transcription,
        tempGroups,
        processedTexts,
        speakerNumbers
      );
    }
  }

  /**
   * Process a segment
   */
  private processSegment(
    segment: { text: string; speaker: string },
    timestamp: string,
    tempGroups: Map<string, any>,
    processedTexts: Map<string, Set<string>>,
    speakerNumbers: Map<string, string>
  ): void {
    const segmentText = segment.text.replace(/^\[[^\]]+\]\s*/, "");
    const { displayName, groupKey, isUserMsg } = this.getSpeakerInfo(
      segment.text,
      segment.speaker,
      speakerNumbers
    );

    this.addToGroup(
      groupKey,
      displayName,
      isUserMsg,
      segmentText,
      timestamp,
      tempGroups,
      processedTexts
    );
  }

  /**
   * Process simple transcription without segments
   */
  private processSimpleTranscription(
    transcription: SpeakerTranscription,
    tempGroups: Map<string, any>,
    processedTexts: Map<string, Set<string>>,
    speakerNumbers: Map<string, string>
  ): void {
    const { displayName, groupKey, isUserMsg } = this.getSpeakerInfo(
      transcription.text,
      transcription.speaker,
      speakerNumbers
    );

    this.addToGroup(
      groupKey,
      displayName,
      isUserMsg,
      transcription.text,
      transcription.timestamp,
      tempGroups,
      processedTexts
    );
  }

  /**
   * Get speaker information
   */
  private getSpeakerInfo(
    text: string,
    speaker: string,
    speakerNumbers: Map<string, string>
  ): { displayName: string; groupKey: string; isUserMsg: boolean } {
    const normalizedSpeaker = speaker;
    const isUserMsg =
      normalizedSpeaker === this.speakerService.getPrimaryUserSpeaker();

    if (isUserMsg) {
      return {
        displayName: this.speakerService.getPrimaryUserSpeaker(),
        groupKey: this.speakerService.getPrimaryUserSpeaker(),
        isUserMsg: true,
      };
    }

    // Extract speaker info for external speakers
    const originalSpeakerMatch = text.match(/^\[([^\]]+)\]/);
    const originalSpeaker = originalSpeakerMatch?.[1]?.trim();

    if (originalSpeaker?.toLowerCase().includes("speaker")) {
      const speakerNumberMatch = originalSpeaker.match(/speaker\s*(\d+)/i);
      const speakerNum = speakerNumberMatch?.[1];

      if (speakerNum) {
        const groupKey = `speaker_${speakerNum}`;
        const displayName = originalSpeaker;
        speakerNumbers.set(groupKey, displayName);
        return { displayName, groupKey, isUserMsg: false };
      }
    }

    // Check text for speaker mentions
    const textSpeakerMatch = text.toLowerCase().match(/speaker\s*(\d+)/i);
    if (textSpeakerMatch?.[1]) {
      const speakerNum = textSpeakerMatch[1];
      const groupKey = `speaker_${speakerNum}`;
      const displayName =
        speakerNumbers.get(groupKey) || `Speaker ${speakerNum}`;
      speakerNumbers.set(groupKey, displayName);
      return { displayName, groupKey, isUserMsg: false };
    }

    return {
      displayName: EXTERNAL_SPEAKER_LABEL,
      groupKey: "external_generic",
      isUserMsg: false,
    };
  }

  /**
   * Add transcription to group
   */
  private addToGroup(
    groupKey: string,
    displayName: string,
    isUser: boolean,
    text: string,
    timestamp: string,
    tempGroups: Map<string, any>,
    processedTexts: Map<string, Set<string>>
  ): void {
    if (!tempGroups.has(groupKey)) {
      tempGroups.set(groupKey, {
        isUser,
        displayName,
        transcriptions: [],
      });
      processedTexts.set(groupKey, new Set());
    }

    const groupTexts = processedTexts.get(groupKey);
    if (groupTexts && !groupTexts.has(text)) {
      const group = tempGroups.get(groupKey);
      if (group) {
        group.transcriptions.push({ text, timestamp });
        groupTexts.add(text);
      }
    }
  }

  /**
   * Format logs output
   */
  private formatLogsOutput(
    tempGroups: Map<string, any>
  ): SpeakerTranscriptionLog[] {
    const logs: SpeakerTranscriptionLog[] = Array.from(
      tempGroups.entries()
    ).map(([groupKey, data]) => {
      const sortedTranscriptions = [...data.transcriptions].sort(
        (a, b) =>
          new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime()
      );

      const formattedTranscriptions = sortedTranscriptions.map((t, index) => {
        const text = index === 0 ? `[${data.displayName}] ${t.text}` : t.text;
        return { text, timestamp: t.timestamp };
      });

      return {
        speaker: data.displayName,
        isUser: data.isUser,
        transcriptions: formattedTranscriptions,
      };
    });

    return this.sortLogs(logs);
  }

  /**
   * Sort logs by speaker priority
   */
  private sortLogs(logs: SpeakerTranscriptionLog[]): SpeakerTranscriptionLog[] {
    return logs.sort((a, b) => {
      if (a.isUser) return -1;
      if (b.isUser) return 1;

      const numA = a.speaker.match(/speaker\s*(\d+)/i)?.[1];
      const numB = b.speaker.match(/speaker\s*(\d+)/i)?.[1];

      if (numA && numB) {
        return parseInt(numA) - parseInt(numB);
      }

      return a.speaker.localeCompare(b.speaker);
    });
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { UIUpdater } from "../../../interfaces/transcription/TranscriptionTypes";

/**
 * TranscriptionUIManager - Single Responsibility: Manage UI updates
 * Follows Interface Segregation principle by depending only on UIUpdater interface
 */
export class TranscriptionUIManager {
  private setTexts: UIUpdater;
  private uiTranscriptionHistory: string[] = [];

  constructor(setTexts: UIUpdater) {
    this.setTexts = setTexts;
  }

  /**
   * Update UI with new transcription
   */
  updateTranscription(transcription: string): void {
    if (!transcription || !transcription.trim()) return;

    const newText = transcription.trim();

    // Check if this is an incremental update
    const isIncremental = this.handleIncrementalUpdate(newText);

    if (!isIncremental) {
      this.addNewTranscription(newText);
    }

    // Update UI with complete history
    this.updateUI();
  }

  /**
   * Handle incremental updates (extending previous messages)
   */
  private handleIncrementalUpdate(newText: string): boolean {
    for (let i = this.uiTranscriptionHistory.length - 1; i >= 0; i--) {
      const existingText = this.uiTranscriptionHistory[i];

      if (newText.startsWith(existingText) && newText !== existingText) {
        console.log(`🛠️ Replacing incremental message:`);
        console.log(`  Previous: "${existingText}"`);
        console.log(`  New: "${newText}"`);

        this.uiTranscriptionHistory[i] = newText;
        return true;
      }
    }
    return false;
  }

  /**
   * Add new transcription to history
   */
  private addNewTranscription(text: string): void {
    if (
      this.uiTranscriptionHistory.length === 0 ||
      this.uiTranscriptionHistory[this.uiTranscriptionHistory.length - 1] !==
        text
    ) {
      console.log(`💾 Adding new message: "${text}"`);
      this.uiTranscriptionHistory.push(text);
    }
  }

  /**
   * Update UI with current transcription history
   */
  private updateUI(): void {
    const fullText = this.uiTranscriptionHistory.join("\n");
    console.log(
      `📄 UI state: ${fullText.length} chars, ${this.uiTranscriptionHistory.length} messages`
    );

    this.setTexts((prev) => ({
      ...prev,
      transcription: fullText,
    }));
  }

  /**
   * Flush all pending transcriptions to UI
   */
  flushToUI(transcriptions: string[]): void {
    if (transcriptions.length === 0) {
      console.log("No transcriptions to flush");
      return;
    }

    const allTranscriptions = transcriptions.join("\n");
    console.log(`Flushing ${transcriptions.length} transcriptions to UI`);

    this.updateTranscription(allTranscriptions);
  }

  /**
   * Update UI with other properties
   */
  updateOther(update: Record<string, unknown>): void {
    this.setTexts((prev) => ({ ...prev, ...update }));
  }

  /**
   * Clear UI transcription history
   */
  clear(): void {
    this.uiTranscriptionHistory = [];
    this.updateUI();
  }

  /**
   * Get current UI transcription history
   */
  getHistory(): string[] {
    return [...this.uiTranscriptionHistory];
  }

  /**
   * Set transcription text directly without managing history
   * Used when we want to show only new transcriptions after sending
   */
  setTranscriptionDirectly(text: string): void {
    console.log(`🔄 Setting UI transcription directly: "${text}"`);

    // Clear history and set new text
    this.uiTranscriptionHistory = text ? [text] : [];

    // Update UI
    this.setTexts((prev) => ({
      ...prev,
      transcription: text,
    }));
  }

  /**
   * Get only new transcriptions text for UI display
   */
  getNewTranscriptionsText(store: any): string {
    const newTranscriptions = store.getNewTranscriptions();
    return newTranscriptions.map((t: any) => t.text).join("\n");
  }

  /**
   * Clear sent transcriptions from UI history
   * Called after transcriptions are marked as sent
   */
  clearSentTranscriptions(): void {
    console.log(`🧹 Clearing UI history after sending transcriptions`);
    this.uiTranscriptionHistory = [];
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Neural Cognitive Processors Export Index
// Symbolic modules for transcription processing pipeline

export { NeuralConfigurationBuilder } from './NeuralConfigurationBuilder';
export { NeuralMemoryRetriever } from './NeuralMemoryRetriever';
export { NeuralSignalEnricher } from './NeuralSignalEnricher';
export { ProcessingResultsSaver } from './ProcessingResultsSaver';
export { ResponseGenerator } from './ResponseGenerator';
export { SessionManager } from './SessionManager';
export { TranscriptionExtractor } from './TranscriptionExtractor';

// Type exports
export type { TranscriptionPromptConfig } from './NeuralConfigurationBuilder';
export type { ProcessorMode } from './NeuralSignalEnricher';

// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { getPrimaryUser } from '../../../../../../config/UserConfig';
import { IMemoryService } from '../../../interfaces/memory/IMemoryService';
import { ITranscriptionStorageService } from '../../../interfaces/transcription/ITranscriptionStorageService';
import { SpeakerTranscription } from '../../../interfaces/transcription/TranscriptionTypes';
import { ISpeakerIdentificationService } from '../../../interfaces/utils/ISpeakerIdentificationService';
import { SessionManager } from './SessionManager';

/**
 * Configuration for transcription prompt processing
 */
export interface TranscriptionPromptConfig {
  transcription: string;
  temporaryContext?: string;
  sessionState: {
    sessionId: string;
    interactionCount: number;
    timestamp: string;
    language: string;
  };
  speakerMetadata: {
    primarySpeaker: string;
    detectedSpeakers: string[];
    speakerTranscriptions: SpeakerTranscription[];
  };
  userContextData?: any;
}

/**
 * Neural configuration synthesis builder
 * Responsible for constructing configuration objects for neural signal extraction
 */
export class NeuralConfigurationBuilder {
  constructor(
    private storageService: ITranscriptionStorageService,
    private memoryService: IMemoryService,
    private speakerService: ISpeakerIdentificationService,
    private sessionManager: SessionManager
  ) {}

  /**
   * Build comprehensive configuration for neural signal extraction
   */
  async buildExtractionConfig(
    transcriptionToSend: string, 
    temporaryContext?: string,
    currentLanguage: string = 'pt-BR'
  ): Promise<TranscriptionPromptConfig> {
    
    const userTranscriptions = this.storageService.getSpeakerTranscriptions()
      .filter(transcription => transcription.speaker.includes(getPrimaryUser()));
    
    const detectedSpeakers = this.storageService.getDetectedSpeakers();
    
    const externalTranscriptions = this.storageService.getSpeakerTranscriptions()
      .filter(transcription => transcription.speaker !== getPrimaryUser());

    const userContextData = await this.memoryService.fetchContextualMemory(
      userTranscriptions,
      externalTranscriptions,
      detectedSpeakers,
      temporaryContext
    );

    return {
      transcription: transcriptionToSend,
      temporaryContext,
      sessionState: {
        sessionId: this.sessionManager.getCurrentSessionId(),
        interactionCount: this.sessionManager.getCurrentInteractionCount(),
        timestamp: new Date().toISOString(),
        language: currentLanguage
      },
      speakerMetadata: {
        primarySpeaker: this.speakerService.getPrimaryUserSpeaker(),
        detectedSpeakers: Array.from(detectedSpeakers),
        speakerTranscriptions: userTranscriptions
      },
      userContextData
    };
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IMemoryService } from "../../../interfaces/memory/IMemoryService";
import {
  NeuralProcessingResult,
  NeuralSignal,
} from "../../../interfaces/neural/NeuralSignalTypes";
import { SymbolicInsight } from "../../../types/SymbolicInsight";
import { LoggingUtils } from "../../../utils/LoggingUtils";
import symbolicCognitionTimelineLogger from "../../utils/SymbolicCognitionTimelineLoggerSingleton";
import { ProcessorMode } from "./NeuralSignalEnricher";

/**
 * Neural memory cognitive retrieval processor
 * Responsible for processing neural signals and retrieving relevant memories
 */
export class NeuralMemoryRetriever {
  constructor(private memoryService: IMemoryService) {}

  /**
   * Process neural signals and retrieve relevant memories based on selected backend
   */
  async processSignals(
    enrichedSignals: NeuralSignal[]
  ): Promise<NeuralProcessingResult[]> {
    LoggingUtils.logInfo(
      `⚡ Segunda fase - Processando ${enrichedSignals.length} áreas cerebrais ativadas...`
    );

    // Log neural signals for symbolic cognition tracking
    for (const signal of enrichedSignals) {
      symbolicCognitionTimelineLogger.logNeuralSignal(
        signal.core,
        {
          query: signal.symbolic_query?.query || "",
          keywords: signal.keywords ?? [],
          filters: signal.filters ?? {},
        },
        signal.intensity,
        signal.topK || 10,
        {}
      );
    }

    const processedSignals = await Promise.all(
      enrichedSignals.map(async (signal) => {
        LoggingUtils.logInfo(
          `→ Activating neural core: ${signal.core} (${(
            signal.intensity * 100
          ).toFixed(1)}%)`
        );

        const memoryResults = await this._retrieveMemoriesForSignal(signal);
        const insights = this._extractInsightsFromSignal(signal);

        // 🔍 DEBUG: Log detalhado dos resultados antes de passar para o timeline
        LoggingUtils.logInfo(
          `🔍 [MEMORY-DEBUG] === RESULTS FOR CORE: ${signal.core} ===`
        );
        LoggingUtils.logInfo(
          `📊 [MEMORY-DEBUG] Memory results: ${JSON.stringify({
            matchCount: memoryResults.matchCount,
            durationMs: memoryResults.durationMs,
            resultsLength: memoryResults.results.length,
            firstResult: memoryResults.results[0]?.substring(0, 50) + "...",
          })}`
        );
        LoggingUtils.logInfo(
          `🧠 [MEMORY-DEBUG] Insights extracted: ${JSON.stringify(insights)}`
        );

        // 🔧 CORREÇÃO: Symbolic retrieval logging com dados corretos
        // Converter insights para array de SymbolicInsight de forma mais robusta
        let insightsArray: SymbolicInsight[] = [];

        // 🔍 PRIORIDADE 1: Usar insights do signal se disponíveis
        if (
          Array.isArray(signal.symbolicInsights) &&
          signal.symbolicInsights.length > 0
        ) {
          insightsArray = signal.symbolicInsights.filter(
            (ins) => ins && typeof ins === "object"
          );
          LoggingUtils.logInfo(
            `🔍 [INSIGHTS-DEBUG] Using signal.symbolicInsights: ${insightsArray.length} insights`
          );
        }
        // 🔧 CORREÇÃO: Criar insights baseados APENAS nos resultados reais de memória
        // 🎯 PRIORIDADE 1: Insights originais do signal (se válidos)
        if (
          Array.isArray(signal.symbolicInsights) &&
          signal.symbolicInsights.length > 0
        ) {
          const validOriginalInsights = signal.symbolicInsights
            .filter((ins) => ins && ins.content && ins.content.trim())
            .map((ins, index) => ({
              id: ins.id || `original_${index}`,
              content: ins.content,
              score: ins.score || 0.6,
              type: ins.type || "original",
            }));
          insightsArray.push(...validOriginalInsights);
        }
        // 🔍 VERIFICAÇÃO: Garantir que matchCount reflete dados reais
        const actualMatchCount = memoryResults.matchCount; // ✅ Usar matchCount real do DuckDB
        const actualDuration = memoryResults.durationMs;

        // 🔍 DEBUG: Log antes de chamar o timeline logger
        LoggingUtils.logInfo(
          `📝 [TIMELINE-DEBUG] Calling logSymbolicRetrieval with: ${JSON.stringify(
            {
              core: signal.core,
              insightsCount: insightsArray.length,
              insightsPreview: insightsArray.slice(0, 2),
              matchCount: actualMatchCount,
              originalMatchCount: memoryResults.matchCount,
              durationMs: actualDuration,
            }
          )}`
        );

        // ✅ CORREÇÃO: Usar dados verificados e incluir keywords do signal
        // 🔧 CORREÇÃO: Adicionar keywords ao insight para manter consistência
        const enrichedInsights = insightsArray.map((insight) => ({
          ...insight,
          keywords: signal.keywords || [], // Adicionar keywords do signal original
        }));

        // 🔍 DEBUG: Log final antes de enviar para timeline
        LoggingUtils.logInfo(
          `🔍 [FINAL-TIMELINE-DEBUG] Final data being sent to timeline: ${JSON.stringify(
            {
              core: signal.core,
              enrichedInsightsCount: enrichedInsights.length,
              actualMatchCount,
              actualDuration,
              enrichedInsightsPreview: enrichedInsights.slice(0, 2),
            }
          )}`
        );

        symbolicCognitionTimelineLogger.logSymbolicRetrieval(
          signal.core,
          enrichedInsights,
          actualMatchCount, // Usar contagem real dos resultados
          actualDuration
        );

        return {
          ...signal,
          pineconeResults: memoryResults.results,
          symbolicInsights: enrichedInsights, // ✅ Usar insights estruturados em vez de campos técnicos
        };
      })
    );

    // Transform to standard neural processing format
    return this._transformToNeuralProcessingResults(processedSignals);
  }

  /**
   * Retrieve memories for a specific neural signal
   */
  private async _retrieveMemoriesForSignal(signal: NeuralSignal): Promise<{
    results: string[];
    matchCount: number;
    durationMs: number;
  }> {
    let results: string[] = [];
    let matchCount = 0;
    const start = Date.now();

    try {
      // 🔧 CORREÇÃO: Usar queryMemoryWithCount para obter contagem real
      const memoryResult = await this._retrieveWithMemoryServiceAndCount(
        signal
      );
      results = memoryResult.results;
      matchCount = memoryResult.matchCount;

      // 🔍 DEBUG: Log detalhado da recuperação de memórias
      LoggingUtils.logInfo(
        `🔍 [MEMORY-RETRIEVAL-DEBUG] Core: ${signal.core} | Results: ${
          results.length
        } | MatchCount: ${matchCount} | First result preview: ${results[0]?.substring(
          0,
          100
        )}...`
      );
    } catch (memoryError) {
      LoggingUtils.logError(
        `Error searching memories for core ${signal.core}`,
        memoryError
      );
      // Garantir que em caso de erro, os valores sejam consistentes
      results = [];
      matchCount = 0;
    }

    const durationMs = Date.now() - start;

    LoggingUtils.logInfo(
      `🔍 [MEMORY-FINAL-DEBUG] Core: ${signal.core} | Final matchCount: ${matchCount} | Results length: ${results.length} | Duration: ${durationMs}ms`
    );

    return { results, matchCount, durationMs };
  }

  /**
   * Retrieve memories using standard memory service
   */
  private async _retrieveWithMemoryService(
    signal: NeuralSignal
  ): Promise<string[]> {
    const query = signal.symbolic_query?.query || "";
    LoggingUtils.logInfo(
      `🧠 [NEURAL-MEMORY] Querying: "${query}" for core: ${signal.core}`
    );
    LoggingUtils.logInfo(
      `🔍 [NEURAL-MEMORY] Keywords: [${
        signal.keywords?.join(", ") || "none"
      }], topK: ${signal.topK || "default"}`
    );

    const result = await this.memoryService.queryExpandedMemory(
      query,
      signal.keywords,
      signal.topK,
      signal.filters
    );

    if (result && result.trim()) {
      const resultArray = Array.isArray(result) ? result : [result];
      LoggingUtils.logInfo(
        `✅ [NEURAL-MEMORY] Found ${resultArray.length} memory results for core: ${signal.core}`
      );
      return resultArray;
    } else {
      LoggingUtils.logWarning(
        `❌ [NEURAL-MEMORY] No memories found for core: ${signal.core} with query: "${query}"`
      );
      return [];
    }
  }

  /**
   * Retrieve memories with accurate match count from DuckDB
   */
  private async _retrieveWithMemoryServiceAndCount(
    signal: NeuralSignal
  ): Promise<{ results: string[]; matchCount: number }> {
    const query = signal.symbolic_query?.query || "";

    try {
      // 🔧 CORREÇÃO: Consultar DuckDB diretamente para obter contagem real
      if (!window.electronAPI?.queryDuckDB) {
        LoggingUtils.logWarning(
          "[NEURAL-MEMORY] DuckDB not available, using fallback"
        );
        const fallbackResults = await this._retrieveWithMemoryService(signal);
        return { results: fallbackResults, matchCount: fallbackResults.length };
      }

      // Criar embedding para a query
      const embeddingService = (this.memoryService as any).embeddingService;
      if (!embeddingService?.isInitialized()) {
        LoggingUtils.logWarning(
          "[NEURAL-MEMORY] Embedding service not initialized"
        );
        const fallbackResults = await this._retrieveWithMemoryService(signal);
        return { results: fallbackResults, matchCount: fallbackResults.length };
      }

      const queryEmbedding = await embeddingService.createEmbedding(query);
      if (!queryEmbedding || queryEmbedding.length === 0) {
        LoggingUtils.logWarning("[NEURAL-MEMORY] Failed to create embedding");
        return { results: [], matchCount: 0 };
      }

      // Consultar DuckDB diretamente
      LoggingUtils.logInfo(
        `🧠 [NEURAL-MEMORY-DIRECT] Querying DuckDB directly for: "${query}"`
      );

      const queryResponse = await window.electronAPI.queryDuckDB(
        queryEmbedding,
        signal.topK || 10,
        signal.keywords || [],
        signal.filters || {}
      );

      // Extrair resultados e contagem real
      const matches = queryResponse.matches || [];

      // 🔍 DEBUG: Log detalhado dos matches
      LoggingUtils.logInfo(
        `🔍 [NEURAL-MEMORY-DEBUG] Raw matches from DuckDB: ${JSON.stringify(
          matches.map((match: any, index: number) => ({
            index,
            hasMetadata: !!match.metadata,
            hasContent: !!(match.metadata && match.metadata.content),
            contentLength: match.metadata?.content?.length || 0,
            contentPreview: match.metadata?.content?.substring(0, 50) || "N/A",
          }))
        )}`
      );

      const validMatches = matches.filter(
        (match: any) => match.metadata && match.metadata.content
      );
      const results = validMatches.map(
        (match: any) => match.metadata.content as string
      );

      const matchCount = validMatches.length; // ✅ Contagem dos matches válidos após filtro

      LoggingUtils.logInfo(
        `✅ [NEURAL-MEMORY-DIRECT] Core: ${signal.core} | DuckDB returned ${matches.length} raw matches | Filtered to ${matchCount} valid matches | Extracted ${results.length} results`
      );

      // 🔍 DEBUG: Log final dos resultados
      LoggingUtils.logInfo(
        `🔍 [NEURAL-MEMORY-FINAL] Final results: ${JSON.stringify({
          matchCount,
          resultsLength: results.length,
          resultsPreview: results.map(
            (r, i) => `${i}: ${r.substring(0, 100)}...`
          ),
        })}`
      );

      return { results, matchCount };
    } catch (error) {
      LoggingUtils.logError(
        `[NEURAL-MEMORY-DIRECT] Error querying DuckDB directly for core ${signal.core}`,
        error
      );
      // Fallback para método original
      const fallbackResults = await this._retrieveWithMemoryService(signal);
      return { results: fallbackResults, matchCount: fallbackResults.length };
    }
  }

  /**
   * Extract and normalize insights from neural signal following Orch-OS theory
   */
  private _extractInsightsFromSignal(
    signal: NeuralSignal
  ): Record<string, unknown> {
    let insights: Record<string, unknown> = {};

    // 🎭 ORCH-OS: Mapear cores para arquétipos junguianos
    const coreArchetypes: Record<
      string,
      { archetype: string; essence: string }
    > = {
      "valência emocional": {
        archetype: "The Mirror",
        essence: "emotional reflection and inner truth",
      },
      "intensidade emocional": {
        archetype: "The Warrior",
        essence: "emotional strength and resilience",
      },
      "conexões interacionais": {
        archetype: "The Lover",
        essence: "bonds and relational harmony",
      },
      "estado emocional": {
        archetype: "The Wanderer",
        essence: "emotional journey and discovery",
      },
      memory: {
        archetype: "The Sage",
        essence: "wisdom and accumulated knowledge",
      },
      metacognitive: {
        archetype: "The Seeker",
        essence: "self-awareness and introspection",
      },
      shadow: {
        archetype: "The Shadow",
        essence: "hidden aspects and inner conflicts",
      },
      soul: { archetype: "The Hero", essence: "purpose and transcendence" },
      self: {
        archetype: "The Pioneer",
        essence: "identity and authentic expression",
      },
    };

    // 🔮 ORCH-OS: Detectar propriedades emergentes baseadas na intensidade e contexto
    const emergentProperties: string[] = [];

    if (signal.intensity > 0.8) {
      emergentProperties.push("High symbolic resonance");
    }
    if (signal.intensity < 0.3) {
      emergentProperties.push("Low response diversity");
    }
    if (signal.keywords && signal.keywords.length > 3) {
      emergentProperties.push("Cognitive dissonance");
    }

    // 🎯 ORCH-OS: Criar insights simbólicos baseados na teoria
    const coreInfo = coreArchetypes[signal.core] || {
      archetype: "The Unknown",
      essence: "unexplored symbolic territory",
    };

    insights = {
      // Query simbólica (se disponível)
      ...(signal.symbolic_query?.query && {
        symbolic_query: {
          query: signal.symbolic_query.query,
          symbolic_tension: "meaning collapse in progress",
        },
      }),

      // Keywords como fragmentos simbólicos
      ...(signal.keywords &&
        signal.keywords.length > 0 && {
          symbolic_fragments: signal.keywords.map((keyword) => ({
            fragment: keyword,
            resonance: "active",
          })),
        }),
    };

    return insights;
  }

  /**
   * Transform processed signals to standard neural processing results format
   */
  private _transformToNeuralProcessingResults(
    processedSignals: any[]
  ): NeuralProcessingResult[] {
    return processedSignals.map((signal) => ({
      core: signal.core,
      intensity: signal.intensity,
      output: signal.pineconeResults.join("\n"),
      insights: Array.isArray(signal.symbolicInsights)
        ? signal.symbolicInsights.reduce(
            (acc: Record<string, unknown>, ins: SymbolicInsight) => {
              if (ins && typeof ins.type === "string") {
                acc[ins.type] = ins;
              }
              return acc;
            },
            {}
          )
        : typeof signal.symbolicInsights === "object" &&
          signal.symbolicInsights !== null
        ? signal.symbolicInsights
        : {},
    }));
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { NeuralSignal } from '../../../interfaces/neural/NeuralSignalTypes';
import { IOpenAIService } from '../../../interfaces/openai/IOpenAIService';
import { LoggingUtils } from '../../../utils/LoggingUtils';

/**
 * Processor mode: OpenAI or HuggingFace
 */
export type ProcessorMode = 'openai' | 'huggingface';

/**
 * Neural signal semantic enrichment processor
 * Responsible for enhancing neural signals with semantic queries and context
 */
export class NeuralSignalEnricher {
  constructor(
    private llmService: IOpenAIService
  ) {}

  /**
   * Enrich neural signals with semantic queries based on selected backend
   */
  async enrichSignals(signals: NeuralSignal[], currentLanguage: string): Promise<NeuralSignal[]> {
    return await Promise.all(
      signals.map(async (signal: NeuralSignal) => {
        try {
          let enrichment: {enrichedQuery: string, keywords: string[]};
          
          enrichment = await this._enrichWithLLM(signal, currentLanguage);

          let topK = signal.topK;
          if (typeof topK !== 'number' || isNaN(topK)) {
            topK = Math.round(5 + (signal.intensity || 0) * 10);
          }

          // Symbolic enrichment logging
          LoggingUtils.logInfo(`[ Enrichment] Core: ${signal.core} | Query: ${enrichment.enrichedQuery} | Keywords: ${JSON.stringify(enrichment.keywords)} | Filters: ${JSON.stringify(signal.filters || {})} | topK: ${topK}`);
          
          return {
            ...signal,
            symbolic_query: {
              ...signal.symbolic_query,
              query: enrichment.enrichedQuery
            },
            keywords: enrichment.keywords,
            filters: signal.filters || undefined,
            topK
          };
        } catch (err) {
          LoggingUtils.logError(`Error enriching query for core ${signal.core}`, err);
          return signal;
        }
      })
    );
  }

  /**
   * Enrich signal using LLM backend
   */
  private async _enrichWithLLM(signal: NeuralSignal, currentLanguage: string): Promise<{enrichedQuery: string, keywords: string[]}> {
    return await this.llmService.enrichSemanticQueryForSignal(
      signal.core,
      signal.symbolic_query?.query || '',
      signal.intensity,
      (typeof signal === 'object' && signal && 'context' in signal) ? (signal.context as string) : undefined,
      currentLanguage
    );
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IMemoryService } from '../../../interfaces/memory/IMemoryService';
import { NeuralProcessingResult, NeuralSignalResponse } from '../../../interfaces/neural/NeuralSignalTypes';
import { ITranscriptionStorageService } from '../../../interfaces/transcription/ITranscriptionStorageService';
import { ISpeakerIdentificationService } from '../../../interfaces/utils/ISpeakerIdentificationService';
import { SymbolicInsight } from '../../../types/SymbolicInsight';
import { LoggingUtils } from '../../../utils/LoggingUtils';
import symbolicCognitionTimelineLogger from '../../utils/SymbolicCognitionTimelineLoggerSingleton';
import { SessionManager } from './SessionManager';

/**
 * Neural processing results cognitive saver
 * Responsible for persisting processing results to memory and symbolic logs
 */
export class ProcessingResultsSaver {
  constructor(
    private memoryService: IMemoryService,
    private storageService: ITranscriptionStorageService,
    private speakerService: ISpeakerIdentificationService,
    private sessionManager: SessionManager
  ) {}

  /**
   * Save comprehensive processing results to memory and logs
   */
  async saveResults(
    transcription: string,
    response: string,
    neuralActivation: NeuralSignalResponse,
    processingResults: NeuralProcessingResult[]
  ): Promise<void> {
    
    // Log symbolic cognitive response
    await this._logSymbolicResponse(response, processingResults);

    // Update conversation history
    this.memoryService.addToConversationHistory({ role: "user", content: transcription });

    // Save to long-term memory
    await this._saveToLongTermMemory(transcription, response);

    // Save neural processing data for brain evolution tracking
    await this._saveNeuralProcessingData(neuralActivation, processingResults);
  }

  /**
   * Log symbolic cognitive response with insights
   */
  private async _logSymbolicResponse(response: string, processingResults: NeuralProcessingResult[]): Promise<void> {
    const symbolicTopics = processingResults.map(r => r.core);
    const importantInsights: SymbolicInsight[] = processingResults
      .flatMap(r => Array.isArray(r.insights) ? r.insights : [])
      .filter(insight => insight && typeof insight.type === 'string');
    
    symbolicCognitionTimelineLogger.logGptResponse({
      response,
      symbolicTopics,
      insights: importantInsights
    });
  }

  /**
   * Save interaction to long-term memory
   */
  private async _saveToLongTermMemory(transcription: string, response: string): Promise<void> {
    await this.memoryService.saveToLongTermMemory(
      transcription,
      response,
      this.storageService.getSpeakerTranscriptions(),
      this.speakerService.getPrimaryUserSpeaker()
    );
  }

  /**
   * Save neural processing data for brain evolution tracking
   */
  private async _saveNeuralProcessingData(
    activation: NeuralSignalResponse,
    processingResults: NeuralProcessingResult[] = []
  ): Promise<void> {
    try {
      const neuralData = {
        sessionId: this.sessionManager.getCurrentSessionId(),
        timestamp: new Date().toISOString(),
        activation,
        results: processingResults,
        interactionCount: this.sessionManager.incrementInteraction()
      };

      // TODO: Implement neural data persistence storage
      LoggingUtils.logInfo(`Neural processing data saved: ${JSON.stringify(neuralData)}`);
    } catch (error) {
      LoggingUtils.logError("Error saving neural processing data", error);
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { IMemoryService } from "../../../interfaces/memory/IMemoryService";
import { IOpenAIService } from "../../../interfaces/openai/IOpenAIService";
import { Message } from "../../../interfaces/transcription/TranscriptionTypes";
import { LoggingUtils } from "../../../utils/LoggingUtils";
import { cleanThinkTags } from "../../../utils/ThinkTagCleaner";
import symbolicCognitionTimelineLogger from "../../utils/SymbolicCognitionTimelineLoggerSingleton";

/**
 * Neural response cognitive generator
 * Responsible for generating responses using different AI backends
 */
export class ResponseGenerator {
  constructor(
    private memoryService: IMemoryService,
    private llmService: IOpenAIService
  ) {}

  /**
   * Generate response using the selected backend
   */
  async generateResponse(
    integratedPrompt: string,
    temporaryContext?: string
  ): Promise<string> {
    symbolicCognitionTimelineLogger.logFusionInitiated();

    // Prepare context messages (shared by both backends)
    const contextMessages = this._prepareContextMessages(temporaryContext);

    if (contextMessages.length > 0) {
      this.memoryService.addContextToHistory(contextMessages);
    }

    const conversationHistory = this.memoryService.getConversationHistory();

    const messages = this.memoryService.buildPromptMessagesForModel(
      integratedPrompt,
      conversationHistory
    );

    return await this._generate(messages);
  }

  /**
   * Prepare context messages for processing
   */
  private _prepareContextMessages(temporaryContext?: string): Message[] {
    const contextMessages: Message[] = [];

    if (temporaryContext?.trim()) {
      contextMessages.push({
        role: "developer",
        content: `🧠 Temporary instructions:\n${temporaryContext.trim()}`,
      });
    }

    return contextMessages;
  }

  /**
   * Generate response using OpenAI backend
   */
  private async _generate(messages: Message[]): Promise<string> {
    try {
      const response = await this.llmService.streamOpenAIResponse(messages);

      // Clean think tags from the final response
      const cleanedResponse = cleanThinkTags(response.responseText);

      return cleanedResponse;
    } catch (error: any) {
      if (error.message?.includes("does not have access to model")) {
        LoggingUtils.logError(
          "Invalid model detected, falling back to gpt-4o-mini",
          error
        );
        // Clear invalid model and retry with default
        if (typeof window !== "undefined" && window.localStorage) {
          window.localStorage.removeItem("chatgptModel");
        }
        throw new Error(
          "Invalid model configuration. Please restart the application."
        );
      }
      throw error;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Neural session cognitive manager
 * Responsible for managing session identifiers and interaction tracking
 */
export class SessionManager {
  private _interactionCount: number = 0;
  private _currentSessionId: string;

  constructor() {
    this._currentSessionId = this._generateSessionId();
  }

  /**
   * Generate unique session identifier with neural patterns
   */
  private _generateSessionId(): string {
    return `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Get current session identifier
   */
  getCurrentSessionId(): string {
    return this._currentSessionId;
  }

  /**
   * Get current interaction count
   */
  getCurrentInteractionCount(): number {
    return this._interactionCount;
  }

  /**
   * Increment interaction count for neural tracking
   */
  incrementInteraction(): number {
    return ++this._interactionCount;
  }

  /**
   * Reset session with new identifier
   */
  resetSession(): void {
    this._currentSessionId = this._generateSessionId();
    this._interactionCount = 0;
  }

  /**
   * Get session metadata for neural processing
   */
  getSessionMetadata(): {
    sessionId: string;
    interactionCount: number;
    timestamp: string;
  } {
    return {
      sessionId: this._currentSessionId,
      interactionCount: this._interactionCount,
      timestamp: new Date().toISOString()
    };
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { ITranscriptionStorageService } from "../../../interfaces/transcription/ITranscriptionStorageService";

/**
 * Neural transcription extraction processor
 * Responsible for extracting new lines that haven't been processed yet
 */
export class TranscriptionExtractor {
  constructor(private storageService: ITranscriptionStorageService) {}

  /**
   * Extract new transcription lines that haven't been processed yet
   * WITHOUT marking them as sent (for preview/validation purposes)
   */
  extractNewLines(): string | null {
    const newTranscriptions = this.storageService.getNewTranscriptions();

    if (newTranscriptions.length === 0) {
      console.log("📭 No new transcriptions to extract");
      return null;
    }

    // Format transcriptions with speaker information
    const formattedLines = newTranscriptions.map((t) => {
      return `${t.speaker}: ${t.text}`;
    });

    console.log(`📤 Extracting ${newTranscriptions.length} new transcriptions`);
    return formattedLines.join("\n");
  }

  /**
   * Extract new transcriptions AND immediately mark them as sent
   * This ensures atomic operation preventing duplicate sends
   */
  extractAndMarkAsSent(): string | null {
    // Use the atomic method from storage service
    const newTranscriptions = this.storageService.extractAndMarkAsSent();

    if (newTranscriptions.length === 0) {
      console.log("📭 No new transcriptions to extract");
      return null;
    }

    // Format transcriptions with speaker information
    const formattedLines = newTranscriptions.map((t) => {
      return `${t.speaker}: ${t.text}`;
    });

    console.log(
      `📤 Extracted and marked ${newTranscriptions.length} transcriptions as sent`
    );

    return formattedLines.join("\n");
  }

  /**
   * Reset extraction state for new session
   */
  reset(): void {
    // No longer need to track index manually
    console.log("🔄 Transcription extractor reset");
  }

  /**
   * Check if there are new transcriptions available
   */
  hasNewTranscriptions(): boolean {
    return this.storageService.hasNewTranscriptions();
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { SpeakerTranscription } from "../../../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../../../interfaces/utils/ISpeakerIdentificationService";
import { LoggingUtils } from "../../../utils/LoggingUtils";

/**
 * TranscriptionProcessor - Single Responsibility: Process transcriptions (duplicates, speakers)
 * Follows DRY principle by centralizing all transcription processing logic
 */
export class TranscriptionProcessor {
  private speakerService: ISpeakerIdentificationService;

  constructor(speakerService: ISpeakerIdentificationService) {
    this.speakerService = speakerService;
  }

  /**
   * Check if transcription is a duplicate
   */
  isDuplicate(
    text: string,
    existingTranscriptions: SpeakerTranscription[]
  ): boolean {
    const cleanText = text.trim();
    const isDuplicate = existingTranscriptions.some(
      (st) =>
        st.text === cleanText &&
        Date.now() - new Date(st.timestamp).getTime() < 2000 // Within 2 seconds
    );

    if (isDuplicate) {
      LoggingUtils.logInfo(`Ignoring duplicate transcription: "${cleanText}"`);
    }

    return isDuplicate;
  }

  /**
   * Process speaker from transcription text
   */
  processSpeaker(
    text: string,
    providedSpeaker?: string,
    currentSpeaker?: string
  ): { speaker: string; cleanText: string } {
    const cleanText = text.trim();

    // Check for explicit speaker markers
    if (this.hasSpeakerMarkers(cleanText)) {
      const segments = this.speakerService.splitMixedTranscription(cleanText);
      if (segments.length > 0) {
        // Return the first segment's speaker
        return {
          speaker: segments[0].speaker,
          cleanText: segments[0].text,
        };
      }
    }

    // Use provided speaker if available
    if (providedSpeaker?.trim()) {
      const normalizedSpeaker = this.speakerService.normalizeAndIdentifySpeaker(
        providedSpeaker.trim()
      );
      return {
        speaker: normalizedSpeaker,
        cleanText,
      };
    }

    // Fall back to current speaker or primary user
    const speakerToUse =
      currentSpeaker || this.speakerService.getPrimaryUserSpeaker();

    LoggingUtils.logInfo(
      `Speaker assigned: "${speakerToUse}" for text without marker: "${cleanText.substring(
        0,
        30
      )}..."`
    );

    return {
      speaker: speakerToUse,
      cleanText,
    };
  }

  /**
   * Check if text has speaker markers
   */
  hasSpeakerMarkers(text: string): boolean {
    return (
      text.includes("[") &&
      text.includes("]") &&
      /\[(Guilherme|Speaker\s*\d+)\]/i.test(text)
    );
  }

  /**
   * Process transcription segments
   */
  processSegments(text: string): Array<{ text: string; speaker: string }> {
    if (!this.hasSpeakerMarkers(text)) {
      return [];
    }

    return this.speakerService.splitMixedTranscription(text);
  }

  /**
   * Filter transcriptions by user
   */
  filterByUser(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): SpeakerTranscription[] {
    return transcriptions.filter((st) => st.speaker === primaryUserSpeaker);
  }

  /**
   * Get last message from user
   */
  getLastUserMessage(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): SpeakerTranscription | null {
    const userMessages = this.filterByUser(transcriptions, primaryUserSpeaker);
    return userMessages.length > 0
      ? userMessages[userMessages.length - 1]
      : null;
  }

  /**
   * Get last messages from external speakers
   */
  getLastExternalMessages(
    transcriptions: SpeakerTranscription[],
    detectedSpeakers: Set<string>
  ): Map<string, SpeakerTranscription> {
    const lastMessages = new Map<string, SpeakerTranscription>();

    if (detectedSpeakers.has("external")) {
      const messages = this.speakerService.filterTranscriptionsBySpeaker(
        "external",
        transcriptions
      );
      if (messages.length > 0) {
        lastMessages.set("external", messages[messages.length - 1]);
      }
    }

    return lastMessages;
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { SpeakerTranscription } from "../../../interfaces/transcription/TranscriptionTypes";

/**
 * TranscriptionStore - Single Responsibility: Store and retrieve transcriptions
 * Follows KISS principle by keeping storage logic simple and focused
 */
export class TranscriptionStore {
  private transcriptionList: string[] = [""];
  private speakerTranscriptions: SpeakerTranscription[] = [];
  private detectedSpeakers: Set<string> = new Set();
  private lastTranscription: string = "";
  private currentSpeaker: string = "";
  private uiTranscriptionList: string[] = [];

  /**
   * Add a transcription to all relevant stores
   */
  addTranscription(text: string, speaker: string, timestamp: string): void {
    // Add to traditional list
    this.transcriptionList.push(text);
    this.lastTranscription = text;

    // Add to speaker-specific list with sent status
    this.speakerTranscriptions.push({
      text: text.trim(),
      speaker,
      timestamp,
      sent: false, // New transcriptions are not sent by default
    });

    // Add speaker to detected speakers
    this.detectedSpeakers.add(speaker);

    // Add to UI transcription list
    this.uiTranscriptionList.push(text.trim());
  }

  /**
   * Get all transcriptions as a single string
   */
  getUITranscriptionText(): string {
    return this.uiTranscriptionList.join("\n");
  }

  /**
   * Get only new transcriptions that haven't been sent yet
   */
  getNewTranscriptions(): SpeakerTranscription[] {
    return this.speakerTranscriptions.filter((t) => !t.sent);
  }

  /**
   * Get all transcriptions with their sent status
   */
  getAllTranscriptionsWithStatus(): SpeakerTranscription[] {
    return this.speakerTranscriptions;
  }

  /**
   * Mark specific transcriptions as sent
   */
  markTranscriptionsAsSent(transcriptions?: SpeakerTranscription[]): void {
    if (transcriptions) {
      // Mark specific transcriptions as sent
      transcriptions.forEach((t) => {
        const index = this.speakerTranscriptions.findIndex(
          (st) => st.timestamp === t.timestamp && st.text === t.text
        );
        if (index !== -1) {
          this.speakerTranscriptions[index].sent = true;
        }
      });
    } else {
      // Mark all unsent transcriptions as sent
      this.speakerTranscriptions.forEach((t) => {
        if (!t.sent) {
          t.sent = true;
        }
      });
    }

    const sentCount = this.speakerTranscriptions.filter((t) => t.sent).length;
    console.log(`📍 Marked transcriptions as sent. Total sent: ${sentCount}`);
  }

  /**
   * Get the transcription list
   */
  getTranscriptionList(): string[] {
    return this.transcriptionList;
  }

  /**
   * Get speaker transcriptions
   */
  getSpeakerTranscriptions(): SpeakerTranscription[] {
    return this.speakerTranscriptions;
  }

  /**
   * Get detected speakers
   */
  getDetectedSpeakers(): Set<string> {
    return this.detectedSpeakers;
  }

  /**
   * Get last transcription
   */
  getLastTranscription(): string {
    return this.lastTranscription;
  }

  /**
   * Get/Set current speaker
   */
  getCurrentSpeaker(): string {
    return this.currentSpeaker;
  }

  setCurrentSpeaker(speaker: string): void {
    this.currentSpeaker = speaker;
  }

  /**
   * Get UI transcription list
   */
  getUITranscriptionList(): string[] {
    return this.uiTranscriptionList;
  }

  /**
   * Check if there are valid transcriptions
   */
  hasValidTranscriptions(): boolean {
    return (
      this.speakerTranscriptions.length > 0 ||
      this.transcriptionList.some((text) => text && text.trim().length > 0)
    );
  }

  /**
   * Check if there are new transcriptions to send
   */
  hasNewTranscriptions(): boolean {
    return this.speakerTranscriptions.some((t) => !t.sent);
  }

  /**
   * Clear all transcription data
   */
  clear(): void {
    this.transcriptionList = [""];
    this.speakerTranscriptions = [];
    this.detectedSpeakers = new Set();
    this.lastTranscription = "";
    this.uiTranscriptionList = [];
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// BatchTranscriptionProcessor.ts
// Implementation of IBatchTranscriptionProcessor

import { IBatchTranscriptionProcessor } from "../../interfaces/transcription/IBatchTranscriptionProcessor";
import { ITranscriptionFormatter } from "../../interfaces/transcription/ITranscriptionFormatter";
import { EXTERNAL_HEADER, EXTERNAL_SPEAKER_LABEL, Message, SpeakerSegment, SpeakerTranscription, USER_HEADER } from "../../interfaces/transcription/TranscriptionTypes";

export class BatchTranscriptionProcessor implements IBatchTranscriptionProcessor {
  private formatter: ITranscriptionFormatter;
  
  constructor(formatter: ITranscriptionFormatter) {
    this.formatter = formatter;
  }
  
  /**
   * Processes and formats transcriptions from multiple speakers
   */
  processTranscriptions(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): SpeakerSegment[] {
    // First, remove duplicates
    const uniqueTranscriptions = this.deduplicateTranscriptions(transcriptions);
    
    // Sort chronologically by timestamp
    const sortedTranscriptions = [...uniqueTranscriptions]
      .sort((a, b) => new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime());
    
    const segments: SpeakerSegment[] = [];
    let lastSpeakerKey = "";
    
    // Process each transcription
    for (const transcription of sortedTranscriptions) {
      // Check if it's a mixed transcription with multiple speakers
      if (transcription.text.includes('[') && transcription.text.includes(']')) {
        // Split mixed transcription into segments
        const mixedSegments = this.formatter.formatMixedTranscription(
          transcription.text, 
          primaryUserSpeaker
        );
        
        // Process each segment individually
        for (const segment of mixedSegments) {
          // Update show speaker flag based on whether speaker changed
          const showSpeaker = (lastSpeakerKey !== segment.speaker);
          lastSpeakerKey = segment.speaker;
          
          segments.push({
            ...segment,
            showSpeaker
          });
        }
      } else {
        // Regular transcription (single speaker)
        let speakerKey = transcription.speaker;
        let speakerLabel: string;
        
        // Determine the correct label for the speaker
        if (speakerKey === primaryUserSpeaker) {
          speakerLabel = primaryUserSpeaker;
        } else {
          // For external speaker, check for original label
          const originalLabel = transcription.text.match(/^\[([^\]]+)\]/)?.[1];
          if (originalLabel && originalLabel.toLowerCase().includes("speaker")) {
            speakerLabel = originalLabel;
            speakerKey = "external";
          } else {
            speakerLabel = EXTERNAL_SPEAKER_LABEL;
            speakerKey = "external";
          }
        }
        
        // Clean the text of any existing speaker markings
        const cleanText = transcription.text.replace(/^\[[^\]]+\]\s*/, '');
        
        // Update show speaker flag based on whether speaker changed
        const showSpeaker = (lastSpeakerKey !== speakerKey);
        lastSpeakerKey = speakerKey;
        
        segments.push({
          speaker: speakerKey,
          text: showSpeaker ? `[${speakerLabel}] ${cleanText}` : cleanText,
          showSpeaker
        });
      }
    }
    
    return segments;
  }
  
  /**
   * Removes duplicate content from transcriptions
   */
  deduplicateTranscriptions(
    transcriptions: SpeakerTranscription[]
  ): SpeakerTranscription[] {
    const processedTexts = new Set<string>();
    const uniqueTranscriptions: SpeakerTranscription[] = [];
    
    for (const transcription of transcriptions) {
      // Skip duplicates
      if (processedTexts.has(transcription.text)) continue;
      
      processedTexts.add(transcription.text);
      uniqueTranscriptions.push(transcription);
    }
    
    return uniqueTranscriptions;
  }
  
  /**
   * Extracts the last message from each speaker
   */
  extractLastMessageBySpeaker(
    transcriptions: SpeakerTranscription[],
    speakers: string[]
  ): Map<string, SpeakerTranscription> {
    const lastMessages = new Map<string, SpeakerTranscription>();
    
    // Helper to get the last message from a specific speaker
    const getLastMessageFrom = (speaker: string): SpeakerTranscription | null => {
      const filteredMessages = transcriptions.filter(st => st.speaker === speaker);
      return filteredMessages.length > 0 ? filteredMessages[filteredMessages.length - 1] : null;
    };
    
    // Get last message for each requested speaker
    for (const speaker of speakers) {
      const lastMessage = getLastMessageFrom(speaker);
      if (lastMessage) {
        lastMessages.set(speaker, lastMessage);
      }
    }
    
    return lastMessages;
  }
  
  /**
   * Formats transcriptions for conversation history
   */
  formatTranscriptionsForHistory(
    transcriptions: SpeakerTranscription[],
    primaryUserSpeaker: string
  ): Message[] {
    const messages: Message[] = [];
    
    // Get the last message from the primary user
    const lastMessages = this.extractLastMessageBySpeaker(
      transcriptions,
      [primaryUserSpeaker, "external"]
    );
    
    // Add the primary user's message
    const lastUserMessage = lastMessages.get(primaryUserSpeaker);
    if (lastUserMessage) {
      messages.push({
        role: "user",
        content: `${USER_HEADER} (last message):\n${lastUserMessage.text}`
      });
    }
    
    // Add external speaker message
    const lastExternalMessage = lastMessages.get("external");
    if (lastExternalMessage) {
      // Extract original label if available
      const originalLabel = lastExternalMessage.text.includes('[') ?
        lastExternalMessage.text.match(/^\[([^\]]+)\]/)?.[1] : null;
        
      // Use original label when available and contains "Speaker"
      const speakerLabel = originalLabel?.includes("Speaker") ?
        originalLabel : EXTERNAL_SPEAKER_LABEL;
        
      // Clean any existing speaker prefix
      const cleanText = lastExternalMessage.text.replace(/^\[[^\]]+\]\s*/, '');
      
      messages.push({
        role: "user",
        content: `${EXTERNAL_HEADER} ${speakerLabel} (last message):\n[${speakerLabel}] ${cleanText}`
      });
    }
    
    return messages;
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Index file for transcription services
// Exports all refactored components for easy import

export { QuestionDetector } from "./detectors/QuestionDetector";
export { TranscriptionLogger } from "./loggers/TranscriptionLogger";
export { TranscriptionUIManager } from "./managers/TranscriptionUIManager";
export { TranscriptionProcessor } from "./processors/TranscriptionProcessor";
export { TranscriptionStore } from "./stores/TranscriptionStore";
export { TranscriptionStorageService } from "./TranscriptionStorageService";
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * LiveTranscriptionProcessor handles processing of transcription data from Deepgram
 * and management of speaker segments.
 */
import { SpeakerBuffer } from "../utils/DeepgramTypes";
import { Logger } from "../utils/Logger";
// Import of the interface for integration with the transcription storage service
import { getPrimaryUser } from "../../../../../config/UserConfig";
import { ITranscriptionStorageService } from "../../interfaces/transcription/ITranscriptionStorageService";
export class LiveTranscriptionProcessor {
  private logger: Logger;
  private activeSpeakerBuffer: Record<number, SpeakerBuffer> = {};
  private currentSpeaker: string = "";
  private transcriptionList: string[] = [];
  private transcriptionCallback: ((event: string, data: any) => void) | null =
    null;
  private transcriptionStorageService: ITranscriptionStorageService | null =
    null;

  constructor() {
    this.logger = new Logger("LiveTranscriptionProcessor");
  }

  /**
   * Configures the transcription storage service for direct sending
   */
  public setTranscriptionStorageService(
    service: ITranscriptionStorageService
  ): void {
    console.log(
      `💾 [INTEGRATION] Storage service received:`,
      service ? "VALID INSTANCE" : "NULL"
    );
    this.transcriptionStorageService = service;
    if (service && typeof service.updateTranscriptionUI === "function") {
      console.log(
        `✅ [INTEGRATION] updateTranscriptionUI available and ready for use`
      );
    } else {
      console.error(
        `❌ [INTEGRATION] updateTranscriptionUI NOT available!`,
        service
      );
    }
    this.logger.info("Storage service configured for direct sending");
  }

  /**
   * Register a callback to receive transcription events
   */
  public registerTranscriptionCallback(
    callback: (event: string, data: any) => void
  ): void {
    this.transcriptionCallback = callback;
    this.logger.info("Callback registered");
  }

  /**
   * Process incoming transcription data from Deepgram
   */
  public handleTranscriptionEvent(data: any): void {
    try {
      console.log("🔄 [PROCESS] Starting transcription processing");

      // Check if this is a final transcription
      // We ignore all interim (non-final) transcriptions
      if (!data.is_final) {
        console.log("⏭️ [PROCESS] Ignoring interim (non-final) transcription");
        return;
      }

      // Check if data contains channel (singular) or channels (plural)
      if (data.channel) {
        // New Deepgram API (v3)
        const channelIndex =
          data.channel_index && data.channel_index[0] !== undefined
            ? data.channel_index[0]
            : 0;

        console.log(`🔄 [PROCESS] Processing channel ${channelIndex}`);

        // Initialize buffer for this channel if not exists
        if (!this.activeSpeakerBuffer[channelIndex]) {
          console.log(
            `🔄 [PROCESS] Initializing buffer for channel ${channelIndex}`
          );
          this.activeSpeakerBuffer[channelIndex] = {
            lastSpeaker:
              channelIndex === 0 ? getPrimaryUser() : `Speaker ${channelIndex}`,
            currentSegment: [],
            formattedSegment: "",
            lastFlushedText: "",
          };
        }

        const alternative = data.channel.alternatives[0];
        if (!alternative?.transcript) {
          return;
        }

        const buffer = this.activeSpeakerBuffer[channelIndex];
        const speakerPrefix =
          this.currentSpeaker !== buffer.lastSpeaker
            ? `[${buffer.lastSpeaker}] `
            : "";

        console.log(
          `🔄 [PROCESS] Current speaker: "${this.currentSpeaker}", Buffer speaker: "${buffer.lastSpeaker}"`
        );
        console.log(`🔄 [PROCESS] Speaker prefix: "${speakerPrefix}"`);

        // Update current speaker
        this.currentSpeaker = buffer.lastSpeaker;

        // Process the transcript content immediately
        const transcriptText = `${speakerPrefix}${alternative.transcript}`;
        console.log(`🔄 [PROCESS] Formatted text: "${transcriptText}"`);

        // Check if this is new or different content
        if (
          transcriptText &&
          transcriptText.trim() &&
          transcriptText !== buffer.lastFlushedText &&
          transcriptText.trim() !== `[${buffer.lastSpeaker}]`
        ) {
          console.log(
            `✅ [PROCESS] Valid and different text: "${transcriptText}"`
          );
          this.logger.info(
            `Final transcription (channel ${channelIndex}): ${transcriptText}`
          );

          // Send only final transcriptions directly via IPC to the main process
          // Explicitly check the storage service for each transcription
          try {
            if (this.transcriptionStorageService) {
              // Ensure the method exists before calling
              if (
                typeof this.transcriptionStorageService.addTranscription ===
                "function"
              ) {
                console.log(
                  `📝 [PROCESS] Sending directly to TranscriptionStorageService: "${transcriptText}"`
                );
                this.transcriptionStorageService.addTranscription(
                  transcriptText
                );
                console.log(
                  `✅ [PROCESS] Transcription successfully sent to storageService`
                );
                // UI update is now handled automatically inside addTranscription
              } else {
                console.error(
                  `❌ [PROCESS] addTranscription NOT available in service!`
                );
              }
            } else {
              console.warn(
                `⚠️ [PROCESS] TranscriptionStorageService NOT available during transcription processing`
              );
            }
          } catch (error) {
            console.error(
              `❌ [PROCESS] Error sending to TranscriptionStorageService:`,
              error
            );
          }

          // Maintain IPC sending for compatibility with panel
          if (typeof window !== "undefined" && window.electronAPI) {
            try {
              console.log(
                `📢 [PROCESS] Sending final transcription via IPC: "${transcriptText}"`
              );
              window.electronAPI.sendAudioTranscription(transcriptText);
            } catch (error) {
              console.error("❌ [PROCESS] Error sending via IPC:", error);
            }
          }

          // Also send via callback for compatibility
          if (this.transcriptionCallback) {
            console.log(
              `📢 [PROCESS] Sending to callback: "${transcriptText}"`
            );
            this.transcriptionCallback("transcript", {
              text: transcriptText,
              isFinal: true, // Always true because we're filtering out non-final transcriptions
              channel: channelIndex,
              speaker: buffer.lastSpeaker,
            });
          } else {
            console.log("❌ [PROCESS] TranscriptionCallback NOT registered");
          }

          // Add to permanent history
          console.log(
            `📝 [PROCESS] Saving final transcription: "${transcriptText}"`
          );
          this.transcriptionList.push(transcriptText);
          buffer.lastFlushedText = transcriptText;
        } else {
          console.log(
            `⚠️ [PROCESS] Text ignored because it was empty or the same as the previous: "${transcriptText}"`
          );
        }

        // Store the formatted version
        buffer.formattedSegment = alternative.transcript;

        // Still accumulate words for speaker change detection
        if (alternative.words && alternative.words.length > 0) {
          console.log(
            `🔄 [PROCESS] Processing ${alternative.words.length} words for speaker detection`
          );
          buffer.currentSegment = alternative.words.map(
            (w: { word: any }) => w.word
          );

          // Process speaker changes in words if available
          let currentSegmentSpeaker = buffer.lastSpeaker;
          for (const word of alternative.words) {
            if (word.speaker) {
              const speaker = `Speaker ${word.speaker}`;
              if (speaker !== currentSegmentSpeaker) {
                // Speaker change detected
                console.log(
                  `👥 [PROCESS] Speaker change detected: ${currentSegmentSpeaker} -> ${speaker}`
                );
                this.logger.info(
                  `Speaker change detected: ${currentSegmentSpeaker} -> ${speaker}`
                );
                currentSegmentSpeaker = speaker;
                buffer.lastSpeaker = speaker;
              }
            }
          }
        } else {
          console.log("⚠️ [PROCESS] No words for speaker detection");
        }
      } else if (data.channels) {
        console.log(
          `🔄 [PROCESS] Multichannel mode, processing ${data.channels.length} channels`
        );
        // Simplified processing for multiple channels
        data.channels.forEach((channel: any, channelIndex: number) => {
          console.log(`🔄 [PROCESS] Processing channel ${channelIndex}`);

          // Initialize buffer for this channel if not exists
          if (!this.activeSpeakerBuffer[channelIndex]) {
            console.log(
              `🔄 [PROCESS] Initializing buffer for channel ${channelIndex}`
            );
            this.activeSpeakerBuffer[channelIndex] = {
              lastSpeaker:
                channelIndex === 0
                  ? getPrimaryUser()
                  : `Speaker ${channelIndex}`,
              currentSegment: [],
              formattedSegment: "",
              lastFlushedText: "",
            };
          }

          const alternative = channel.alternatives[0];
          if (!alternative?.transcript) {
            console.log(
              `❌ [PROCESS] Channel ${channelIndex} without transcription, skipping`
            );
            return;
          }

          const buffer = this.activeSpeakerBuffer[channelIndex];
          const transcriptText = `[${buffer.lastSpeaker}] ${alternative.transcript}`;

          // Send final transcriptions via IPC to the main process
          if (
            typeof window !== "undefined" &&
            window.electronAPI &&
            transcriptText.trim() &&
            transcriptText !== buffer.lastFlushedText
          ) {
            try {
              console.log(
                `📢 [PROCESS] Sending final transcription via IPC: "${transcriptText}"`
              );
              window.electronAPI.sendAudioTranscription(transcriptText);
            } catch (error) {
              console.error("❌ [PROCESS] Error sending via IPC:", error);
            }
          }

          // Also forward to storage service for unified processing
          if (this.transcriptionStorageService) {
            this.transcriptionStorageService.addTranscription(transcriptText);
          }

          // Also send via callback for compatibility
          if (
            this.transcriptionCallback &&
            transcriptText.trim() &&
            transcriptText !== buffer.lastFlushedText
          ) {
            console.log(
              `📢 [PROCESS] Sending to callback: "${transcriptText}"`
            );
            this.transcriptionCallback("transcript", {
              text: transcriptText,
              isFinal: true, // Always true because we're filtering out non-final transcriptions
              channel: channelIndex,
              speaker: buffer.lastSpeaker,
            });

            // Save final transcription
            console.log(
              `📝 [PROCESS] Saving final transcription: "${transcriptText}"`
            );
            this.transcriptionList.push(transcriptText);
            buffer.lastFlushedText = transcriptText;
          } else {
            console.log(
              `⚠️ [PROCESS] Text ignored: empty=${!transcriptText.trim()}, repeated=${
                transcriptText === buffer.lastFlushedText
              }`
            );
          }

          // Store formatted version
          buffer.formattedSegment = alternative.transcript;
        });
      } else {
        console.log(
          "❌ [PROCESS] Unrecognized data format:",
          Object.keys(data)
        );
      }

      console.log("✅ [PROCESS] Transcription processing completed");
    } catch (error) {
      console.log("❌ [PROCESS] Error during transcription processing:", error);
      this.logger.error("Error processing transcription", error);
    }
  }

  /**
   * Flush the current speaker segment and return formatted text
   */
  public flushSpeakerSegment(channelIndex: number): string | null {
    const buffer = this.activeSpeakerBuffer[channelIndex];
    if (!buffer || buffer.currentSegment.length === 0) return null;

    // Use the original formatted text instead of reconstructing from words
    const content =
      buffer.formattedSegment || buffer.currentSegment.join(" ").trim();
    if (!content) return null;

    // Check if speaker has changed from last transcription segment
    const speakerPrefix =
      this.currentSpeaker !== buffer.lastSpeaker
        ? `[${buffer.lastSpeaker}] `
        : "";

    // Update current speaker
    this.currentSpeaker = buffer.lastSpeaker;

    const text = `${speakerPrefix}${content}`;
    buffer.currentSegment = [];
    buffer.formattedSegment = "";
    return text;
  }

  /**
   * Get the stored transcription list
   */
  public getTranscriptionList(): string[] {
    return [...this.transcriptionList];
  }

  /**
   * Clear the transcription history
   */
  public clearTranscriptionList(): void {
    this.transcriptionList = [];
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionContextManager.ts
// Manages the temporary context of transcriptions

/**
 * Context manager that maintains the temporaryContext between calls
 * and ensures it is not lost when dynamic objects are modified
 */
export class TranscriptionContextManager {
    // Singleton instance
    private static instance: TranscriptionContextManager;

    // Stores the current temporary context
    private currentTemporaryContext: string = '';

    // Stores the memory associated with the temporary context
    private temporaryContextMemory: string = '';

    // Stores the last temporary context queried in Pinecone
    // to avoid repeated queries with the same context
    private lastQueriedTemporaryContext: string = '';

    // Use the getInstance() method to obtain the unique instance
    private constructor() { }

    /**
     * Gets the unique instance of the context manager
     */
    public static getInstance(): TranscriptionContextManager {
        if (!TranscriptionContextManager.instance) {
            TranscriptionContextManager.instance = new TranscriptionContextManager();
        }

        return TranscriptionContextManager.instance;
    }

    /**
     * Sets or updates the temporary context
     * @param context The new temporary context
     */
    public setTemporaryContext(context: string | undefined): void {
        if (context === undefined) {
            return; // Does not clear the context if undefined is passed
        }

        // Updates the temporary context with the new value, even if it is an empty string
        // (Note: an empty string does not clear the context, it only serves to replace
        // the previous context with an empty context)
        this.currentTemporaryContext = context;
    }

    /**
     * Gets the current temporary context
     * @returns The current temporary context or empty string
     */
    public getTemporaryContext(): string {
        return this.currentTemporaryContext;
    }

    /**
     * Sets the memory associated with the temporary context
     * @param memory The memory associated with the context
     */
    public setTemporaryContextMemory(memory: string): void {
        this.temporaryContextMemory = memory;
    }

    /**
     * Gets the memory associated with the temporary context
     * @returns The memory associated with the temporary context
     */
    public getTemporaryContextMemory(): string {
        return this.temporaryContextMemory;
    }

    /**
     * Checks if the current temporary context is different from the last queried context
     * @param context The temporary context to be verified
     * @returns true if the context is different from the last queried context, false otherwise
     */
    public hasTemporaryContextChanged(context: string): boolean {
        // Normalize the context (remove extra spaces)
        const normalizedContext = (context || "").trim();
        
        // If the context is empty, we don't consider it as a change
        // Always returns false to avoid unnecessary new queries
        if (!normalizedContext) {
            return false;
        }
        
        // If we get here, the context is non-empty and needs to be compared
        const normalizedLastContext = (this.lastQueriedTemporaryContext || "").trim();
        
        // Only returns true if it is different from the last (to make a new query)
        return normalizedContext !== normalizedLastContext;
    }

    /**
     * Updates the last queried temporary context
     * @param context The temporary context consulted
     */
    public updateLastQueriedTemporaryContext(context: string): void {
        this.lastQueriedTemporaryContext = context;
    }

    /**
     * Clears the stored temporary context
     */
    public clearTemporaryContext(): void {
        this.currentTemporaryContext = '';
        this.temporaryContextMemory = '';
        this.lastQueriedTemporaryContext = '';
    }

    /**
     * Checks if there is a defined temporary context
     */
    public hasTemporaryContext(): boolean {
        return this.currentTemporaryContext.trim().length > 0;
    }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionFormatter.ts
// Implementation of ITranscriptionFormatter

import { ITranscriptionFormatter } from "../../interfaces/transcription/ITranscriptionFormatter";
import { SpeakerSegment } from "../../interfaces/transcription/TranscriptionTypes";

export class TranscriptionFormatter implements ITranscriptionFormatter {
  /**
   * Formats mixed transcriptions with speaker labels
   */
  formatMixedTranscription(text: string, primaryUserSpeaker: string): SpeakerSegment[] {
    const results: SpeakerSegment[] = [];
    const speakerPattern = /\[([^\]]+)\]\s*(.*?)(?=\s*\[[^\]]+\]|$)/gs;
    let match;
    let lastSpeaker = "";
    
    while ((match = speakerPattern.exec(text)) !== null) {
      if (match[1] && match[2]) {
        const rawSpeaker = match[1].trim();
        const spokenText = match[2].trim();
        
        if (spokenText) {
          let speaker: string;
          
          // Simplified speaker attribution rules
          if (rawSpeaker === primaryUserSpeaker) {
            speaker = primaryUserSpeaker;
          } else if (rawSpeaker.toLowerCase().includes("speaker")) {
            speaker = "external";
          } else {
            speaker = "external";
          }
          
          // Only show speaker label when it changes
          const showSpeaker = (lastSpeaker !== speaker);
          lastSpeaker = speaker;
          
          results.push({
            speaker,
            text: `[${rawSpeaker}] ${spokenText}`,
            showSpeaker
          });
        }
      }
    }
    
    return results;
  }
  
  /**
   * Formats external speaker content to ensure correct labeling
   */
  formatExternalSpeakerContent(content: string): string {
    if (!content) return "";
    
    const lines = content.split('\n');
    const formattedLines = lines.map(line => {
      if (!line.trim()) return line;
      
      // Check if it already has a [Speaker X] prefix
      if (line.match(/^\[[^\]]+\]/)) {
        return line; // Already has prefix, keep as is
      }
      
      // Check if it contains "Speaker" without prefix
      if (line.toLowerCase().includes("speaker")) {
        // Extract speaker number if present
        const speakerMatch = line.match(/speaker\s*(\d+)/i);
        const speakerLabel = speakerMatch ? 
          `Speaker ${speakerMatch[1]}` : "External Participant";
          
        // Add prefix
        return `[${speakerLabel}] ${line}`;
      }
      
      return line;
    });
    
    return formattedLines.join('\n');
  }
  
  /**
   * Sanitizes memory content and fixes speaker attributions
   */
  sanitizeMemoryContent(content: string, isSpeakerContent: boolean = false): string {
    if (!content) return "";
    
    // If already speaker content or doesn't contain "Speaker", return as is
    if (isSpeakerContent || !content.toLowerCase().includes("speaker")) {
      return content;
    }
    
    // Process text lines to add speaker prefixes to lines mentioning "Speaker"
    const lines = content.split('\n');
    const processedLines = lines.map(line => {
      if (line.toLowerCase().includes("speaker")) {
        // Check if already has Speaker prefix
        if (line.match(/^\[[^\]]+\]/)) {
          return line; // Already has prefix, keep as is
        }
        
        // Extract speaker number if present
        const speakerMatch = line.match(/speaker\s*(\d+)/i);
        const speakerLabel = speakerMatch ? 
          `Speaker ${speakerMatch[1]}` : "External Participant";
          
        // Add prefix
        return `[${speakerLabel}] ${line}`;
      }
      return line;
    });
    
    return processedLines.join('\n');
  }
  
  /**
   * Combines speaker segments into a coherent conversation
   */
  buildConversationFromSegments(
    segments: SpeakerSegment[], 
    preserveSpeakerLabels: boolean = true
  ): string {
    const conversation: string[] = [];
    
    for (const segment of segments) {
      // Extract the original speaker label if available
      let text = segment.text;
      
      if (!preserveSpeakerLabels) {
        // Remove any existing speaker labels
        text = text.replace(/^\[[^\]]+\]\s*/, '');
      }
      
      conversation.push(text);
    }
    
    return conversation.join('\n');
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import {
  NeuralProcessingResult,
  NeuralSignalResponse,
} from "../../interfaces/neural/NeuralSignalTypes";
import { NeuralSignalExtractor } from "../../symbolic-cortex/activation/NeuralSignalExtractor";
import { LoggingUtils } from "../../utils/LoggingUtils";
import symbolicCognitionTimelineLogger from "../utils/SymbolicCognitionTimelineLoggerSingleton";
import {
  getOption,
  STORAGE_KEYS,
} from "./../../../../../services/StorageService";

// Services interfaces
import { IMemoryService } from "../../interfaces/memory/IMemoryService";
import { IOpenAIService } from "../../interfaces/openai/IOpenAIService";
import { ITranscriptionStorageService } from "../../interfaces/transcription/ITranscriptionStorageService";
import { ISpeakerIdentificationService } from "../../interfaces/utils/ISpeakerIdentificationService";
import { IUIUpdateService } from "../../interfaces/utils/IUIUpdateService";
import { INeuralIntegrationService } from "../../symbolic-cortex/integration/INeuralIntegrationService";

// Neural processors
import { cleanThinkTags } from "../../utils/ThinkTagCleaner";
import {
  NeuralConfigurationBuilder,
  NeuralMemoryRetriever,
  NeuralSignalEnricher,
  ProcessingResultsSaver,
  ProcessorMode,
  ResponseGenerator,
  SessionManager,
  TranscriptionExtractor,
} from "./processors";

/**
 * Response from transcription processing
 */
export interface TranscriptionProcessingResponse {
  response: string;
  neuralActivation: NeuralSignalResponse;
  processingResults: NeuralProcessingResult[];
}

/**
 * Neural transcription cognitive orchestrator
 * Coordinates specialized processors for transcription prompt processing
 * Supports both Ollama and HuggingFace backends for cognitive diversity
 */
export class TranscriptionPromptProcessor {
  private isProcessingPrompt: boolean = false;
  private currentLanguage: string;

  // Neural components
  private _neuralSignalExtractor: NeuralSignalExtractor;

  // Specialized processors (SOLID architecture)
  private transcriptionExtractor!: TranscriptionExtractor;
  private configurationBuilder!: NeuralConfigurationBuilder;
  private sessionManager!: SessionManager;
  private signalEnricher!: NeuralSignalEnricher;
  private memoryRetriever!: NeuralMemoryRetriever;
  private responseGenerator!: ResponseGenerator;
  private resultsSaver!: ProcessingResultsSaver;

  constructor(
    private storageService: ITranscriptionStorageService,
    private memoryService: IMemoryService,
    private llmService: IOpenAIService, // Pode ser Ollama ou HuggingFace, já abstraído
    private uiService: IUIUpdateService,
    private speakerService: ISpeakerIdentificationService,
    private neuralIntegrationService: INeuralIntegrationService
  ) {
    this.currentLanguage = getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "pt-BR";
    this._neuralSignalExtractor = new NeuralSignalExtractor(this.llmService);

    // Inicializa os processors com a instância já abstraída (Ollama ou HuggingFace)
    this._initializeProcessors();
  }

  /**
   * Initialize all specialized neural processors following SOLID principles
   */
  private _initializeProcessors(): void {
    this.transcriptionExtractor = new TranscriptionExtractor(
      this.storageService
    );
    this.sessionManager = new SessionManager();

    this.configurationBuilder = new NeuralConfigurationBuilder(
      this.storageService,
      this.memoryService,
      this.speakerService,
      this.sessionManager
    );

    this.signalEnricher = new NeuralSignalEnricher(this.llmService);

    this.memoryRetriever = new NeuralMemoryRetriever(this.memoryService);

    this.responseGenerator = new ResponseGenerator(
      this.memoryService,
      this.llmService
    );

    this.resultsSaver = new ProcessingResultsSaver(
      this.memoryService,
      this.storageService,
      this.speakerService,
      this.sessionManager
    );
  }

  /**
   * Process transcription with LLM backend (Ollama in Advanced mode, OpenAI compatible)
   * Full neural processing with symbolic cognition
   */
  async processWithOpenAI(temporaryContext?: string): Promise<void> {
    await this._processTranscriptionPrompt("openai", temporaryContext);
  }

  /**
   * Process transcription with HuggingFace backend
   * Local neural processing with enhanced privacy
   */
  async processWithHuggingFace(temporaryContext?: string): Promise<void> {
    await this._processTranscriptionPrompt("huggingface", temporaryContext);
  }

  /**
   * Process direct message from chat (no transcription)
   * @param message The message from chat input
   * @param temporaryContext Optional additional context
   */
  async processDirectMessage(
    message: string,
    temporaryContext?: string
  ): Promise<void> {
    const mode = this.llmService.constructor.name.includes("HuggingFace")
      ? "huggingface"
      : "openai";
    await this._processDirectMessage(mode, message, temporaryContext);
  }

  /**
   * Process a direct message from chat interface
   */
  private async _processDirectMessage(
    mode: ProcessorMode,
    message: string,
    temporaryContext?: string
  ): Promise<void> {
    // Prevent concurrent processing
    if (this.isProcessingPrompt) {
      LoggingUtils.logWarning(
        "Blocking prompt request: Already processing another prompt"
      );
      return;
    }

    this.uiService.updateUI({ aiResponse: "Processing..." });

    try {
      this.isProcessingPrompt = true;

      // Garante que o backend já abstraído está pronto
      if (!(await this.llmService.ensureOpenAIClient())) return;

      LoggingUtils.logInfo("Processing direct message from chat");

      // Log cognitive activities
      symbolicCognitionTimelineLogger.logRawPrompt(message);
      if (temporaryContext?.trim()) {
        LoggingUtils.logInfo(`Using additional context: "${temporaryContext}"`);
        symbolicCognitionTimelineLogger.logTemporaryContext(temporaryContext);
      }

      // Notify processing start
      this.uiService.notifyPromptProcessingStarted(temporaryContext);

      LoggingUtils.logInfo(
        `Processing message: "${message.substring(0, 50)}..."${
          temporaryContext ? " with additional context" : ""
        }`
      );

      // Process using orchestrated pipeline
      const result = await this._executeProcessingPipeline(
        mode,
        message,
        temporaryContext
      );

      // Update UI and complete processing
      this.uiService.updateUI({ aiResponse: result.response });
      this.uiService.notifyPromptComplete(result.response);
    } catch (error: Error | unknown) {
      const errorMessage =
        error instanceof Error ? error.message : "Unknown error";
      LoggingUtils.logError("Error processing message", error);
      this.uiService.updateUI({ aiResponse: `Error: ${errorMessage}` });
      this.uiService.notifyPromptError(errorMessage);
    } finally {
      this.isProcessingPrompt = false;
      LoggingUtils.logInfo("Message processing completed, releasing lock");
    }
  }

  /**
   * Main transcription processing orchestration - adaptable to different backends
   */
  private async _processTranscriptionPrompt(
    mode: ProcessorMode,
    temporaryContext?: string
  ): Promise<void> {
    // Prevent concurrent processing
    if (this.isProcessingPrompt) {
      LoggingUtils.logWarning(
        "Blocking prompt request: Already processing another prompt"
      );
      return;
    }

    this.uiService.updateUI({ aiResponse: "Processing..." });

    try {
      this.isProcessingPrompt = true;

      // Garante que o backend já abstraído está pronto (Ollama ou HuggingFace)
      if (!(await this.llmService.ensureOpenAIClient())) return;

      // Validate and extract transcriptions
      const hasTranscriptions = this.storageService.hasValidTranscriptions();

      if (!hasTranscriptions) {
        LoggingUtils.logWarning("No transcription detected");

        // Verify if there is text in lastTranscription
        const lastTranscription = this.storageService.getLastTranscription();
        if (lastTranscription) {
          LoggingUtils.logInfo(
            `Using last known transcription: "${lastTranscription}"`
          );
        } else {
          // Notify error if there is no transcription
          this.uiService.notifyPromptError(
            "No transcription detected for processing"
          );
          LoggingUtils.logInfo(`No transcription detected for processing`);
          return;
        }
      }

      // Notify processing start
      this.uiService.notifyPromptProcessingStarted(temporaryContext);

      // Extract new transcription lines AND mark as sent atomically
      const extractedLines = this.transcriptionExtractor.extractAndMarkAsSent();
      let promptText: string | null = extractedLines;

      if (!promptText || promptText.trim().length === 0) {
        LoggingUtils.logInfo("No new transcription to send.");
        return;
      }

      // Log cognitive activities
      symbolicCognitionTimelineLogger.logRawPrompt(promptText);

      // Log temporary context if provided
      if (temporaryContext?.trim()) {
        LoggingUtils.logInfo(`Using additional context: "${temporaryContext}"`);
        symbolicCognitionTimelineLogger.logTemporaryContext(temporaryContext);
      }

      LoggingUtils.logInfo(
        `Processing transcription: "${promptText.substring(0, 50)}..."${
          temporaryContext ? " with additional context" : ""
        }`
      );

      // Process using orchestrated pipeline
      const result = await this._executeProcessingPipeline(
        mode,
        promptText,
        temporaryContext
      );

      // Update UI and complete processing
      this.uiService.updateUI({ aiResponse: result.response });
      this.uiService.notifyPromptComplete(result.response);

      // Note: Transcriptions were already marked as sent during extraction
      // This prevents race conditions and duplicate sends

      // Update UI to show only new transcriptions (empty initially)
      if (this.storageService.updateUIWithNewTranscriptions) {
        this.storageService.updateUIWithNewTranscriptions();
      }
    } catch (error: Error | unknown) {
      const errorMessage =
        error instanceof Error ? error.message : "Unknown error";
      LoggingUtils.logError("Error processing prompt", error);
      this.uiService.updateUI({ aiResponse: `Error: ${errorMessage}` });
      this.uiService.notifyPromptError(errorMessage);
    } finally {
      this.isProcessingPrompt = false;
      LoggingUtils.logInfo("Prompt processing completed, releasing lock");
    }
  }

  /**
   * Execute the full neural processing pipeline using specialized processors
   */
  private async _executeProcessingPipeline(
    mode: ProcessorMode,
    transcriptionToSend: string,
    temporaryContext?: string
  ): Promise<TranscriptionProcessingResponse> {
    // PHASE 1: Neural Signal Extraction
    LoggingUtils.logInfo(
      "🧠 Starting neural system: Phase 1 - Sensory analysis..."
    );

    const extractionConfig =
      await this.configurationBuilder.buildExtractionConfig(
        transcriptionToSend,
        temporaryContext,
        this.currentLanguage
      );
    const neuralActivation =
      await this._neuralSignalExtractor.extractNeuralSignals(extractionConfig);

    // PHASE 2: Query Enrichment & Memory Retrieval
    const enrichedSignals = await this.signalEnricher.enrichSignals(
      neuralActivation.signals,
      this.currentLanguage
    );
    const processingResults = await this.memoryRetriever.processSignals(
      enrichedSignals
    );

    // PHASE 3: Neural Integration
    LoggingUtils.logInfo(
      "💥 Third phase - Integrating neural processing into final prompt..."
    );
    const integratedPrompt = await this.neuralIntegrationService.integrate(
      processingResults,
      transcriptionToSend,
      this.currentLanguage
    );

    // Symbolic context synthesis log (clean think tags before logging)
    const cleanIntegratedPrompt = cleanThinkTags(integratedPrompt);
    symbolicCognitionTimelineLogger.logSymbolicContextSynthesized({
      summary: cleanIntegratedPrompt, // summary is required in SymbolicContext
      modules: processingResults.map((r) => ({
        core: r.core,
        intensity: r.intensity,
      })),
    });

    // PHASE 4: Generate Response
    const fullResponse = await this.responseGenerator.generateResponse(
      cleanIntegratedPrompt,
      temporaryContext
    );

    const response = cleanThinkTags(fullResponse);

    // PHASE 5: Save and Log Results
    await this.resultsSaver.saveResults(
      transcriptionToSend,
      response,
      neuralActivation,
      processingResults
    );

    return {
      response,
      neuralActivation,
      processingResults,
    };
  }

  /**
   * Check if currently processing a prompt
   */
  public isProcessingPromptRequest(): boolean {
    return this.isProcessingPrompt;
  }

  /**
   * Update current language for processing
   */
  public setLanguage(language: string): void {
    this.currentLanguage = language;
  }

  /**
   * Get current processing language
   */
  public getCurrentLanguage(): string {
    return this.currentLanguage;
  }

  /**
   * Reset transcription processing state across all processors
   */
  public reset(): void {
    this.isProcessingPrompt = false;
    this.transcriptionExtractor.reset();
    this.sessionManager.resetSession();
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionSnapshotTracker.ts
// Tracks previously sent transcription lines to prevent duplication

/**
 * A class responsible for tracking transcription lines that have already
 * been sent to OpenAI to prevent duplication in future requests.
 */
export class TranscriptionSnapshotTracker {
  private sentLines: Set<string> = new Set<string>();
  
  /**
   * Filters a transcription text to only include lines that haven't been sent before
   * @param transcription The full transcription text to filter
   * @returns A filtered transcription containing only new content
   */
  public filterTranscription(transcription: string): string {
    if (!transcription?.trim()) return '';
    
    // Split the transcription into lines and normalize each line
    const lines = transcription.split('\n')
      .map(line => this.normalizeLine(line))
      .filter(line => line.length > 0);
    
    // Filter out lines that have already been sent
    const newLines = lines.filter(line => !this.sentLines.has(line));
    
    // If no new lines, return empty string
    if (newLines.length === 0) return '';
    
    // Return the filtered transcription
    return newLines.join('\n');
  }
  
  /**
   * Updates the snapshot with lines that were just sent to OpenAI
   * @param transcription The transcription that was actually sent
   */
  public updateSnapshot(transcription: string): void {
    if (!transcription?.trim()) return;
    
    // Split the transcription into lines and normalize each line
    const lines = transcription.split('\n')
      .map(line => this.normalizeLine(line))
      .filter(line => line.length > 0);
    
    for (const line of lines) {
      this.sentLines.add(line);
    }
  }
  
  /**
   * Checks if a transcription contains only content that has already been sent
   * @param transcription The transcription to check
   * @returns True if all content has already been sent
   */
  public isAllContentSent(transcription: string): boolean {
    if (!transcription?.trim()) return true;
    
    const lines = transcription.split('\n')
      .map(line => this.normalizeLine(line))
      .filter(line => line.length > 0);
    
    // Check if all lines are already in the sent lines set
    return lines.every(line => this.sentLines.has(line));
  }
  
  /**
   * Resets the snapshot tracker, clearing all tracked lines
   */
  public reset(): void {
    this.sentLines.clear();
  }
  
  /**
   * Normalizes a line of text by trimming and collapsing whitespace
   * @param line The line of text to normalize
   * @returns Normalized line
   */
  private normalizeLine(line: string): string {
    if (!line) return '';
    
    // Trim the line and collapse multiple whitespaces within the line
    return line.trim().replace(/\s+/g, ' ');
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionStorageService.ts
// Refactored using SOLID, DRY, KISS, and YAGNI principles
// Acts as a Facade orchestrating smaller, focused components

import { ITranscriptionStorageService } from "../../interfaces/transcription/ITranscriptionStorageService";
import {
  SpeakerTranscription,
  SpeakerTranscriptionLog,
  UIUpdater,
} from "../../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../../interfaces/utils/ISpeakerIdentificationService";
import { LoggingUtils } from "../../utils/LoggingUtils";
import { DeepgramTranscriptionService } from "../DeepgramTranscriptionService";

// Import refactored components
import { QuestionDetector } from "./detectors/QuestionDetector";
import { TranscriptionLogger } from "./loggers/TranscriptionLogger";
import { TranscriptionUIManager } from "./managers/TranscriptionUIManager";
import { TranscriptionProcessor } from "./processors/TranscriptionProcessor";
import { TranscriptionStore } from "./stores/TranscriptionStore";

/**
 * TranscriptionStorageService - Refactored using SOLID principles
 * Acts as a Facade Pattern orchestrating smaller, focused components
 * Each component has a single responsibility, making the code more maintainable
 */
export class TranscriptionStorageService
  implements ITranscriptionStorageService
{
  // Components following Single Responsibility Principle
  private store: TranscriptionStore;
  private questionDetector: QuestionDetector;
  private processor: TranscriptionProcessor;
  private logger: TranscriptionLogger;
  private uiManager: TranscriptionUIManager;

  // Service dependencies
  private speakerService: ISpeakerIdentificationService;
  private transcriptionService: DeepgramTranscriptionService | null = null;

  constructor(
    speakerService: ISpeakerIdentificationService,
    setTexts: UIUpdater
  ) {
    // Initialize all components - Dependency Injection
    this.speakerService = speakerService;
    this.store = new TranscriptionStore();
    this.questionDetector = new QuestionDetector();
    this.processor = new TranscriptionProcessor(speakerService);
    this.logger = new TranscriptionLogger(speakerService);
    this.uiManager = new TranscriptionUIManager(setTexts);
  }

  /**
   * Sets the transcription service for auto-prompt functionality
   */
  setTranscriptionService(service: DeepgramTranscriptionService): void {
    this.transcriptionService = service;
  }

  /**
   * Adds a new transcription to the storage
   * Delegates to appropriate components following Single Responsibility
   */
  addTranscription(text: string, speaker?: string): void {
    if (!text || !text.trim()) return;

    const cleanText = text.trim();

    // Handle question cycle interruption
    this.questionDetector.handleNewTranscriptionDuringCycle(cleanText);

    // Check for duplicates
    if (
      this.processor.isDuplicate(
        cleanText,
        this.store.getSpeakerTranscriptions()
      )
    ) {
      return;
    }

    // Process segments if text has speaker markers
    if (this.processor.hasSpeakerMarkers(cleanText)) {
      const segments = this.processor.processSegments(cleanText);

      for (const segment of segments) {
        if (
          !this.processor.isDuplicate(
            segment.text,
            this.store.getSpeakerTranscriptions()
          )
        ) {
          this.addSingleSpeakerTranscription(segment.text, segment.speaker);
          this.store.setCurrentSpeaker(segment.speaker);
        }
      }
      return;
    }

    // Process speaker without markers
    const { speaker: processedSpeaker, cleanText: processedText } =
      this.processor.processSpeaker(
        cleanText,
        speaker,
        this.store.getCurrentSpeaker()
      );

    this.addSingleSpeakerTranscription(processedText, processedSpeaker);
    this.store.setCurrentSpeaker(processedSpeaker);
  }

  /**
   * Adds a single speaker transcription
   * Simplified by delegating to components
   */
  addSingleSpeakerTranscription(text: string, speaker: string): void {
    if (!text || !text.trim()) return;

    LoggingUtils.logInfo(`Adding transcription for "${speaker}": "${text}"`);

    // Store the transcription
    this.store.addTranscription(text, speaker, new Date().toISOString());

    // Always update UI to show only new (unsent) transcriptions
    // This prevents accumulation of sent transcriptions in the display
    this.updateUIWithNewTranscriptions();

    // Check if we should detect questions
    if (
      this.questionDetector.isAutoDetectionEnabled() &&
      this.transcriptionService &&
      this.questionDetector.isQuestion(text) &&
      speaker === this.speakerService.getPrimaryUserSpeaker() &&
      this.transcriptionService.isOnlyUserSpeaking() &&
      !this.questionDetector.isDuplicateQuestion(text) &&
      !this.questionDetector.isInQuestionCycle()
    ) {
      this.questionDetector.startQuestionCycle(text);
    }
  }

  /**
   * Flush all accumulated transcriptions to the UI
   * Delegates to UI manager
   */
  public flushTranscriptionsToUI(): void {
    const transcriptions = this.store.getUITranscriptionList();
    LoggingUtils.logInfo(
      `Flushing ${transcriptions.length} transcriptions to UI`
    );
    this.uiManager.flushToUI(transcriptions);
  }

  /**
   * Update transcription UI directly
   * Delegates to UI manager
   */
  public updateTranscriptionUI(transcription: string): void {
    console.log(
      `🖥️ [TranscriptionStorageService] Updating UI with: "${transcription}"`
    );
    // Always show only new transcriptions, not the full history
    // This prevents accumulation of sent transcriptions in the UI
    this.updateUIWithNewTranscriptions();
  }

  /**
   * Enable or disable automatic question detection
   * Delegates to question detector
   */
  setAutoQuestionDetection(enabled: boolean): void {
    this.questionDetector.setAutoDetection(enabled);
  }

  /**
   * Cancel pending question timer
   * Delegates to question detector
   */
  cancelPendingQuestionTimer(): void {
    this.questionDetector.cancelPendingTimer();
  }

  /**
   * Get UI transcription text
   * Delegates to store
   */
  getUITranscriptionText(): string {
    return this.store.getUITranscriptionText();
  }

  /**
   * Alias for getUITranscriptionText
   */
  getTranscriptionPromptText(): string {
    return this.getUITranscriptionText();
  }

  /**
   * Get transcription list
   * Delegates to store
   */
  getTranscriptionList(): string[] {
    return this.store.getTranscriptionList();
  }

  /**
   * Get speaker transcriptions
   * Delegates to store
   */
  getSpeakerTranscriptions(): SpeakerTranscription[] {
    return this.store.getSpeakerTranscriptions();
  }

  /**
   * Get transcription logs
   * Delegates to logger
   */
  getTranscriptionLogs(): SpeakerTranscriptionLog[] {
    return this.logger.generateLogs(this.store.getSpeakerTranscriptions());
  }

  /**
   * Clear transcription data
   * Delegates to components
   */
  clearTranscriptionData(): void {
    this.store.clear();
    this.uiManager.clear();
    this.updateUI({ transcription: "" });
  }

  /**
   * Check if has valid transcriptions
   * Delegates to store
   */
  hasValidTranscriptions(): boolean {
    return this.store.hasValidTranscriptions();
  }

  /**
   * Get last transcription
   * Delegates to store
   */
  getLastTranscription(): string {
    return this.store.getLastTranscription();
  }

  /**
   * Get last message from user
   * Delegates to processor
   */
  getLastMessageFromUser(): SpeakerTranscription | null {
    return this.processor.getLastUserMessage(
      this.store.getSpeakerTranscriptions(),
      this.speakerService.getPrimaryUserSpeaker()
    );
  }

  /**
   * Get last messages from external speakers
   * Delegates to processor
   */
  getLastMessagesFromExternalSpeakers(): Map<string, SpeakerTranscription> {
    return this.processor.getLastExternalMessages(
      this.store.getSpeakerTranscriptions(),
      this.store.getDetectedSpeakers()
    );
  }

  /**
   * Get detected speakers
   * Delegates to store
   */
  getDetectedSpeakers(): Set<string> {
    return this.store.getDetectedSpeakers();
  }

  /**
   * Set current speaker
   * Delegates to store
   */
  setCurrentSpeaker(speaker: string): void {
    this.store.setCurrentSpeaker(speaker);
  }

  /**
   * Get current speaker
   * Delegates to store
   */
  getCurrentSpeaker(): string {
    return this.store.getCurrentSpeaker();
  }

  /**
   * Private method to update UI with other properties
   * Uses UI manager for consistency
   */
  private updateUI(update: Record<string, unknown>): void {
    this.uiManager.updateOther(update);
  }

  /**
   * Get only new transcriptions that haven't been sent yet
   */
  getNewTranscriptions(): SpeakerTranscription[] {
    return this.store.getNewTranscriptions();
  }

  /**
   * Mark current transcriptions as sent
   */
  markTranscriptionsAsSent(): void {
    this.store.markTranscriptionsAsSent();
  }

  /**
   * Check if there are new transcriptions to send
   */
  hasNewTranscriptions(): boolean {
    return this.store.hasNewTranscriptions();
  }

  /**
   * Update UI to show only new transcriptions
   */
  updateUIWithNewTranscriptions(): void {
    const newTranscriptionsText = this.uiManager.getNewTranscriptionsText(
      this.store
    );
    // Use setTranscriptionDirectly to avoid adding to history
    this.uiManager.setTranscriptionDirectly(newTranscriptionsText);
    console.log(
      `🖥️ Updated UI with ${
        this.store.getNewTranscriptions().length
      } new transcriptions`
    );
  }

  /**
   * Extract new transcriptions and mark them as sent atomically
   * This prevents race conditions where transcriptions could be added
   * between extraction and marking
   */
  extractAndMarkAsSent(): SpeakerTranscription[] {
    // Get new transcriptions
    const newTranscriptions = this.store.getNewTranscriptions();

    if (newTranscriptions.length > 0) {
      // Mark them as sent immediately
      this.store.markTranscriptionsAsSent(newTranscriptions);
      console.log(
        `🚀 Extracted and marked ${newTranscriptions.length} transcriptions as sent`
      );

      // Clear UI history to prevent showing sent transcriptions
      this.uiManager.clearSentTranscriptions();

      // Update UI to show empty (no new transcriptions)
      this.updateUIWithNewTranscriptions();
    }

    return newTranscriptions;
  }

  /**
   * Get all transcriptions with their sent status
   * Used by UI to show which transcriptions have been sent
   */
  getAllTranscriptionsWithStatus(): SpeakerTranscription[] {
    return this.store.getAllTranscriptionsWithStatus();
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Interface for cognitive log exporters
import { CognitionEvent } from '../../types/CognitionEvent';

export interface CognitionLogExporter {
  label: string;
  export(log: CognitionEvent[], filename?: string): void;
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Factory for cognitive log exporters
import { CognitionLogExporter } from './CognitionLogExporter';
import { CognitionLogJsonExporter } from './CognitionLogJsonExporter';
import { CognitionLogTxtExporter } from './CognitionLogTxtExporter';

/**
 * Factory responsible for creating and providing cognitive log exporters
 * Following the Factory Method pattern of SOLID
 */
export class CognitionLogExporterFactory {
  private static instance: CognitionLogExporterFactory;
  private exporters: CognitionLogExporter[] = [];

  private constructor() {
    // Initializes default exporters
    this.registerDefaultExporters();
  }

  /**
   * Gets the unique instance of the factory (Singleton)
   */
  public static getInstance(): CognitionLogExporterFactory {
    if (!CognitionLogExporterFactory.instance) {
      CognitionLogExporterFactory.instance = new CognitionLogExporterFactory();
    }
    return CognitionLogExporterFactory.instance;
  }

  /**
   * Registers default exporters of the system
   */
  private registerDefaultExporters(): void {
    this.exporters = [
      new CognitionLogJsonExporter(),
      new CognitionLogTxtExporter()
    ];
  }

  /**
   * Adds a new exporter to the list
   * @param exporter Exporter to be added
   */
  public registerExporter(exporter: CognitionLogExporter): void {
    this.exporters.push(exporter);
  }

  /**
   * Removes an exporter based on the label
   * @param label Label of the exporter to be removed
   */
  public unregisterExporter(label: string): void {
    this.exporters = this.exporters.filter(e => e.label !== label);
  }

  /**
   * Gets all registered exporters
   */
  public getExporters(): CognitionLogExporter[] {
    return [...this.exporters];
  }
}

// Export a singleton instance of the factory for global use
export default CognitionLogExporterFactory.getInstance();
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// JSON exporter for cognitive log
import { CognitionEvent } from '../../types/CognitionEvent';
import { CognitionLogExporter } from './CognitionLogExporter';

export class CognitionLogJsonExporter implements CognitionLogExporter {
  label = 'Export cognitive log (JSON)';
  export(log: CognitionEvent[], filename = 'symbolic_cognition_session.json') {
    const blob = new Blob([JSON.stringify(log, null, 2)], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    a.click();
    setTimeout(() => URL.revokeObjectURL(url), 1000);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TXT exporter for cognitive log
import { CognitionEvent } from '../../types/CognitionEvent';
import { CognitionLogExporter } from './CognitionLogExporter';

export class CognitionLogTxtExporter implements CognitionLogExporter {
  label = 'Export cognitive log (TXT)';
  export(log: CognitionEvent[], filename = 'symbolic_cognition_session.txt') {
    const txt = log.map(e => JSON.stringify(e, null, 2)).join('\n---\n');
    const blob = new Blob([txt], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    a.click();
    setTimeout(() => URL.revokeObjectURL(url), 1000);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Common types and interfaces used across Deepgram service modules
 */
import { ListenLiveClient } from "@deepgram/sdk";
import { ConnectionState } from "../../interfaces/deepgram/IDeepgramService";

/**
 * Speaker segment buffer structure
 */
export interface SpeakerBuffer {
  lastSpeaker: string;
  currentSegment: string[];
  formattedSegment: string;
  lastFlushedText: string;
}

/**
 * Transcription callback data structure
 */
export interface TranscriptionData {
  text: string;
  isFinal: boolean;
  channel: number;
  speaker: string;
}

/**
 * Connection status information
 */
export interface ConnectionStatus {
  state: ConnectionState;
  stateRef: ConnectionState;
  hasConnectionObject: boolean;
  readyState: number | null;
  active: boolean;
}

/**
 * Audio processing result
 */
export interface AudioProcessingResult {
  buffer: ArrayBuffer | null;
  valid: boolean;
}

/**
 * Connection state update callback
 */
export type ConnectionStateCallback = (state: ConnectionState) => void;

/**
 * Connection object update callback
 */
export type ConnectionCallback = (connection: ListenLiveClient | null) => void;

/**
 * Transcription event callback
 */
export type TranscriptionEventCallback = (event: string, data: any) => void;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/**
 * Logger utility for Deepgram services
 * Provides consistent logging throughout the Deepgram service modules
 */
export class Logger {
  private context: string;
  
  constructor(context: string) {
    this.context = context;
  }
  
  /**
   * Log informational message
   */
  public info(message: string): void {
    console.log(`✅ [${this.context}] ${message}`);
  }
  
  /**
   * Log debug message with optional data
   */
  public debug(message: string, data?: any): void {
    if (data) {
      console.log(`🔍 [${this.context}] ${message}:`, data);
    } else {
      console.log(`🔍 [${this.context}] ${message}`);
    }
  }
  
  /**
   * Log warning message with optional error
   */
  public warning(message: string, error?: any): void {
    if (error) {
      console.warn(`⚠️ [${this.context}] ${message}:`, error);
    } else {
      console.warn(`⚠️ [${this.context}] ${message}`);
    }
  }
  
  /**
   * Log error message with optional error object
   */
  public error(message: string, error?: any): void {
    if (error) {
      console.error(`❌ [${this.context}] ${message}:`, error);
    } else {
      console.error(`❌ [${this.context}] ${message}`);
    }
  }
  
  /**
   * Handle error with context
   */
  public handleError(context: string, error: any): void {
    console.error(`❌ [${this.context}] ${context}:`, error);
  }
}

export default Logger;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// SpeakerIdentificationService.ts
// Service responsible for speaker identification and normalization

import { getPrimaryUser } from '../../../../../config/UserConfig';
import { EXTERNAL_SPEAKER_LABEL, SpeakerSegment, SpeakerTranscription } from "../../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../../interfaces/utils/ISpeakerIdentificationService";
import { LoggingUtils } from "../../utils/LoggingUtils";

export class SpeakerIdentificationService implements ISpeakerIdentificationService {
  private primaryUserSpeaker: string = getPrimaryUser();
  private currentSpeaker: string = "";

  constructor(primaryUserSpeaker?: string) {
    if (primaryUserSpeaker) {
      this.primaryUserSpeaker = primaryUserSpeaker;
    }
  }

  /**
   * Sets the name of the primary speaker (user)
   */
  setPrimaryUserSpeaker(name: string): void {
    if (name && name.trim()) {
      this.primaryUserSpeaker = name.trim();
      LoggingUtils.logInfo(`Primary speaker set as: "${this.primaryUserSpeaker}"`);
    }
  }
  
  /**
   * Gets the primary speaker (user)
   */
  getPrimaryUserSpeaker(): string {
    return this.primaryUserSpeaker;
  }

  /**
   * Normalizes the speaker identification for comparison
   * Converts variations like "user", "usuario", etc. to the primaryUserSpeaker
   */
  normalizeAndIdentifySpeaker(speaker: string): string {
    // If the input text is empty, return the primary speaker
    if (!speaker || !speaker.trim()) {
      return this.primaryUserSpeaker;
    }
    
    // Remove brackets if they exist
    const cleanSpeaker = speaker.replace(/^\[|\]$/g, '').trim();
    
    // Simple rule:
    // 1. If it is exactly "Guilherme", return primaryUserSpeaker
    // 2. If it is a pattern "Speaker N", return "external"
    // 3. For other cases, assume primaryUserSpeaker
    
    if (cleanSpeaker === this.primaryUserSpeaker) {
      return this.primaryUserSpeaker;
    }
    
    if (/^speaker\s*\d+$/i.test(cleanSpeaker)) {
      return "external";
    }
    
    // If it is a variation of "user", return primaryUserSpeaker
    if (/^(user|usuario|usuário)$/i.test(cleanSpeaker)) {
      return this.primaryUserSpeaker;
    }
    
    // By default, assume it is the primary user
    return this.primaryUserSpeaker;
  }

  /**
   * Extracts speech segments from a transcription that mixes speakers
   * Example: "[Guilherme] Olá [Speaker 0] Como vai? [Guilherme] Estou bem"
   * → [{ speaker: "Guilherme", text: "Olá" }, { speaker: "Speaker 0", text: "Como vai?" }, ...]
   */
  splitMixedTranscription(text: string): SpeakerSegment[] {
    const segments: SpeakerSegment[] = [];
    
    // Sanitize the input text
    const cleanText = text?.trim() || "";
    if (!cleanText) {
      return segments;
    }
    
    // Regex simplified to capture patterns like [Speaker] text until the next [Speaker]
    // Captures group 1: speaker name between brackets
    // Captures group 2: text spoken until the next speaker or end of the string
    const speakerPattern = /\[([^\]]+)\]\s*(.*?)(?=\s*\[[^\]]+\]|$)/gs;
    
    let match;
    let matchFound = false;
    let lastSpeakerKey = "";
    
    while ((match = speakerPattern.exec(cleanText)) !== null) {
      matchFound = true;
      const rawSpeaker = match[1].trim();
      const spokenText = match[2].trim();
      
      if (spokenText) {
        // Simple rule: If the speaker is "Guilherme", assign to the user
        // If the speaker is "Speaker N", assign as external
        let speakerToUse;
        
        if (rawSpeaker === this.primaryUserSpeaker) {
          speakerToUse = this.primaryUserSpeaker;
        } else if (/^speaker\s*\d+$/i.test(rawSpeaker)) {
          speakerToUse = "external";
        } else {
          // For other cases, use normalization
          speakerToUse = this.normalizeAndIdentifySpeaker(rawSpeaker);
        }
        
        // Show speaker if different from the previous one
        const showSpeaker = (lastSpeakerKey !== speakerToUse);
        lastSpeakerKey = speakerToUse;
        
        LoggingUtils.logInfo(`Segmento [${rawSpeaker}] → ${speakerToUse}: "${spokenText.substring(0, 30)}..."`);
        
        segments.push({
          speaker: speakerToUse,
          text: spokenText,
          showSpeaker: showSpeaker
        });
      }
    }
    
    // If no speaker pattern is found, consider the complete text
    if (!matchFound && cleanText) {
      // Use the current speaker of the service, if available
      const speakerToUse = this.currentSpeaker || this.primaryUserSpeaker;
      
      segments.push({
        speaker: speakerToUse,
        text: cleanText,
        showSpeaker: true // Always show when there is no explicit pattern
      });
      
      LoggingUtils.logInfo(`Text without speaker labels, assigned to: "${speakerToUse}"`);
    }
    
    return segments;
  }

  /**
   * Filters transcriptions by specific speaker
   */
  filterTranscriptionsBySpeaker(speaker: string, transcriptions: SpeakerTranscription[]): SpeakerTranscription[] {
    return transcriptions.filter(
      st => st.speaker === speaker
    );
  }

  /**
   * Filters transcriptions by the primary user,
   * extracting only segments where the user is speaking
   */
  filterTranscriptionsByUser(transcriptions: SpeakerTranscription[]): SpeakerTranscription[] {
    const userTranscriptions = transcriptions.filter(st => {
      // Basic verification: speaker must be the primary user
      if (st.speaker !== this.primaryUserSpeaker) {
        LoggingUtils.logInfo(`[FiltroUser] Rejeitado: speaker diferente do usuário principal (${st.speaker}) - texto: ${st.text.substring(0, 60)}`);
        return false;
      }
      // If the text contains speaker delimiters [...], ensure no external segments are present
      if (st.text.includes('[') && st.text.includes(']')) {
        const segments = this.splitMixedTranscription(st.text);
        if (segments.some(segment => segment.speaker !== this.primaryUserSpeaker)) {
          LoggingUtils.logInfo(`[FiltroUser] Rejeitado: transcrição mista com segmento externo detectado - texto: ${st.text.substring(0, 60)}`);
          return false;
        }
      }
      // If the text starts with an external speaker pattern, it is not from the user
      if (/^speaker\s*\d+\s*:/i.test(st.text)) {
        LoggingUtils.logInfo(`[FiltroUser] Rejeitado: começa como falante externo - texto: ${st.text.substring(0, 60)}`);
        return false;
      }
      // If the text explicitly mentions '[Speaker' at the beginning, log for diagnosis
      if (/^\[Speaker\s*\d+\]/i.test(st.text)) {
        LoggingUtils.logInfo(`[FiltroUser] Rejeitado: possível segmento externo no início - texto: ${st.text.substring(0, 60)}`);
        return false;
      }
      return true;
    });
    
    LoggingUtils.logInfo(`Filtrado ${userTranscriptions.length} transcrições do usuário principal`);
    return userTranscriptions;
  }

  /**
   * Verifies if only the primary user is speaking
   */
  isOnlyUserSpeaking(transcriptions: SpeakerTranscription[]): boolean {
    if (transcriptions.length === 0) {
      return true; // No transcriptions yet, assume only user
    }
    
    // For each transcription, verify if it contains only the user speaking
    for (const transcription of transcriptions) {
      // If the transcription contains speaker markers [Speaker]
      if (transcription.text.includes('[') && transcription.text.includes(']')) {
        const segments = this.splitMixedTranscription(transcription.text);
        
        // If any segment is not from the primary user, it is not just the user speaking
        const hasNonUserSegment = segments.some(segment => 
          this.normalizeAndIdentifySpeaker(segment.speaker) !== this.primaryUserSpeaker
        );
        
        if (hasNonUserSegment) {
          return false;
        }
      } else {
        // If no markers, verify by the speaker field
        if (this.normalizeAndIdentifySpeaker(transcription.speaker) !== this.primaryUserSpeaker) {
          return false;
        }
      }
    }
    
    // If it got here, all transcriptions are from the user
    return true;
  }

  /**
   * Prepares the transcription text for sending, combining all inputs
   */
  prepareTranscriptionText(
    transcriptionList: string[],
    speakerTranscriptions: SpeakerTranscription[],
    lastTranscription: string
  ): string {
    // If we have transcriptions by speaker, use primarily these
    if (speakerTranscriptions.length > 0) {
      // Use a Set to avoid duplicate text
      const processedTexts = new Set<string>();
      
      // First, sort by timestamp to ensure chronological order
      const sortedTranscriptions = [...speakerTranscriptions]
        .sort((a, b) => new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime());
      
      // Process each transcription, maintaining all speakers in sequence
      const conversationPieces: string[] = [];
      let lastSpeakerKey = "";
      
      for (const st of sortedTranscriptions) {
        // Avoid duplicate text
        if (processedTexts.has(st.text)) continue;
        processedTexts.add(st.text);
        
        // For transcriptions already with [Speaker] format
        if (st.text.includes('[') && st.text.includes(']')) {
          // Divide into segments to preserve multiple speakers within the same sentence
          const speakerPattern = /\[([^\]]+)\]\s*(.*?)(?=\s*\[[^\]]+\]|$)/gs;
          let match;
          let segmentFound = false;
          
          while ((match = speakerPattern.exec(st.text)) !== null) {
            segmentFound = true;
            if (match[1] && match[2]) {
              const rawSpeaker = match[1].trim();
              const spokenText = match[2].trim();
              
              // For each segment, determine if to show the speaker or not
              const speakerKey = /^speaker\s*\d+$/i.test(rawSpeaker) ? 
                `speaker_${rawSpeaker}` : rawSpeaker;
              
              // Show speaker if different from the previous one
              const showSpeaker = (lastSpeakerKey !== speakerKey);
              
              // Format the text appropriately
              if (showSpeaker && spokenText) {
                conversationPieces.push(`[${rawSpeaker}] ${spokenText}`);
              } else if (spokenText) {
                conversationPieces.push(spokenText);
              }
              
              lastSpeakerKey = speakerKey;
            }
          }
          
          // If no segments found, add the text as is
          if (!segmentFound && st.text.trim()) {
            conversationPieces.push(st.text.trim());
          }
        } else {
          // For transcriptions without explicit [Speaker] format
          // Determine the correct speaker
          let speakerLabel: string;
          let speakerKey: string;
          
          if (st.speaker === this.primaryUserSpeaker) {
            speakerLabel = this.primaryUserSpeaker;
            speakerKey = this.primaryUserSpeaker;
          } else {
            // For external speakers, verify if they have a specific number using a more precise regex
            const speakerMatch = st.text.match(/\bspeaker\s*(\d+)\b/i);
            if (speakerMatch && speakerMatch[1]) {
              speakerLabel = `Speaker ${speakerMatch[1]}`;
              speakerKey = `speaker_${speakerMatch[1]}`;
            } else {
              speakerLabel = EXTERNAL_SPEAKER_LABEL;
              speakerKey = "external";
            }
          }
          
          // Show speaker if different from the previous one
          const showSpeaker = (lastSpeakerKey !== speakerKey);
          
          // Format the text appropriately
          if (showSpeaker) {
            conversationPieces.push(`[${speakerLabel}] ${st.text}`);
          } else {
            conversationPieces.push(st.text);
          }
          
          lastSpeakerKey = speakerKey;
        }
      }
      
      return conversationPieces.join("\n");
    }
    
    // Alternative case: use the traditional list
    const validTranscriptions = transcriptionList
      .filter(text => text && text.trim().length > 0);
    
    if (validTranscriptions.length > 0) {
      // Remove duplicates while maintaining order
      const uniqueTranscriptions = [...new Set(validTranscriptions)];
      return uniqueTranscriptions.join(" ").trim();
    }
    
    // Last option: use the last known transcription
    if (lastTranscription && lastTranscription.trim()) {
      return lastTranscription.trim();
    }
    
    return "Please respond based only on the context provided.";
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// SymbolicCognitionTimelineLogger.ts
// Logging structure for symbolic timeline with precise timestamps

import { CognitionEvent } from '../../types/CognitionEvent';
import { SymbolicInsight } from '../../types/SymbolicInsight';
import { SymbolicQuery } from '../../types/SymbolicQuery';
import { SymbolicContext } from '../../types/SymbolicContext';
import { LoggingUtils } from '../../utils/LoggingUtils';
import { UserIntentWeights } from '../../symbolic-cortex/integration/ICollapseStrategyService';

export class SymbolicCognitionTimelineLogger {
  private timeline: CognitionEvent[] = [];

  private now(): string {
    return new Date().toISOString();
  }

  /**
   * Records a raw prompt in the timeline.
   * @param content Textual content of the prompt sent to the system.
   */
  logRawPrompt(content: string): void {
    this.timeline.push({ type: 'raw_prompt', timestamp: this.now(), content });
  }

  /**
   * Records a temporary context in the timeline.
   * @param context Temporary textual context used in processing.
   */
  logTemporaryContext(context: string): void {
    this.timeline.push({ type: 'temporary_context', timestamp: this.now(), context });
  }

  /**
   * Records a neural signal in the timeline, including symbolic query and parameters.
   * @param core Neural signal core (e.g., memory, emotion).
   * @param symbolic_query Symbolic query associated with the signal.
   * @param intensity Signal intensity.
   * @param topK TopK parameter used in search.
   * @param params Additional signal parameters.
   */
  logNeuralSignal(core: string, symbolic_query: SymbolicQuery, intensity: number, topK: number, params: Record<string, unknown>): void {
    this.timeline.push({
      type: 'neural_signal',
      timestamp: this.now(),
      core,
      symbolic_query,
      intensity,
      topK,
      params
    });
  }

  /**
   * Records the result of a symbolic retrieval in the timeline.
   * @param core Core of the associated signal.
   * @param insights Array of extracted symbolic insights.
   * @param matchCount Number of matches found.
   * @param durationMs Search duration in milliseconds.
   */
  logSymbolicRetrieval(core: string, insights: SymbolicInsight[], matchCount: number, durationMs: number): void {
    const safeInsights = Array.isArray(insights) ? insights : [];
    if (!insights || safeInsights.length === 0) {
      LoggingUtils.logInfo(`No insights extracted for core: ${core}`);
    }
    this.timeline.push({
      type: 'symbolic_retrieval',
      timestamp: this.now(),
      core,
      insights: safeInsights,
      matchCount,
      durationMs
    });
  }

  /**
   * Records the initiation of a symbolic fusion process in the timeline.
   */
  logFusionInitiated(): void {
    this.timeline.push({ type: 'fusion_initiated', timestamp: this.now() });
  }

  /**
   * Logs a neural collapse event in the timeline
   * @param isDeterministic Indicates if the collapse was deterministic or probabilistic
   * @param selectedCore Neural core selected for collapse
   * @param numCandidates Number of candidates available before collapse
   * @param emotionalWeight Emotional weight of the result
   * @param contradictionScore Contradiction value of the result
   * @param temperature Symbolic temperature used (only for non-deterministic collapses)
   * @param justification Textual justification for the collapse decision
   * @param userIntent User intent weights inferred from the original text
   * @param insights Symbolic insights associated with the collapse
   * @param emergentProperties Emergent properties detected in the neural response patterns
   */
  logNeuralCollapse(isDeterministic: boolean, selectedCore: string, numCandidates: number, emotionalWeight: number, contradictionScore: number, temperature?: number, justification?: string, userIntent?: UserIntentWeights, insights?: SymbolicInsight[], emergentProperties?: string[]): void {
    this.timeline.push({ 
      type: 'neural_collapse',
      timestamp: this.now(),
      isDeterministic,
      selectedCore,
      numCandidates,
      temperature,
      emotionalWeight,
      contradictionScore,
      justification,
      userIntent,
      insights,
      emergentProperties
    });
  }

  /**
   * Records the synthesized symbolic context in the timeline.
   * @param context Synthesized symbolic context object.
   */
  logSymbolicContextSynthesized(context: SymbolicContext): void {
    this.timeline.push({ type: 'symbolic_context_synthesized', timestamp: this.now(), context });
  }

  /**
   * Records the GPT model response in the timeline, including symbolic topics and insights.
   * @param data Response string or detailed object with topics and insights.
   */
  logGptResponse(data: string | { response: string; symbolicTopics?: string[]; insights?: SymbolicInsight[] }): void {
    if (typeof data === 'string') {
      LoggingUtils.logInfo('No insights extracted in GPT response (string).');
      this.timeline.push({ 
        type: 'gpt_response', 
        timestamp: this.now(), 
        response: data, 
        insights: []
      });
    } else {
      const hasInsights = data.insights && Array.isArray(data.insights) && data.insights.length > 0;
      if (!hasInsights) {
        LoggingUtils.logInfo('No insights extracted in GPT response (object).');
      }
      this.timeline.push({
        type: 'gpt_response',
        timestamp: this.now(),
        response: data.response,
        symbolicTopics: data.symbolicTopics,
        insights: hasInsights ? data.insights : []
      });
    }
  }

  /**
   * Returns the complete timeline of recorded cognitive events.
   */
  getTimeline(): CognitionEvent[] {
    return this.timeline;
  }

  /**
   * Logs detected emergent symbolic patterns.
   * This is part of the Orch-OS scientific introspection layer for tracking
   * emergent cognitive phenomena across processing cycles.
   * 
   * @param patterns Array of emergent symbolic pattern descriptions
   * @param metrics Scientific metrics associated with the patterns
   */
  logEmergentPatterns(patterns: string[], metrics?: { 
    archetypalStability?: number; 
    cycleEntropy?: number; 
    insightDepth?: number 
  }): void {
    this.timeline.push({
      type: 'emergent_patterns',
      timestamp: this.now(),
      patterns,
      metrics
    });
    
    // Log para debugging/monitoramento
    LoggingUtils.logInfo(`[Timeline] Logged ${patterns.length} emergent patterns`);
  }

  clear() {
    this.timeline = [];
  }
}

export default SymbolicCognitionTimelineLogger;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// SymbolicCognitionTimelineLoggerSingleton.ts
import SymbolicCognitionTimelineLogger from './SymbolicCognitionTimelineLogger';

const symbolicCognitionTimelineLogger = new SymbolicCognitionTimelineLogger();
export default symbolicCognitionTimelineLogger;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// UIUpdateService.ts
// Service responsible for updating the UI and sending notifications

import { UIUpdater } from "../../interfaces/transcription/TranscriptionTypes";
import { IUIUpdateService } from "../../interfaces/utils/IUIUpdateService";
import { LoggingUtils } from "../../utils/LoggingUtils";

export class UIUpdateService implements IUIUpdateService {
  private setTexts: UIUpdater;
  
  constructor(setTexts: UIUpdater) {
    this.setTexts = setTexts;
  }
  
  /**
   * Updates the UI with new values
   */
  updateUI(update: Record<string, any>): void {
    this.setTexts((prev: any) => ({ ...prev, ...update }));
  }
  
  /**
   * Notifies the start of prompt processing via IPC
   */
  notifyPromptProcessingStarted(temporaryContext?: string): void {
    if (typeof window !== 'undefined' && window.electronAPI) {
      try {
        // 1. Notify that we are sending the prompt (for UI to show loading)
        if (window.electronAPI.sendPromptUpdate) {
          window.electronAPI.sendPromptUpdate('partial', "Processing...");
        }
        
        // 2. Send command to main process via IPC
        if (window.electronAPI.sendNeuralPrompt) {
          window.electronAPI.sendNeuralPrompt(temporaryContext);
          LoggingUtils.logInfo("Prompt sent to main process");
        }
      } catch (e) {
        LoggingUtils.logError("Error sending notifications via IPC", e);
      }
    }
  }
  
  /**
   * Notifies the completion of the prompt via IPC
   */
  notifyPromptComplete(response: string): void {
    if (typeof window !== 'undefined' && window.electronAPI?.sendPromptUpdate) {
      try {
        window.electronAPI.sendPromptUpdate('complete', response);
        LoggingUtils.logInfo("Final response sent via IPC");
      } catch (e) {
        LoggingUtils.logError("Error sending final response via IPC", e);
      }
    }
  }
  
  /**
   * Notifies an error in prompt processing via IPC
   */
  notifyPromptError(errorMessage: string): void {
    if (typeof window !== 'undefined' && window.electronAPI?.sendPromptUpdate) {
      try {
        window.electronAPI.sendPromptUpdate('error', errorMessage);
      } catch (e) {
        LoggingUtils.logError("Error sending error notification via IPC", e);
      }
    }
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
import { getOption, STORAGE_KEYS } from "./../../../../services/StorageService";
// Copyright (c) 2025 Guilherme Ferrari Brescia

// DeepgramTranscriptionService.ts
// Main transcription service for Deepgram that orchestrates other services

import { getPrimaryUser } from "../../../../config/UserConfig";
import { ModeService, OrchOSModeEnum } from "../../../../services/ModeService";
import { IDeepgramTranscriptionService } from "../interfaces/deepgram/IDeepgramService";
import { IMemoryService } from "../interfaces/memory/IMemoryService";
import { IOpenAIService } from "../interfaces/openai/IOpenAIService";
import { ITranscriptionStorageService } from "../interfaces/transcription/ITranscriptionStorageService";
import {
  SpeakerTranscription,
  SpeakerTranscriptionLog,
  UIUpdater,
} from "../interfaces/transcription/TranscriptionTypes";
import { ISpeakerIdentificationService } from "../interfaces/utils/ISpeakerIdentificationService";
import { IUIUpdateService } from "../interfaces/utils/IUIUpdateService";
import { DefaultNeuralIntegrationService } from "../symbolic-cortex/integration/DefaultNeuralIntegrationService";
import { INeuralIntegrationService } from "../symbolic-cortex/integration/INeuralIntegrationService";
import { LoggingUtils } from "../utils/LoggingUtils";
import { MemoryService } from "./memory/MemoryService";
import { TranscriptionPromptProcessor } from "./transcription/TranscriptionPromptProcessor";
import { TranscriptionStorageService } from "./transcription/TranscriptionStorageService";
import { SpeakerIdentificationService } from "./utils/SpeakerIdentificationService";
import { UIUpdateService } from "./utils/UIUpdateService";

export class DeepgramTranscriptionService
  implements IDeepgramTranscriptionService
{
  // Essential services
  private speakerService: ISpeakerIdentificationService;
  private storageService: ITranscriptionStorageService;
  private memoryService: IMemoryService;
  // Generic LLM service (could be Ollama or HuggingFace facade)
  private llmService: IOpenAIService;
  private uiService: IUIUpdateService;

  // Configuration
  private model: string =
    getOption(STORAGE_KEYS.DEEPGRAM_MODEL) || "nova-2-general";
  private interimResultsEnabled: boolean = true;
  private useSimplifiedHistory: boolean = false;
  private currentLanguage: string =
    getOption(STORAGE_KEYS.DEEPGRAM_LANGUAGE) || "pt-BR";

  // Properties for the neural system (kept for backward compatibility)
  private _neuralMemory: Array<{
    timestamp: number;
    core: string;
    intensity: number;
    pattern?: string;
  }> = [];

  /**
   * Returns the current state of prompt processing
   * @returns true if a prompt is currently being processed, false otherwise
   */
  public isProcessingPromptRequest(): boolean {
    return this.transcriptionPromptProcessor.isProcessingPromptRequest();
  }

  // Neural integration service
  private neuralIntegrationService: INeuralIntegrationService;

  // Transcription prompt processor
  private transcriptionPromptProcessor: TranscriptionPromptProcessor;

  constructor(
    setTexts: UIUpdater,
    llmService: IOpenAIService,
    primaryUserSpeaker: string = getPrimaryUser()
  ) {
    // Initialize services
    this.speakerService = new SpeakerIdentificationService(primaryUserSpeaker);
    this.storageService = new TranscriptionStorageService(
      this.speakerService,
      setTexts
    );
    this.llmService = llmService; // May be HuggingFaceServiceFacade in Basic mode
    this.memoryService = new MemoryService(this.llmService);
    this.uiService = new UIUpdateService(setTexts);

    // Initialize the neural integration service
    this.neuralIntegrationService = new DefaultNeuralIntegrationService(
      this.llmService
    );

    // Initialize the transcription prompt processor
    this.transcriptionPromptProcessor = new TranscriptionPromptProcessor(
      this.storageService,
      this.memoryService,
      this.llmService,
      this.uiService,
      this.speakerService,
      this.neuralIntegrationService
    );

    // Set reference back to this service in the storage service to enable auto-triggering
    if (this.storageService instanceof TranscriptionStorageService) {
      this.storageService.setTranscriptionService(this);
    }

    // Load API key
    this.loadApiKey();
  }

  // Main interface methods

  /**
   * Sets the name of the primary speaker (user)
   */
  setPrimaryUserSpeaker(name: string): void {
    this.speakerService.setPrimaryUserSpeaker(name);
  }

  /**
   * Adds a new transcription received from the Deepgram service
   */
  addTranscription(text: string, speaker?: string): void {
    this.storageService.addTranscription(text, speaker);
  }

  /**
   * Clears transcription data
   */
  clearTranscriptionData(): void {
    this.storageService.clearTranscriptionData();
    this.memoryService.clearMemoryData();
    this.memoryService.resetTranscriptionSnapshot();
  }

  /**
   * Returns the current list of transcriptions
   */
  getTranscriptionList(): string[] {
    return this.storageService.getTranscriptionList();
  }

  /**
   * Returns transcriptions organized by speaker
   */
  getSpeakerTranscriptions(): SpeakerTranscription[] {
    return this.storageService.getSpeakerTranscriptions();
  }

  /**
   * Returns transcription logs grouped by speaker
   */
  getTranscriptionLogs(): SpeakerTranscriptionLog[] {
    return this.storageService.getTranscriptionLogs();
  }

  /**
   * Accesses the internal storage service directly
   * @returns The instance of the storage service
   * @internal Only for internal use
   */
  getStorageServiceForIntegration(): ITranscriptionStorageService {
    return this.storageService;
  }

  /**
   * Verifies if only the primary user speaker is speaking
   */
  isOnlyUserSpeaking(): boolean {
    return this.speakerService.isOnlyUserSpeaking(
      this.storageService.getSpeakerTranscriptions()
    );
  }

  /**
   * Activates or deactivates the simplified history mode
   */
  setSimplifiedHistoryMode(enabled: boolean): void {
    this.useSimplifiedHistory = enabled;
    this.memoryService.setSimplifiedHistoryMode(enabled);
    LoggingUtils.logInfo(
      `Simplified history mode: ${enabled ? "activated" : "deactivated"}`
    );
  }

  /**
   * Sets the processing language for transcription and neural processing
   */
  setProcessingLanguage(language: string): void {
    this.currentLanguage = language;
    this.transcriptionPromptProcessor.setLanguage(language);
    LoggingUtils.logInfo(`Processing language updated to: ${language}`);
  }

  /**
   * Gets the current processing language
   */
  getProcessingLanguage(): string {
    return this.transcriptionPromptProcessor.getCurrentLanguage();
  }

  /**
   * Processes the transcription using the appropriate AI service based on current mode
   * Automatically selects between Ollama (advanced mode) and HuggingFace (basic mode)
   */
  async sendTranscriptionPrompt(temporaryContext?: string): Promise<void> {
    console.log("🚀 [DEEPGRAM_SERVICE] sendTranscriptionPrompt called:", {
      temporaryContext,
      hasContext: !!temporaryContext,
      currentMode: ModeService.getMode(),
      timestamp: new Date().toISOString(),
    });

    const currentMode = ModeService.getMode();

    try {
      if (currentMode === OrchOSModeEnum.BASIC) {
        LoggingUtils.logInfo("🤖 Using HuggingFace service (Basic mode)");
        console.log("📤 [DEEPGRAM_SERVICE] Calling processWithHuggingFace");
        await this.transcriptionPromptProcessor.processWithHuggingFace(
          temporaryContext
        );
        console.log("✅ [DEEPGRAM_SERVICE] processWithHuggingFace completed");
      } else {
        LoggingUtils.logInfo("🧠 Using Ollama service (Advanced mode)");
        console.log("📤 [DEEPGRAM_SERVICE] Calling processWithOpenAI");
        await this.transcriptionPromptProcessor.processWithOpenAI(
          temporaryContext
        );
        console.log("✅ [DEEPGRAM_SERVICE] processWithOpenAI completed");
      }
    } catch (error) {
      console.error(
        "❌ [DEEPGRAM_SERVICE] Error in sendTranscriptionPrompt:",
        error
      );
      throw error;
    }
  }

  /**
   * Processes a direct message from chat interface
   * @param message The message from chat input
   * @param temporaryContext Optional additional context
   */
  async sendDirectMessage(
    message: string,
    temporaryContext?: string
  ): Promise<void> {
    console.log("💬 [DEEPGRAM_SERVICE] sendDirectMessage called:", {
      message: message.substring(0, 50),
      hasContext: !!temporaryContext,
      timestamp: new Date().toISOString(),
    });

    try {
      await this.transcriptionPromptProcessor.processDirectMessage(
        message,
        temporaryContext
      );
      console.log("✅ [DEEPGRAM_SERVICE] sendDirectMessage completed");
    } catch (error) {
      console.error("❌ [DEEPGRAM_SERVICE] Error in sendDirectMessage:", error);
      throw error;
    }
  }

  /**
   * Processes the transcription using HuggingFace backend specifically
   * Always uses HuggingFace regardless of the current mode
   */
  async sendTranscriptionPromptWithHuggingFace(
    temporaryContext?: string
  ): Promise<void> {
    LoggingUtils.logInfo("🤖 Explicitly using HuggingFace service");
    return await this.transcriptionPromptProcessor.processWithHuggingFace(
      temporaryContext
    );
  }

  /**
   * Loads the LLM service API key from the environment
   */
  private async loadApiKey(): Promise<void> {
    await this.llmService.loadApiKey();
  }

  // Implementation of IDeepgramTranscriptionService methods

  async connect(language?: string): Promise<void> {
    if (language) {
      this.currentLanguage = language;
      this.transcriptionPromptProcessor.setLanguage(language);
    }
    LoggingUtils.logInfo(
      `Connecting transcription service. Language: ${this.currentLanguage}`
    );
    return Promise.resolve();
  }

  async disconnect(): Promise<void> {
    LoggingUtils.logInfo("Disconnecting transcription service");
    return Promise.resolve();
  }

 