
      // Create transcription service with UI updater callback
      const transcriptionService = new DeepgramTranscriptionService(
        (updater: any) => {
          if (updater.transcription !== undefined) {
            handleTranscriptionData(updater.transcription);
          }
          if (updater.interim !== undefined) {
            handleInterimUpdate(updater.interim);
          }
        },
        aiService
      );

      // Get storage service for integration
      const storageService =
        transcriptionService.getStorageServiceForIntegration();
      console.log(
        "💾 Storage service obtained:",
        storageService ? "OK" : "NULL"
      );

      // Initialize connection service
      services.current.deepgramConnection = new DeepgramConnectionService(
        setConnectionState,
        setConnection,
        analyzer,
        storageService
      );

      // Store references
      deepgramTranscriptionRef.current = transcriptionService;
      deepgramConnectionRef.current = services.current.deepgramConnection;

      // Register transcription callback
      services.current.deepgramConnection.registerTranscriptionCallback(
        (event: string, data: any) => {
          if (event === "transcript") {
            handleTranscriptionData(data);
            if (data?.text && deepgramTranscriptionRef.current) {
              deepgramTranscriptionRef.current.addTranscription(data.text);
              console.log(`📝 Transcription sent to service: "${data.text}"`);
            }
          } else if (event === "metadata") {
            console.log("Metadata received:", data);
          }
        }
      );

      // Configure initial preferences
      if (settings.deepgramModel) {
        transcriptionService.setModel(settings.deepgramModel);
      }
      transcriptionService.toggleInterimResults(settings.showInterimResults);
    };

    initializeServices();

    // Cleanup on unmount
    return () => {
      if (services.current.deepgramConnection) {
        services.current.deepgramConnection.cleanup();
      }
      if (deepgramTranscriptionRef.current) {
        deepgramTranscriptionRef.current.reset();
      }
      services.current.audioContext.closeAudioContext();
    };
  }, [
    analyzer,
    settings.deepgramModel,
    settings.showInterimResults,
    handleTranscriptionData,
    handleInterimUpdate,
  ]);

  // Configure IPC event receiver
  useEffect(() => {
    if (typeof window !== "undefined" && window.electronAPI) {
      const removeListener = window.electronAPI.onSendChunk(() => {
        // Intentionally empty - DeepgramConnectionService handles this directly
      });
      return () => removeListener();
    }
  }, []);

  // Update preferences when settings change
  useEffect(() => {
    if (deepgramTranscriptionRef.current) {
      if (settings.deepgramModel) {
        deepgramTranscriptionRef.current.setModel(settings.deepgramModel);
      }
      deepgramTranscriptionRef.current.toggleInterimResults(
        settings.showInterimResults
      );
    }
  }, [settings.deepgramModel, settings.showInterimResults]);

  // Connection management functions
  const connectToDeepgram = useCallback(async () => {
    try {
      if (
        state.deepgramState === DeepgramState.Connected ||
        state.deepgramState === DeepgramState.Connecting
      ) {
        console.log("🔍 Connection already active or in progress");
        return state.isConnected;
      }

      const { deepgramConnection } = services.current;
      if (!deepgramConnection) return false;

      dispatch({ type: "SET_STATE", payload: DeepgramState.Connecting });
      await deepgramConnection.connectToDeepgram(state.language);

      const connected = await deepgramConnection.hasActiveConnection();

      if (connected) {
        dispatch({ type: "SET_STATE", payload: DeepgramState.Connected });
        dispatch({ type: "SET_CONNECTED", payload: true });
      } else {
        dispatch({ type: "SET_STATE", payload: DeepgramState.Error });
        dispatch({ type: "SET_CONNECTED", payload: false });
      }

      return connected;
    } catch (error) {
      console.error("❌ Error connecting to Deepgram:", error);
      dispatch({ type: "SET_STATE", payload: DeepgramState.Error });
      dispatch({ type: "SET_CONNECTED", payload: false });
      return false;
    }
  }, [state.deepgramState, state.isConnected, state.language]);

  const disconnectFromDeepgram = useCallback(async () => {
    try {
      if (
        state.deepgramState === DeepgramState.NotConnected ||
        state.deepgramState === DeepgramState.Disconnecting
      ) {
        console.log("🔍 Connection already inactive or disconnecting");
        return;
      }

      const { deepgramConnection } = services.current;
      if (!deepgramConnection) return;

      dispatch({ type: "SET_STATE", payload: DeepgramState.Disconnecting });
      await deepgramConnection.disconnectFromDeepgram();

      dispatch({ type: "SET_STATE", payload: DeepgramState.NotConnected });
      dispatch({ type: "SET_CONNECTED", payload: false });
    } catch (error) {
      console.error("❌ Error disconnecting from Deepgram:", error);
    }
  }, [state.deepgramState]);

  // Utility functions
  const sendAudioChunk = useCallback(async (chunk: Blob | Uint8Array) => {
    return services.current.deepgramConnection?.sendAudioChunk(chunk) || false;
  }, []);

  const stopProcessing = useCallback(() => {
    dispatch({ type: "SET_PROCESSING", payload: false });
  }, []);

  const setLanguage = useCallback((language: string) => {
    dispatch({ type: "SET_LANGUAGE", payload: language });
  }, []);

  const setModel = useCallback((model: string) => {
    dispatch({ type: "SET_MODEL", payload: model });
  }, []);

  const resetState = useCallback(() => {
    dispatch({ type: "RESET_STATE" });
  }, []);

  // Export service instances for UI/integration
  const getServiceInstances = () => {
    let transcriptionServiceInstance: any = undefined;
    let memoryServiceInstance: any = undefined;

    if (
      deepgramTranscriptionRef.current instanceof DeepgramTranscriptionService
    ) {
      transcriptionServiceInstance = (deepgramTranscriptionRef.current as any)[
        "storageService"
      ];
      const memoryService = (deepgramTranscriptionRef.current as any)[
        "memoryService"
      ];
      if (memoryService?.persistenceService) {
        memoryServiceInstance = memoryService["persistenceService"];
      }
    }

    return { transcriptionServiceInstance, memoryServiceInstance };
  };

  const { transcriptionServiceInstance, memoryServiceInstance } =
    getServiceInstances();

  // Context value - Following Interface Segregation Principle
  const contextValue: IDeepgramContext = {
    // Connection state
    connection,
    connectionState,
    isConnected: state.isConnected,
    deepgramState: state.deepgramState,

    // Transcription data
    transcriptionList: transcriptionData,

    // Processing state
    isProcessing: state.isProcessing,

    // Language and model
    language: state.language,
    model: state.model,

    // Connection management
    connectToDeepgram,
    disconnectFromDeepgram,
    sendAudioChunk,
    hasActiveConnection: () =>
      deepgramConnectionRef.current?.hasActiveConnection() || false,
    getConnectionStatus: () =>
      deepgramConnectionRef.current?.getConnectionStatus() || {
        state: ConnectionState.CLOSED,
        active: false,
      },
    waitForConnectionState: (targetState, timeoutMs) =>
      deepgramConnectionRef.current?.waitForConnectionState(
        targetState,
        timeoutMs
      ) || Promise.resolve(false),

    // Transcription processing
    sendTranscriptionPrompt,
    sendDirectMessage,
    flushTranscriptionsToUI,
    setAutoQuestionDetection,
    clearTranscriptionData,

    // Get all transcriptions with status
    getAllTranscriptionsWithStatus: () => {
      if (transcriptionServiceInstance?.getAllTranscriptionsWithStatus) {
        return transcriptionServiceInstance.getAllTranscriptionsWithStatus();
      }
      return [];
    },

    // State management
    stopProcessing,
    setLanguage,
    setModel,
    resetState,

    // Service instances
    transcriptionService: transcriptionServiceInstance,
    memoryService: memoryServiceInstance,

    // Debug functions (only in development)
    debugDatabase,
    testDatabaseDiagnosis,
    testEmbeddingModel,
  };

  return (
    <DeepgramContext.Provider value={contextValue}>
      {children}
    </DeepgramContext.Provider>
  );
};

export default DeepgramProvider;
// @ts-nocheck
/** @type {import('ts-jest').JestConfigWithTsJest} */
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  transform: {
    '^.+\\.tsx?$': 'ts-jest',
  },
  moduleFileExtensions: ['ts', 'tsx', 'js', 'jsx', 'json', 'node'],
}; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IAudioContextService.ts
// Interface for managing the Web Audio API audio context

export interface IAudioContextService {
  getAudioContext: () => AudioContext | null;
  setupAudioContext: () => void;
  closeAudioContext: () => Promise<void>;
  getMerger: () => ChannelMergerNode | null;
  getDestination: () => MediaStreamAudioDestinationNode | null;
  getChannelInfo: () => { 
    count: number, 
    mode: ChannelCountMode, 
    interpretation: ChannelInterpretation 
  } | null;
  connectMicrophoneSource: (source: AudioNode) => void;
  disconnectMicrophoneSource: () => void;
  connectSystemAudioSource: (source: AudioNode) => void;
  disconnectSystemAudioSource: () => void;
  isMicrophoneConnected: () => boolean;
  isSystemAudioConnected: () => boolean;
  getConnectionStatus: () => { microphone: boolean, systemAudio: boolean };
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IAudioDeviceService.ts
// Interface for managing audio devices and their connections


// Interface for audio source
export interface AudioSource {
  stream: MediaStream;
  source: MediaStreamAudioSourceNode;
  speakerName: string;
  isSystemAudio?: boolean;
  oscillator?: OscillatorNode;
}

export interface IAudioDeviceService {
  getAudioDevices: () => MediaDeviceInfo[];
  setAudioDevices: (devices: MediaDeviceInfo[]) => void;
  getSources: () => Record<string, AudioSource>;
  connectDevice: (deviceId: string | null) => Promise<boolean>;
  disconnectDevice: (deviceId: string) => void;
  getSpeakerNameForDevice: (device: MediaDeviceInfo) => string;
  createSilentSource: (channelIndex: number, speakerName: string) => string | null;
  isSystemAudioDevice: (device: MediaDeviceInfo) => boolean;
  filterDevicesForUI: () => { 
    microphoneDevices: MediaDeviceInfo[],
    systemAudioDevices: MediaDeviceInfo[]
  };
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IMicrophoneContext.ts
// Interface for the microphone context

// Enums for microphone states and events
export enum MicrophoneEvents {
  DataAvailable = "dataavailable",
  Error = "error",
  Pause = "pause",
  Resume = "resume",
  Start = "start",
  Stop = "stop",
}

export enum MicrophoneState {
  NotSetup = -1,
  SettingUp = 0,
  Ready = 1,
  Opening = 2,
  Open = 3,
  Error = 4,
  Stopping = 5,
  Stopped = 6,
  Resuming = 7
}

// Basic types
export type SelectedDevices = {
  microphone: string | null;
  systemAudio: string | null;
};

export type SpeakerMapping = {
  [deviceId: string]: string;
};

// Interface for channel analysis
export interface ChannelAnalysis {
  channelCount: number;
  totalSamples: number;
  sampleRate: number;
  durationSeconds?: number;
  channels: {
    avgVolume: number;
    rmsVolume?: number;
    peakVolume: number;
    hasAudio: boolean;
    sampleValues?: number[];
  }[];
  error?: string;
}

// Interface for the microphone context that will be exposed to components
export interface IMicrophoneContext {
  microphone: MediaRecorder | null;
  startMicrophone: () => void;
  stopMicrophone: (forceReset?: boolean) => void;
  setupMicrophone: (deviceIds?: string[]) => Promise<boolean>;
  resetAudioSystem: (autoRestart?: boolean) => Promise<void>;
  microphoneState: MicrophoneState;
  getCurrentMicrophoneState: () => MicrophoneState;
  audioDevices: MediaDeviceInfo[];
  selectedDevices: SelectedDevices;
  setSelectedDevices: React.Dispatch<React.SetStateAction<SelectedDevices>>;
  disconnectSource: (deviceId: string) => void;
  handleDeviceChange: (deviceId: string, isSystemAudio: boolean) => void;
  setIsMicrophoneOn: React.Dispatch<React.SetStateAction<boolean>>;
  setIsSystemAudioOn: React.Dispatch<React.SetStateAction<boolean>>;
  isMicrophoneOn: boolean;
  isSystemAudioOn: boolean;
  speakerMappings: SpeakerMapping;
  generateTestWAV: () => Promise<ChannelAnalysis | null>;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// IRecorderService.ts
// Interface for managing audio recording with MediaRecorder


export interface IRecorderService {
  createMediaRecorder: () => MediaRecorder | null;
  startRecording: () => void;
  stopRecording: () => void;
  configureRecorderEvents: (recorder: MediaRecorder) => void;
  getCurrentRecorder: () => MediaRecorder | null;
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// AudioContextService.ts
// Implementation of the Web Audio API audio context service

import { IAudioContextService } from "../interfaces/IAudioContextService";

export class AudioContextService implements IAudioContextService {
  private audioContext: AudioContext | null = null;
  private merger: ChannelMergerNode | null = null;
  private destination: MediaStreamAudioDestinationNode | null = null;
  
  // Track audio sources connected to the merger
  private microphoneSource: AudioNode | null = null;
  private systemAudioSource: AudioNode | null = null;
  
  // Nodes for automatic gain control
  private microphoneGain: GainNode | null = null;
  private systemAudioGain: GainNode | null = null;
  private microphoneAnalyser: AnalyserNode | null = null;
  private systemAudioAnalyser: AnalyserNode | null = null;
  private microphoneFilter: BiquadFilterNode | null = null;
  
  // Parameters for automatic gain control
  private readonly MIN_VOLUME_THRESHOLD = 0.01; // Minimum volume level to consider amplification
  private readonly TARGET_VOLUME = 0.3;         // Target volume after amplification
  private readonly DEFAULT_MAX_GAIN = 5.0;      // Maximum gain for common microphones
  private readonly AIRPODS_MAX_GAIN = 12.0;     // Maximum gain for AirPods
  private readonly ANALYSIS_INTERVAL_MS = 100;  // Analysis interval in ms
  
  // Track device type
  private isMicrophoneAirPods = false;
  
  // Analysis timers
  private microphoneAnalysisTimer: NodeJS.Timeout | null = null;
  private systemAudioAnalysisTimer: NodeJS.Timeout | null = null;
  
  private readonly CHANNEL_COUNT = 2;
  private readonly LATENCY_HINT = 'interactive';

  // Access methods - return audio context components
  getAudioContext() {
    return this.audioContext;
  }

  getMerger() {
    return this.merger;
  }

  getDestination() {
    return this.destination;
  }

  // Method to verify the number of channels being processed
  getChannelInfo(): { count: number, mode: ChannelCountMode, interpretation: ChannelInterpretation } | null {
    if (!this.destination) return null;
    
    return {
      count: this.destination.channelCount,
      mode: this.destination.channelCountMode,
      interpretation: this.destination.channelInterpretation
    };
  }

  // Setup the audio context and its components
  setupAudioContext(): void {
    try {
      // Create the audio context if it doesn't exist
      if (!this.audioContext) {
        this.createNewAudioContext();
      } else if (this.audioContext.state === "suspended") {
        this.resumeAudioContext();
      }
    } catch (error) {
      console.error("❌ Error configuring AudioContext:", error);
    }
  }

  // Main method to configure and connect microphone source
  configureAndConnectMicrophoneSource(source: AudioNode, deviceInfo?: MediaDeviceInfo): void {
    if (!this.merger || !this.audioContext) {
      console.error("❌ AudioContext or merger not available to connect microphone");
      return;
    }
    
    try {
      // Disconnect previous source if it exists
      this.disconnectMicrophoneSource();
      
      // Check if the device is AirPods
      this.isMicrophoneAirPods = false;
      if (deviceInfo && deviceInfo.label) {
        this.isMicrophoneAirPods = deviceInfo.label.toLowerCase().includes("airpods");
        console.log(`🎤 Device detected: ${deviceInfo.label} ${this.isMicrophoneAirPods ? '(AirPods)' : ''}`);
      }
      
      // Create processing nodes for analysis and automatic gain control
      this.microphoneAnalyser = this.audioContext.createAnalyser();
      this.microphoneAnalyser.fftSize = 256;
      this.microphoneAnalyser.smoothingTimeConstant = 0.8;
      
      // Create equalization filter to improve voice
      this.microphoneFilter = this.audioContext.createBiquadFilter();
      this.microphoneFilter.type = "peaking";
      this.microphoneFilter.frequency.value = 1500;
      this.microphoneFilter.Q.value = 1;
      this.microphoneFilter.gain.value = 4;
      
      // Create gain node for volume control
      this.microphoneGain = this.audioContext.createGain();
      this.microphoneGain.gain.value = 1.0; // Start with neutral gain
      
      // Connect processing chain: source -> analyzer -> filter -> gain -> merger
      source.connect(this.microphoneAnalyser);
      this.microphoneAnalyser.connect(this.microphoneFilter);
      this.microphoneFilter.connect(this.microphoneGain);
      this.microphoneGain.connect(this.merger, 0, 0);
      
      // Store source
      this.microphoneSource = source;
      
      const deviceType = this.isMicrophoneAirPods ? 'AirPods' : 'default';
      console.log(`🎤 Microphone source (${deviceType}) connected to merger channel 0 with EQ and adaptive gain`);
      
      // Start periodic analysis for gain adjustment
      this.startMicrophoneVolumeAnalysis();
    } catch (error) {
      console.error("❌ Error configuring and connecting microphone source:", error);
    }
  }

  // Connect microphone source to merger (Channel 0)
  connectMicrophoneSource(source: AudioNode, deviceInfo?: MediaDeviceInfo): void {
    this.configureAndConnectMicrophoneSource(source, deviceInfo);
  }
  
  // Disconnect microphone source
  disconnectMicrophoneSource(): void {
    if (!this.merger || !this.microphoneSource) return;
    
    try {
      // Verify if the source is really connected before trying to disconnect
      // using try/catch to capture possible errors
      this.microphoneSource.disconnect(this.microphoneAnalyser || this.merger);
      this.microphoneSource = null;
      
      // Disconnect and clean processing nodes
      if (this.microphoneAnalyser) {
        this.microphoneAnalyser.disconnect();
        this.microphoneAnalyser = null;
      }
      
      if (this.microphoneFilter) {
        this.microphoneFilter.disconnect();
        this.microphoneFilter = null;
      }
      
      if (this.microphoneGain) {
        this.microphoneGain.disconnect();
        this.microphoneGain = null;
      }
      
      // Stop volume analysis
      this.stopMicrophoneVolumeAnalysis();
      
      console.log("🎤 Microphone source disconnected from merger");
    } catch (error) {
      // Verify if the error is about disconnecting a node that is not connected
      if (error instanceof DOMException && 
          error.message.includes("the given destination is not connected")) {
        // Apenas limpar a referência ao nó, já que ele não está conectado
        this.microphoneSource = null;
        console.log("🎤 Microphone source reference cleared (not connected)");
      } else {
        // Reportar outros tipos de erros
        console.error("❌ Error disconnecting microphone source:", error);
      }
    }
  }
  
  // Connect system audio source to merger (Channel 1)
  connectSystemAudioSource(source: AudioNode): void {
    if (!this.merger || !this.audioContext) {
      console.error("❌ AudioContext or merger not available to connect system audio");
      return;
    }
    
    try {
      // Disconnect previous source if it exists
      this.disconnectSystemAudioSource();
      
      // Create processing nodes for analysis and automatic gain control
      this.systemAudioAnalyser = this.audioContext.createAnalyser();
      this.systemAudioAnalyser.fftSize = 256;
      this.systemAudioAnalyser.smoothingTimeConstant = 0.8;
      
      this.systemAudioGain = this.audioContext.createGain();
      this.systemAudioGain.gain.value = 1.0; // Iniciar com ganho neutro
      
      // Connect nodes: source -> analyzer -> gain -> merger
      source.connect(this.systemAudioAnalyser);
      this.systemAudioAnalyser.connect(this.systemAudioGain);
      this.systemAudioGain.connect(this.merger, 0, 1);
      
      // Store source
      this.systemAudioSource = source;
      
      console.log("🔊 System audio source connected to channel 1 of merger with automatic gain control");
      
      // Start periodic analysis for gain adjustment
      this.startSystemAudioVolumeAnalysis();
    } catch (error) {
      console.error("❌ Error connecting system audio source:", error);
    }
  }
  
  // Disconnect system audio source
  disconnectSystemAudioSource(): void {
    if (!this.merger || !this.systemAudioSource) return;
    
    try {
      // Verify if the source is really connected before trying to disconnect
      // using try/catch to capture possible errors
      this.systemAudioSource.disconnect(this.systemAudioAnalyser || this.merger);
      this.systemAudioSource = null;
      
      // Disconnect and clean processing nodes
      if (this.systemAudioAnalyser) {
        this.systemAudioAnalyser.disconnect();
        this.systemAudioAnalyser = null;
      }
      
      if (this.systemAudioGain) {
        this.systemAudioGain.disconnect();
        this.systemAudioGain = null;
      }
      
      // Stop volume analysis
      this.stopSystemAudioVolumeAnalysis();
      
      console.log("🔊 System audio source disconnected from merger");
    } catch (error) {
      // Verify if the error is about disconnecting a node that is not connected
      if (error instanceof DOMException && 
          error.message.includes("the given destination is not connected")) {
        // Just clear the node reference, since it's not connected
        this.systemAudioSource = null;
        console.log("🔊 System audio source reference cleared (not connected)");
      } else {
        // Report other types of errors
        console.error("❌ Error disconnecting system audio source:", error);
      }
    }
  }

  // Reset the audio system, optionally forcing a complete closure
  async resetAudioSystem(forceClose: boolean = false): Promise<void> {
    console.log(`🔄 Resetting audio system (forceClose: ${forceClose})`);
    
    try {
      // Stop volume analyses to avoid pending references
      this.stopMicrophoneVolumeAnalysis();
      this.stopSystemAudioVolumeAnalysis();
      
      // Disconnect all audio sources
      this.disconnectMicrophoneSource();
      this.disconnectSystemAudioSource();
      
      if (forceClose && this.audioContext) {
        console.log("🔒 Closing AudioContext completely for clean restart");
        // Close the audio context completely
        await this.closeAudioContext();
        
        // Small pause to ensure the closure is completed
        await new Promise(resolve => setTimeout(resolve, 300));
        
        // Create a new audio context from scratch
        this.createNewAudioContext();
        console.log("✅ Audio system restarted with new AudioContext");
      } else if (this.audioContext) {
        // If the context is reused, ensure it is in a usable state
        if (this.audioContext.state === "suspended") {
          console.log("🔄 Resuming existing AudioContext");
          await this.audioContext.resume();
        }
        
        // Verify if essential nodes exist, recreate if necessary
        if (!this.merger || !this.destination) {
          console.log("🔄 Recreating audio nodes");
          this.setupAudioNodes();
          this.connectAudioNodes();
        }
        
        console.log("✅ Audio system reset");
      } else {
        // No audio context, create a new one
        console.log("🔄 Creating new AudioContext (none existing)");
        this.createNewAudioContext();
        console.log("✅ New audio system initialized");
      }
      
      return;
    } catch (error) {
      console.error("❌ Error resetting audio system:", error);
      
      // In case of a critical error, try to clean everything and start over
      this.resetState();
      
      // Try to create a new context even after error
      try {
        this.createNewAudioContext();
        console.log("⚠️ Audio system re-created after error");
      } catch (secondError) {
        console.error("❌ Critical error re-creating audio system:", secondError);
        throw new Error("Audio system restart failed");
      }
    }
  }

  // Close the audio context and clean resources
  async closeAudioContext(): Promise<void> {
    if (!this.audioContext) return;
    
    try {
      console.log("🔄 Closing AudioContext...");
      
      // Disconnect all sources first
      this.disconnectMicrophoneSource();
      this.disconnectSystemAudioSource();
      
      // Stop volume analyses to avoid pending callbacks
      this.stopMicrophoneVolumeAnalysis();
      this.stopSystemAudioVolumeAnalysis();
      
      // Verify if there are audio nodes and disconnect them explicitly
      if (this.merger && this.destination) {
        try {
          this.merger.disconnect(this.destination);
          console.log("🔌 Audio nodes disconnected explicitly");
        } catch (err) {
          console.warn("⚠️ Could not disconnect audio nodes:", err);
        }
      }
      
      // Close the audio context
      await this.audioContext.close();
      
      // Reset state variables
      this.resetState();
      console.log("✅ AudioContext closed successfully and resources released");
    } catch (error) {
      console.error("❌ Error closing AudioContext:", error);
      // In case of an error, try to reset the state anyway
      throw error;
    }
  }

  // Verify if a microphone source is connected
  isMicrophoneConnected(): boolean {
    return this.microphoneSource !== null;
  }
  
  // Verify if a system audio source is connected
  isSystemAudioConnected(): boolean {
    return this.systemAudioSource !== null;
  }
  
  // Returns the connection status of both channels
  getConnectionStatus(): { microphone: boolean, systemAudio: boolean } {
    return {
      microphone: this.isMicrophoneConnected(),
      systemAudio: this.isSystemAudioConnected()
    };
  }

  // Start microphone volume analysis
  private startMicrophoneVolumeAnalysis(): void {
    if (this.microphoneAnalysisTimer) {
      clearInterval(this.microphoneAnalysisTimer);
    }
    
    this.microphoneAnalysisTimer = setInterval(() => {
      if (!this.microphoneAnalyser || !this.microphoneGain) return;
      
      const volume = this.getVolumeFromAnalyser(this.microphoneAnalyser);
      this.adjustGainForVolume(volume, this.microphoneGain, "microphone", this.isMicrophoneAirPods);
    }, this.ANALYSIS_INTERVAL_MS);
  }
  
  // Stop microphone volume analysis
  private stopMicrophoneVolumeAnalysis(): void {
    if (this.microphoneAnalysisTimer) {
      clearInterval(this.microphoneAnalysisTimer);
      this.microphoneAnalysisTimer = null;
    }
  }
  
  // Start system audio volume analysis
  private startSystemAudioVolumeAnalysis(): void {
    if (this.systemAudioAnalysisTimer) {
      clearInterval(this.systemAudioAnalysisTimer);
    }
    
    this.systemAudioAnalysisTimer = setInterval(() => {
      if (!this.systemAudioAnalyser || !this.systemAudioGain) return;
      
      const volume = this.getVolumeFromAnalyser(this.systemAudioAnalyser);
      this.adjustGainForVolume(volume, this.systemAudioGain, "system", false);
    }, this.ANALYSIS_INTERVAL_MS);
  }
  
  // Stop system audio volume analysis
  private stopSystemAudioVolumeAnalysis(): void {
    if (this.systemAudioAnalysisTimer) {
      clearInterval(this.systemAudioAnalysisTimer);
      this.systemAudioAnalysisTimer = null;
    }
  }
  
  // Get the current volume from the analyzer
  private getVolumeFromAnalyser(analyser: AnalyserNode): number {
    const dataArray = new Uint8Array(analyser.frequencyBinCount);
    analyser.getByteTimeDomainData(dataArray);
    
    // Calculate RMS (Root Mean Square) - standard metric for audio volume
    let sumSquares = 0;
    for (let i = 0; i < dataArray.length; i++) {
      // Converter de [0, 255] para [-1, 1]
      const normalized = (dataArray[i] - 128) / 128;
      sumSquares += normalized * normalized;
    }
    
    const rms = Math.sqrt(sumSquares / dataArray.length);
    return rms;
  }
  
  // Adjust gain based on volume
  private adjustGainForVolume(
    volume: number, 
    gainNode: GainNode, 
    sourceType: string, 
    isAirPods: boolean = false
  ): void {
    // Set maximum gain based on device type
    const maxGain = isAirPods ? this.AIRPODS_MAX_GAIN : this.DEFAULT_MAX_GAIN;
    const deviceTypeStr = isAirPods ? "AirPods" : "default";
    
    // Verify if the volume is below the minimum threshold
    if (volume < this.MIN_VOLUME_THRESHOLD && volume > 0.001) {
      // Calculate the gain needed to reach the target volume, but limit to maximum
      const requiredGain = Math.min(this.TARGET_VOLUME / volume, maxGain);
      
      // Adjust gain gradually to avoid clicks
      const currentGain = gainNode.gain.value;
      const newGain = currentGain * 0.8 + requiredGain * 0.2; // Blend suave
      
      gainNode.gain.setValueAtTime(newGain, this.audioContext?.currentTime || 0);
      
      // Log ocasional for diagnosis (1 in every 10 adjustments)
      if (Math.random() < 0.1) {
        console.log(`🔊 Adjusting ${sourceType} (${deviceTypeStr}): volume=${volume.toFixed(3)}, gain=${newGain.toFixed(2)}x, max=${maxGain}x`);
      }
    } else if (volume >= this.MIN_VOLUME_THRESHOLD || volume <= 0.001) {
      // If the volume is above the threshold or effectively silent, gradually return to normal gain
      const currentGain = gainNode.gain.value;
      
      // If we are significantly above the normal gain, gradually reduce
      if (currentGain > 1.2) {
        const newGain = currentGain * 0.95 + 1.0 * 0.05; // Gradually return to gain 1.0
        gainNode.gain.setValueAtTime(newGain, this.audioContext?.currentTime || 0);
        
        // Log ocasional
        if (Math.random() < 0.1) {
          console.log(`🔊 Normalizing gain of ${sourceType} (${deviceTypeStr}): ${newGain.toFixed(2)}x, volume=${volume.toFixed(3)}`);
        }
      }
    }
  }

  // Private methods for better organization
  
  private createNewAudioContext(): void {
    console.log("🔊 Creating new AudioContext");
    
    try {
      // Create the context with ideal parameters for speech processing
      this.audioContext = new AudioContext({
        latencyHint: this.LATENCY_HINT,
        sampleRate: 16000
      });
      
      // Verify if the context started correctly
      this.ensureRunningState();
      
      // Configure the audio processing components
      this.setupAudioNodes();
      
      // Connect the audio nodes
      this.connectAudioNodes();
      
      // Log of complete configuration
      this.logAudioSetup();
    } catch (error) {
      console.error("❌ Error creating AudioContext:", error);
      this.resetState();
      throw new Error("Could not create audio context");
    }
  }
  
  private ensureRunningState(): void {
    if (!this.audioContext) return;
    
    if (this.audioContext.state !== "running") {
      console.log(`⚠️ AudioContext in state ${this.audioContext.state}, trying to resume...`);
      
      this.audioContext.resume()
        .then(() => {
          if (this.audioContext) {
            console.log(`✅ AudioContext now: ${this.audioContext.state}`);
          }
        })
        .catch(error => {
          console.error("❌ Error resuming AudioContext:", error);
        });
    }
  }
  
  private resumeAudioContext(): void {
    if (!this.audioContext) return;
    
    console.log("🔄 Resuming suspended AudioContext");
    
    this.audioContext.resume()
      .catch(error => {
        console.error("❌ Error resuming AudioContext suspenso:", error);
      });
  }
  
  private setupAudioNodes(): void {
    if (!this.audioContext) return;
    
    // Configure the merger to combine audio channels
    console.log("🔀 Creating ChannelMerger");
    this.merger = this.audioContext.createChannelMerger(this.CHANNEL_COUNT);
    this.merger.channelInterpretation = 'discrete';
    this.merger.channelCountMode = 'explicit';
    
    // Configure the destination to receive processed audio
    console.log("🎯 Configurando MediaStreamAudioDestinationNode");
    this.destination = this.audioContext.createMediaStreamDestination();
    this.destination.channelCount = this.CHANNEL_COUNT;
    this.destination.channelCountMode = 'explicit';
    this.destination.channelInterpretation = 'discrete';
  }
  
  private connectAudioNodes(): void {
    if (!this.merger || !this.destination) {
      console.error("❌ Audio nodes not available for connection");
      return;
    }
    
    try {
      this.merger.connect(this.destination);
      console.log("🔌 ChannelMerger connected to MediaStreamAudioDestinationNode");
    } catch (error) {
      console.error("❌ Error connecting audio nodes:", error);
    }
  }
  
  private logAudioSetup(): void {
    if (!this.audioContext || !this.destination) return;
    
    console.log(`🎛️ Native sample rate: ${this.audioContext.sampleRate}Hz`);
    console.log(`🎚️ Number of channels: ${this.destination.channelCount}`);
    
    if (this.destination.stream) {
      console.log(`🔍 Destination stream has ${this.destination.stream.getAudioTracks().length} audio tracks`);
    }
  }
  
  private resetState(): void {
    // Stop volume analyses
    this.stopMicrophoneVolumeAnalysis();
    this.stopSystemAudioVolumeAnalysis();
    
    this.microphoneSource = null;
    this.systemAudioSource = null;
    this.microphoneGain = null;
    this.systemAudioGain = null;
    this.microphoneAnalyser = null;
    this.systemAudioAnalyser = null;
    this.microphoneFilter = null;
    this.audioContext = null;
    this.merger = null; 
    this.destination = null;
    this.isMicrophoneAirPods = false;
  }
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// AudioDeviceService.ts
// Implementation of the audio device service

import { getPrimaryUser } from "../../../config/UserConfig";
import { IAudioContextService } from "../interfaces/IAudioContextService";
import {
  AudioSource,
  IAudioDeviceService,
} from "../interfaces/IAudioDeviceService";

// Audio configuration constants
const AUDIO_CONFIG = {
  SYSTEM_AUDIO_IDENTIFIERS: ["blackhole", "dipper"],
  DEFAULT_USER_NAME: getPrimaryUser(),
  CHANNEL: {
    MICROPHONE: 0,
    SYSTEM_AUDIO: 1,
  },
  GAIN: {
    SILENT: 0.001, // -60dB, practically inaudible
  },
  FREQUENCY: {
    MICROPHONE: 440, // A4
    SYSTEM_AUDIO: 880, // A5
  },
};

export class AudioDeviceService implements IAudioDeviceService {
  private _audioDevices: MediaDeviceInfo[] = [];
  private sources: Record<string, AudioSource> = {};
  private audioContextService: IAudioContextService;

  constructor(audioContextService: IAudioContextService) {
    this.audioContextService = audioContextService;
  }

  // --- Methods for device management ---

  getAudioDevices() {
    return this._audioDevices;
  }

  setAudioDevices(devices: MediaDeviceInfo[]) {
    this._audioDevices = devices;
  }

  getSources() {
    return this.sources;
  }

  // Filter devices for microphone and system audio
  filterDevicesForUI(): {
    microphoneDevices: MediaDeviceInfo[];
    systemAudioDevices: MediaDeviceInfo[];
  } {
    const microphoneDevices: MediaDeviceInfo[] = [];
    const systemAudioDevices: MediaDeviceInfo[] = [];

    this._audioDevices.forEach((device) => {
      if (this.isSystemAudioDevice(device)) {
        systemAudioDevices.push(device);
      } else {
        microphoneDevices.push(device);
      }
    });

    return { microphoneDevices, systemAudioDevices };
  }

  // --- Methods for device type detection ---

  // Determine speaker name based on device
  getSpeakerNameForDevice(device: MediaDeviceInfo): string {
    return this.isSystemAudioDevice(device)
      ? ""
      : AUDIO_CONFIG.DEFAULT_USER_NAME;
  }

  // Determine if a device is system audio
  isSystemAudioDevice(device: MediaDeviceInfo): boolean {
    const label = device.label.toLowerCase();
    return AUDIO_CONFIG.SYSTEM_AUDIO_IDENTIFIERS.some((id) =>
      label.includes(id)
    );
  }

  // --- Methods for connecting cognitive audio devices ---

  // Method to connect an audio device for brain input
  async connectDevice(deviceId: string | null): Promise<boolean> {
    if (!this.isValidDeviceId(deviceId) || this.sources[deviceId!]) {
      return false;
    }

    try {
      // Ensure the AudioContext is configured
      this.audioContextService.setupAudioContext();

      const audioContext = this.audioContextService.getAudioContext();
      const merger = this.audioContextService.getMerger();

      if (!this.validateAudioComponents(audioContext, merger)) return false;

      // Find the corresponding device
      const device = this._audioDevices.find((d) => d.deviceId === deviceId);
      if (!device) {
        console.log(
          "🎚️ [COGNITIVE-AUDIO-DEVICE] Selected audio device for brain input not found:",
          deviceId
        );
        return false;
      }

      const isSystemAudio = this.isSystemAudioDevice(device);
      const deviceType = isSystemAudio ? "System Audio" : "Microphone";
      console.log(
        "🎚️ [COGNITIVE-AUDIO-DEVICE] Connecting audio device for brain input:",
        deviceType,
        device.label,
        deviceId
      );

      // Get the audio stream from the device
      const stream = await this.captureDeviceStream(deviceId!);
      if (!stream || stream.getAudioTracks().length === 0) {
        console.log(
          "❌ [COGNITIVE-AUDIO-DEVICE] No audio tracks found for device:",
          deviceId
        );
        return false;
      }

      // Log the stream settings
      this.logStreamSettings(stream);

      // Create and connect the source node
      const source = audioContext!.createMediaStreamSource(stream);
      const channelIndex = isSystemAudio
        ? AUDIO_CONFIG.CHANNEL.SYSTEM_AUDIO
        : AUDIO_CONFIG.CHANNEL.MICROPHONE;
      const speakerName = this.getSpeakerNameForDevice(device);

      // Configure and connect the source node
      this.configureAndConnectSource(source, merger!, channelIndex);

      // Store the source
      this.sources[deviceId!] = {
        stream,
        source,
        speakerName,
        isSystemAudio,
      };

      console.log(
        "✅ [COGNITIVE-AUDIO-DEVICE] Audio device connected to channel",
        channelIndex,
        "with speaker:",
        speakerName || "Default diarization"
      );
      return true;
    } catch (error) {
      console.log(
        "❌ [COGNITIVE-AUDIO-DEVICE] Error connecting audio device for brain input:",
        error
      );
      return false;
    }
  }

  // Disconnect an audio source
  disconnectDevice(deviceId: string) {
    if (!this.isValidDeviceId(deviceId)) return;

    if (this.sources[deviceId]) {
      try {
        // Disconnect the audio source
        this.sources[deviceId].source.disconnect();

        // Stop the oscillator if it exists (for silent sources)
        const sourceObj = this.sources[deviceId];
        const oscillator = sourceObj.oscillator;
        if (oscillator) {
          oscillator.stop();
          oscillator.disconnect();
        }

        // Stop all audio tracks
        for (const track of this.sources[deviceId].stream.getAudioTracks()) {
          track.enabled = false;
          track.stop();
        }

        // Remove the source
        delete this.sources[deviceId];
        console.log(`✅ Source ${deviceId} disconnected`);
      } catch (err) {
        console.error(`❌ Error disconnecting source ${deviceId}:`, err);
      }
    }
  }

  // Create a silent audio source for a specific channel
  createSilentSource(channelIndex: number, speakerName: string): string | null {
    const audioContext = this.audioContextService.getAudioContext();
    const merger = this.audioContextService.getMerger();

    if (!this.validateAudioComponents(audioContext, merger)) return null;

    // Security check for valid channels
    if (!this.isValidChannelIndex(channelIndex)) return null;

    try {
      console.log(`🔊 Creating silent source for channel ${channelIndex}...`);

      // Create silent audio source
      const { oscillator, gainNode } = this.createSilentAudioSource(
        audioContext!,
        channelIndex
      );

      // Connect to merger at specific channel
      gainNode.connect(merger!, 0, channelIndex);
      oscillator.start();

      // Create unique ID for this source
      const fakeDeviceId = `silent-channel-${channelIndex}-${Date.now()}`;
      const isSystemAudio = channelIndex === AUDIO_CONFIG.CHANNEL.SYSTEM_AUDIO;

      // Store reference
      this.sources[fakeDeviceId] = {
        stream: new MediaStream(),
        source: gainNode as unknown as MediaStreamAudioSourceNode,
        speakerName,
        isSystemAudio,
        oscillator,
      };

      console.log(
        `✅ Silent source created successfully for channel ${channelIndex}`
      );
      this.verifyDestinationTracks();

      return fakeDeviceId;
    } catch (error) {
      console.error("❌ Error creating silent source:", error);
      return null;
    }
  }

  // --- Private helper methods ---

  private isValidDeviceId(deviceId: string | null): boolean {
    return Boolean(deviceId && deviceId !== "N/A" && deviceId !== "");
  }

  private isValidChannelIndex(channelIndex: number): boolean {
    if (
      channelIndex !== AUDIO_CONFIG.CHANNEL.MICROPHONE &&
      channelIndex !== AUDIO_CONFIG.CHANNEL.SYSTEM_AUDIO
    ) {
      console.error(
        `❌ Invalid channel ${channelIndex}. Only channels 0 and 1 are allowed!`
      );
      return false;
    }
    return true;
  }

  private validateAudioComponents(
    audioContext: AudioContext | null,
    merger: ChannelMergerNode | null
  ): boolean {
    if (!audioContext || !merger) {
      console.error("❌ AudioContext or merger not available");
      return false;
    }
    return true;
  }

  private async captureDeviceStream(
    deviceId: string
  ): Promise<MediaStream | null> {
    try {
      // Get device information
      const device = this._audioDevices.find((d) => d.deviceId === deviceId);
      if (!device) {
        console.error(`❌ Device not found for ID: ${deviceId}`);
        return null;
      }

      const deviceLabel = device.label.toLowerCase();
      console.log(`🎤 Requesting device: ${deviceLabel}`);

      // Check and request microphone permissions on macOS
      if (typeof window !== "undefined" && window.electronAPI) {
        try {
          console.log("🔒 Checking microphone permissions...");
          const permissionResult =
            await window.electronAPI.requestMicrophonePermission();

          if (!permissionResult.success) {
            console.error(
              "❌ Microphone permission denied:",
              permissionResult.error
            );
            throw new Error(
              permissionResult.error || "Microphone permission was denied"
            );
          }

          console.log("✅ Microphone permission granted");
        } catch (permissionError) {
          console.error(
            "❌ Error checking microphone permissions:",
            permissionError
          );
          throw new Error(
            `Permission check failed: ${
              permissionError instanceof Error
                ? permissionError.message
                : "Unknown error"
            }`
          );
        }
      }

      // Check if device is available and not in use
      const isDeviceAvailable = await this.checkDeviceAvailability(deviceId);
      if (!isDeviceAvailable) {
        throw new Error(
          "Device is not available or may be in use by another application"
        );
      }

      // Try with different constraint configurations for better compatibility
      const constraintVariations = [
        // First try: Optimal settings for transcription
        {
          audio: {
            deviceId: { exact: deviceId },
            echoCancellation: false,
            noiseSuppression: false,
            channelCount: 1,
          },
        },
        // Second try: More permissive settings
        {
          audio: {
            deviceId: { exact: deviceId },
            echoCancellation: true,
            noiseSuppression: true,
          },
        },
        // Third try: Minimal constraints
        {
          audio: {
            deviceId: { exact: deviceId },
          },
        },
        // Fourth try: Just deviceId preference
        {
          audio: {
            deviceId: { ideal: deviceId },
          },
        },
      ];

      let stream: MediaStream | null = null;
      let lastError: Error | null = null;

      for (let i = 0; i < constraintVariations.length; i++) {
        try {
          console.log(
            `🔄 Trying constraint variation ${i + 1}/${
              constraintVariations.length
            }`
          );
          stream = await navigator.mediaDevices.getUserMedia(
            constraintVariations[i]
          );
          console.log(
            `✅ Successfully connected with constraint variation ${i + 1}`
          );
          break;
        } catch (error) {
          console.log(`⚠️ Constraint variation ${i + 1} failed:`, error);
          lastError = error as Error;

          // Wait a bit before trying next variation
          if (i < constraintVariations.length - 1) {
            await new Promise((resolve) => setTimeout(resolve, 500));
          }
        }
      }

      if (!stream) {
        throw lastError || new Error("All constraint variations failed");
      }

      // Log the actual settings obtained
      this.logStreamSettings(stream);

      return stream;
    } catch (error) {
      console.error("❌ Error capturing stream:", error);

      // Enhanced error logging for debugging
      if (error instanceof DOMException) {
        console.error("❌ DOMException details:", {
          name: error.name,
          message: error.message,
          code: error.code,
          stack: error.stack,
        });

        // Specific handling for common errors
        if (error.name === "NotAllowedError") {
          console.error("❌ NotAllowedError - possible causes:");
          console.error("   1. Browser permissions not granted");
          console.error("   2. Device is being used by another application");
          console.error("   3. Hardware access blocked by system");
          console.error(
            "   4. Electron webContents permission handler blocking access"
          );
        } else if (error.name === "NotFoundError") {
          console.error("❌ NotFoundError - device not found or unavailable");
        } else if (error.name === "OverconstrainedError") {
          console.error(
            "❌ OverconstrainedError - constraints cannot be satisfied"
          );
        }
      }

      return null;
    }
  }

  private configureAndConnectSource(
    source: MediaStreamAudioSourceNode,
    merger: ChannelMergerNode,
    channelIndex: number
  ): void {
    // Source configurations
    source.channelCount = 1;
    source.channelCountMode = "explicit";
    source.channelInterpretation = "discrete";

    // Connect to specific channel in merger
    source.connect(merger, 0, channelIndex);
    console.log(`🔀 Source connected to channel ${channelIndex} of merger`);
  }

  private createSilentAudioSource(
    audioContext: AudioContext,
    channelIndex: number
  ): {
    oscillator: OscillatorNode;
    gainNode: GainNode;
  } {
    // Frequency based on channel
    const frequency =
      channelIndex === AUDIO_CONFIG.CHANNEL.MICROPHONE
        ? AUDIO_CONFIG.FREQUENCY.MICROPHONE
        : AUDIO_CONFIG.FREQUENCY.SYSTEM_AUDIO;

    // Create oscillator
    const oscillator = audioContext.createOscillator();
    oscillator.type = "sine";
    oscillator.frequency.value = frequency;

    // Create gain node to control volume
    const gainNode = audioContext.createGain();
    gainNode.gain.value = AUDIO_CONFIG.GAIN.SILENT;

    // Configure gain node parameters
    gainNode.channelCount = 1;
    gainNode.channelCountMode = "explicit";
    gainNode.channelInterpretation = "discrete";

    // Connect oscillator to gain node
    oscillator.connect(gainNode);

    return { oscillator, gainNode };
  }

  private logStreamSettings(stream: MediaStream): void {
    const trackSettings = stream.getAudioTracks()[0].getSettings();
    console.log(
      `🎛️ Stream: sampleRate=${
        trackSettings.sampleRate || "unknown"
      }, channelCount=${trackSettings.channelCount || "unknown"}`
    );
  }

  private verifyDestinationTracks(): void {
    setTimeout(() => {
      const destination = this.audioContextService.getDestination();
      if (destination) {
        const trackCount = destination.stream.getAudioTracks().length;
        console.log(`🔍 Destination stream has ${trackCount} audio tracks`);
      }
    }, 100);
  }

  private async checkDeviceAvailability(deviceId: string): Promise<boolean> {
    try {
      // Try to enumerate devices first to see if device still exists
      const devices = await navigator.mediaDevices.enumerateDevices();
      const targetDevice = devices.find((d) => d.deviceId === deviceId);

      if (!targetDevice) {
        console.error("❌ Device not found in enumeration");
        return false;
      }

      // Quick test with minimal constraints to check availability
      try {
        const testStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            deviceId: { exact: deviceId },
          },
        });

        // Immediately release the test stream
        testStream.getTracks().forEach((track) => track.stop());
        console.log("✅ Device availability test passed");
        return true;
      } catch (error) {
        console.log("⚠️ Device availability test failed:", error);
        return false;
      }
    } catch (error) {
      console.error("❌ Error checking device availability:", error);
      return false;
    }
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// MicrophoneContextProvider.tsx
// Microphone context provider

import React, { createContext, Dispatch, SetStateAction, useCallback, useContext, useEffect, useReducer, useRef } from 'react';
import { getPrimaryUser } from '../../../config/UserConfig';
import {
  ChannelAnalysis,
  IMicrophoneContext,
  MicrophoneState,
  SelectedDevices,
  SpeakerMapping
} from '../interfaces/IMicrophoneContext';
import { AudioContextService } from './AudioContextService';
import { AudioDeviceService } from './AudioDeviceService';
import { RecorderService } from './RecorderService';

// Microphone context
export const MicrophoneContext = createContext<IMicrophoneContext | null>(null);

// Hook for usage in consumer components
export function useMicrophone() {
  const context = useContext(MicrophoneContext);
  if (!context) {
    throw new Error('useMicrophone must be used within a MicrophoneProvider');
  }
  return context;
}

// Tipo para o estado do microfone
type MicrophoneStateType = {
  microphone: MediaRecorder | null;
  microphoneState: MicrophoneState;
  audioDevices: MediaDeviceInfo[];
  selectedDevices: SelectedDevices;
  isMicrophoneOn: boolean;
  isSystemAudioOn: boolean;
  speakerMappings: SpeakerMapping;
}

// Tipo para ações do reducer
type MicrophoneAction = 
  | { type: 'SET_STATE'; payload: MicrophoneState }
  | { type: 'SET_MICROPHONE'; payload: MediaRecorder | null }
  | { type: 'SET_AUDIO_DEVICES'; payload: MediaDeviceInfo[] }
  | { type: 'SET_SELECTED_DEVICES'; payload: Partial<SelectedDevices> }
  | { type: 'SET_MICROPHONE_ON'; payload: boolean }
  | { type: 'SET_SYSTEM_AUDIO_ON'; payload: boolean }
  | { type: 'UPDATE_SPEAKER_MAPPING'; payload: SpeakerMapping }
  | { type: 'RESET_STATE' };

// Estado inicial
const initialState: MicrophoneStateType = {
  microphone: null,
  microphoneState: MicrophoneState.NotSetup,
  audioDevices: [],
  selectedDevices: { microphone: null, systemAudio: null },
  isMicrophoneOn: false,
  isSystemAudioOn: false,
  speakerMappings: {}
};

// Reducer to manage state
function microphoneReducer(state: MicrophoneStateType, action: MicrophoneAction): MicrophoneStateType {
  switch (action.type) {
    case 'SET_STATE':
      return { ...state, microphoneState: action.payload };
    case 'SET_MICROPHONE':
      return { ...state, microphone: action.payload };
    case 'SET_AUDIO_DEVICES':
      return { ...state, audioDevices: action.payload };
    case 'SET_SELECTED_DEVICES':
      return { 
        ...state, 
        selectedDevices: { ...state.selectedDevices, ...action.payload } 
      };
    case 'SET_MICROPHONE_ON':
      return { ...state, isMicrophoneOn: action.payload };
    case 'SET_SYSTEM_AUDIO_ON':
      return { ...state, isSystemAudioOn: action.payload };
    case 'UPDATE_SPEAKER_MAPPING':
      return { ...state, speakerMappings: { ...state.speakerMappings, ...action.payload } };
    case 'RESET_STATE':
      return { 
        ...initialState,
        audioDevices: state.audioDevices
      };
    default:
      return state;
  }
}

// Microphone context provider
export default function MicrophoneProvider({ children }: { children: React.ReactNode }) {
  // State management using reducer
  const [state, dispatch] = useReducer(microphoneReducer, initialState);
  
  // Reference to the current state (for use in async callbacks)
  const stateRef = useRef<MicrophoneState>(MicrophoneState.NotSetup);
  
  // Services
  const services = useRef({
    audioContext: new AudioContextService(),
    audioDevice: null as AudioDeviceService | null,
    recorder: null as RecorderService | null
  });
  
  // Update state reference when state changes
  useEffect(() => {
    stateRef.current = state.microphoneState;
  }, [state.microphoneState]);
  
  // Initialize services and configure event listeners
  useEffect(() => {
    // Initialize services
    const initializeServices = () => {
      // Initialize AudioDeviceService
      services.current.audioDevice = new AudioDeviceService(services.current.audioContext);
      
      // Initialize RecorderService
      services.current.recorder = new RecorderService(
        services.current.audioContext,
        (newState) => dispatch({ type: 'SET_STATE', payload: newState }),
        (recorder) => dispatch({ type: 'SET_MICROPHONE', payload: recorder })
      );
    };
    
    // Initialize services
    initializeServices();
    
    // Enumerate audio devices
    enumerateAudioDevices();
    
    // Configure event listener for device changes
    navigator.mediaDevices.addEventListener('devicechange', handleDeviceChangeEvent);
    
    // Handler for cleaning up resources before closing/reloading the page
    const handleBeforeUnload = (event: BeforeUnloadEvent) => {
      console.log("🔄 Page being closed or reloaded, shutting down audio system...");
      
      // Force audio system shutdown
      const { audioContext, audioDevice, recorder } = services.current;
      
      try {
        // Stop recording
        if (recorder) {
          recorder.stopRecording();
        }
        
        // Disconnect all sources
        if (audioDevice) {
          const sources = audioDevice.getSources();
          Object.keys(sources).forEach(deviceId => {
            audioDevice.disconnectDevice(deviceId);
          });
        }
        
        // Disconnect merger sources
        if (audioContext) {
          audioContext.disconnectMicrophoneSource();
          audioContext.disconnectSystemAudioSource();
          
          // Try to close the audio context synchronously
          try {
            audioContext.getAudioContext()?.close();
          } catch (error) {
            console.error("❌ Error closing AudioContext during unload:", error);
          }
        }
      } catch (error) {
        console.error("❌ Error cleaning up audio resources during unload:", error);
      }
      
      // Prevent system from keeping open resources
      return null;
    };
    
    // Add handler for beforeunload event
    window.addEventListener('beforeunload', handleBeforeUnload);
    
    // Check and turn off audio system if already running in the background
    const checkExistingAudioSystem = () => {
      const { audioContext } = services.current;
      if (audioContext && audioContext.getAudioContext()?.state === 'running') {
        console.warn("⚠️ Audio system already running, resetting to prevent resource leaks");
        resetAudioSystem(false);
      }
    };
    
    // Check existing audio system on startup
    checkExistingAudioSystem();
    
    // Cleanup on unmount
    return () => {
      stopMicrophone(true);
      navigator.mediaDevices.removeEventListener('devicechange', handleDeviceChangeEvent);
      window.removeEventListener('beforeunload', handleBeforeUnload);
      resetAudioSystem(false);
    };
  }, []);
  
  // Handle device change events
  const handleDeviceChangeEvent = useCallback(() => {
    console.log("🔄 Device change detected, re-enumerating devices");
    enumerateAudioDevices();
  }, []);
  
  // Enumerate audio devices
  const enumerateAudioDevices = async () => {
    try {
      const { audioDevice } = services.current;
      if (!audioDevice) return;
      
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputDevices = devices.filter(device => device.kind === 'audioinput');
      
      if (audioInputDevices.length > 0) {
        console.log(`🎙️ Found ${audioInputDevices.length} audio input devices`);
        
        // Map speakers to devices
        const speakerMap: SpeakerMapping = {};
        
        audioInputDevices.forEach((device) => {
          const speakerName = audioDevice.getSpeakerNameForDevice(device);
          if (speakerName) {
            speakerMap[device.deviceId] = speakerName;
          }
        });
        
        // Update device list and speaker mapping
        dispatch({ type: 'SET_AUDIO_DEVICES', payload: audioInputDevices });
        dispatch({ type: 'UPDATE_SPEAKER_MAPPING', payload: speakerMap });
        
        // Update device service
        audioDevice.setAudioDevices(audioInputDevices);
        
        // Automatically select the first available microphone and system audio device if none are selected
        const { systemAudioDevices, microphoneDevices } = audioDevice.filterDevicesForUI();
        
        if (!state.selectedDevices.microphone && microphoneDevices.length > 0) {
          console.log(`🎤 Automatically selecting the first available microphone: ${microphoneDevices[0].label}`);
          dispatch({ 
            type: 'SET_SELECTED_DEVICES', 
            payload: { microphone: microphoneDevices[0].deviceId } 
          });
        }
        
        if (!state.selectedDevices.systemAudio && systemAudioDevices.length > 0) {
          console.log(`🔊 Automatically selecting the first available system audio device: ${systemAudioDevices[0].label}`);
          dispatch({ 
            type: 'SET_SELECTED_DEVICES', 
            payload: { systemAudio: systemAudioDevices[0].deviceId } 
          });
        }
      } else {
        console.warn("⚠️ No audio input devices found!");
      }
    } catch (error) {
      console.error("❌ Error enumerating audio devices:", error);
    }
  };
  
  // Disconnect a specific audio source
  const disconnectSource = useCallback((deviceId: string) => {
    const { audioDevice } = services.current;
    if (!audioDevice) return;
    
    audioDevice.disconnectDevice(deviceId);
  }, []);
  
  // Handle device change
  const handleDeviceChange = useCallback(async (deviceId: string, isSystemAudio: boolean) => {
    const { audioDevice, audioContext } = services.current;
    if (!audioDevice) return;
    
    // Determine device type
    const deviceType = isSystemAudio ? 'systemAudio' : 'microphone';
    
    // Check if the device is already selected
    const currentDevice = state.selectedDevices[deviceType];
    if (currentDevice === deviceId) return;
    
    // Disconnect the current device if it exists
    if (currentDevice) {
      disconnectSource(currentDevice);
    }
    
    // Update selected device
    dispatch({ 
      type: 'SET_SELECTED_DEVICES', 
      payload: { [deviceType]: deviceId } 
    });
    
    // Check if this type of device is active
    const isActive = isSystemAudio ? state.isSystemAudioOn : state.isMicrophoneOn;
    
    // If the device type is active, connect the new device
    if (isActive) {
      // Ensure the AudioContext is configured
      audioContext.setupAudioContext();
      
      // Connect the new device
      const connected = await audioDevice.connectDevice(deviceId);
      
      if (!connected) {
        console.error(`❌ Failed to connect ${isSystemAudio ? 'system audio' : 'microphone'}: ${deviceId}`);
        
        // Reset selection on failure
        dispatch({ 
          type: 'SET_SELECTED_DEVICES', 
          payload: { [deviceType]: null } 
        });
      }
    }
  }, [state.selectedDevices, state.isMicrophoneOn, state.isSystemAudioOn, disconnectSource]);
  
  // Configure microphone
  const setupMicrophone = async () => {
    console.log("🔄 Configuring microphone...");
    
    try {
      const { audioContext, recorder } = services.current;
      if (!recorder) return false;
      
      // Update state
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.SettingUp });
      stateRef.current = MicrophoneState.SettingUp;
      
      // Configure AudioContext
      audioContext.setupAudioContext();
      
      // Reset recorder
      dispatch({ type: 'SET_MICROPHONE', payload: null });
      
      // Create new MediaRecorder
      const newRecorder = recorder.createMediaRecorder();
      if (!newRecorder) {
        throw new Error("Failed to create MediaRecorder");
      }
      
      console.log("✅ Microphone configuration completed");
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Ready });
      stateRef.current = MicrophoneState.Ready;
      
      return true;
    } catch (error) {
      console.error("❌ Error configuring microphone:", error);
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Error });
      stateRef.current = MicrophoneState.Error;
      
      return false;
    }
  };
  
  // Helper function to connect active devices
  const connectActiveDevices = async () => {
    const { audioContext, audioDevice } = services.current;
    if (!audioContext || !audioDevice) return;
    
    // Connect microphone if active
    if (state.isMicrophoneOn) {
      if (state.selectedDevices.microphone) {
        // First, connect the device to obtain the audio source
        const connected = await audioDevice.connectDevice(state.selectedDevices.microphone);
        if (!connected) {
          console.warn(`⚠️ Failed to connect microphone ${state.selectedDevices.microphone}`);
        } else {
          // Now, connect the source to the correct channel in the merger
          const sources = audioDevice.getSources();
          const source = sources[state.selectedDevices.microphone];
          if (source) {
            audioContext.connectMicrophoneSource(source.source);
            console.log("✅ Microphone source connected to merger");
          }
        }
      } else {
        // Create silent source for the microphone channel
        const silentDeviceId = audioDevice.createSilentSource(0, getPrimaryUser());
        if (silentDeviceId) {
          console.log("🔇 Created silent source for channel 0 (microphone)");
          // Connect silent source to the microphone channel
          const sources = audioDevice.getSources();
          const source = sources[silentDeviceId];
          if (source) {
            audioContext.connectMicrophoneSource(source.source);
          }
        }
      }
    } else {
      // If microphone is disabled, ensure it is disconnected
      audioContext.disconnectMicrophoneSource();
    }
    
    // Connect system audio if active
    if (state.isSystemAudioOn) {
      if (state.selectedDevices.systemAudio) {
        // First, connect the device to obtain the audio source
        const connected = await audioDevice.connectDevice(state.selectedDevices.systemAudio);
        if (!connected) {
          console.warn(`⚠️ Failed to connect system audio ${state.selectedDevices.systemAudio}`);
        } else {
          // Now, connect the source to the correct channel in the merger
          const sources = audioDevice.getSources();
          const source = sources[state.selectedDevices.systemAudio];
          if (source) {
            audioContext.connectSystemAudioSource(source.source);
            console.log("✅ System audio source connected to merger");
          }
        }
      } else {
        // Create silent source for the system audio channel
        const silentDeviceId = audioDevice.createSilentSource(1, "");
        if (silentDeviceId) {
          console.log("🔇 Created silent source for channel 1 (system audio)");
          // Connect silent source to the system audio channel
          const sources = audioDevice.getSources();
          const source = sources[silentDeviceId];
          if (source) {
            audioContext.connectSystemAudioSource(source.source);
          }
        }
      }
    } else {
      // If system audio is disabled, ensure it is disconnected
      audioContext.disconnectSystemAudioSource();
    }
  };
  
  // Start recording
  const startMicrophone = useCallback(async () => {
    console.log("🎙️ Starting microphone...");
    
    try {
      const { audioContext, recorder } = services.current;
      if (!audioContext || !recorder) {
        throw new Error("Services not initialized");
      }
      
      // If not configured, configure first
      if (stateRef.current === MicrophoneState.NotSetup) {
        await setupMicrophone();
      }
      
      // Update state
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Opening });
      stateRef.current = MicrophoneState.Opening;
      
      // Configure AudioContext
      audioContext.setupAudioContext();
      
      // Connect active devices
      await connectActiveDevices();
      
      // Check if we have a recorder
      let currentRecorder = recorder.getCurrentRecorder();
      if (!currentRecorder) {
        currentRecorder = recorder.createMediaRecorder();
        if (!currentRecorder) {
          throw new Error("Failed to create MediaRecorder");
        }
      }
      
      // Start recording
      recorder.startRecording();
      
      console.log("✅ Microphone started successfully");
    } catch (error) {
      console.error("❌ Error starting microphone:", error);
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Error });
      stateRef.current = MicrophoneState.Error;
    }
  }, []);
  
  // Stop recording
  const stopMicrophone = useCallback(async (forceReset: boolean = false) => {
    console.log("🛑 Stopping microphone...");
    
    try {
      const { recorder } = services.current;
      if (!recorder) return;
      
      // Update state
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Stopping });
      stateRef.current = MicrophoneState.Stopping;
      
      // Stop recording
      recorder.stopRecording();
      
      // Reset if necessary
      if (forceReset) {
        resetAudioSystem(false);
      }
      
      console.log("✅ Microphone stopped successfully");
    } catch (error) {
      console.error("❌ Error stopping microphone:", error);
      dispatch({ type: 'SET_STATE', payload: MicrophoneState.Error });
      stateRef.current = MicrophoneState.Error;
    }
  }, []);
  
  // Reset the entire audio system
  const resetAudioSystem = async (autoRestart: boolean = false) => {
    console.log("🧹 Resetting audio system...");
    
    try {
      const { audioContext, audioDevice, recorder } = services.current;
      if (!audioContext || !audioDevice) return;
      
      // Stop recording
      if (recorder) {
        recorder.stopRecording();
      }
      
      // Clear sources
      const sources = audioDevice.getSources();
      Object.keys(sources).forEach(deviceId => {
        audioDevice.disconnectDevice(deviceId);
      });
      
      // Close AudioContext
      await audioContext.closeAudioContext();
      
      // Reset state
      dispatch({ type: 'RESET_STATE' });
      stateRef.current = MicrophoneState.NotSetup;
      
      console.log("✅ Audio system reset successfully");  
      
      // Restart if necessary
      if (autoRestart) {
        setTimeout(() => setupMicrophone(), 500);
      }
    } catch (error) {
      console.error("❌ Error resetting audio system:", error);
    }
  };
  
  // Get current microphone state
  const getCurrentMicrophoneState = useCallback(() => {
    return stateRef.current;
  }, []);
  
  // Generate test WAV file for analysis
  const generateTestWAV = async (): Promise<ChannelAnalysis | null> => {
    const { audioContext } = services.current;
    if (!audioContext) return null;
    
    const context = audioContext.getAudioContext();
    if (!context) return null;
    
    // Use the native sample rate of the context
    const sampleRate = context.sampleRate;
    const duration = 0.5; // 500ms
    const samples = Math.floor(sampleRate * duration);
    
    const buffer = context.createBuffer(2, samples, sampleRate);
    
    // Generate different tones in each channel
    for (let channel = 0; channel < 2; channel++) {
      const channelData = buffer.getChannelData(channel);
      const frequency = channel === 0 ? 440 : 880; // A4 for channel 0, A5 for channel 1
      
      for (let i = 0; i < samples; i++) {
        const t = i / sampleRate;
        channelData[i] = 0.5 * Math.sin(2 * Math.PI * frequency * t);
      }
    }
    
    // Analyze the generated buffer
    const analysis: ChannelAnalysis = {
      channelCount: 2,
      totalSamples: samples,
      sampleRate: sampleRate,
      durationSeconds: duration,
      channels: [
        { avgVolume: 0.5, peakVolume: 0.5, hasAudio: true },
        { avgVolume: 0.5, peakVolume: 0.5, hasAudio: true }
      ]
    };
    
    return analysis;
  };
  
  // Functions adapted to match the expected signatures of the IMicrophoneContext interface
  
  // Define microphone state - adapted for SetStateAction
  const setIsMicrophoneOn: Dispatch<SetStateAction<boolean>> = useCallback((value) => {
    const prevValue = state.isMicrophoneOn;
    const newValue = typeof value === 'function' ? value(prevValue) : value;
    
    // If no change, do nothing
    if (prevValue === newValue) return;
    
    console.log(`🎙️ Microphone state changed: ${prevValue} -> ${newValue}`);
    
    // Update state
    dispatch({ type: 'SET_MICROPHONE_ON', payload: newValue });
    
    // Use try/catch to facilitate diagnosis
    try {
      const { audioContext, audioDevice } = services.current;
      if (!audioContext) {
        throw new Error("AudioContext não disponível");
      }
      if (!audioDevice) {
        throw new Error("AudioDevice não disponível");
      }
      
      // If activating the microphone
      if (newValue) {
        console.log("🔵 Activating microphone channel");
        
        // Ensure the AudioContext is configured and active
        audioContext.setupAudioContext();
        
        const status = audioContext.getConnectionStatus();
        console.log(`🔍 Connection status - before activation: Microphone = ${status.microphone}, System = ${status.systemAudio}`);
        
        // Check if we have a microphone device selected
        let microphoneDeviceId = state.selectedDevices.microphone;
        
        // If no device selected, try selecting the first available
        if (!microphoneDeviceId) {
          console.log("🔍 No microphone device selected, trying to select the first available");
          const { microphoneDevices } = audioDevice.filterDevicesForUI();
          
          if (microphoneDevices.length > 0) {
            microphoneDeviceId = microphoneDevices[0].deviceId;
            console.log(`✅ Automatically selected: ${microphoneDevices[0].label}`);
            
            // Update state with the selected device
            dispatch({ 
              type: 'SET_SELECTED_DEVICES', 
              payload: { microphone: microphoneDeviceId } 
            });
          } else {
            console.warn("⚠️ No microphone device available to select");
          }
        }
        
        // Connect the selected microphone device, if available
        if (microphoneDeviceId) {
          console.log(`🔄 Trying to connect microphone: ${microphoneDeviceId}`);
          
          // Using async/await directly (chained promises may lose context)
          (async () => {
            const connected = await audioDevice.connectDevice(microphoneDeviceId!);
            
            console.log(`🔄 Trying to connect microphone ${microphoneDeviceId}: ${connected ? 'Success' : 'Failure'}`);
            
            if (connected) {
              // Get the audio source and connect it to the microphone channel (0)
              const sources = audioDevice.getSources();
              const source = sources[microphoneDeviceId!];
              
              if (source) {
                audioContext.connectMicrophoneSource(source.source);
                console.log("✅ Success: Microphone source connected to merger");
                
                const finalStatus = audioContext.getConnectionStatus();
                console.log(`🔍 Final connection status: Microphone = ${finalStatus.microphone}, System = ${finalStatus.systemAudio}`);
              } else {
                console.error("❌ Error: Microphone source not found after connection");
              }
            }
          })();
        } else {
          console.log("⚠️ No microphone device selected or available, creating silent source");
          
          // Create silent source for the microphone channel
          const silentDeviceId = audioDevice.createSilentSource(0, getPrimaryUser());
          if (silentDeviceId) {
            console.log("🔇 Created silent source for channel 0 (microphone)");
            // Connect silent source to the microphone channel
            const sources = audioDevice.getSources();
            const source = sources[silentDeviceId];
            if (source) {
              audioContext.connectMicrophoneSource(source.source);
              console.log("✅ Silent source connected to microphone channel");
            }
          }
        }
      }
      // If deactivating the microphone
      else {
        console.log("🔴 Deactivating microphone channel");
        
        // Debug of status before disconnection
        const statusBefore = audioContext.getConnectionStatus();
        console.log(`🔍 Status of connections - before disconnection: Microphone = ${statusBefore.microphone}, System = ${statusBefore.systemAudio}`);
        
        // Disconnect the microphone source from the merger - now synchronous
        audioContext.disconnectMicrophoneSource();
        
        // Debug of status after disconnection from the merger
        const statusAfterMerger = audioContext.getConnectionStatus();
        console.log(`🔍 Status after disconnecting from merger: Microphone = ${statusAfterMerger.microphone}, System = ${statusAfterMerger.systemAudio}`);
        
        // If the device is connected, disconnect
        if (state.selectedDevices.microphone) {
          console.log(`🔄 Disconnecting microphone device: ${state.selectedDevices.microphone}`);
          audioDevice.disconnectDevice(state.selectedDevices.microphone);
          console.log("✅ Success: Microphone device disconnected");
        }
        
        // Debug of final status
        const statusAfter = audioContext.getConnectionStatus();
        console.log(`🔍 Final connection status: Microphone = ${statusAfter.microphone}, System = ${statusAfter.systemAudio}`);
      }
    } catch (error) {
      console.error("❌ Error handling microphone:", error);
    }
  }, [state.isMicrophoneOn, state.selectedDevices.microphone]);
  
  // Define audio system state - adapted for SetStateAction
  const setIsSystemAudioOn: Dispatch<SetStateAction<boolean>> = useCallback((value) => {
    const prevValue = state.isSystemAudioOn;
    const newValue = typeof value === 'function' ? value(prevValue) : value;
    
    // If no change, do nothing
    if (prevValue === newValue) return;
    
    console.log(`🔊 Alterando estado do áudio do sistema: ${prevValue} -> ${newValue}`);
    
    // Update state
    dispatch({ type: 'SET_SYSTEM_AUDIO_ON', payload: newValue });
    
    // Uso do try/catch para facilitar diagnóstico
    try {
      const { audioContext, audioDevice } = services.current;
      if (!audioContext) {
        throw new Error("AudioContext não disponível");
      }
      if (!audioDevice) {
        throw new Error("AudioDevice não disponível");
      }
      
      // If activating the system audio
      if (newValue) {
        console.log("🔵 Activating system audio channel");
        
        // Ensure the AudioContext is configured and active
        audioContext.setupAudioContext();
        
        const status = audioContext.getConnectionStatus();
        console.log(`🔍 Status of connections - before activation: Microphone = ${status.microphone}, System = ${status.systemAudio}`);
        
        // Verify if we have a system audio device selected
        let systemAudioDeviceId = state.selectedDevices.systemAudio;
        
        // If no device selected, try selecting the first available
        if (!systemAudioDeviceId) {
          console.log("🔍 No system audio device selected, trying to select the first available");
          const { systemAudioDevices } = audioDevice.filterDevicesForUI();
          
          if (systemAudioDevices.length > 0) {
            systemAudioDeviceId = systemAudioDevices[0].deviceId;
            console.log(`✅ Automatically selected: ${systemAudioDevices[0].label}`);
            
            // Update state with the selected device
            dispatch({ 
              type: 'SET_SELECTED_DEVICES', 
              payload: { systemAudio: systemAudioDeviceId } 
            });
          } else {
            console.warn("⚠️ No system audio device available to select");
          }
        }
        
        // Connect the selected system audio device, if available
        if (systemAudioDeviceId) {
          console.log(`🔄 Trying to connect system audio: ${systemAudioDeviceId}`);
          
          // Using async/await directly
          (async () => {
            const connected = await audioDevice.connectDevice(systemAudioDeviceId!);
            
            console.log(`🔄 Trying to connect system audio ${systemAudioDeviceId}: ${connected ? 'Success' : 'Failure'}`);
            
            if (connected) {
              // Get the audio source and connect it to the system audio channel (1)
              const sources = audioDevice.getSources();
              const source = sources[systemAudioDeviceId!];
              
              if (source) {
                audioContext.connectSystemAudioSource(source.source);
                console.log("✅ Success: System audio source connected to merger");
                
                const finalStatus = audioContext.getConnectionStatus();
                console.log(`🔍 Final connection status: Microphone = ${finalStatus.microphone}, System = ${finalStatus.systemAudio}`);
              } else {
                console.error("❌ Error: System audio source not found after connection");
              }
            }
          })();
        } else {
          console.log("⚠️ No system audio device selected or available, creating silent source");
          
          // Create silent source for the system audio channel
          const silentDeviceId = audioDevice.createSilentSource(1, "");
          if (silentDeviceId) {
            console.log("🔇 Created silent source for channel 1 (system)");
            // Connect silent source to the system audio channel
            const sources = audioDevice.getSources();
            const source = sources[silentDeviceId];
            if (source) {
              audioContext.connectSystemAudioSource(source.source);
              console.log("✅ Silent source connected to system audio channel");
            }
          }
        }
      } 
      // If deactivating the system audio
      else {
        console.log("🔴 Deactivating system audio channel");
        
        // Debug of status before disconnection
        const statusBefore = audioContext.getConnectionStatus();
        console.log(`🔍 Status of connections - before disconnection: Microphone = ${statusBefore.microphone}, System = ${statusBefore.systemAudio}`);
        
        // Disconnect the system audio source from the merger - now synchronous
        audioContext.disconnectSystemAudioSource();
        
        // Debug of status after disconnection from the merger
        const statusAfterMerger = audioContext.getConnectionStatus();
        console.log(`🔍 Status after disconnecting from merger: Microphone = ${statusAfterMerger.microphone}, System = ${statusAfterMerger.systemAudio}`);
        
        // If the device is connected, disconnect
        if (state.selectedDevices.systemAudio) {
          console.log(`🔄 Disconnecting system audio device: ${state.selectedDevices.systemAudio}`);
          audioDevice.disconnectDevice(state.selectedDevices.systemAudio);
          console.log("✅ Success: System audio device disconnected");
        }
        
        // Debug of final status
        const statusAfter = audioContext.getConnectionStatus();
        console.log(`🔍 Final connection status: Microphone = ${statusAfter.microphone}, System = ${statusAfter.systemAudio}`);
      }
    } catch (error) {
      console.error("❌ Error handling system audio:", error);
    }
  }, [state.isSystemAudioOn, state.selectedDevices.systemAudio]);
  
  // Define selected devices - adapted for SetStateAction
  const setSelectedDevices: Dispatch<SetStateAction<SelectedDevices>> = useCallback((value) => {
    if (typeof value === 'function') {
      const newDevices = value(state.selectedDevices);
      dispatch({ type: 'SET_SELECTED_DEVICES', payload: newDevices });
    } else {
      dispatch({ type: 'SET_SELECTED_DEVICES', payload: value });
    }
  }, [state.selectedDevices]);
  
  // Context value to be provided
  const contextValue: IMicrophoneContext = {
    microphone: state.microphone,
    startMicrophone,
    stopMicrophone,
    setupMicrophone,
    resetAudioSystem,
    microphoneState: state.microphoneState,
    getCurrentMicrophoneState,
    audioDevices: state.audioDevices,
    selectedDevices: state.selectedDevices,
    setSelectedDevices,
    disconnectSource,
    handleDeviceChange,
    setIsMicrophoneOn,
    setIsSystemAudioOn,
    isMicrophoneOn: state.isMicrophoneOn,
    isSystemAudioOn: state.isSystemAudioOn,
    speakerMappings: state.speakerMappings,
    generateTestWAV
  };
  
  return (
    <MicrophoneContext.Provider value={contextValue}>
      {children}
    </MicrophoneContext.Provider>
  );
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// RecorderService.ts
// Implementation of the audio recording service

import { IAudioContextService } from "../interfaces/IAudioContextService";
import { MicrophoneState } from "../interfaces/IMicrophoneContext";
import { IRecorderService } from "../interfaces/IRecorderService";

// Configuration constants for audio recording
const RECORDER_CONFIG = {
  TIMESLICE: 2000,  // milliseconds between audio chunks
  AUDIO_BIT_RATE: 256000,  // 16 bits * 16000Hz * 1 channel = 256kbps (aligned with Deepgram sample_rate)
  MIME_TYPES: [
    "audio/wav",              // WAV - uncompressed format with headers (more stable for testing)
    "audio/webm;codecs=pcm",  // PCM uncompressed - ideal for STT (alternative if WAV fails)
    "audio/webm",             // WebM - modern format supported and widely compatible
    "audio/ogg;codecs=opus"   // Ogg Opus - last resort
  ]
};

export class RecorderService implements IRecorderService {
  private audioContextService: IAudioContextService;
  private setMicrophoneState: (state: MicrophoneState) => void;
  private setMicrophone: (recorder: MediaRecorder | null) => void;
  private microphone: MediaRecorder | null = null;
  private activeMimeType: string | null = null;

  constructor(
    audioContextService: IAudioContextService,
    setMicrophoneState: (state: MicrophoneState) => void,
    setMicrophone: (recorder: MediaRecorder | null) => void
  ) {
    this.audioContextService = audioContextService;
    this.setMicrophoneState = setMicrophoneState;
    this.setMicrophone = setMicrophone;
  }

  /**
   * Returns the current recorder, if it exists
   */
  getCurrentRecorder(): MediaRecorder | null {
    return this.microphone;
  }

  /**
   * Creates a new MediaRecorder
   */
  createMediaRecorder(): MediaRecorder | null {
    try {
      // Get the audio destination node
      const destination = this.validateAudioDestination();
      if (!destination) return null;

      // Create the MediaRecorder with the best available format
      const recorder = this.createOptimalRecorder(destination.stream);
      if (!recorder) return null;

      // Configure events and store references
      this.configureRecorderEvents(recorder);
      this.microphone = recorder;
      this.setMicrophone(recorder);
      
      console.log(`✅ MediaRecorder created with format: ${this.activeMimeType}`);
      return recorder;
    } catch (error) {
      console.error("❌ Error creating MediaRecorder:", error);
      return null;
    }
  }

  /**
   * Starts the recording
   */
  startRecording(): void {
    if (!this.microphone) {
      console.log("🎤 [COGNITIVE-RECORDER] Attempting to start recording without a MediaRecorder");
      return;
    }

    if (this.microphone.state === "recording") {
      console.log("ℹ️ [COGNITIVE-RECORDER] Recording already in progress for cognitive input.");
      return;
    }

    try {
      console.log("🎤 [COGNITIVE-RECORDER] Starting audio capture for brain memory...");
      this.microphone.start(RECORDER_CONFIG.TIMESLICE);
    } catch (error) {
      console.log("❌ [COGNITIVE-RECORDER] Error starting audio capture for brain memory:", error);
      this.setMicrophoneState(MicrophoneState.Error);
      this.resetRecorder();
    }
  }

  /**
   * Stops the recording
   */
  stopRecording(): void {
    if (!this.microphone) {
      console.log("ℹ️ No recording in progress to stop");
      return;
    }

    if (this.microphone.state !== "recording") {
      console.log(`ℹ️ Recorder not recording (state: ${this.microphone.state})`);
      return;
    }

    try {
      console.log("🛑 [COGNITIVE-RECORDER] Stopping audio capture for brain memory...");
      this.microphone.stop();
      this.resetRecorder();
    } catch (error) {
      console.log("❌ [COGNITIVE-RECORDER] Error stopping audio capture for brain memory:", error);
      this.setMicrophoneState(MicrophoneState.Error);
      this.resetRecorder();
    }
  }

  /**
   * Configures the MediaRecorder events
   * Public implementation for compatibility with IRecorderService
   */
  configureRecorderEvents(recorder: MediaRecorder): void {
    // Process audio data
    recorder.ondataavailable = this.handleAudioData.bind(this);

    // State change events
    recorder.onstart = () => {
      console.log("🎤 [COGNITIVE-RECORDER] Recording started");
      this.setMicrophoneState(MicrophoneState.Open);
    };

    recorder.onpause = () => {
      console.log("⏸️ Recording paused");
      this.setMicrophoneState(MicrophoneState.Stopped);
    };

    recorder.onresume = () => {
      console.log("▶️ Recording resumed");
      this.setMicrophoneState(MicrophoneState.Open);
    };

    recorder.onstop = () => {
      console.log("⏹️ Recording stopped");
      this.setMicrophoneState(MicrophoneState.Stopped);
    };

    recorder.onerror = (event) => {
      console.error("🎤 [COGNITIVE-RECORDER] Error in MediaRecorder:", event);
      this.setMicrophoneState(MicrophoneState.Error);
    };
  }

  // --- Private Helper Methods ---

  /**
   * Validates and returns the audio destination node
   */
  private validateAudioDestination(): MediaStreamAudioDestinationNode | null {
    const destination = this.audioContextService.getDestination();
    if (!destination) {
      console.error("❌ [COGNITIVE-RECORDER] No audio destination available for recording");
      return null;
    }

    // Check if there are audio tracks
    if (destination.stream.getAudioTracks().length === 0) {
      console.error("❌ [COGNITIVE-RECORDER] No audio tracks available for recording");
      return null;
    }

    return destination;
  }

  /**
   * Creates a MediaRecorder with the best available format
   */
  private createOptimalRecorder(stream: MediaStream): MediaRecorder | null {
    // Try creating the MediaRecorder with the preferred formats
    for (const mimeType of RECORDER_CONFIG.MIME_TYPES) {
      if (MediaRecorder.isTypeSupported(mimeType)) {
        try {
          const options: MediaRecorderOptions = {
            mimeType,
            audioBitsPerSecond: RECORDER_CONFIG.AUDIO_BIT_RATE
          };
          
          console.log(`🎤 [COGNITIVE-RECORDER] Using format: ${mimeType}`);
          const recorder = new MediaRecorder(stream, options);
          this.activeMimeType = mimeType;
          return recorder;
        } catch (err) {
          console.warn(`⚠️ Format ${mimeType} failed:`, err);
        }
      }
    }

    // Final attempt without specifying format
    try {
      console.log("⚠️ Using default browser format");
      const recorder = new MediaRecorder(stream);
      this.activeMimeType = recorder.mimeType;
      return recorder;
    } catch (err) {
      console.error("❌ [COGNITIVE-RECORDER] Unable to create MediaRecorder:", err);
      return null;
    }
  }

  /**
   * Processes the received audio data
   */
  private async handleAudioData(event: BlobEvent): Promise<void> {
    if (event.data.size <= 0) return;

    try {
      // Convert the Blob to ArrayBuffer
      const arrayBuffer = await event.data.arrayBuffer();
      
      // Convert to Uint8Array as expected by the API - enviar diretamente sem processamento
      const audioData = new Uint8Array(arrayBuffer);
      
      // Log the format being used (only for debugging)
      if (Math.random() < 0.05) { // Reduce log frequency
        console.log(`🎤 [COGNITIVE-RECORDER] Sending ${audioData.length} bytes of audio (format: ${this.activeMimeType || 'unknown'})`);
      }
      
      // Send the audio without additional modifications
      if (window.electronAPI) {
        window.electronAPI.sendAudioChunk(audioData);
        
        // Occasional log for debugging
        if (Math.random() < 0.05) {
          console.log("🎤 [COGNITIVE-RECORDER] Audio data received for brain memory:", audioData);
        }
      } else {
        console.warn("⚠️ Electron API not available for audio transmission");
      }
    } catch (error) {
      console.error("❌ [COGNITIVE-RECORDER] Error processing audio data:", error);
    }
  }

  /**
   * Resets the recorder after use
   */
  private resetRecorder(): void {
    // Use setTimeout to ensure pending operations complete
    setTimeout(() => {
      this.microphone = null;
      this.setMicrophone(null);
      this.activeMimeType = null;
    }, 100);
  }

} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { createContext, useContext, useState } from 'react';

interface Settings {
  deepgramModel: string;
  showInterimResults: boolean;
}

const defaultSettings: Settings = {
  deepgramModel: 'nova-2',
  showInterimResults: false
};

const SettingsContext = createContext<{
  settings: Settings;
  updateSettings: (newSettings: Partial<Settings>) => void;
}>({
  settings: defaultSettings,
  updateSettings: () => {}
});

export const useSettings = () => useContext(SettingsContext);

export const SettingsProvider: React.FC<{children: React.ReactNode}> = ({ children }) => {
  const [settings, setSettings] = useState<Settings>(defaultSettings);
  
  const updateSettings = (newSettings: Partial<Settings>) => {
    setSettings(prev => ({ ...prev, ...newSettings }));
  };
  
  return (
    <SettingsContext.Provider value={{ settings, updateSettings }}>
      {children}
    </SettingsContext.Provider>
  );
}; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// index.ts
// Exportation of the transcription context

export { TranscriptionProvider, useTranscription } from './TranscriptionContext';
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// TranscriptionContext.tsx
// Context for managing transcriptions

import React, { createContext, useCallback, useContext, useState } from 'react';

interface TranscriptionContextType {
  texts: {
    transcription: string;
    aiResponse: string;
  };
  setTexts: React.Dispatch<React.SetStateAction<{ transcription: string; aiResponse: string }>>;
  updateTranscription: (text: string, isFinal: boolean) => void;
  clearTranscription: () => void;
}

const TranscriptionContext = createContext<TranscriptionContextType | null>(null);

export function useTranscription() {
  return useContext(TranscriptionContext);
}

export function TranscriptionProvider({ children }: { children: React.ReactNode }) {
  const [texts, setTexts] = useState({
    transcription: '',
    aiResponse: '',
  });

  // Function to update transcription with new text
  const updateTranscription = useCallback((text: string, isFinal: boolean) => {
    if (!text.trim()) return;
    
    setTexts(prev => {
      // For final text, add a paragraph or send for processing
      if (isFinal) {
        return {
          ...prev,
          transcription: prev.transcription ? `${prev.transcription}\n${text}` : text,
        };
      } 
      // For temporary text (while speaking), update the end of the transcription
      else {
        // Find the last line (where we added the temporary text)
        const lines = prev.transcription.split('\n');
        const lastLineIndex = lines.length - 1;
        
        // Update only the last line
        if (lastLineIndex >= 0) {
          lines[lastLineIndex] = text;
        } else {
          lines.push(text);
        }
        
        return {
          ...prev,
          transcription: lines.join('\n'),
        };
      }
    });
  }, []);

  // Function to clear the transcription
  const clearTranscription = useCallback(() => {
    setTexts(prev => ({
      ...prev,
      transcription: '',
    }));
  }, []);

  const value = {
    texts,
    setTexts,
    updateTranscription,
    clearTranscription,
  };

  return (
    <TranscriptionContext.Provider value={value}>
      {children}
    </TranscriptionContext.Provider>
  );
} // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// CognitionLogContext.tsx
// Context for managing cognition logs in the application

import React, { createContext, useContext, useEffect, useState } from 'react';
import { CognitionEvent } from './deepgram/types/CognitionEvent';
import symbolicCognitionTimelineLogger from './deepgram/services/utils/SymbolicCognitionTimelineLoggerSingleton';
import { CognitionLogExporter } from './deepgram/services/utils/CognitionLogExporter';
import cognitionLogExporterFactory from './deepgram/services/utils/CognitionLogExporterFactory';

/**
 * Interface for the cognition log context
 */
interface CognitionLogContextType {
  /** Current cognition events */
  events: CognitionEvent[];
  /** Available exporters */
  exporters: CognitionLogExporter[];
  /** Clear all cognition events */
  clearEvents: () => void;
  /** Export events using the specified exporter */
  exportEvents: (exporterLabel: string) => void;
}

/**
 * Context for managing cognition logs in the application
 */
const CognitionLogContext = createContext<CognitionLogContextType | null>(null);

/**
 * Provider for the cognition log context
 */
export const CognitionLogProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [events, setEvents] = useState<CognitionEvent[]>([]);
  const [exporters] = useState<CognitionLogExporter[]>(
    cognitionLogExporterFactory.getExporters()
  );

  // Updates events periodically
  useEffect(() => {
    const updateEvents = () => {
      setEvents([...symbolicCognitionTimelineLogger.getTimeline()]);
    };

    // Initial update
    updateEvents();

    // Update every second
    const interval = setInterval(updateEvents, 1000);
    return () => clearInterval(interval);
  }, []);

  // Clears all events
  const clearEvents = () => {
    symbolicCognitionTimelineLogger.clear();
    setEvents([]);
  };

  // Exports events using the specified exporter
  const exportEvents = (exporterLabel: string) => {
    const exporter = exporters.find(e => e.label === exporterLabel);
    if (exporter) {
      exporter.export(events);
    }
  };

  return (
    <CognitionLogContext.Provider value={{ events, exporters, clearEvents, exportEvents }}>
      {children}
    </CognitionLogContext.Provider>
  );
};

/**
 * Hook to access the cognition log context
 */
export const useCognitionLog = (): CognitionLogContextType => {
  const context = useContext(CognitionLogContext);
  if (!context) {
    throw new Error('useCognitionLog must be used within a CognitionLogProvider');
  }
  return context;
};

export default CognitionLogContext;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// index.ts
// Export organization of all contexts and services

// Interfaces
export * from './deepgram/interfaces/deepgram/IDeepgramContext';
export * from './deepgram/interfaces/deepgram/IDeepgramService';
export * from './interfaces/IAudioContextService';
export * from './interfaces/IAudioDeviceService';
export * from './interfaces/IMicrophoneContext';
export * from './interfaces/IRecorderService';

// Microphone services
export { AudioContextService } from './microphone/AudioContextService';
export { AudioDeviceService } from './microphone/AudioDeviceService';
export { default as MicrophoneProvider, useMicrophone } from './microphone/MicrophoneContextProvider';
export { RecorderService } from './microphone/RecorderService';

// Deepgram services
export { DeepgramAudioAnalyzer } from './deepgram/AudioAnalyzer';
export { DeepgramConnectionService } from './deepgram/DeepgramConnectionService';
export { default as DeepgramProvider, useDeepgram } from './deepgram/DeepgramContextProvider';
export { DeepgramTranscriptionService } from './deepgram/services/DeepgramTranscriptionService';

// Transcription context
export { TranscriptionProvider, useTranscription } from './transcription';

// Enums
export { ConnectionState } from './deepgram/interfaces/deepgram/IDeepgramService';
export { MicrophoneEvents, MicrophoneState } from './interfaces/IMicrophoneContext';
 // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { createContext, useState, useEffect } from "react";
import { getOption, setOption, subscribeToStorageChanges, STORAGE_KEYS } from "../../services/StorageService";

/**
 * Hook personalizado para gerenciar configurações do storage
 * Permite um "espelhamento neural" de uma configuração global do storage
 */
function useStorageSetting<T>(key: string, defaultValue: T): [T, (value: T) => void] {
  // Estado local que espelha o valor no storage
  const [value, setValue] = useState<T>(() => {
    const storedValue = getOption<T>(key);
    return storedValue !== undefined ? storedValue : defaultValue;
  });

  // Efeito para sincronizar com mudanças no storage
  useEffect(() => {
    const handleStorageChange = (changedKey: string, newValue: any) => {
      if (changedKey === key && newValue !== undefined) {
        console.log(`💾 [STORAGE-MIRROR] Valor atualizado para ${key}:`, newValue);
        setValue(newValue as T);
      }
    };
    
    // Inscreve para mudanças e retorna função de limpeza
    return subscribeToStorageChanges(handleStorageChange);
  }, [key]);

  // Função para atualizar o valor
  const updateValue = (newValue: T) => {
    setValue(newValue);
    setOption(key, newValue);
  };

  return [value, updateValue];
}

// =====================================
// Contexto de linguagem do Orch-OS
// =====================================

export const LanguageContext = createContext<{  
  language: string;
  setLanguage: (language: string) => void;
}>({ language: "pt-BR", setLanguage: () => {} });

export const LanguageProvider = ({ children }: { children: React.ReactNode }) => {
  // Espelha configurações diretamente do storage neural
  const [language, setLanguageInternal] = useStorageSetting(STORAGE_KEYS.DEEPGRAM_LANGUAGE, "pt-BR");
  
  // Simples wrapper para manter a API consistente
  const setLanguage = (newLanguage: string) => {
    setLanguageInternal(newLanguage);
    console.log(`🌎 LanguageContext: Idioma alterado para ${newLanguage}`);
  };
  
  // O contexto agora é sempre atualizado automaticamente graças ao espelhamento neural

  return (
    <LanguageContext.Provider value={{ language, setLanguage }}>
      {children}
    </LanguageContext.Provider>
  );
};// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import { useEffect, useRef, useState } from "react";
import { MicrophoneState, useMicrophone } from "../../context";

const AudioVisualizer = ({
  width = 500,
  height = 150,
}: {
  width?: number;
  height?: number;
}) => {
  // Analysis frequency (visualizer update rate)
  const ANALYSIS_FREQUENCY = 100; // 100ms = 10fps
  
  // Initialize canvas and context
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const canvasCtxRef = useRef<CanvasRenderingContext2D | null>(null);
  
  // State to control display
  const [isConnected, setIsConnected] = useState(false);
  
  // Access microphone context
  const { microphoneState } = useMicrophone();
  
  // Initialize visualizer and analysis
  useEffect(() => {
    // Verify if the canvas exists
    if (!canvasRef.current) return;
    
    // Get 2D context for canvas drawing
    const canvas = canvasRef.current;
    canvasCtxRef.current = canvas.getContext('2d');
    
    // Configure canvas size
    canvas.width = width;
    canvas.height = height;
    
    // Clear canvas (draw black background)
    if (canvasCtxRef.current) {
      canvasCtxRef.current.fillStyle = 'rgba(0, 0, 0, 0.2)';
      canvasCtxRef.current.fillRect(0, 0, width, height);
    }
  }, [width, height]);
  
  // Observe microphone state to update visualizer
  useEffect(() => {
    // Visualize only when the microphone is active
    const shouldVisualize = microphoneState === MicrophoneState.Open;
    setIsConnected(shouldVisualize);
    
    // Reference for animation timer
    let animationTimer: ReturnType<typeof setInterval> | null = null;
    
    // Start visualization when active
    if (shouldVisualize) {
      animationTimer = setInterval(() => renderFrame(), ANALYSIS_FREQUENCY);
    }
    
    // Function to render a frame of the visualization
    const renderFrame = () => {
      if (!canvasCtxRef.current) return;
      
      const ctx = canvasCtxRef.current;
      const canvas = canvasRef.current;
      if (!canvas) return;
      
      // Fadeout effect
      ctx.fillStyle = 'rgba(0, 0, 0, 0.1)';
      ctx.fillRect(0, 0, canvas.width, canvas.height);
      
      // Generate simulated visualization (when there are no real audio data)
      
      // Draw color lines simulating audio activity
      const centerY = canvas.height / 2;
      const maxHeight = canvas.height * 0.4;
      
      // Generate "random" values but with some continuity for simulation
      const now = Date.now() / 1000;
      
      for (let x = 0; x < canvas.width; x++) {
        // Generate "waves" using sine functions with different frequencies and phases
        // to create a more natural and less random effect
        const phase1 = now * 2;
        const phase2 = now * 3.7;
        
        // Combination of waves to create a more complex shape
        const value = (
          Math.sin(x * 0.02 + phase1) * 0.5 + 
          Math.sin(x * 0.04 + phase2) * 0.3
        ) * maxHeight;
        
        // Height varies based on generated value
        const height = Math.abs(value);
        
        // Y position (centered vertically)
        const y = centerY - (value < 0 ? 0 : height);
        
        // Color based on position and height (gradient effect)
        const hue = (x / canvas.width) * 180 + 180; // blue to purple
        ctx.fillStyle = `hsla(${hue}, 80%, 50%, 0.8)`;
        
        // Draw bar
        ctx.fillRect(x, y, 1, height);
      }
    };
    
    // Clear timer on unmount or state change
    return () => {
      if (animationTimer) {
        clearInterval(animationTimer);
      }
    };
  }, [microphoneState, width, height]);
  
  return (
    <div className="relative w-full rounded-lg overflow-hidden bg-gradient-to-b from-black/20 to-black/50 backdrop-blur-sm">
      <canvas
        ref={canvasRef}
        className="w-full h-auto"
        width={width}
        height={height}
      />
      
      {/* Overlay status when not connected */}
      {!isConnected && (
        <div className="absolute inset-0 flex items-center justify-center text-white/60 text-sm">
          Audio visualizer inactive
        </div>
      )}
    </div>
  );
};

export default AudioVisualizer; // SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { CognitionEvent } from "../../../../context/deepgram/types/CognitionEvent";

/**
 * Pure UI component for rendering a single cognition event in the timeline.
 * No logic, no mutation, only visual representation.
 * SOLID: Single Responsibility Principle.
 */
export interface CognitionEventUIProps {
  event: CognitionEvent;
  idx: number;
  onClick?: (event: CognitionEvent) => void;
  duration?: { value: string; color: string };
}

// Define the background colors for each event type to match modal colors
const eventColors: Record<string, string> = {
  raw_prompt: "bg-blue-700",
  temporary_context: "bg-purple-700",
  neural_signal: "bg-amber-700",
  neural_collapse: "bg-pink-700",
  symbolic_retrieval: "bg-green-700",
  fusion_initiated: "bg-orange-700",
  symbolic_context_synthesized: "bg-indigo-700",
  gpt_response: "bg-red-700",
  emergent_patterns: "bg-teal-700",
};

// Use the colors to create the icons
const eventTypeIcons: Record<string, React.ReactNode> = {
  raw_prompt: (
    <div
      className={`w-9 h-9 rounded-full ${eventColors.raw_prompt} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🧠
    </div>
  ),
  temporary_context: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.temporary_context} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🧠
    </div>
  ),
  neural_signal: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.neural_signal} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      ⚡
    </div>
  ),
  neural_collapse: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.neural_collapse} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      💥
    </div>
  ),
  symbolic_retrieval: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.symbolic_retrieval} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🔍
    </div>
  ),
  fusion_initiated: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.fusion_initiated} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🔥
    </div>
  ),
  symbolic_context_synthesized: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.symbolic_context_synthesized} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🔗
    </div>
  ),
  gpt_response: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.gpt_response} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      💬
    </div>
  ),
  emergent_patterns: (
    <div
      className={`w-8 h-8 rounded-full ${eventColors.emergent_patterns} flex items-center justify-center text-white overflow-hidden text-xl`}
    >
      🌊
    </div>
  ),
};

// Format event type for display
const formatEventType = (type: string): string => {
  // Para Neural Signal e Raw Prompt, usamos formatos específicos
  if (type === "neural_signal") return "Neural Signal";
  if (type === "raw_prompt") return "Raw Prompt";

  // Para eventos regulares, usamos Title Case
  return type
    .replace(/_/g, " ")
    .split(" ")
    .map((word) => word.charAt(0).toUpperCase() + word.slice(1))
    .join(" ");
};

export const CognitionEventUI: React.FC<CognitionEventUIProps> = React.memo(
  ({ event, idx, onClick, duration }) => {
    // Verificar se é um evento de resposta GPT (geralmente o último)
    const isGptResponse = event.type === "gpt_response";
    const icon = eventTypeIcons[event.type] || (
      <span className="text-gray-400">⬛</span>
    );

    // Extract information based on event type
    const isNeuralSignal = event.type === "neural_signal";
    const isRawPrompt = event.type === "raw_prompt";

    // For neural signals, extract core and intensity
    const neuralCore = isNeuralSignal ? event.core : null;
    // Garantir que intensity seja tratado como número e convertido para percentual
    // Multiplicamos por 100 para converter o valor decimal (0-1) para percentual (0-100)
    const intensityValue = isNeuralSignal
      ? Math.round(event.intensity * 100)
      : 0;
    const neuralValue = isNeuralSignal ? `${intensityValue}%` : null;

    /**
     * Sistema de cores para todos os cores neurais do Orch-OS
     * - Cores escolhidas para refletir os domínios cognitivos específicos
     * - Codificação por cores facilita o reconhecimento de padrões (processamento pré-atentivo)
     * - Cores com boa visibilidade e associações semânticas relevantes
     */
    const getNeuralTypeColor = (type: string | null) => {
      if (!type) return "text-amber-400"; // Default

      switch (type.toLowerCase()) {
        // Cores cognitivos/memória
        case "memory":
          return "text-orange-400"; // Memória - Laranja (lembranças/recuperação)
        case "associative":
          return "text-amber-500"; // Associativo - Âmbar escuro (conexões entre memórias)

        // Cores emocionais/afetivos
        case "valence":
          return "text-amber-400"; // Valência - Âmbar (emoções/afeto)
        case "soul":
          return "text-rose-400"; // Alma - Rosa avermelhado (essência emocional profunda)

        // Cores de linguagem/comunicação
        case "language":
          return "text-indigo-400"; // Linguagem - Índigo (expressão verbal)

        // Cores de metacognição/planejamento
        case "metacognitive":
          return "text-teal-400"; // Metacognitivo - Turquesa (reflexão sobre pensamento)
        case "planning":
          return "text-sky-400"; // Planejamento - Azul céu (estruturação futura)
        case "will":
          return "text-blue-500"; // Vontade - Azul forte (força de intenção)

        // Cores inconscientes/arquetípicos
        case "unconscious":
          return "text-fuchsia-400"; // Inconsciente - Fuchsia (conteúdos não conscientes)
        case "archetype":
          return "text-purple-400"; // Arquétipo - Roxo (padrões universais)
        case "shadow":
          return "text-violet-500"; // Sombra - Violeta escuro (aspectos reprimidos)

        // Cores físicos/encarnação
        case "body":
          return "text-green-500"; // Corpo - Verde escuro (sensações físicas)

        // Cores sociais/identidade
        case "social":
          return "text-blue-400"; // Social - Azul (interações interpessoais)
        case "self":
          return "text-green-400"; // Self - Verde (identidade/auto-percepção)

        // Cores criativos/intuitivos
        case "creativity":
          return "text-pink-400"; // Criatividade - Rosa (inovação/expressão)
        case "intuition":
          return "text-purple-500"; // Intuição - Roxo intenso (conhecimento direto)

        // Default para qualquer outro tipo
        default:
          return "text-amber-400"; // Default - Âmbar como fallback
      }
    };

    /**
     * Cores otimizadas para barras de progresso com maior visibilidade
     * Correspondendo ao sistema de cores do texto, mas com ajustes de intensidade
     * para garantir boa visibilidade contra o fundo escuro
     */
    /**
     * Cores otimizadas para barras de progresso com alta visibilidade
     * Especialmente contra fundos escuros (bg-gray-800/60) usado no tema do Orch-OS
     *
     * Cores problemáticas contra fundos escuros foram ajustadas para:
     * 1. Tons de azul/roxo/violeta - aumentados para 500/600 (roxo e azul escuro absorvem mais luz)
     * 2. Tons de verde escuro - aumentados para 500/600 (verde escuro tem menor percepção luminosa)
     * 3. Cores neutras - aumentadas para 500/600 (cinzas e cores neutras se misturam com o fundo)
     */
    const getProgressBarColor = (type: string | null) => {
      if (!type) return "bg-amber-500"; // Default usando 500 para maior visibilidade

      switch (type.toLowerCase()) {
        // Cores cognitivos/memória
        case "memory":
          return "bg-orange-500"; // Memória - Laranja 500 (boa visibilidade natural)
        case "associative":
          return "bg-amber-600"; // Associativo - Âmbar 600 (mais intenso para maior contraste)

        // Cores emocionais/afetivos
        case "valence":
          return "bg-amber-500"; // Valência - Âmbar 500 (boa visibilidade natural)
        case "soul":
          return "bg-rose-500"; // Alma - Rosa 500 (bom contraste natural)

        // Cores de linguagem/comunicação
        case "language":
          return "bg-indigo-600"; // Linguagem - Índigo 600 (intensificado pois índigo tem baixa visibilidade)

        // Cores de metacognição/planejamento
        case "metacognitive":
          return "bg-teal-500"; // Metacognitivo - Teal 500 (boa visibilidade)
        case "planning":
          return "bg-sky-500"; // Planejamento - Sky 500 (boa visibilidade)
        case "will":
          return "bg-blue-600"; // Vontade - Azul 600 (intensificado para contraste)

        // Cores inconscientes/arquetípicos - Grupo mais problemático para visibilidade
        case "unconscious":
          return "bg-fuchsia-600"; // Inconsciente - Fuchsia 600 (cores roxas/violetas precisam de maior intensidade)
        case "archetype":
          return "bg-purple-600"; // Arquétipo - Roxo 600 (intensificado para garantir visibilidade)
        case "shadow":
          return "bg-violet-600"; // Sombra - Violeta 600 (intensificado para contraste)

        // Cores físicos/encarnação
        case "body":
          return "bg-green-600"; // Corpo - Verde 600 (verdes escuros precisam de maior intensidade)

        // Cores sociais/identidade
        case "social":
          return "bg-blue-600"; // Social - Azul 600 (intensificado pois azul escuro tem menor contraste)
        case "self":
          return "bg-green-500"; // Self - Verde 500 (verde médio tem boa visibilidade)

        // Cores criativos/intuitivos
        case "creativity":
          return "bg-pink-500"; // Criatividade - Rosa 500 (bom contraste natural)
        case "intuition":
          return "bg-purple-600"; // Intuição - Roxo 600 (intensificado para visibilidade)

        // Default para qualquer outro tipo
        default:
          return "bg-amber-500"; // Default usando âmbar 500 para melhor visibilidade
      }
    };

    // Format time as HH:MM:SS.mmm manually to include milliseconds
    const formattedTime = event.timestamp
      ? (() => {
          const date = new Date(event.timestamp);
          const hours = date.getHours().toString().padStart(2, "0");
          const minutes = date.getMinutes().toString().padStart(2, "0");
          const seconds = date.getSeconds().toString().padStart(2, "0");
          const ms = date.getMilliseconds().toString().padStart(3, "0");
          return `${hours}:${minutes}:${seconds}.${ms}`;
        })()
      : "";

    return (
      <div
        className={`relative rounded-xl bg-gray-900/90 p-3 shadow-lg transition-colors cursor-pointer ${
          onClick ? "hover:bg-gray-800/90" : ""
        } ${isGptResponse ? "mb-6 pb-4" : ""}`}
        onClick={() => onClick && onClick(event)}
      >
        {/* Vertical timeline line - visible between consecutive events */}
        {idx > 0 && (
          <div className="absolute left-7 top-0 bottom-0 w-0.5 bg-cyan-800/30 -translate-y-3 h-4" />
        )}
        <div
          className="group flex items-stretch bg-gray-900/90 hover:bg-gray-800/90 rounded-lg border border-gray-800/80 hover:border-gray-700 transition-all duration-200 overflow-hidden cursor-pointer"
          tabIndex={0}
          role="button"
          aria-label={event.type}
        >
          {/* Left column with icon */}
          <div className="w-14 flex-shrink-0 flex items-center justify-center text-2xl select-none border-r border-gray-800/30">
            {icon}
          </div>

          {/* Content section */}
          <div className="flex-1 px-3 py-2">
            <div className="flex items-center justify-between mb-1">
              {/* Event type label */}
              <div className="flex items-center">
                <span
                  className={`inline-block px-2 py-0.5 rounded text-xs font-medium text-white ${
                    eventColors[event.type] || "bg-gray-800"
                  }`}
                >
                  {formatEventType(event.type)}
                </span>
              </div>

              {/* Timestamp and duration */}
              <div className="flex items-center space-x-2">
                <span className="whitespace-nowrap font-mono text-xs text-gray-400">
                  {formattedTime}
                </span>
                {duration && (
                  <span className="flex items-center">
                    <span
                      className={`whitespace-nowrap font-mono text-xs ${duration.color}`}
                    >
                      {duration.value}
                    </span>
                  </span>
                )}
              </div>
            </div>

            {/* Content based on event type */}
            {isNeuralSignal && neuralValue ? (
              <div className="text-base font-medium flex items-center gap-1.5">
                <span className={`${getNeuralTypeColor(neuralCore)} font-mono`}>
                  {neuralCore ? neuralCore.toLowerCase() : ""}
                </span>
                <span className={`${getNeuralTypeColor(neuralCore)} font-mono`}>
                  {neuralValue}
                </span>
                {/* Barra de intensidade junto do percentual */}
                <div className="w-28 h-2 bg-gray-800/60 rounded-full overflow-hidden ml-1">
                  {/* Aplicando largura dinâmica com classes CSS */}
                  <div
                    className={`h-full ${getProgressBarColor(neuralCore)}`}
                    style={{ width: `${intensityValue}%` }}
                  />
                </div>
              </div>
            ) : isRawPrompt ? (
              <div className="text-base text-gray-300 font-medium">
                {event.content
                  ? event.content.substring(0, 50) +
                    (event.content.length > 50 ? "..." : "")
                  : ""}
              </div>
            ) : event.type === "gpt_response" ? (
              <div className="text-base text-gray-300 font-medium">
                {event.response
                  ? event.response.substring(0, 50) +
                    (event.response.length > 50 ? "..." : "")
                  : ""}
              </div>
            ) : (
              <div className="text-base text-gray-300 font-medium">
                {/* Display summary based on event type */}
                {event.type === "symbolic_retrieval"
                  ? `${event.matchCount || 0} matches found`
                  : event.type === "neural_collapse"
                  ? `Collapse: ${event.selectedCore}`
                  : event.type === "emergent_patterns"
                  ? `${event.patterns?.length || 0} patterns`
                  : ""}
              </div>
            )}
          </div>
        </div>
      </div>
    );
  }
);

// Exporta o componente memoizado
export default CognitionEventUI;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from 'react';
import { CognitionEvent } from '../../../../context/deepgram/types/CognitionEvent';
import { CognitionEventUI } from '../Event/CognitionEventUI';
import styles from '../../CognitionTimeline.module.css';

/**
 * UI component for a visually grouped cognition cycle (RawPrompt → ... → GPTResponse),
 * accordion-exclusivo: só um ciclo aberto por vez, com scroll interno.
 * SOLID: Single Responsibility Principle.
 */
export interface CognitionLogGroupUIProps {
  events: CognitionEvent[]; // Events in this group, in order
  groupIdx: number;
  onEventClick?: (event: CognitionEvent) => void;
  getDuration?: (idx: number) => { value: string; color: string };
  expanded?: boolean;
  onExpand?: () => void;
}

const MAX_GROUP_HEIGHT = 450; // px, ajustado para o container global

export const CognitionLogGroupUI: React.FC<CognitionLogGroupUIProps> = ({ events, groupIdx, onEventClick, getDuration, expanded = false, onExpand }) => {
  if (!events.length) return null;
  const isExpanded = expanded;
  return (
    <div className="mt-3 mb-0">
      <button
        type="button"
        className="w-full relative flex items-center justify-between px-5 py-2.5 bg-[#121936] rounded-2xl border border-cyan-400/80 shadow-[0_0_6px_rgba(34,211,238,0.2)] group transition-all duration-300 focus:outline-none focus:ring-1 focus:ring-cyan-400 focus:ring-opacity-50 min-h-[48px]"
        onClick={onExpand}
        aria-expanded={isExpanded}
        aria-controls={`cognitive-group-${groupIdx}`}
        aria-label={`Cognitive Cycle ${groupIdx + 1} with ${events.length} events. Click to ${isExpanded ? 'collapse' : 'expand'}`}
      >
        {/* Left side with icon and title */}
        <div className="flex items-center gap-4">
          {/* Neural network brain icon with glow */}
          <div className="relative flex-shrink-0 flex items-center justify-center h-9 w-9 rounded-full shadow-[0_0_8px_rgba(34,211,238,0.2)]">
            <svg 
              className="w-7 h-7 text-cyan-300 drop-shadow-[0_0_3px_rgba(34,211,238,0.4)]" 
              viewBox="0 0 24 24" 
              fill="none" 
              xmlns="http://www.w3.org/2000/svg"
              aria-hidden="true"
            >
              <path d="M22 14.3529C22 19.0968 17.0751 23 10.9999 23C5.33439 23 1 19.2323 1 14.8235C1 10.9343 3.35098 8.34576 6.70584 7.21671C8.73574 6.5246 9.74705 6.17854 10.5291 5.15517C11.3112 4.13179 11.5349 2.74591 11.9823 0" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M16 10.5C16 8.01472 13.9853 6 11.5 6C9.01472 6 7 8.01472 7 10.5C7 12.9853 9.01472 15 11.5 15C13.9853 15 16 12.9853 16 10.5Z" stroke="currentColor" strokeWidth="1.5"/>
              <path d="M11.5 6V4.5" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M16 10.5H17.5" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M11.5 15V16.5" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M7 10.5H5.5" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M14.5281 7.47192L15.5888 6.41116" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M14.5281 13.5281L15.5888 14.5888" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M8.47192 13.5281L7.41116 14.5888" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
              <path d="M8.47192 7.47192L7.41116 6.41116" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round"/>
            </svg>
            <div className="absolute inset-0 z-0 bg-gradient-to-br from-[#121936]/90 to-cyan-950/20 rounded-full"></div>
          </div>
          
          {/* Title */}
          <h3 className="text-white font-bold text-xl tracking-wide select-none">
            Cognitive Cycle #{groupIdx + 1}
          </h3>
        </div>
        
        {/* Right side with badge and arrow */}
        <div className="flex items-center gap-3">
          {/* Event count badge matching Export buttons */}
          <div className="px-4 py-1 rounded-full bg-cyan-400 text-[#121936] text-base font-bold flex items-center justify-center min-w-[2.5rem] shadow-[0_0_5px_rgba(34,211,238,0.3)]">
            {events.length}
          </div>
          
          {/* Chevron that changes direction when expanded */}
          <div className="flex items-center justify-center w-6 h-6 text-cyan-400">
            <span className={`text-xl font-bold transition-transform duration-200 inline-block ${isExpanded ? 'rotate-90' : 'rotate-0'}`}>
              &gt;
            </span>
          </div>
        </div>
      </button>
      <div
        id={`cognitive-group-${groupIdx}`}
        className={expanded ? `${styles.fadeIn}` : ''}
        style={{
          // Quando expandido, não limitar altura (controle feito pelo container externo)
          maxHeight: expanded ? 'none' : 0,
          overflow: 'hidden',
          transition: expanded ? 'opacity 0.2s' : 'max-height 0.4s cubic-bezier(0.16,1,0.3,1), opacity 0.2s',
          opacity: expanded ? 1 : 0,
        }}
      >
        {expanded && (
          <div
            className="px-2"
            style={{
              maxHeight: MAX_GROUP_HEIGHT - 30,
              overflowY: 'auto',
              scrollbarWidth: 'thin',
              boxSizing: 'border-box',
              WebkitOverflowScrolling: 'touch'
            }}
          >
            {events.map((event, idx) => (
              <CognitionEventUI
                key={event.timestamp + '-' + event.type + '-' + groupIdx + '-' + idx}
                event={event}
                idx={idx}
                onClick={onEventClick}
                duration={getDuration ? getDuration(idx) : undefined}
              />
            ))}
          </div>
        )}
      </div>
    </div>
  );
};

export default CognitionLogGroupUI;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useState, useRef, useEffect } from 'react';
import { CognitionEvent } from './../../../context/deepgram/types/CognitionEvent';
import CognitionLogGroupUI from './Group/CognitionLogGroupUI';
import { VariableSizeList as List } from 'react-window';

/**
 * UI utility: Groups events into cognitive cycles (RawPrompt to GPTResponse).
 * Exclusive accordion: only one cycle open at a time, but allows all collapsed.
 * SOLID: Single Responsibility Principle.
 */
export class CognitionLogGrouper {
  static groupEvents(events: CognitionEvent[]): CognitionEvent[][] {
    const groups: CognitionEvent[][] = [];
    let current: CognitionEvent[] = [];
    for (const event of events) {
      if (event.type === 'raw_prompt' && current.length > 0) {
        groups.push(current);
        current = [];
      }
      current.push(event);
      if (event.type === 'gpt_response') {
        groups.push(current);
        current = [];
      }
    }
    if (current.length > 0) groups.push(current);
    return groups;
  }
}

export interface CognitionTimelineGroupedUIProps {
  events: CognitionEvent[];
  onEventClick?: (event: CognitionEvent) => void;
  getDuration?: (idx: number) => { value: string; color: string };
}

const MAX_CONTAINER_HEIGHT = 600; // px, ajustável conforme layout do app

export const CognitionTimelineGroupedUI: React.FC<CognitionTimelineGroupedUIProps> = ({ events, onEventClick, getDuration }) => {
  // Memoize event grouping to avoid unnecessary recalculations
  const groups = React.useMemo(() => CognitionLogGrouper.groupEvents(events), [events]);
  const [expandedIdx, setExpandedIdx] = useState<number | null>(0); // null = todos colapsados
  const listRef = useRef<List>(null);

  // Fixed heights for each group
  const COLLAPSED_HEIGHT = 60;
  const EXPANDED_HEIGHT = 380;

  // Calculate item height based on expansion state
  const getItemSize = (index: number) => (expandedIdx === index ? EXPANDED_HEIGHT : COLLAPSED_HEIGHT);

  // When expanding/collapsing, update state and force height recalculation
  // useCallback prevents unnecessary function recreations on each render
  const handleExpand = React.useCallback((idx: number) => {
    setExpandedIdx(current => (current === idx ? null : idx));
    // O efeito abaixo força o recálculo das alturas
    setTimeout(() => {
      if (listRef.current) listRef.current.resetAfterIndex(0);
    }, 0);
  }, []);

  if (!groups.length) return <div className="text-indigo-300 text-sm p-4">No cognitive logs available</div>;

  // If any group is expanded, render ONLY it and navigation controls
  if (expandedIdx !== null) {
    return (
      <div className="h-[450px] w-full overflow-y-auto overflow-scrolling-touch">
        {/* Neural-Symbolic Header - design reflecting cognitive orchestration */}
        <div
          className="flex items-center justify-between px-3 py-2 bg-gradient-to-r from-indigo-950/80 via-indigo-900/70 to-indigo-950/80 backdrop-blur-md border-b border-cyan-400/30 shadow-sm rounded-t-lg transition-all"
          style={{
            backgroundImage: `radial-gradient(circle at 15% 50%, rgba(99, 102, 241, 0.08) 0%, transparent 25%), 
                           radial-gradient(circle at 85% 30%, rgba(6, 182, 212, 0.08) 0%, transparent 25%)`
          }}
        >
          {/* Neural Button (Back) - cognitive return action */}
          <button
            onClick={() => setExpandedIdx(null)}
            className="group flex items-center gap-1 text-cyan-300 hover:text-cyan-100 font-medium text-xs px-2 py-1 rounded-md transition-all duration-200 focus:outline-none focus-visible:ring-1 focus-visible:ring-cyan-400 active:scale-95 border border-transparent hover:border-cyan-500/20 hover:bg-cyan-900/10"
          >
            <span className="text-base transition-transform duration-200 group-hover:-translate-x-0.5">←</span>
            <span>Back</span>
          </button>

          {/* Cognitive Center - focal point of neural information */}
          <div className="relative flex-1 text-center px-1">
            <div className="absolute left-0 right-0 top-1/2 -translate-y-1/2 h-[1px] bg-gradient-to-r from-transparent via-cyan-500/20 to-transparent"></div>
            <span className="relative inline-flex items-center gap-1.5 px-3 py-0.5 bg-indigo-900/40 rounded-full border border-indigo-700/30 shadow-inner">
              <span className="inline-block w-1.5 h-1.5 rounded-full bg-cyan-400/80"></span>
              <span className="text-cyan-100 font-medium text-sm tracking-wide">
                Cycle #{expandedIdx + 1}
              </span>
              <span className="text-cyan-300/70 text-xs font-normal">of {groups.length}</span>
            </span>
          </div>

          {/* Neural Navigation Controls - directional synapses */}
          <div className="flex gap-1">
            <button
              onClick={() => setExpandedIdx(curr => Math.max(0, (curr || 0) - 1))}
              disabled={expandedIdx <= 0}
              className="w-6 h-6 flex items-center justify-center text-cyan-300 hover:text-cyan-100 disabled:text-cyan-800/50 disabled:hover:bg-transparent font-medium rounded-full transition-all duration-200 hover:bg-cyan-900/20 focus:outline-none focus-visible:ring-1 focus-visible:ring-cyan-400 active:scale-95 border border-transparent hover:border-cyan-500/20"
              aria-label="Previous cycle"
            >
              <span className="text-sm">←</span>
            </button>
            <button
              onClick={() => setExpandedIdx(curr => Math.min(groups.length - 1, (curr || 0) + 1))}
              disabled={expandedIdx >= groups.length - 1}
              className="w-6 h-6 flex items-center justify-center text-cyan-300 hover:text-cyan-100 disabled:text-cyan-800/50 disabled:hover:bg-transparent font-medium rounded-full transition-all duration-200 hover:bg-cyan-900/20 focus:outline-none focus-visible:ring-1 focus-visible:ring-cyan-400 active:scale-95 border border-transparent hover:border-cyan-500/20"
              aria-label="Next cycle"
            >
              <span className="text-sm">→</span>
            </button>
          </div>
        </div>
        <div className="pt-5">
          <CognitionLogGroupUI
            key={expandedIdx}
            events={groups[expandedIdx]}
            groupIdx={expandedIdx}
            onEventClick={onEventClick}
            getDuration={getDuration}
            expanded={true}
            onExpand={() => handleExpand(expandedIdx)}
          />
          {/* Removido div fantasma que criava espaço em branco excessivo */}
        </div>
      </div>
    );
  }

  // If all are collapsed, use virtualization
  return (
    <div style={{ height: 450, width: '100%' }}>
      <List
        ref={listRef}
        height={450}
        width={'100%'}
        itemCount={groups.length}
        itemSize={getItemSize}
        overscanCount={4}
      >
        {({ index, style }) => {
          // Apply mb-4 except on the last card for tighter spacing like in the image
          const isLast = index === groups.length - 1;
          return (
            <div style={{ ...style, marginBottom: isLast ? 0 : 16 }}>
              <CognitionLogGroupUI
                key={index}
                events={groups[index]}
                groupIdx={index}
                onEventClick={onEventClick}
                getDuration={getDuration}
                expanded={expandedIdx === index}
                onExpand={() => handleExpand(index)}
              />
            </div>
          );
        }}
      </List>
    </div>
  );
};

export default CognitionTimelineGroupedUI;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React from "react";
import { CognitionEvent } from "../../context/deepgram/types/CognitionEvent";
import { SymbolicInsight } from "../../context/deepgram/types/SymbolicInsight";

interface CognitionDetailModalProps {
  isOpen: boolean;
  onClose: () => void;
  event: CognitionEvent | null;
}

const CognitionDetailModal: React.FC<CognitionDetailModalProps> = ({
  isOpen,
  onClose,
  event,
}) => {
  if (!event) return null;

  // Prevent scrolling on body when modal is open
  React.useEffect(() => {
    const toggleBodyScroll = (disable: boolean) => {
      if (disable) {
        document.body.classList.add("overflow-hidden");
      } else {
        document.body.classList.remove("overflow-hidden");
      }
    };

    toggleBodyScroll(isOpen);

    return () => {
      toggleBodyScroll(false);
    };
  }, [isOpen]);

  // Handle click outside modal to close
  const handleBackdropClick = (e: React.MouseEvent<HTMLDivElement>) => {
    if (e.target === e.currentTarget) {
      onClose();
    }
  };

  // Format event type for display
  const formatEventType = (type: string): string => {
    return type
      .replace(/_/g, " ")
      .split(" ")
      .map((word) => word.charAt(0).toUpperCase() + word.slice(1))
      .join(" ");
  };

  // Get event color for display
  const getEventTypeStyles = (
    type: string
  ): { bgColor: string; textColor: string; borderColor: string } => {
    switch (type) {
      case "raw_prompt":
        return {
          bgColor: "bg-blue-700",
          textColor: "text-blue-100",
          borderColor: "border-blue-500",
        };
      case "temporary_context":
        return {
          bgColor: "bg-purple-700",
          textColor: "text-purple-100",
          borderColor: "border-purple-500",
        };
      case "neural_signal":
        return {
          bgColor: "bg-amber-700",
          textColor: "text-amber-100",
          borderColor: "border-amber-500",
        };
      case "neural_collapse":
        return {
          bgColor: "bg-pink-700",
          textColor: "text-pink-100",
          borderColor: "border-pink-500",
        };
      case "symbolic_retrieval":
        return {
          bgColor: "bg-green-700",
          textColor: "text-green-100",
          borderColor: "border-green-500",
        };
      case "fusion_initiated":
        return {
          bgColor: "bg-orange-700",
          textColor: "text-orange-100",
          borderColor: "border-orange-500",
        };
      case "symbolic_context_synthesized":
        return {
          bgColor: "bg-indigo-700",
          textColor: "text-indigo-100",
          borderColor: "border-indigo-500",
        };
      case "gpt_response":
        return {
          bgColor: "bg-red-700",
          textColor: "text-red-100",
          borderColor: "border-red-500",
        };
      case "emergent_patterns":
        return {
          bgColor: "bg-teal-700",
          textColor: "text-teal-100",
          borderColor: "border-teal-500",
        };
      default:
        return {
          bgColor: "bg-gray-700",
          textColor: "text-gray-100",
          borderColor: "border-gray-600",
        };
    }
  };

  // Helper function to render insight items
  const renderInsights = (insights: any[]) => {
    return (
      <ul className="space-y-2 mt-3">
        {insights.map((insight, i) => {
          // Handle string insights
          if (typeof insight === 'string') {
            return (
              <li key={i} className="text-sm bg-gray-800/50 p-2 rounded border border-gray-700">
                {insight}
              </li>
            );
          }
          // Handle SymbolicInsight objects
          else if (insight && typeof insight === 'object' && 'type' in insight) {
            const insightObj = insight as SymbolicInsight;

            return (
              <li key={i} className="text-sm bg-gray-800/50 p-2 rounded border border-gray-700">
                <span className="text-teal-400 font-medium">{insightObj.type}</span>: {' '}
                <span>{String(insightObj.content || '')}</span>
              </li>
            );
          }
          // Fallback for other structures
          return (
            <li key={i} className="text-sm bg-gray-800/50 p-2 rounded border border-gray-700">
              {JSON.stringify(insight)}
            </li>
          );
        })}
      </ul>
    );
  };

  // Helper function to render patterns
  const renderPatterns = (patterns: string[]) => {
    return (
      <ul className="space-y-2 mt-3">
        {patterns.map((pattern, i) => (
          <li
            key={i}
            className="text-sm bg-gray-800/50 p-2 rounded border border-gray-700 text-teal-300"
          >
            {pattern}
          </li>
        ))}
      </ul>
    );
  };

  // Render event details based on type
  const renderEventDetails = (): React.ReactNode => {
    if (!event) return null;

    switch (event.type) {
      case "raw_prompt":
        return (
          <div className="space-y-4">
            <div className="text-lg text-blue-300 font-medium">Raw Prompt</div>
            <div className="p-4 bg-gray-800/50 rounded-lg border border-blue-500/30 whitespace-pre-wrap text-gray-200">
              {event.content}
            </div>
          </div>
        );

      case "temporary_context":
        return (
          <div className="space-y-4">
            <div className="text-lg text-purple-300 font-medium">
              Temporary Context
            </div>
            <div className="p-4 bg-gray-800/50 rounded-lg border border-purple-500/30 whitespace-pre-wrap text-gray-200">
              {event.context}
            </div>
          </div>
        );

      case "neural_signal":
        return (
          <div className="space-y-4">
            <div className="text-lg text-amber-300 font-medium">
              Neural Signal
            </div>

            <div className="flex flex-wrap gap-4">
              <div className="bg-gray-800/50 p-3 rounded-lg border border-amber-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Core</div>
                <div className="text-white font-medium">{event.core}</div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-amber-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Intensity</div>
                <div className="text-amber-400 font-medium text-xl">
                  {Math.round(event.intensity * 100)}%
                </div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-amber-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Top K</div>
                <div className="text-white font-medium">{event.topK}</div>
              </div>
            </div>

            <div>
              <div className="text-gray-400 mb-2">Symbolic Query</div>
              <div className="p-4 bg-gray-800/50 rounded-lg border border-amber-500/30 whitespace-pre-wrap text-gray-200">
                {JSON.stringify(event.symbolic_query, null, 2)}
              </div>
            </div>

            {Object.keys(event.params).length > 0 && (
              <div>
                <div className="text-gray-400 mb-2">Parameters</div>
                <div className="p-4 bg-gray-800/50 rounded-lg border border-amber-500/30 whitespace-pre-wrap text-gray-200 font-mono text-sm">
                  {JSON.stringify(event.params, null, 2)}
                </div>
              </div>
            )}
          </div>
        );

      case "emergent_patterns":
        return (
          <div className="space-y-4">
            <div className="text-lg text-teal-300 font-medium">
              Emergent Patterns
            </div>

            {event.metrics && (
              <div className="flex flex-wrap gap-4">
                {event.metrics.archetypalStability !== undefined && (
                  <div className="bg-gray-800/50 p-3 rounded-lg border border-teal-500/30 flex-1">
                    <div className="text-gray-400 text-sm mb-1">
                      Archetypal Stability
                    </div>
                    <div className="text-teal-400 font-medium text-xl">
                      {event.metrics.archetypalStability.toFixed(3)}
                    </div>
                  </div>
                )}

                {event.metrics.cycleEntropy !== undefined && (
                  <div className="bg-gray-800/50 p-3 rounded-lg border border-amber-500/30 flex-1">
                    <div className="text-gray-400 text-sm mb-1">
                      Cycle Entropy
                    </div>
                    <div className="text-amber-400 font-medium text-xl">
                      {event.metrics.cycleEntropy.toFixed(3)}
                    </div>
                  </div>
                )}

                {event.metrics.insightDepth !== undefined && (
                  <div className="bg-gray-800/50 p-3 rounded-lg border border-cyan-500/30 flex-1">
                    <div className="text-gray-400 text-sm mb-1">
                      Insight Depth
                    </div>
                    <div className="text-cyan-400 font-medium text-xl">
                      {event.metrics.insightDepth.toFixed(3)}
                    </div>
                  </div>
                )}
              </div>
            )}

            {event.patterns.length > 0 ? (
              <div>
                <div className="text-gray-400 mb-2">Detected Patterns</div>
                {renderPatterns(event.patterns)}
              </div>
            ) : (
              <div className="p-4 bg-gray-800/50 rounded-lg border border-gray-700 text-gray-400 italic">
                No patterns detected
              </div>
            )}
          </div>
        );

      case "symbolic_retrieval":
        return (
          <div className="space-y-4">
            <div className="text-lg text-green-300 font-medium">
              Symbolic Retrieval
            </div>

            <div className="flex flex-wrap gap-4">
              <div className="bg-gray-800/50 p-3 rounded-lg border border-green-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Core</div>
                <div className="text-white font-medium">{event.core}</div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-green-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Memory Matches</div>
                <div className="text-white font-medium">{event.matchCount}</div>
              </div>
              
              <div className="bg-gray-800/50 p-3 rounded-lg border border-green-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Duration</div>
                <div className="text-green-400 font-medium">
                  {event.durationMs}ms
                </div>
              </div>
            </div>
            
            {event.insights && event.insights.length > 0 && (
              <div>
                <div className="text-blue-400 mb-2 font-medium">Insights</div>
                {renderInsights(event.insights)}

                {/* 🔧 CORREÇÃO: Mostrar keywords dos insights se disponíveis */}
                {event.insights.some(
                  (insight: any) =>
                    insight.keywords && insight.keywords.length > 0
                ) && (
                  <div className="mt-4">
                    <div className="text-gray-400 mb-2">Keywords</div>
                    <div className="flex flex-wrap gap-2">
                      {event.insights
                        .filter(
                          (insight: any) =>
                            insight.keywords && insight.keywords.length > 0
                        )
                        .flatMap((insight: any) => insight.keywords)
                        .filter(
                          (keyword: string, index: number, arr: string[]) =>
                            arr.indexOf(keyword) === index
                        ) // Remove duplicatas
                        .map((keyword: string, i: number) => (
                          <span
                            key={i}
                            className="px-2 py-1 bg-gray-800 rounded-full text-xs text-green-300 border border-green-500/30"
                          >
                            "{keyword}"
                          </span>
                        ))}
                    </div>
                  </div>
                )}
              </div>
            )}
          </div>
        );

      case "neural_collapse":
        return (
          <div className="space-y-4">
            <div className="text-lg text-pink-300 font-medium">
              Neural Collapse
            </div>

            <div className="flex flex-wrap gap-4">
              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Selected Core</div>
                <div className="text-white font-medium">
                  {event.selectedCore}
                </div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Process</div>
                <div className="text-white font-medium">
                  {event.isDeterministic ? "Deterministic" : "Stochastic"}
                </div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Candidates</div>
                <div className="text-white font-medium">
                  {event.numCandidates}
                </div>
              </div>
            </div>

            <div className="flex flex-wrap gap-4">
              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Emotional Weight</div>
                <div className="text-pink-400 font-medium">{event.emotionalWeight.toFixed(2)}</div>
              </div>

              <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                <div className="text-gray-400 text-sm mb-1">Contradiction Score</div>
                <div className="text-pink-400 font-medium">{event.contradictionScore.toFixed(2)}</div>
              </div>

              {event.temperature !== undefined && (
                <div className="bg-gray-800/50 p-3 rounded-lg border border-pink-500/30 flex-1">
                  <div className="text-gray-400 text-sm mb-1">Temperature</div>
                  <div className="text-pink-400 font-medium">
                    {event.temperature.toFixed(2)}
                  </div>
                </div>
              )}
            </div>

            {event.justification && (
              <div>
                <div className="text-gray-400 mb-2">Justification</div>
                <div className="p-4 bg-gray-800/50 rounded-lg border border-pink-500/30 whitespace-pre-wrap text-gray-200">
                  {event.justification}
                </div>
              </div>
            )}

            {event.insights && event.insights.length > 0 && (
              <div>
                <div className="text-blue-400 mb-2 font-medium">Insights</div>
                {renderInsights(event.insights)}
              </div>
            )}

            {event.emergentProperties &&
              event.emergentProperties.length > 0 && (
                <div>
                  <div className="text-teal-400 mb-2 font-medium">
                    Emergent Properties
                  </div>
                  {renderPatterns(event.emergentProperties)}
                </div>
              )}
          </div>
        );

      case "symbolic_context_synthesized":
        return (
          <div className="space-y-4">
            <div className="text-lg text-indigo-300 font-medium">
              Symbolic Context Synthesized
            </div>
            <div className="p-4 bg-gray-800/50 rounded-lg border border-indigo-500/30 whitespace-pre-wrap text-gray-200 font-mono text-sm">
              {JSON.stringify(event.context, null, 2)}
            </div>
          </div>
        );

      case "gpt_response":
        return (
          <div className="space-y-4">
            <div className="text-lg text-red-300 font-medium">GPT Response</div>

            <div className="p-4 bg-gray-800/50 rounded-lg border border-red-500/30 whitespace-pre-wrap text-gray-200">
              {event.response}
            </div>

            {event.symbolicTopics && event.symbolicTopics.length > 0 && (
              <div>
                <div className="text-gray-400 mb-2">Symbolic Topics</div>
                <div className="flex flex-wrap gap-2">
                  {event.symbolicTopics.map((topic: string, i: number) => (
                    <span key={i} className="px-2 py-1 bg-gray-800 rounded-full text-xs text-red-300 border border-red-500/30">
                      {topic}
                    </span>
                  ))}
                </div>
              </div>
            )}

            {event.insights && event.insights.length > 0 && (
              <div>
                <div className="text-blue-400 mb-2 font-medium">Insights</div>
                {renderInsights(event.insights)}
              </div>
            )}
          </div>
        );

      case "fusion_initiated":
        return (
          <div className="p-4 bg-gray-800/50 rounded-lg border border-orange-500/30 text-center">
            <div className="text-orange-300 font-medium text-lg mb-2">
              Fusion Process Initiated
            </div>
            <div className="text-gray-400">
              The neural-symbolic fusion process has been triggered
            </div>
          </div>
        );

      default:
        return (
          <div className="p-4 bg-gray-800/50 rounded-lg border border-gray-700 text-gray-400">
            Unknown event type: {(event as any).type}
          </div>
        );
    }
  };

  const typeStyle = getEventTypeStyles(event.type);

  return (
    <>
      {isOpen && (
        <div
          className="fixed inset-0 z-50 flex items-start justify-center overflow-y-auto bg-black/70 backdrop-blur-sm p-4 pt-[2vh] pb-[2vh]"
          onClick={handleBackdropClick}
        >
          <div
            className={`relative bg-gray-900 rounded-lg shadow-2xl max-w-3xl w-full border-t-4 flex flex-col max-h-[96vh] animate-in fade-in slide-in-from-bottom-4 duration-300 ${typeStyle.borderColor}`}
            onClick={(e) => e.stopPropagation()}
          >
            {/* Modal header */}
            <div className="flex items-center justify-between px-6 py-4 border-b border-gray-800">
              <div className="flex items-center">
                <span
                  className={`inline-block px-3 py-1 rounded text-sm font-medium mr-3 ${typeStyle.bgColor} ${typeStyle.textColor}`}
                >
                  {formatEventType(event.type)}
                </span>
                <span className="text-gray-400 text-sm">
                  {new Date(event.timestamp).toLocaleTimeString()}.
                  {new Date(event.timestamp)
                    .getMilliseconds()
                    .toString()
                    .padStart(3, "0")}
                </span>
              </div>
              <button
                onClick={onClose}
                className="text-gray-400 hover:text-white transition-colors focus:outline-none focus:ring-2 focus:ring-indigo-500 rounded-full p-1"
                aria-label="Close modal"
              >
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-6 w-6"
                  fill="none"
                  viewBox="0 0 24 24"
                  stroke="currentColor"
                >
                  <path
                    strokeLinecap="round"
                    strokeLinejoin="round"
                    strokeWidth={2}
                    d="M6 18L18 6M6 6l12 12"
                  />
                </svg>
              </button>
            </div>

            {/* Modal content - event details */}
            <div className="p-6 overflow-y-auto max-h-[calc(90vh-160px)]">
              {renderEventDetails()}
            </div>

            {/* Modal footer */}
            <div className="px-6 py-4 border-t border-gray-800 flex justify-end">
              <button
                onClick={onClose}
                className="px-4 py-2 bg-gray-800 hover:bg-gray-700 text-white rounded-md transition-colors focus:outline-none focus:ring-2 focus:ring-indigo-500"
              >
                Close
              </button>
            </div>
          </div>
        </div>
      )}
    </>
  );
};

export default CognitionDetailModal;
/* SPDX-License-Identifier: MIT OR Apache-2.0 */
/* Copyright (c) 2025 Guilherme Ferrari Brescia */

/* Animations for cognition timeline component */
.fadeIn {
  animation: fadeIn 0.2s ease-in-out;
}

.slideIn {
  animation: slideIn 0.2s cubic-bezier(0.16, 1, 0.3, 1);
}

.pulse {
  animation: pulse 2s infinite;
}

/* Modal padding */
.modalPadding {
  padding: 2vh 1rem;
}

/* Modal body content */
.modalBody {
  padding: 1.5rem;
  overflow-y: auto;
  max-height: calc(90vh - 160px);
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }
  100% {
    opacity: 1;
  }
}

@keyframes slideIn {
  0% {
    transform: translateY(10px);
    opacity: 0;
  }
  100% {
    transform: translateY(0);
    opacity: 1;
  }
}

@keyframes pulse {
  0% {
    box-shadow: 0 0 0 0 rgba(99, 102, 241, 0.4);
  }
  70% {
    box-shadow: 0 0 0 10px rgba(99, 102, 241, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(99, 102, 241, 0);
  }
}

.eventCard {
  transition: all 0.2s ease;
}

.eventCard:hover {
  transform: translateY(-1px);
}

/* Neural glow effects for different event types */
.neuralGlow {
  position: relative;
  overflow: hidden;
}

.neuralGlow::after {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: radial-gradient(circle at var(--x, 50%) var(--y, 50%), rgba(99, 102, 241, 0.15) 0%, transparent 50%);
  opacity: 0;
  transition: opacity 0.5s;
  pointer-events: none;
}

.neuralGlow:hover::after {
  opacity: 1;
}

/* Modal backdrop animation */
.modalBackdrop {
  -webkit-backdrop-filter: blur(0px);
  backdrop-filter: blur(0px);
  background-color: rgba(0, 0, 0, 0);
  transition: -webkit-backdrop-filter 0.3s ease, backdrop-filter 0.3s ease, background-color 0.3s ease;
}

.modalBackdropActive {
  -webkit-backdrop-filter: blur(5px);
  backdrop-filter: blur(5px);
  background-color: rgba(0, 0, 0, 0.7);
}

/* Modal layout styles */
.modalWrapper {
  position: fixed;
  inset: 0;
  z-index: 50;
  display: flex;
  align-items: flex-start;
  justify-content: center;
  padding: 2vh 1rem;
  background-color: rgba(0, 0, 0, 0.7);
  -webkit-backdrop-filter: blur(5px);
  backdrop-filter: blur(5px);
  overflow-y: auto;
}

.modalContent {
  position: relative;
  background-color: #1f2937;
  border-radius: 0.5rem;
  box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.25);
  max-width: 48rem;
  width: 100%;
  max-height: 96vh;
  display: flex;
  flex-direction: column;
  margin: 2vh 0;
}

/* Event type indicator dot animation */
.eventIndicator {
  position: relative;
}

.eventIndicator::before {
  content: '';
  position: absolute;
  left: -8px;
  top: 50%;
  transform: translateY(-50%);
  width: 4px;
  height: 4px;
  border-radius: 50%;
  background-color: currentColor;
  box-shadow: 0 0 0 rgba(255, 255, 255, 0.7);
  animation: indicatorPulse 2s infinite;
}

@keyframes indicatorPulse {
  0% {
    box-shadow: 0 0 0 0 rgba(255, 255, 255, 0.7);
  }
  70% {
    box-shadow: 0 0 0 4px rgba(255, 255, 255, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(255, 255, 255, 0);
  }
}
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useState } from 'react';
import { CognitionEvent } from '../../context/deepgram/types/CognitionEvent';
import CognitionTimelineGroupedUI from './ui/CognitionTimelineGroupedUI';
import CognitionDetailModal from './CognitionDetailModal';

/**
 * CognitionTimeline (UI grouped version)
 * - UI/UX lendária, disruptiva e simbólica.
 * - Não altera dados, modelo ou lógica.
 * - Segue SOLID e Clean Architecture.
 */
interface CognitionTimelineProps {
  events: CognitionEvent[];
}

export const CognitionTimeline: React.FC<CognitionTimelineProps> = ({ events }) => {
  const [selectedEvent, setSelectedEvent] = useState<CognitionEvent | null>(null);
  const [isModalOpen, setIsModalOpen] = useState(false);

  // Open modal with event details
  const openEventDetails = (event: CognitionEvent) => {
    setSelectedEvent(event);
    setIsModalOpen(true);
  };

  // Calculate duration between consecutive events (for UI only)
  const calcDuration = (events: CognitionEvent[]) => (idx: number): { value: string; color: string } => {
    if (idx === 0) return { value: '-', color: 'text-gray-500' };
    try {
      const current = new Date(events[idx].timestamp).getTime();
      const previous = new Date(events[idx - 1].timestamp).getTime();
      const diff = current - previous;
      let color = 'text-green-400';
      if (diff > 1000) color = 'text-amber-400';
      if (diff > 3000) color = 'text-red-400';
      if (diff < 1000) return { value: `+${diff}ms`, color };
      return { value: `+${(diff / 1000).toFixed(2)}s`, color };
    } catch (e) {
      return { value: '-', color: 'text-gray-500' };
    }
  };

  if (!events || events.length === 0) {
    return (
      <div className="flex items-center justify-center h-32 bg-gray-900/40 rounded-lg border border-gray-800 text-gray-400 italic">
        <div className="flex flex-col items-center">
          <svg xmlns="http://www.w3.org/2000/svg" className="h-8 w-8 mb-2 text-gray-600" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
          </svg>
          <span>No cognitive events recorded</span>
        </div>
      </div>
    );
  }

  return (
    <div className="relative">
      {/* UI grouped cognition cycles */}
      <CognitionTimelineGroupedUI
        events={events}
        onEventClick={openEventDetails}
        getDuration={calcDuration(events)}
      />
      {/* Modal for event details */}
      <CognitionDetailModal
        isOpen={isModalOpen}
        onClose={() => setIsModalOpen(false)}
        event={selectedEvent}
      />
    </div>
  );
};

export default CognitionTimeline;
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

// Exportação de todos os componentes de visualização quântica
export { QuantumSuperposition } from './QuantumSuperposition';
export { WaveCollapse } from './WaveCollapse';
export { QuantumEntanglement } from './QuantumEntanglement';
export { ProbabilityFields } from './ProbabilityFields';
export { InterferencePatterns } from './InterferencePatterns';
export { QuantumField } from './QuantumField';
export { Observer, getCorePosition, getAge } from './QuantumUtils';// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import React, { useRef } from 'react';
import { useFrame } from '@react-three/fiber';
import * as THREE from 'three';

/**
 * Quantum interference patterns
 * 
 * Na teoria Orch OR, os padrões de interferência são fundamentais para entender
 * como a informação quântica é processada nos microtúbulos:
 * 
 * 1. Demonstram o comportamento ondulatório quântico, onde ondas de probabilidade
 *    interferem construtivamente e destrutivamente
 * 
 * 2. Representam como diferentes estados de superposição interagem dentro da 
 *    estrutura dos microtúbulos
 * 
 * 3. São a base da computação quântica nos microtúbulos, permitindo o processamento
 *    paralelo massivo de informação, crucial na teoria Orch OR
 * 
 * 4. A interferência quântica conecta o processamento quântico nos microtúbulos
 *    a fenômenos de campo mais amplos na atividade neural global
 */
interface InterferencePatternsProps {
  coherence?: number;
  collapseActive?: boolean;
}

export function InterferencePatterns({ coherence = 0.3, collapseActive = false }: InterferencePatternsProps) {
  const patterns = useRef<THREE.Group>(null);
  
  // Criando conjuntos de ondas circulares que representam
  // os padrões de interferência quântica nos microtúbulos
  useFrame(({ clock }) => {
    if (patterns.current) {
      const t = clock.getElapsedTime();
      
      // Rotação lenta de todo o grupo
      // Representa a evolução temporal dos padrões de interferência
      patterns.current.rotation.x = Math.sin(t * 0.2) * 0.3;
      patterns.current.rotation.y = t * 0.1;
      
      // Animar materiais independentemente
      // Cada padrão evolui com sua própria dinâmica, mas todos estão inter-relacionados
      patterns.current.children.forEach((child, i) => {
        const material = (child as THREE.Mesh).material as THREE.MeshBasicMaterial;
        
        // Pulsação da opacidade - representa flutuações quânticas
        // Na teoria Orch OR, os estados quânticos oscilam antes do colapso
        // Opacidade dos anéis modulada por coerência global e evento OR
// Opacidade mínima muito baixa em repouso (0.01), crescendo suavemente com coherence
material.opacity = collapseActive ? 1 : (0.01 + 0.9 * coherence) + Math.sin(t * (collapseActive ? 1.2 : (0.5 + i * 0.2))) * 0.2;
        
        // Animar escala para simular propagação de onda quântica
        // Representa a propagação da informação quântica através dos microtúbulos
        child.scale.setScalar(1 + Math.sin(t * (0.3 + i * 0.1)) * 0.3);
      });
    }
  });
  
  return (
    <group ref={patterns}>
      {/* Padrões de interferência - anéis sobrepostos em múltiplos planos */}
      {Array(5).fill(0).map((_, i) => (
        <React.Fragment key={i}>
          {/* Plano horizontal - representa um plano de interferência quântica */}
          <mesh rotation={[Math.PI / 2, 0, 0]}>
            <ringGeometry args={[1.0 + i * 0.4, 1.05 + i * 0.4, 128]} />
            <meshBasicMaterial 
              // Cores alternadas representam diferentes fases de interferência
              // Azul claro/azul escuro - associados a estados coerentes de baixa energia
              color={i % 2 === 0 ? "#80FFFF" : "#8080FF"} 
              transparent 
              opacity={0.3} 
              side={THREE.DoubleSide} 
            />
          </mesh>
          
          {/* Plano vertical - interferência ortogonal ao primeiro plano */}
          <mesh rotation={[0, Math.PI / 2, 0]}>
            <ringGeometry args={[1.1 + i * 0.4, 1.15 + i * 0.4, 128]} />
            <meshBasicMaterial 
              // Rosa/amarelo - associados a estados coerentes de alta energia
              // A alternância de cores representa interferência construtiva/destrutiva
              color={i % 2 === 0 ? "#FF80FF" : "#FFFF80"} 
              transparent 
              opacity={0.3} 
              side={THREE.DoubleSide} 
            />
          </mesh>
        </React.Fragment>
      ))}
    </group>
  );
}// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

/* eslint-disable react/no-unknown-property */
import { useFrame } from '@react-three/fiber';
import React, { useCallback, useMemo, useRef } from 'react';
import * as THREE from 'three';

/**
 * Flowing quantum probability fields
 * 
 * Na teoria Orch OR de Penrose-Hameroff, os campos de probabilidade quântica são fundamentais:
 * 
 * 1. Representam a função de onda quântica distribuída através dos microtúbulos
 * 2. Demonstram as propriedades de "não-localidade" quântica do sistema neural
 * 3. Visualizam como os estados de superposição existem como campos de probabilidade
 *    antes do colapso (OR)
 * 
 * Nesta representação, cada partícula representa um componente do campo de probabilidade
 * quântica descrito pela equação de Schrödinger, antes da redução objetiva.
 */
interface ProbabilityFieldsProps {
  particleCount?: number;
  coherence?: number;
  collapseActive?: boolean;
}

const ProbabilityFields = React.memo<ProbabilityFieldsProps>(({ 
  particleCount = 150, 
  coherence = 0.3, 
  collapseActive = false 
}) => {
  const particles = useRef<THREE.Points>(null);
  
  // Criando posições iniciais para partículas
  // Cada partícula representa um "elemento" da função de onda quântica
  const positions = useMemo(() => {
    const pos = [];
    for (let i = 0; i < particleCount; i++) {
      const theta = Math.random() * Math.PI * 2;
      const phi = Math.random() * Math.PI;
      // Distribuir em volume esférico - representa o campo de probabilidade 3D
      const radius = 0.5 + Math.random() * 1.5;
      
      pos.push(
        radius * Math.sin(phi) * Math.cos(theta),
        radius * Math.sin(phi) * Math.sin(theta),
        radius * Math.cos(phi)
      );
    }
    return new Float32Array(pos);
  }, [particleCount]);
  
  // Cores para as partículas - representam diferentes estados quânticos
  // Em Orch OR, os estados quânticos envolvem diferentes configurações de elétrons
  // em proteínas tubulina, cada uma com diferentes níveis energéticos
  const colors = useMemo(() => {
    const cols = [];
    for (let i = 0; i < particleCount; i++) {
      // Gradiente de cor de azul a violeta - representa espectro de energia quântica
      // Azul: Estados de baixa energia
      // Violeta: Estados de alta energia (próximos de colapso OR)
      const h = 180 + (Math.random() * 80); // 180-260 (azul a violeta)
      const s = 60 + (Math.random() * 40);  // Saturação moderada a alta
      const l = 50 + (Math.random() * 30);  // Luminosidade média a alta
      
      const color = new THREE.Color(`hsl(${h}, ${s}%, ${l}%)`);
      cols.push(color.r, color.g, color.b);
    }
    return new Float32Array(cols);
  }, [particleCount]);

  // Optimized animation callback for probability fields
  const animateProbabilityFields = useCallback((state: { clock: { getElapsedTime: () => number } }) => {
    if (!particles.current) return;
    
    const t = state.clock.getElapsedTime();
    const positions = particles.current.geometry.attributes.position.array as Float32Array;
    
    for (let i = 0; i < positions.length; i += 3) {
      const i3 = i / 3;
      
      // Usa funções senoidais para criar movimento fluido
      // Isto simula a evolução da função de onda quântica
      const x = positions[i];
      const y = positions[i + 1];
      const z = positions[i + 2];
      
      // Cálculo de deslocamento baseado em funções senoidais entrelaçadas
      // Simula as interações não-locais dos campos quânticos
      const modulation = Math.sin(t * 0.5 + i3 * 0.1);
      const phase = t * 0.2 + i3 * 0.05;
      
      // Movimento em espiral - representa evolução da função de onda
      // que caracteriza os estados quânticos em proteínas tubulina
      positions[i] = x + Math.sin(phase + y * 0.5) * 0.01 * modulation;
      positions[i + 1] = y + Math.cos(phase + x * 0.5) * 0.01 * modulation;
      positions[i + 2] = z + Math.sin(phase * 1.5) * 0.01 * modulation;
    }
    
    particles.current.geometry.attributes.position.needsUpdate = true;
    
    // Rotação lenta do sistema de partículas
    // Representa a dinâmica global do campo quântico
    particles.current.rotation.y = t * 0.05;
    particles.current.rotation.x = Math.sin(t * 0.1) * 0.2;
  }, []);
  
  // Animação do campo de probabilidade
  // Na teoria Orch OR, os campos quânticos evoluem de acordo com a equação de Schrödinger
  // até atingirem um limiar de massa-energia para colapso gravitacional
  useFrame(animateProbabilityFields);

  // Memoized geometry for better performance
  const geometry = useMemo(() => {
    const geom = new THREE.BufferGeometry();
    geom.setAttribute('position', new THREE.BufferAttribute(positions, 3));
    geom.setAttribute('color', new THREE.BufferAttribute(colors, 3));
    return geom;
  }, [positions, colors]);

  // Memoized material for better performance
  const material = useMemo(() => {
    return new THREE.PointsMaterial({
      size: 0.05,
      vertexColors: true,
      transparent: true,
      opacity: collapseActive ? 1 : (0.03 + 0.8 * coherence),
      sizeAttenuation: true
    });
  }, [collapseActive, coherence]);
  
  return (
    <points ref={particles} geometry={geometry} material={material} />
  );
});

ProbabilityFields.displayName = 'ProbabilityFields';

export { ProbabilityFields };
// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useRef, useMemo } from 'react';
import * as THREE from 'three';
import { useFrame } from '@react-three/fiber';

/**
 * Propriedades para o componente de visualização da decoerência quântica
 */
interface QuantumDecoherenceProps {
  /** Intensidade da decoerência (0-1) */
  intensity: number;
  /** Posição do efeito no espaço 3D */
  position?: [number, number, number];
  /** Escala visual do efeito */
  scale?: number;
  /** Velocidade da animação */
  speed?: number;
  /** Direção da propagação da decoerência */
  direction?: [number, number, number];
  /** Cor base do efeito */
  color?: string;
}

/**
 * Componente que visualiza o processo de decoerência quântica
 * 
 * A decoerência quântica é o processo pelo qual estados quânticos perdem
 * suas propriedades de superposição devido à interação com o ambiente.
 * Este é um desafio fundamental para a teoria Orch-OR, que propõe que
 * os microtúbulos possuem mecanismos para proteger contra a decoerência.
 */
export const QuantumDecoherence: React.FC<QuantumDecoherenceProps> = ({
  intensity,
  position = [0, 0, 0],
  scale = 1,
  speed = 1,
  direction = [1, 0, 0],
  color = '#ff3300'
}) => {
  // Referências para os elementos visuais
  const groupRef = useRef<THREE.Group>(null!);
  const particlesRef = useRef<THREE.Points>(null!);
  const waveRef = useRef<THREE.Mesh>(null!);
  
  // Número de partículas baseado na intensidade
  const particleCount = useMemo(() => Math.round(100 * intensity), [intensity]);
  
  // Gerar geometria para as partículas de decoerência
  const particlesGeometry = useMemo(() => {
    const geometry = new THREE.BufferGeometry();
    const positions = new Float32Array(particleCount * 3);
    const sizes = new Float32Array(particleCount);
    const colors = new Float32Array(particleCount * 3);
    
    const colorObj = new THREE.Color(color);
    
    for (let i = 0; i < particleCount; i++) {
      // Posição aleatória em uma esfera
      const radius = 0.1 + Math.random() * 0.2;
      const theta = Math.random() * Math.PI * 2;
      const phi = Math.random() * Math.PI;
      
      positions[i * 3] = radius * Math.sin(phi) * Math.cos(theta);
      positions[i * 3 + 1] = radius * Math.sin(phi) * Math.sin(theta);
      positions[i * 3 + 2] = radius * Math.cos(phi);
      
      // Tamanho baseado na distância do centro (partículas mais distantes são menores)
      sizes[i] = 0.01 + 0.03 * (1 - Math.random() * 0.3);
      
      // Cores gradientes baseadas na distância
      const distance = Math.sqrt(
        positions[i * 3] ** 2 +
        positions[i * 3 + 1] ** 2 +
        positions[i * 3 + 2] ** 2
      );
      
      const colorFactor = Math.max(0, 1 - distance * 2);
      colors[i * 3] = colorObj.r * colorFactor;
      colors[i * 3 + 1] = colorObj.g * colorFactor;
      colors[i * 3 + 2] = colorObj.b * colorFactor;
    }
    
    geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
    geometry.setAttribute('size', new THREE.BufferAttribute(sizes, 1));
    geometry.setAttribute('color', new THREE.BufferAttribute(colors, 3));
    
    return geometry;
  }, [particleCount, color]);
  
  // Material para partículas de decoerência
  const particlesMaterial = useMemo(() => {
    return new THREE.PointsMaterial({
      size: 0.02,
      transparent: true,
      opacity: 0.7,
      vertexColors: true,
      blending: THREE.AdditiveBlending,
      depthWrite: false,
      sizeAttenuation: true
    });
  }, []);
  
  // Criar o material para a onda de decoerência
  const waveMaterial = useMemo(() => {
    return new THREE.MeshBasicMaterial({
      color: new THREE.Color(color).lerp(new THREE.Color('#ffffff'), 0.5),
      transparent: true,
      opacity: 0.3,
      side: THREE.DoubleSide,
      depthWrite: false,
      blending: THREE.AdditiveBlending
    });
  }, [color]);
  
  // Animação da decoerência
  useFrame((state) => {
    if (!groupRef.current || !particlesRef.current || !waveRef.current) return;
    
    // Garantir intensidade mínima para visibilidade em estado de repouso
    const baseIntensity = Math.max(intensity, 0.35);
    const t = state.clock.getElapsedTime() * speed;
    
    // Rotação suave do grupo - mais evidente no estado de repouso
    groupRef.current.rotation.x = Math.sin(t * 0.3) * 0.3;
    groupRef.current.rotation.y = Math.sin(t * 0.2) * 0.4;
    
    // Atualizar posições das partículas para simular dissipação
    const positions = particlesRef.current.geometry.attributes.position.array as Float32Array;
    const sizes = particlesRef.current.geometry.attributes.size.array as Float32Array;
    
    for (let i = 0; i < particleCount; i++) {
      // Movimento para fora (decoerência)
      const idx = i * 3;
      const dirX = positions[idx];
      const dirY = positions[idx + 1];
      const dirZ = positions[idx + 2];
      
      // Normalizar direção
      const len = Math.sqrt(dirX * dirX + dirY * dirY + dirZ * dirZ);
      const normalizedX = dirX / len;
      const normalizedY = dirY / len;
      const normalizedZ = dirZ / len;
      
      // Movimento para fora + direção preferencial - mais visível mesmo com baixa intensidade
      // Adiciona movimento oscilatório mais visível
      const oscillation = Math.sin(t * 2 + i * 0.5) * 0.8 + 0.2;
      positions[idx] += (normalizedX * 0.002 * baseIntensity + direction[0] * 0.001) * oscillation;
      positions[idx + 1] += (normalizedY * 0.002 * baseIntensity + direction[1] * 0.001) * oscillation;
      positions[idx + 2] += (normalizedZ * 0.002 * baseIntensity + direction[2] * 0.001) * oscillation;
      
      // Reduzir tamanho com o tempo (partículas desvanecem) mas manter tamanho mínimo
      sizes[i] = Math.max(sizes[i] * 0.998, 0.01);
    }
    
    particlesRef.current.geometry.attributes.position.needsUpdate = true;
    particlesRef.current.geometry.attributes.size.needsUpdate = true;
    
    // Animar a onda de decoerência
    if (waveRef.current) {
      // Expandir a onda para fora
      waveRef.current.scale.setScalar(1 + 0.2 * Math.sin(t * 2) + 0.3 * intensity);
      
      // Reduzir opacidade da onda com o tempo
      const material = waveRef.current.material as THREE.MeshBasicMaterial;
      material.opacity = 0.3 * (0.5 + 0.5 * Math.sin(t * 3)) * intensity;
    }
  });
  
  return (
    <group ref={groupRef} position={position} scale={scale}>
      {/* Partículas de decoerência */}
      <points ref={particlesRef} geometry={particlesGeometry} material={particlesMaterial} />
      
      {/* 
      Removida a onda esférica central que causava o efeito de bolinha intermitente
      Mantendo apenas as partículas de decoerência, que são mais consistentes visualmente
      e representam melhor o fenômeno físico de perda de coerência quântica
      */}
    </group>
  );
};
